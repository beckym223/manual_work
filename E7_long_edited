


---Economics-2013-0-02.txt---
I. Introduction
Central banks since 2008 in many countries have greatly expanded their balance
sheets, rapidly creating large amounts of what used to be called "high powered
money," without creating inflation. The European Central Bank's policies and pro
posals for Europe-wide bank supervision are at the center of hot disputes because of
their fiscal policy implications. The United States and Japan are accumulating debt
at rates that are unprecedented in peacetime, which some worry may eventually
generate inflation. Most central banks now pay interest on reserve deposits, making
those deposits part of the government's interest-bearing debt. These developments
make it clear that monetary and fiscal policy are tied together, and that conventional
macro models with non-interest-bearing high-powered money, a "money multi
plier," and a tight relation between the price level and the quantity of "money" are
inadequate as a framework for current policy discussions.
The literature on the fiscal theory of the price level (FTPL) integrates discussion
of monetary and fiscal policy, recognizing that fiscal policy can be a determinant,
or even the sole determinant, of the price level.1 The first papers in the area may
have seemed technical—they showed that when the government budget constraint


---Economics-2013-0-03.txt---
is properly taken into account, conditions for existence and uniqueness of the price
level in dynamic general equilibrium were different from what they seemed in
conventional models. They may also have seemed esoteric—they questioned con
ventional analysis most strongly in policy configurations that in the 1990s seemed
unlike any observed in rich economies in recent history.2
This paper tries to bring FTPL down to earth. It begins by citing some results from
FTPL analysis and showing how they apply to current policy discussions. It then
presents a couple of simple models illustrating FTPL, using them to make clear how
the results used earlier in the paper arise and to refute some of the objections to and
fallacies concerning FTPL that still circulate among economists.
II. Insights from FTPL and Their Application
A. Monetary Policy Actions, to be Effective,
Must Induce a Fiscal Policy Response
This is easiest to understand in high-inflation, high nominal debt economies where
fiscal policy is frozen by political deadlock or chicanery. In such an economy, the
interest rate will be high, and with a high level of debt, the interest expense com
ponent of the budget is substantial, possibly even dominant. If inflation rises still
higher, the usual monetary policy prescription would be for the policy interest rate
to increase, by even more than the rise in the inflation rate.3 But if the legislature
in such an economy is gridlocked, the central bank may realize that an interest rate
increase will pass right through the government budget, with an increased rate of
issue of nominal debt the only fiscal effect of the interest rate rise. If this is indeed
the situation, and private sector bond buyers understand the situation, the interest
rate rise will have no contractionary effect. Indeed, it will increase the rate of infla
tion rather than decrease it. The central bank, understanding this, may then forgo
following the conventional policy prescription. In doing so, it is not accelerating the
inflation, it is damping it.4
In the 1990s, this may have seemed an analysis that applied at most to some mis
managed Latin American economies, but consider the reasoning many economists
(including me) have used to argue that the great expansion of the balance sheet of
the US Federal Reserve system in the last four years need not generate inflationary
pressure. The Fed now has the authority to pay interest on reserve balances. While
the rates are now low (though still in excess of rates on short term US Treasury
Bills), the Fed is free to raise them if inflationary pressures arise. Even if it under
takes no open market operations to change the amount of reserves, raising the rates
on reserves would have a powerful contractionary effect. Banks would have little
incentive to expand their lending if perfectly safe reserve deposits paid interest at a
rate comparable to loan rates.


---Economics-2013-0-04.txt---
But this story, like any story about the effects of monetary policy, has a fiscal pol
icy backstory. Because reserve deposits and Treasury securities are close substitutes,
raising the rate on reserve deposits would also raise rates on government debt gener
ally. The level of US government debt relative to GDP is at unprecedented levels. If
debt were at 100 percent of GDP, a rise in interest rates to 6 percent from its current
level of about 2 percent would bring interest expense, now less than 10 percent of
total Federal government expenditure, to 30 percent of government expenditure,
increasing the conventionally measured deficit drastically if there were no response
of fiscal policy. Would there be a response? Some years ago the answer might have
been, "Surely yes." But the increase in the conventional deficit would be so large
that the response would have to involve substantial increases in tax revenue. With
recent repeated congressional games of chicken over the debt limit and inability to
bargain to a resolution of long-term budget problems, the answer may now be in
some doubt.
With the central bank keeping interest rates stable in the face of inflation fluctua
tions and fiscal authorities not increasing primary surpluses in response to increased
real debt, the price level is still likely to be determinate. But the main determinant
of inflation becomes the fiscal deficit, rather than changes in the usual instruments
of monetary policy.
B. Paper Money Requires Fiscal Backing
It is easy to construct models of economies in which unbacked paper money can
have value, but in such models it is generally also possible for money to be valueless,
or to dwindle rapidly in value so that the economy approaches a barter equilibrium.
In such models, introducing taxation either to pay interest on government liabilities
or to contract the supply of non-interest-bearing liabilities (and thus, via deflation,
create a real return) tends to resolve the indeterminacy and provide a uniquely deter
mined price level. The first two examples in Section III below show quite different
models in which this pattern of results hold.
Depending on the institutional setup, the fiscal backing can be apparent in
equilibrium, as with taxation to service a stable volume of nominal debt, or it
can be implicit, invoked only under unusual circumstances, as with a commit
ment to treasury transfers to the central bank if the central bank balance sheet
deteriorates. But in evaluating monetary and fiscal institutions, the question of
the nature of fiscal backing for the price level is a useful starting point. It led me
(Sims 2004; 1999) to think about where fiscal backing could come from in the
European Monetary Union (EMU) and what kinds of conditions might force the
EMU to confront the need for fiscal backing. Those two papers speculated about
policy dilemmas that at the time might have been seen as obscure and unlikely,
but now seem practically relevant. The policy discussion in the EMU during this
recent crisis period has focused on fiscal transfers that will arise as partial default
on Greek and possibly other euro area sovereign debt occurs. While resolving the
allocation of these losses that have already occurred is important, controversy
over them has hindered discussion of ways to provide clear fiscal backing for the
euro, which is in many ways an easier problem. I discuss these issues in more
detail in Sims (2012).


---Economics-2013-0-05.txt---
Monetary and Fiscal Policy
Formerly, there were monetary economists who argued that the central bank bal
ance sheet is an accounting fiction, of no substantive interest. It is true that the
implications of having negative net worth at current market values are different for
a central bank and an ordinary firm or private bank. A firm with negative net worth
is likely to find its creditors demanding payment, and is unlikely to be able to pay
them all. A central bank can "print money"—offer deposits as payment for its bills.
It will not be subject to the usual sort of run, then, in which creditors fear not being
paid and hence demand immediate payment. Its liabilities are denominated in gov
ernment paper, which it can produce at will.
On top of this, most economists have thought of central banks as part of the gov
ernment, with the only balance sheet that really matters being that of the government
as a whole.
But both of these arguments come apart when the central bank aims at control
ling the price level and fiscal and monetary policy are not set jointly. Traditional
contractionary open market operations require selling assets to shrink the amount
of reserve deposits and currency. If the central bank is in the red, an aggressively
contractionary policy may not be possible, because people will see that it would
require selling more assets than the central bank actually has.5 If the negative-net
worth central bank tries to contract by raising interest rates on reserves, yet wants to
avoid expanding reserves, it is likely to need to sell assets to finance the interest on
reserves, again putting it on an unsustainable path. Of course, if the treasury stands
ready to back up the central bank—providing additional assets to the central bank in
the form of interest-bearing securities whenever necessary—then the central bank
balance sheet is indeed irrelevant.
In thinking about central bank policy when fiscal backing from the treasury is
absent or uncertain, it helps to consider what "fiscal backing" the central bank can
provide on its own, without assistance from the treasury. Of course central banks
cannot impose explicit taxes, but they do have access to an implicit tax: seignor
age. Even if its balance sheet shows negative net worth at current market values, a
central bank can maintain a uniquely determined price level by using its seignorage
revenues to restore its balance sheet. But seignorage revenue depends on the infla
tion rate, generally increasing with the rate of inflation except at extremely high
inflation rates. A central bank with a severely enough impaired balance sheet may
not be able to pin down the price level without treasury assistance, but modestly
negative net worth can generally be "worked off' by seignorage. Of course most
central banks see their task as maintaining low inflation, so balance sheet problems,
by requiring inflation to generate seignorage, can be an obstacle to the central bank's
achieving its policy objectives. Even a central bank with positive net worth may be
inhibited in taking some policy actions by fear of the consequences of negative net
worth. Lender of last resort operations, for example, even when they have positive
expected return, generally pose some risk of losses. A central bank with uncertain


---Economics-2013-0-06.txt---
fiscal backing may hesitate to undertake such operations out of fear of their balance
sheet consequences.
A simple model of balance sheet dynamics for a central bank with negative net
worth and no fiscal backing appears in my earlier paper, Sims (2005). However, that
paper assumes the central bank has non-interest-bearing liabilities and short-term
treasury bonds as assets. The US Federal Reserve system pays interest on its reserve
liabilities and holds substantial amounts of long-maturity debt. It has been argued
(by me, among others) that the Fed can take contractionary action by raising the rate
paid on reserves, without necessarily selling its assets. But this argument assumes
fiscal backing. Without it, high interest on reserves, while interest on long-maturity
debt remains low, can create negative seignorage, even in the presence of inflation.
In the United States and the euro area today, it is not certain that fiscal repair of
central bank balance sheets would emerge. In the euro area, there is a formal "capi
tal key," specifying in what proportions governments in the EMU should provide
capital when the European Central Bank (ECB) calls for a capital infusion. But if
the capital called for were substantial, and the call came in the wake of ECB policy
actions that were politically unpopular in some countries, the provision of capital
might not be automatic. Perhaps equally important is that, foreseeing the risk of a
capital call and its implicit fiscal transfers, the ECB's governing board might refuse
to authorize market-stabilizing actions by the ECB that an ordinary central bank
would have undertaken.
In the United States, the risk is that the need for capital infusion would most likely
arise in the wake of stringent monetary policy tightening that caused capital losses
on the Fed's long term debt holdings and required an increased stream of inter
est payments on reserves. These actions would be restraining growth and forcing
Congress to confront increased deficits arising from increased interest expense. In
an environment where Congress cannot agree to let the debt limit increase to accom
modate its own spending and revenue legislation, it is not hard to imagine Congress
blaming the Fed for the painful decisions it faces and in the process casting doubt
on its commitment to recapitalize the Fed.
D. Nominal Debt and Real Debt are Very Different
Real sovereign debt promises future payments of something the government may
not have available—gold, under the gold standard, euros for individual country
members of the EMU, and dollars for developing countries that borrow mainly in
foreign currency. Nominal sovereign debt promises only future payments of gov
ernment paper, which is always available. Both types of debt must satisfy the equi
librium condition that the real value of the country's debt is the discounted present
value of future primary surpluses—revenues in excess of expenditures other than
interest payments. But if an adverse fiscal development increases debt, the increased
real debt will require increased future primary surpluses, whereas with nominal debt
there are two other ways to restore balance—inflation, which directly reduces the
real value of future commitments, and changes in the nominal interest rate, which
will change the current market value of long term debt.
Obviously outright default on nominal debt is much less likely than default on real
debt. So long as the country is capable of generating any positive stream of primary


---Economics-2013-0-07.txt---
Figure 1. Surprise Gain or Loss to US Debt Holders as Proportion of GDP
surpluses, its debt will have non-zero real value. But if debt is real and the country
finds itself unable to maintain primary surpluses above its predetermined real debt
service commitment, it must default, even if in absolute terms it is running substan
tial primary surpluses.
Nominal Debt is a Cushion, Like Equity.—In a deterministic steady state, inves
tors will insist on nominal interest rates high enough to compensate for inflation's
effect on the real value of their debt holdings. Real returns on government debt will
be the same whether it is nominal or real. But if nominal interest rates fall after the
date of issue of long-term nominal debt with a fixed coupon rate, the market value
of the debt will rise, providing the debt holder with an unanticipated higher return.
If inflation occurs at a higher than expected rate, the real value of nominal debt,
whatever its maturity, suffers an unanticipated decline. These mechanisms can cush
ion the impact of unexpected changes in the fiscal situation. We live in a stochastic
world, and surprises in returns on government debt from these two mechanisms
are substantial. Figure 1 shows a time series of surprise gains and losses on US
government debt as a fraction of GDP.6 The surprise gains and losses relative to
GDP have been of the same order of magnitude as year to year fluctuations in the


---Economics-2013-0-08.txt---
fiscal deficit. Were they displayed as fractions of the value of outstanding debt, so
they became surprises in rates of return, they would be much larger. It is clearly not
a good approximation to model the US economy as if debt were real, even though a
considerable part of the literature on optimal fiscal policy does so.
The southern countries in the euro area are now reckoning with the consequences
of their having, by joining the euro, made their sovereign debt real. The 2008-2009
crisis led to great expansion of their debts, and the nominal debt cushion is not avail
able to them. Greece already has defaulted on its debt, and quite possibly before the
crisis resolves other southern tier EMU members will as well.
When only distorting taxes are available, there is a benefit to keeping tax rates
stable. A highly variable time path of tax rates produces higher deadweight loss than
a more stable path that delivers the same cumulative revenue. With nominal debt,
flexible prices, and costless inflation, it is optimal to keep revenue very stable, allow
ing inflation to absorb most of any fiscal shocks. While this result is well known,
Siu (2004) and Schmidt-Grohé and Uribe (2001) have shown that when nominal
rigidities are present, variation in inflation becomes costly, and that this leads to very
little use of inflation to smooth tax rates except (as Siu shows) when fiscal distur
bances are very large. The model in Section IIIC below argues that this conclusion is
sensitive to those papers having allowed only for one-period debt. With longer debt,
the costs of tax smoothing via surprise inflation can be much lower. In any case,
the economic situation in the southern-tier European countries probably reflects the
"very large fiscal shock" case.
Nominal Debt is (Almost) Non-Defaultable, Hence Important to the Lender of Last
Resort.—By the usual indicators, the first two countries in Table 1 are not in notice
ably worse shape than the last three. But the last three are selling their bonds at much
lower interest rates. This reflects the fact that the last three issue mainly nominal debt,
denominated in their own country currency, while the first two have issued real (i.e.,
euro) debt. There is a non-trivial probability that the first two will default in some
form, while the latter three are quite unlikely to default, because their debt is nominal.
Economists and journalists sometimes treat inflation as a form of default, but it is
not. Default is a situation where the contracted payments cannot be delivered, and
the contract does not specify what happens in that eventuality. For private firms, this
leads to renegotiation and/or court proceedings. There can be a long period in which
investors cannot get access to their investments and the amount that will be returned
to them remains unknown. Creditors holding different maturities or types of debt


---Economics-2013-0-09.txt---
may suffer different degrees of loss, and the allocation of losses across creditors
may be uncertain. For example, a minor default may involve a modest delay in
returning principal of a short term debt. Other creditors may be unaffected, or, if
the holder of the short debt goes to court, all debtors may find themselves impaired.
Similar, or perhaps more severe, uncertainties surround sovereign default.
Unanticipated high inflation does impose losses on investors in nominal sover
eign debt, but it does not involve renegotiation or court proceedings. Contracted
payments are made. The securities remain tradable. In the same state of uncertainty
about future primary surpluses, therefore, investors are likely to be much more
uncertain about the return on their own investment in sovereign debt when reso
lution of the fiscal imbalance has to come from default, rather than inflation and
nominal interest rate changes.
In a financial panic, counterparty risk becomes pervasive among market partici
pants and credit markets freeze up. An institution of unquestioned soundness and
liquidity can remedy the situation by lending freely. While large private banks can
and have historically sometimes acted as such a lender of last resort, any private
institution that attempts it risks itself becoming subject to worries about liquidity. A
central bank, backed by a treasury that can run primary surpluses and issue nominal
debt, is an ideal lender of last resort. Because it can create reserve money, it need
never default. If it takes capital losses, and it is not backed by a fiscal authority, it
could be forced to run a high inflation to restore its balance sheet, but this will not
be a problem if it has fiscal backing. Europe, in setting up its Monetary Union, did
not contemplate the ECB's taking on a lender of last resort role. Individual country
central banks can no longer play the role, because they have no independent author
ity to create reserve money and their country treasuries issue only real debt. During
the recent crisis the ECB has in fact played a lender of last resort role, though its
effectiveness is limited because its actions and announcements in this role are regu
larly criticized by some northern-tier economic officials.
III. Models
A. Samuelson's Pure Consumption Loan Model with Storage
This model is one where, without tax backing for debt or money, the price level
is indeterminate. The model in that case has one stable price level, in which the
real allocation is efficient, and a continuum of other possible initial price levels,
each of which corresponds to an inefficient equilibrium in which the real value of
government debt or money shrinks toward zero. If the government runs a primary
surplus (revenues in excess of non-interest expenditures), private agents see the
future taxes as reducing their spending power. They will therefore save (attempt
to accumulate money or government debt), until the price level is low enough that
the value of their government paper matches the present value of their future taxes.
This mechanism eliminates the non-uniqueness, no matter how small the primary
surplus, and for small levels of primary surplus, the real allocation is arbitrarily
close to the efficient one.
There is an infinite sequence of periods, in each of which the same number of two
period-lived agents is born and endowed with one unit of the consumption good,


---Economics-2013-0-10.txt---
grain. The grain can be stored, but decays in storage by a factor 6. There is also
government debt, denominated in dollars. Its amount at the initial date t = 1 is B0,
and it is held by the initial old, who redeem it with the government, receiving in
return new one-period debt in the amount B] — R0B0. Since this new government
paper is worthless to the initial old, they attempt to sell it to the initial young, for
grain. The price level at date t is the rate at which grain trades for newly issued gov
ernment debt. This process repeats thereafter for t = 1,..., oo.
Formally, the generation born at t maximizes its lifetime utility U(CU, C2j+ 0 sub
ject to the constraints
(1) C„ + S, + y = 1
(2) c2,+1 = ^L + est
rf+I
(3) S, >0, B, > 0.
Because the government is doing nothing but rolling over the debt each period, the
market clearing condition is simply R,B, = Br+l. The government sets an arbitrary
value for R, each period. The first-order conditions for an agent in generation t,
assuming perfect foresight about next period's P, are
(4) dCx: Dx U(CU, C2,t+1) = A,
(5) c>C2: D2U(Cu, C2t+i) = n,+\
(6) dB,: iffi;>0
"t "t+1
(7) dSX, = Q[i,+\, if S, > 0.
The B and S first order conditions tell us, as we would expect, that if agents are stor
ing grain and also buying debt, their returns must match, so that in that case
(8) jr- = 9.
rr+i
In order to get easily computed solutions that give us some insight into how the
model works, we assume R, is constant and
U(cu, C2,,+1) = logic,,) + log(C2f+1).
Then the Lagrange multipliers can be solved out to deliver
(9) ifS;>0
rt+1


---Economics-2013-0-11.txt---
Cj r+1
et = -£±, if s, > o.
Let savings be represented by W, — S, + Bt/Pt. Logarithmic utility makes solution
easy because it implies that whatever the rate of return to savings, call it p„ we will
have p = C2,r+i/Clr, and this in turn implies that
Thus savings is always half the endowment, i.e., 0.5.
This economy has an equilibrium in which there is no storage and nominal debt
has value (i.e., P, < oo). With no storage, Cu = 0.5 and, since savings is all used to
buy debt from the older generation, C2, = 0.5 also. This means p, = 1 and therefore
R = P,+l/P„ all t. In other words, the price level grows at the gross interest rate and
the real value of both newly issued and maturing debt is constant at 0.5. In order for
this equilibrium to prevail, the initial price level P¡ must be 2Bt, i.e., 2RB0.
The economy also has equilibria in which S(>0, however. In these equilibria, of
course, p = 9 = RPt/Pt+l. In other words, The price level grows not at the rate R,
but at the higher rate R/0. The nominal debt still grows at the rate R, however, so the
real debt shrinks over time, with
The economy can start with any Bx/ Px < 0.5. Storage will then be 5] = 0.5 —
Bx/P{. In subsequent periods, S, increases toward 0.5 as the real value of savings in
the form of nominal bonds shrinks toward zero.
In other words, every initial price level PA that exceeds 2RB0, including P{ = oo
(in which case bonds are valueless and all savings is in the form of storage), corre
sponds to a perfect-foresight equilibrium in this economy. This is an economy with
an indeterminate price level.
Note that the economy's resource constraint is Cu + C2, + S, — 1 + 9S,_i: con
sumption and storage by the young plus consumption by the old is endowment of
the young plus the proceeds from storage by the old. Since in all the equilibria
with positive storage S is either increasing or (when P, = oo) constant, and since
Clt = 0.5, C2, < 0.5 in all these equilibria with Sr>0. That is, these equilibria
with S, > 0 are strictly worse than the one in which S, = 0. It may be comforting to
believe that somehow these worse equilibria would be avoided, but there is nothing
in the structure of the model that should make the worse equilibria less likely.
When we use the "6" and "/?" notation as here, it is perhaps unsurprising that
we get an indeterminate equilibrium when the government issues debt without
any backing from taxation. But if we replace B by M and set R = 1, this becomes
Samuelson's model of "money" and is sometimes taken as a useful metaphor to aid
understanding of how fiat money can have value.



---Economics-2013-0-12.txt---
But back to thinking of it as debt. What if we do provide tax backing for the debt?
Suppose everything is as before, but now the government imposes a lump sum tax r
on the young each period. The government budget constraint is now
/ x jB, RBt_ i
<12> i=-ir-T'
Suppose there were an equilibrium in which savings is in the form of both bonds and
storage. Then both must have real gross rate of return 6. That makes the government
budget constraint
(13) T = eT±"r
't rr-l
This is a stable difference equation in B,/Pt. If it starts operation at t = 0, we will
have
B, B()
(14) ^ = ^_rr + 0< °
*t j=0 *0
But notice that, since 8 < 1, the right-hand side of this expression eventually becomes
negative, converging as t —> oo to — r/ ( 1 —6). That is, if the economy started on a
path satisfying this condition, eventually it would reach a point where the govern
ment is putting grain in the amount r on the market to exchange for mature debt, but
no one would have any debt to exchange for it. Anyone foreseeing this would have
a motive for holding on to some debt to exchange for grain at an extremely favor
able price ratio when everyone else had run out. So these paths cannot be equilibria.
By imposing the tax, no matter how small, the government has eliminated all those
equilibria in which storage and debt coexist. It has also eliminated the equilibria in
which debt is valueless (P, = oo) for the same reason: the government is trying to
exchange r units of grain per capita for mature debt, so the mature debt is necessar
ily of some value.
If there is no storage, the rate of return on debt can be positive. The tax is recog
nized by individual agents as reducing their wealth, so first-period consumption is
reduced. Formally, the private budget constraint in the first period is now
(15) cu + ^ + t= 1.
We will still have, from the first-order conditions, RP,/P,+l = p, = C2,r+i/Q, where
p, is just notation for the real rate of return. Using these last two expressions to
rewrite the first-period budget constraint, we have
~ i i ^2,t+i . P/C],
(16) Clt + t — — 1 — Cu -j — I- r.


---Economics-2013-0-13.txt---
with, as usual with log utility, first-period consumption being half of total wealth
1 — r. With no storage, Cit+ C2t = 1, which implies
(17) C2, = i±±.
Note that in this unique equilibrium, the utility of each generation is log( 1 — r) +
log(l + r) + 21og(l/2), which is less than the upper bound of 21og(l/2). Thus with
r = 0, the utility-maximizing equilibrium exists, but is not unique, while small posi
tive values of r make equilibrium unique, and can approach the utility of the opti  mum for small r.
The debt valuation equation holds in these equilibria. The gross real interest rate
is p, = C2j+i/Cu = (1 + t)/(1 — r). From the government budget constraint, then,
we see that
B,
-i
p Pt-i p r>
t "t~)
and since B/P = C2/p is constant in the equilibrium,
B, T
(18)
P, p- 1
Note that as r approaches zero, B/P does not approach zero, as this formula might
suggest, p = C2/Cl = (1 + r) / ( 1 — r) in equilibrium and substituting this for p in
(18) gives us
(19) Tr'-^r
Because p —* 1 as r —» 0, in other words, real debt converges to one half, its value in
the utility-maximizing equilibrium, as r —> 0, even though the debt valuation equa
tion (18) continues to hold.
To see how the initial price level is determined, we look at the initial government
budget constraint. R_x and B_x are given by history, so
/ s S0 i — T R_,B._,
(20) — = = - t.
y } P0 2 Pt
This equation can be solved for a unique, positive value of P0, so long as
R,-\B,_x > 0. The subsequent sequence of prices is determined by the sequence of
policy choices for R„ with higher R, values producing higher inflation.


---Economics-2013-0-14.txt---
What if initial R,_xBt_{ = 0? So long as we maintain the constraint that B0 > 0,
fiscal policy cannot then at t = 0 be simply to set r to its constant value. The old at
time 0 in this case have no way to finance consumption. It is plausible then to sup
pose that the government imposes the tax ron the young, issues new debt bought by
the young, and uses the proceeds to provide a subsidy to the time-0 old. From that
point on the equilibrium would be as we have calculated above.
B. Fiscal Backup for a Taylor Rule
Cochrane (2007) has argued against attempts to claim a determinate price level in
models with Taylor-rule monetary policy by invoking "fiscal backing" that comes
into play only off the equilibrium path. I don't understand his reasoning, but in any
case the simple model of this section shows that we can also justify uniqueness of
the price level with a Taylor rule by invoking fiscal backing that is always in play,
even in equilibrium, but is negligibly small in size. The equilibrium is then arbitrarily
close to that of the model without fiscal backing. In this and the preceding model,
we conclude that the existence of fiscal backing is important for stable prices, but
that if market perception of the backing is there, the size of the backing can be quite
small in equilibrium. Institutions like those in the EMU that make it unclear where
the fiscal backing would come from, or even whether it exists, are destabilizing; yet
in normal times, because large fiscal-backing interventions do not occur in equilib
rium, it is easy for the importance of fiscal backing to be lost sight of.
The model is very simple, a continuous time extension of Leeper's (1991) original
framework. The monetary policy rule is
(21) r = i(0p - (r- p)).
This makes the nominal interest rate r respond with a delay (larger 7 means less
delay) to inflation p. The "Taylor principle" that the interest rate should eventually
respond more than one for one to inflation changes corresponds here to 9 > 1.
We assume a constant real rate p and a no-risk-aversion Fisher equation connect
ing a constant real rate p and the nominal rate:
(22) r = p + p.
The p notation represents the right time derivative of the expected path of the log of
the price level. On a perfect foresight solution path, p = p at all dates after the initial
date, but p can move discontinuously at the initial date. These two equations (21)
and (22) can be solved to yield a second order differential equation in p:
(23) p = -y(0-l)p,
which holds after the initial date / = 0 on any perfect foresight equilibrium path.
With 9 > 1, this is an unstable differential equation, with solutions of the form p,
= p0elly6~r'1'.
Leeper assumed that such explosive paths for the price level were not equilibrium
paths and focused on the one stable solution to the equation, p = 0. On such a path


---Economics-2013-0-15.txt---
r=p from (22). From (21) this implies also that r — 0. The policy equation (21),
since it holds in actual (not expected-right) derivatives, implies that the time path of
r — ~¡0p is differentiable, even at the initial date t = 0, but this leaves it possible that
both p and r jump discontinuously at t = 0, so long as the jumps satisfy Ar = *y9Ap.
This makes the initial price level determinate. Using r$, pô to indicate the left limits
of these variables at time 0 (i.e., their pre-jump values), we have
(24) Ar0 = p - ro = oy6(p0 -pô) = 7&V0.
This equation can be solved for a unique value of p0 (right limit of the log of the
price level at time 0) as a function of p, rjj", and pô
If the initial price level should be below this level, r and thus p would also be
lower, which implies inflation tends to — oo at an exponential rate. The policy
rule (21) cannot possibly be maintained on such a path, as it would require pushing
r to negative values. It is natural to suppose that there would be a shift in the rule at
very low inflation rates, with fiscal policy ruling out such a path. If the initial price
level and inflation rate are above the steady state level, the inflation rate rises at an
exponential rate. The opportunity costs of holding non-interest-bearing money bal
ances become arbitrarily high. If real balances are essential (utility is driven to — oo
as M/P —> 0), these explosive paths may be viable equilibria. If not, there may be
an upper bound on the interest rate above which real balances become zero. Paths on
which real balances shrink to zero in finite time may also be viable equilibria. Here
again, though, we can postulate a shift in policy at very high inflation rates that elim
inates these unstable paths, while leaving the stationary p = 0 equilibrium viable.
Cochrane finds these hypothetical policy shifts at high and low inflation rates,
which then never are observed in equilibrium, implausible. But suppose we add a
government budget constraint and fiscal policy, as did Leeper. The budget constraint,
with real debt (not log of real debt) denoted as b and primary surplus denoted as r, is
(25) b = (r — p)b — r.
A version of what Leeper calls a passive fiscal policy is
(26) r= -00 + (f>xb.
Along a perfect foresight path, where r — p + p, this gives us
(27) b = pb + (p0 — 4>x b.
With 4>x > p, this is a stable equation in b. No matter what the equilibrium time path
of p, real debt converges to 0o/(^i — p)- This equation can therefore play no role in
determining the price level, and thus cannot resolve the indeterminacy.
But what if we add to the right-hand side of (26) a positive response of the pri
mary surplus to inflation, i.e., replace (26) with
(28)  r= -4> o + <j>xb + 02vr?


---Economics-2013-0-16.txt---
Then (27) becomes
(29) ¿ = (p-0,)¿ +00-02
The three-equation differential equation system formed by (21), (22), and (29) is
recursive, since b appears only in the last equation. That means that the solution
paths that make it explode up or down that we observed when considering the first
two equations alone are still mathematical solution paths for the three-equation sys
tem. But now notice what happens to b along a path on which p —> oo. From the
debt equation (29) we see that on such a path b eventually becomes negative and
more negative over time. This implies that b goes to zero in finite time. From the
point of view of private agents in the economy, since we assume they can't borrow
from the government, this means that their future tax obligations exceed their wealth
in the form of government debt, and thus that they cannot finance their planned con
sumption with the income and wealth they have. They will therefore reduce con
sumption and try to save. If they truly have perfect foresight, this would instantly,
at the initial date t = 0, bring pQ back to the level consistent with stability. If it
takes agents some time to realize what kind of a path they are on, the adjustment
might come with a delay, still producing the same reversion to the stable solu
tion. Unstable paths with accelerating deflation can also be ruled out. On such
paths, real debt would rise without bound, while primary surpluses shrank and
eventually became negative. Now people would see their wealth in the form of
government debt growing without bound, with no offsetting increase in future tax
obligations. They would therefore spend, raising prices, bringing the economy
back to its stable path.
These arguments do not depend on the size of 4>2, so long as it is positive. In equi
librium, 7r will be zero or (if people are imperfectly foresighted, or if we add random
disturbances to the system) fluctuate in a narrow range. If 4>2 is small enough, its
presence might be difficult to detect from data. In any case its presence would have
no effect on the first two equations of the system or on the equilibrium time path
of prices and interest rates, except for its elimination of the unstable solutions as
equilibria of the economy.
C. Debt as a Fiscal Cushion
Barro (1979) showed in a simple, stylized model that in the presence of distort
ing taxation it is not optimal to rapidly pay off public debt, because the deadweight
losses from heavy initial taxation to reduce the debt are not offset by the present
value of lower future deadweight losses after the debt is reduced. Instead, in his
model, debt and tax revenue optimally follow a martingale process, with E,bt+l
= b„ E,t,+i = Tt. (Here r is total tax revenue.) In his framework, r increases with
increases in b. Lucas and Stokey (1983) showed that when the government can issue
contingent liabilities, it is actually optimal for taxes to be set without reference to
the current level of debt. Chari, Christiano, and Kehoe ( 1994) showed that monetary
policy, by determining inflation, can create appropriate contingencies in the return to
debt. I showed (Sims 2001) that if these insights are brought back to Barro's (1979)
stylized framework, we get a simple and stark conclusion—r should be constant,


---Economics-2013-0-17.txt---
with b brought in line with stochastically fluctuating future government spending by
surprise inflation and deflation.
However, these results all depend on surprise inflation and deflation being cost
less. In a Keynesian model with sticky prices or wages, or in a model with incom
plete markets and borrowing and lending via standard debt contracts in nominal
terms, surprise inflation has a substantial cost. Schmidt-Grohé and Uribe (2001)
showed that in a New Keynesian model with one-period government debt, opti
mal policy is much closer to Barro's (1979) initial prescription than to a constant-r
policy. It makes a great deal of difference, though, whether government debt is long
or short term. When debt is short, as in Schmitt-Grohé and Uribe's setup, inflation
or deflation is the only way to change its market value in response to government
spending surprises. But if the debt is long term, large changes in the value of the debt
can be produced by changes in the nominal interest rate, with much smaller changes
in inflation. Interest rates fluctuate widely and their fluctuations are not thought of
as very costly, while price fluctuations may generate inefficient output and employ
ment fluctuations. The model of this section revisits Barro's (1979) framework,
adding endogenous price determination and allowing for short or long debt. It con
cludes that substantial use of the nominal debt fiscal cushion to limit tax fluctuations
may be optimal if debt maturity is long.
Following Barro (1979), we model the government as wanting to minimize the
deadweight loss from taxation, modeled as proportional to r2, the square of total
revenue. But we add to his specification a concern with wide swings in inflation,
leading to the objective function
Î"H£-■)')]■
It simplifies notation for us to use the single symbol ut — P,_x/P, to denote the
inverse of the gross inflation rate from this point on. There is a constant real interest
rate, and private sector behavior requires the real rate to match the expected nominal
return on a bond:
(31) R,Etut+1 = p.
The government budget constraint is
(32) b, = R,_iU,bl_l - T, + g„
where b is real debt and g, is government spending, which we treat as an exogenous
stochastic process. The government then maximizes (30) by choosing R, P, b, and
T subject to the two constraints (31) and (32). The first order conditions for an opti
mum are
(33) dr: t, = X,
(30)
(34)
db: A t = 0RtE,[ut+l\t+l]


---Economics-2013-0-18.txt---
dR\ H,E,ut+1 = ¡3Et[ist+l\t+1]b,
(36) dv. (yt- 1) = — A tRt_xb,_x +n,_lR,_lp.
These first order conditions look complicated, but when 9 = 0, so that there is no
cost to inflation, they collapse to a surprisingly simple solution. To keep things
neat, we assume /3p = 1. From the b and R FOCs and the Fisher equation (31) we
can derive
(37) b,X, = n,p.
Substituting into the v first order condition, we arrive at
(38) {y, - 1) = (—A, + \,-\)Rt-\bt_x = (r, - T,_x)Rt_ib,_
If 9 = 0, this lets us conclude that r, = rf_j, so long as Rt_\ and b,_l are both posi
tive. With T, constant and g, exogenous and stochastic, (31) is an unstable equation.
Feasibility (b > 0) and transversality (b, —> oo while future r's are constant cannot
be optimal) imply that b must not explode. This implies that we can solve the budget
constraint (32) forward to produce
(39)
b,
P- 1
-E,
PC
E s= 1
P gt+S
In the special case where g is i.i.d., b is constant, b is maintained at these stability
consistent values by fluctuations in u„ the inverse inflation rate, that offset the effects
of g on the real value of the debt.
So far, we have derived an analogue in this simple model of the Lucas-Stokey
result, by setting 9 = 0. We can also consider 9 = oo, i.e., a case where the price
level is kept constant and only real government debt exists. Then we drop the v first
order condition, because v is no longer freely chosen, and use the fact that vt = 1.
Then the b first order condition lets us conclude that E,rt+l = r„ and we are back
to Barro's (1979) conclusion (since we are now back to Barro's model, which had
only real debt).
The cases of most interest, though, are those with 0 < 9 < oo. For these cases,
we want to contrast this version of the model with one in which government debt is
not only nominal, but long term. We will consider the extreme case of consol debt,
which pays a stream of one "dollar" per period forever, never returning principal.
The number of consols held by the public is A, and the price, in dollars, of a consol
is Q,. (So 1/(2, is the long term interest rate). Then the Fisher equation requires that
the expected one-period yield on a consol be equal to p, i.e.,



---Economics-2013-0-19.txt---
and the budget constraint becomes
(41)
Qt vt + 1
a-. - r, + gt.
Because these systems with non-trivial 6 become difficult to handle analytically,
we omit laying out the first order conditions for the consol-debt case, and we solve,
numerically, locally linearized versions of both the short debt and long debt models.
We assume g, is independent across time, with constant mean Eg, = ~g = 1. We set
p = /?_1 = 1.1, and r = 2 in the initial steady state. Because this makes r — ~g = 1,
and the net real rate p — 1 =0.1, initial steady state real debt b = 10.
With real debt, as in Barro's (1979) original framework (i.e., 0 — oo), a unit
increase in g, above its mean ~g requires r and b to move to new levels that could be
sustained forever if future g values reverted to ~g. So with our parameter settings,
10/11 of the g shock goes into b, 1/11 into r. The increased ris exactly enough
to service the increased debt at the 10 percent interest rate. The new values of b
and T are sustained forever in the absence of new shocks. The interest rate remains
constant.
At the opposite extreme, with perfectly flexible prices (9 = 0) and nominal debt,
optimal policy absorbs all of the fiscal surprise in surprise inflation. The inflation
proportionally reduces the real value of maturing debt, which is 11, and must be
sufficient to offset the unit increase in g, since r will optimally not change at all.
The result is an inflation of 10 percent, with b, r, and the interest rate all unchanged.
The inflation is limited to the initial period, after that returning to zero. Here again,
the interest rate remains constant.
With oj = 10, optimal policy depends on whether we have one-year or consol
debt. With one-year debt, optimal policy allows 43 percent of the g shock to flow
into b and permanently adjusts r by 4.3 percent, to cover the increased debt ser
vice. This leaves some of the g shock unaccounted for, though, and that is absorbed
in a one-time surprise inflation of 4.8 percent. And once again the interest rate
remains constant.
With u = 10 and consol debt, only 6.9 percent of the g shock passes into increased
b, and r increases by only 0.69 percent. Most of the shock is absorbed by simultane
ous, permanent small increases in the nominal interest rate (1/(2) ar>d the inflation
rate. The interest rate increases by 0.84 percentage points and the inflation rate by
0.76 percentage points. These small changes in the interest rate and the inflation rate
are enough to create a capital loss for consol holders that offsets most of the g shock.
It may seem, since the interest rate increases by more than the inflation rate, that
the real rate has increased, even though we have assumed constant p. However, this
happens only because of the definition of the "nominal rate" as \/Q. This is a good
approximation when there is no inflation, but when there is steady inflation, as in the
wake of this shock, a consol's constant stream of nominal payments is front-loaded
in real terms, so that in fact the constant real rate is preserved by this combination of
permanent changes in inflation and 1 /Q.
The response to the g shock in the four cases we have discussed in the preceding
paragraphs is displayed in Figure 2. Each column of plots shows the time path of
changes in the four variables listed on the left side of the chart, in the case labeled at


---Economics-2013-0-20.txt---

the top of the column. Note that the 1-year debt case with uj = 10 is about halfway
between the pure real debt case of Barro (1979) and the flex-price case. The consol
case is very close to the flex-price case for the time paths of the real variables b and
r, though its time path for inflation and interest rates is quite different.
The point of this comparison is not to claim that a combination of long debt and
low response of taxes to fiscal shocks is optimal. The model is extremely stylized,
and the costs of inflation have been calibrated only to a value that makes contrasts
between cases easy to see. As is by now well understood, the first-order accurate
solution obtained as here by local linearization does not allow us directly to com
pute expected welfare, even for the stylized objective function. Since both taxes
and inflation vary much less in the consol solution, it seems likely that it delivers
higher welfare, but because the budget constraint is nonlinear, we can't be certain of
this. The amount of shock absorption available from surprise changes in prices and
interest rates depends on the size of the real debt, and the real debt is locally non
stationary in this solution.
However, the results with long and short debt contrast so sharply that this example
does provide a reason for caution in interpreting the results of analyses like those
of Schmitt-Grohé and Uribe (2001) and Siu (2004). These papers conclude that
in normal (non-war) times, it is optimal to make very little use of surprise inflation
in cushioning fiscal shocks, but both papers assume that all debt is one-period debt.
It is likely that their conclusions are sensitive to this assumption.


---Economics-2013-0-21.txt---
The kinds of models that have been the staple of undergraduate macroeconom
ics teaching, with price level determined by balance between "money supply" and
"money demand," and money supply described using the "money multiplier," are
obsolete and provide little insight into the policy issues facing fiscal and mone
tary authorities in the last few years. There are relatively simple models available,
though, that could be taught in undergraduate and graduate courses and that would
allow discussion of current policy issues using clearer analytic foundations.



---Economics-2013-0-22.txt---


---Economics-2014-0-02.txt---
I. Converging Roles
Of the many advances in society and the economy in the last century, the converging
roles of men and women are among the grandest. A narrowing has occurred
between men and women in labor force participation, paid hours of work, hours of
work at home, life-time labor force experience, occupations, college majors, and
education, where there has been an overtaking by females.1 And there has also been
convergence in earnings, on which this essay will focus. Although my evidence is
for the United States, the themes developed here are more broadly applicable.


---Economics-2014-0-03.txt---
These parts of the grand gender convergence occupy various metaphorical chapters
in the history of gender roles in the economy and society. But what must be in
the "last" chapter for there to be real equality?
The answer may come as a surprise. The solution does not (necessarily) have to
involve government intervention. It does not have to improve women's bargaining
skills and desire to compete. And it does not necessarily have to make men
more responsible in the home (although that wouldn't hurt). But it must involve
alterations in the labor market, in particular changing how jobs are structured and
remunerated to enhance temporal flexibility. The gender gap in pay would be considerably
reduced and might even vanish if firms did not have an incentive to disproportionately
reward individuals who worked long hours and who worked particular
hours. Such change has already occurred in various sectors, but not in enough.
Before I discuss what is needed to close the gender gap and what must be in the
last chapter, I should first discuss what is contained in the preceding figurative chapters.
That will set the stage for the detective work necessary to uncover what the last
chapter must contain.
The preceding metaphorical chapters unfolded across at least the last century.
Narrowing occurred in a host of economic areas. Changes in labor force participation
and the reasons for the changes were discussed in my Ely Lecture (Goldin
2006). A grand convergence occurred in labor force participation for adult women
from the early twentieth century to more recently. But a plateau in participation
has emerged for US women in most age groups, even for college graduate women,
since around the 1990s. The plateau may be related to the relative earnings issues
that I will soon discuss. If certain women are disadvantaged in the labor market their
participation will be stymied.2
Lifetime job experience rose along with labor force participation. Years of education
for women increased more than it did for men and it changed in content
for secondary and college education toward more investment-oriented and fewer
consumption-oriented courses and concentrations. Professional and graduate program
enrollment increased for women so that about half of all law and medical
enrollments today are women, and women lead men in fields such as the biological
sciences, pharmacy, optometry, and veterinary medicine.
Women, particularly college graduates, increased their desire to attain "career
and family."3 Hours of work for women increased in the market and decreased in
the home relative to those of men. Female earnings rose relative to males in an era
that saw women "swimming against the tide" of generally rising income inequality.4
Thus the various metaphorical chapters that precede the "last" chapter explored here
are those of a grand gender convergence.
Convergence in some economic outcomes has also occurred within various groups
of women. Until the 1970s most non-employed adult women had not been in the
workforce since they were first married or since having their first child. Currently


---Economics-2014-0-04.txt---
employed women, however, had worked most years since leaving school.5 With
increased labor market participation women were no longer divided as much along
the lines of currently or not currently employed.
II. Gender Gaps in Earnings over the Life-Cycle and by Occupation
Even though there are many ways to measure the degree of gender equality in
the economy, the one that stands out is earnings, particularly earnings per unit time
or the wage. Because relative earnings often signify how individuals are valued
socially and economically, earnings ratios between men and women have been banners
for social movements. The mantra of the women's movement in the 1970s was
"59 cents on the dollar" and a more recent crusade for pay equality has adopted
"77 cents on the dollar."
The wage is also a summary statistic for an individual's education, training, -prior
labor force experience, and expected future participation. The gender gap in wages
is a summary statistic for gender differences in work. For a long time the gender gap
in wages has been viewed as summarizing human capital differences between men's
and women's productivity as well as differential treatment of men and women in the
labor market. As the grand gender convergence has proceeded, underlying differences
between the human capital capabilities of women and men have been vastly
reduced and in many cases eliminated.6
What do we know about how much of the difference between male and female
wages is due to differential treatment in the labor market and how much to differences
in productive characteristics? That question has been addressed by many and
I will briefly summarize the findings and provide further comment.
Most of the gender wage gap studies have produced estimates of an "explained"
and a "residual" portion.7 The "residual" is often termed "wage discrimination"
since it is the difference in earnings between observationally identical males and
females.
The explained portion of the gender wage gap decreased over time as human
capital investments between men and women converged. Differences in years of
education, in the content of college and in accumulated labor market experience narrowed.
In consequence, the residual portion of the gap rose relative to the explained
portion.8
But what can explain the residual portion of the gap that now remains? There are
many contenders. Some would claim that earnings differences for the same position
are due to actual discrimination. To others it is due to women's lower ability


---Economics-2014-0-05.txt---
to bargain and their lesser desire to compete.9 Still others blame it on differential
employer promotion standards due to gender differences in the probability of
leaving.10
The existing explanations for the residual gender pay gap regarding how women
compete and bargain relative to men have some merit. But they do not explain why
different amounts of time out of the labor force and different numbers of hours
worked per day or per week have a large effect on the time-adjusted earnings in
some occupations but not in others. They do not explicate why some positions have
a highly nonlinear (convex) pay structure with regard to hours worked and some are
almost perfectly linear. 1 1
The alternative reasons for the residual gender pay gap do not help illuminate why
earnings differences by sex expand so greatly with age. They also do not explain
why women without children generally have higher earnings than women with children
and why the former's earnings are almost equal to those of comparable men.12
A better answer, I will demonstrate, can be found in an application of personnel
economics.13 The explanation will rely on labor market equilibrium with compensating
differentials and endogenous job design.
As women have increased their productivity enhancing characteristics and as
they "look" more like men, the human capital part of the wage difference has been
squeezed out. What remains is largely how firms reward individuals who differ in
their desire for various amenities. These amenities are various aspects of workplace
flexibility. Workplace flexibility is a complicated, multidimensional concept. The
term incorporates the number of hours to be worked and also the particular hours
worked, being "on call," providing "face time," being around for clients, group
meetings, and the like. Because these idiosyncratic temporal demands are generally
more important for the highly-educated workers, I will emphasize the college educated
and occupations at the higher end of the earnings distribution.
Jobs for which bargaining and competing matter the most, I will demonstrate, are
also positions that have the greatest nonlinearities (meaning convexity) of pay with
respect to time worked. Field and laboratory experiments often show that women
shy away from competition.14 But these experiments do not consider the types of
jobs that reward competition the most. Often those are winner-take-all positions,
such as partner in a firm, tenured professor at a university, or top manager. These are
also positions for which considerable work hours leads to a higher chance of obtaining
the reward, and it is often the case that hours alone get rewarded. Persistence in
these positions and continuous time on the job probably matters far more to one's
success than a desire and ability to compete.


---Economics-2014-0-06.txt---
But I have gotten ahead of myself. Let us first look at the evolution of gender gaps
in earnings over the life-cycle and differences by occupation. These hold clues to
what must be in the last chapter for it to be the finale of the grand gender convergence.
A. In the Aggregate and over the Life-Cycle
The ratio of (mean) annual earnings between male and female workers (full-time,
full-year, 25 to 69 years) was 0.72 in 2010 and that of the medians was 0.77. The
ratio of the medians for the same group was 0.74 in 2000 but 0.56 in 1980. 15 These
aggregate ratios have been somewhat sticky for the last ten years or so after greatly
increasing in the preceding decades, especially in the 1980s. The same is true looking
only at college graduates, for whom the ratios are lower - 0.65 in 2010 for the
means and 0.72 for the medians, about the same as it was in 2000. Interestingly,
across the past decade the gender pay gap has narrowed within almost all age groups
even though the aggregate has not budged as much. How can that be?
The answer concerns what happens to the gender gap over the life-cycle. The
ratio of female to male earnings greatly decreases for some time as cohorts age. It
is lower for individuals in their forties compared with the same individuals in their
twenties. And because the baby boom is still working its way through the population,
the aggregate ratio can be fairly stable even though the underlying components
are actually increasing.
One way to see change in the earnings gender gap by age is to construct synthetic
birth cohorts, as shown in Figure 1, part A for college graduate men and
women working full-time, full-year and in Figure 1, part B for college graduates
with controls for hours, weeks, and further education.16 The data used are from the
US Census and the American Community Survey (ACS) for the years from 1970
to 2010.
The most obvious and important findings from these depictions are that each
cohort has a higher ratio of female to male earnings than the preceding one and that
the ratio is closer to parity for younger individuals than it is for older individuals, at
least up to some age. One part of the story of the preceding metaphorical chapters is
that there have been large gains in the earnings of women relative to men. An important
clue to what it will take to create gender equality in earnings is that something
happens that decreases women's earnings relative to those of men as they age.
Men and women begin their employment with earnings that are fairly similar,
both for full-time year-round workers and for all workers with controls for hours and
weeks. In the case of the latter group, relative earnings are in the 90 percent range
for the most recent cohorts even without any other controls. But these ratios soon
decline and in some cases plummet to below the 70 percent level.


---Economics-2014-0-07.txt---

---Economics-2014-0-08.txt---
Interestingly, in most cases the ratio increases again when individuals are in their
forties (for the most recent of the cohorts to be old enough to be in that age bracket).
Why it increases is beyond the scope of the present work. It would appear to be less
a function of selection since in most cases the women who left would be drawn disproportionately
from the lower part of the earnings distribution and those returning
would presumably be the same individuals with less accumulated human capital. If
anything, the function should increase and then decrease.
The main conclusion from the aggregate earnings gender gaps is that the difference
in earnings by sex greatly increases during the first several decades of working
life. That conclusion will be reaffirmed by the findings of studies of several highly
specific occupations for which the training for both men and women is identical.
The two degrees are MBA and JD. The data for these occupations, moreover, is longitudinal
(or retrospective) thereby containing actual cohorts, not synthetic ones. In
addition, the data contains detailed productivity-related characteristics.
B. By Occupation
Within versus Between Occupation Differences by Gender. - Another important
clue concerning what the last chapter must contain arises from the fact that
the majority of the current earnings gap comes from within occupation differences
in earnings rather than from between occupation differences. What happens
within each occupation is far more important than the occupations in which
women wind up.
The fact can be demonstrated several ways. One is by observing the coefficient on
female in a log earnings regression when a full set of three-digit occupation dummies
are added. Table 1 gives the results for four samples from the 2009 to 2011
ACS: two for all education groups and two limited to college (BA) graduates. For
each of these samples, one version is for all workers and one is for those working
full-time, full-year. All regressions include age as a quartic, race, and year. Measures
of time worked (log hours, log weeks) and education levels (above college for the
college graduates) are successively added. Occupation dummies (three-digit level)
are included in the most complete specification.
Absorbing the effect of all occupations decreases the coefficient on female by
no more than one-third. Take the case of college graduates working full-time, fullyear
("full-time, BA"). The female coefficient is -0.285 (a ratio of 0.752) with
no additional variables. Adding log hours and log weeks reduces the coefficient to
-0.230 (0.795). Absorbing all occupations reduces the coefficient on female to
-0.163 (0.850), or almost 30 percent of the distance to equality. In the case of all
education groups, the inclusion of all occupations decreases the gap by somewhat
less. For the full-time, full-year sample that includes the education variables, the gap
decreases from -0.247 (0.781) to -0.192 (0.825) or by just 22 percent.
Another way to measure the effect of occupation is to ask what would happen to
the aggregate gender gap if one equalized earnings by gender within each occupation
or, instead, evened their proportions for each occupation. The answer is that
equalizing earnings within each occupation matters far more than equalizing the
proportions by each occupation. The precise results of the exercise will depend on
the choice of weights.


---Economics-2014-0-09.txt---
Taking the case of gender gaps by occupation for college graduates (full-time,
full-year), the aggregate gap is 0.323 log points. Of that difference, 68 percent is
due to the within gap and 32 percent to the between gap when the male weights and
the female earnings are used. If the opposite is used (the female weights and male
earnings) 58 percent is due to the within gap and 42 percent to the between.
The main takeaway is that what is going on within occupations - even when there
are 469 of them as in the case of the Census and ACS - is far more important to the
gender gap in earnings than is the distribution of men and women by occupations.
That is an extremely useful clue to what must be in the last chapter. If earnings gaps
within occupations are more important than the distribution of individuals by occupations
then looking at specific occupations should provide further evidence on how
to equalize earnings by gender. Furthermore, it means that changing the gender mix
of occupations will not do the trick.
Gender Differences in Pay for High-Earning Occupations. - To further understand
differences by occupations, I estimate log earnings equations using the 2009
to 2011 ACS including various observables, such as a quartic in age, education
dummies, race, years, log hours, and log weeks. The regression also includes occupation
dummies, a female dummy, and an interaction of occupation and female.
Three versions of the residual gender difference by occupation have been graphed


---Economics-2014-0-10.txt---
Figure 2 A. Gender Pay Gaps by Occupation: 2009 to 201 1
Notes: Sample consists of full-time, full-year individuals 25 to 64 years old excluding those in  the military using trimmed annual earnings data (exceeding 1,400 hours x 0.5 x 2009 mini-  mum wage). Regression contains age in a quartic, race, log hours, log weeks, education lev-  els, census year, all occupations (469), and an interaction with female and occupation. Part A  contains all full-time, full-year workers (2,603,968 observations); part B has those who gradu-  ated (BA) college (964,705 observations); part C has the group < 45 years old among those  included in part A (1,333,013 observations). Each of the symbols in part A is an occupation  for which the mean annual income for males exceeds $60K (current $) and is limited to occu-  pations with at least 25 males and at least 25 females. For parts B and C the same occupations  are graphed.
Source: American Community Survey 2009 to 201 1.
in Figure 2. Each is for full-time, full-year workers. Part A gives the whole sample;
part B is for only college graduates; and part C includes only "young" (less than
45 years old) workers.
The graphs give the coefficients for approximately the top 95 occupations ranked
by male (wage and business) income. I have graphed only the top occupations
because they are more easily grouped by occupation type.17 In addition, they include
a large fraction of all college graduate workers: 61 percent of all college graduate
men and 45 percent of all college graduate women are in the top group depicted in
Figure 2 using the full-time, full-year college sample.18


---Economics-2014-0-11.txt---



---Economics-2014-0-12.txt---

The findings gleaned from each of the graphs are similar although the levels are
a bit different. In almost all cases the coefficient on female for each of the occupations
is negative. That should not come as a surprise since it is a reflection of
the lower earnings women receive relative to men in almost all occupations. If the
individual's past employment history was included, as it will be for specific occupations
presented later, the coefficients would be considerably smaller. Presented as in
Figure 2, the coefficients give the raw gender gap in pay adjusted for age, education
and time worked.
One way to think about the coefficient is that it is the penalty to being a woman
relative to a man of equal education and age, given hours and weeks of work for
each of the occupations. But why should the penalty differ so greatly by occupation,
even for occupations that are high paying?
Each of the occupations has been categorized into one of five sectors: Business,
Health, Science, Technology, and a miscellaneous group called "Other." Although
the categorization is generally clear (e.g., engineers in Technology; physicians in
Health), occupation descriptions and groupings of the occupations in 0*Net were
used for less obvious cases.19 The list of occupations by category is given in online
Appendix Table Al .


---Economics-2014-0-13.txt---
The clear finding is that the occupations grouped as Business have the largest negative
coefficients and that occupations grouped as Technology and Science have the
smallest ones. That is, given age and time worked residual differences for Business
occupations are large and residual differences in Technology and Science are small.
In fact, for the "young" group (less than 45 years old) some Technology and Science
occupations have positive coefficients.20
For the full-time, full-year sample including log hours, log weeks, and education
in years (in addition to the basic set of variables), the residual difference for
the Business occupations is -0.240 and the residual difference for the Technology
and Science occupations combined is -0.1 14. For the sample of college graduates,
the differences are -0.227 for Business and -0.102 for Science and Technology.
Residual differences for the Health and Other groups are heterogeneous.21 -
These residual differences by occupation provide another important clue about
what must be in the last chapter for there to be gender equality. If one can isolate
the features of occupations that have high and low residual differences by gender
one can figure out what factors make for more equal pay. But before I explore the
reasons for these differences I must address the possibility that the coefficients for
some of the occupations, in particular the "technology" occupations in which there
are relatively few women, are largely driven by selection. My answer will be that
selection is not the dominant reason for the small penalty to being a woman working
in the technology and science fields.
Potential Biases: Technology Occupations. - The fact that individuals in the technology
occupations have among the lowest residual gender gaps may be greeted
with some skepticism. These are not occupations in which women are a large fraction
and the fields of training for many of them are also not those in which women
are abundant. Perhaps the finding is due to selection: the best men and the worst
women could leave technology occupations after a brief tenure. The men who leave
could begin their own businesses and have titles like CEO and the women who leave
could become science teachers. These individuals would not show up in the technology
occupations. Another issue is whether the low gender gaps in recent data are
because of the industries in which these individuals are hired rather than something
about the technology occupations.
I use the National Survey of College Graduates, 2003 (NSCG03) to explore if
women with technology degrees have different labor force participation rates than
those of other college graduate women. The answer is that they do not have lower
participation rates given age (entered as a quartic). In fact, women with BAs or
higher degrees in technology fields have somewhat greater participation than other
women.22 One reason for their slightly higher participation is that having young children
(less than two years old) reduces participation for all college graduate women
but there is a lesser impact on those with technology degrees. 'Tech" appears to
enable women to work part-time or to work more flexibly.


---Economics-2014-0-14.txt---
Does the smaller gender gap result mainly from the characteristics of the technology
occupations or from the features of the industries of the technology employees?
The answer is that it results far more from the occupation than from the industry. To
examine whether industry is the locus of greater gender equality rather than occupation
I create a variable measuring the fraction of the industry's workforce in one of
the identified technology occupations.
To see whether the industry of the technology workers matters, a log earnings
regression is estimated similar to the previous ones but is limited to individuals in
one of the technology occupations. I add a variable measuring the fraction of the
industry workforce in technology (see online Appendix Table A2). Some industries,
such as "computer systems design," have a large fraction of their workers in technology
occupations (around 60 percent) whereas others, such as "motor vehicle manufacturing,
" are moderate (around 12 percent) and still others are very low. Engineers
and information technology workers are hired in almost all industries. The technology
industry variable is also interacted with female.
The results are that technology workers in industries with more technology
workers earn considerably more than workers in other industries, even with occupation
fixed effects. But women do not earn disproportionately more than men
within the technology industries. The bottom line is that technology occupations
and not technology industries more generally are associated with greater gender
equality in earnings.23
III. A Personnel Economics Theory of Occupational Pay Differences
A. Micro-foundations of Compensating Differentials
Residual differences by occupation in earnings by gender, I will demonstrate, are
largely due to the value placed on the hours and job continuity of workers, including
the self-employed.24 Individuals in some occupations work 70 hours a week and
receive far more than twice the earnings of those who work 35 hours a week. But
in other occupations they do not. Some occupations exhibit linearity with respect to
time worked whereas others exhibit nonlinearity (convexity).25 When earnings are
linear with respect to time worked the gender gap is low; when there is nonlinearity
the gender gap is higher.
Total hours worked are generally a good metric for time on the job. But often what
counts are the particular hours worked. The employee who is around when others are
as well may be rewarded more than the employee who leaves at 1 1 am for two hours


---Economics-2014-0-15.txt---
but is hard at work for two additional hours in the evening. Even the self-employed
may have nonlinear earnings because they cannot fully delegate responsibility.
Gender differences in earnings across occupations and occupational groups substantially
concern job flexibility and continuity. By job flexibility I mean a multitude
of temporal matters including the number of hours, precise times, predictability and
ability to schedule one's own hours.
I will now provide some micro-foundations for the notion that nonlinear pay
with respect to hours worked is responsible for the majority of the residual differences
observed in earnings by gender. These notions are the micro-foundations
underlying the compensating differentials model of pay with respect to the amenity
"job-flexibility."
In many workplaces employees meet with clients and accumulate knowledge
about them. If an employee is unavailable and communicating the information
to another employee is costly, the value of the individual to the firm will decline.
Equivalently, employees often gain from interacting with each other in meetings
or through random exchanges. If an employee is not around that individual will be
excluded from the information conveyed during these interactions and has lower
value unless the information can be fully transferred in a low cost manner.
The point is quite simple. Whenever an employee does not have a perfect substitute
nonlinearities can arise.26 When there are perfect substitutes for particular workers
and zero transactions costs, there is never a premium in earnings with respect to
the number or the timing of hours. If there were perfect substitutes earnings would
be linear with respect to hours. But if there are transactions costs that render workers
imperfect substitutes for each other, there will be penalties from low hours depending
on the value to the firm. A sparse framework will demonstrate these points and
develop them further.
B. Framework to Understand the Nonlinear ( Convex ) Hours-Wages Relationship
Assume that each employee, i, invests in training (e.g., MBA, MD) only prior
to the job and that the training is valuable in a hierarchy of positions, j. The positions
can be separate occupations or they can be different varieties of the same
occupation. Let 0 < A < 1 be the fraction of full-time employment worked by the
employee or some metric concerning which hours are worked. Output, Q, for an
employee is given by
m o = i X'kj if Xi > Aj*
I 'kj • (1 - Sj) if a,. < a; '
where k} is output per unit time when time exceeds some amount, Sj is the reduction
in output because the employee works less than some amount in occupation j. The


---Economics-2014-0-16.txt---
Figure 4. A Theory of Occupational Pay Differences
Notes: Each of the lines gives the relationship between output, Q, in some occupation and the  time input, A, of a worker where 0 < A < Amax. When the time input is reduced below some  level, A*, output decreases discretely for occupations 1 and 2. Occupation r has a linear rela-  tionship between time worked and earnings throughout. An individual who works between X'  and Amax will be in occupation 1, an individual between '' and A ' will work in 2 and all others  will work in r, if they remain in the labor force.
setup given by (1) contains a discontinuity in productivity if the worker is absent
more than some amount.
Several occupations or positions may exist among which individual i can choose.
To begin with, assume two positions exist such that kx > k2 and that output is
reduced when hours do not exceed some level such that ¿1 > S2. In addition, assume
fct(l - <5|) < k2( 1 - S2 ) so that one occupation or work setting does not dominate
the other. Now add a third position, r, characterized by linearity (Sr = 0) for which
kr < k2. That position, which can be called the reservation occupation, will dominate
the other two when A is sufficiently low. Also assume that k2( 1 - ô2) < kr .
As shown in Figure 4, an employee will work in occupation 1 as long as A > Aj
and will then shift to occupation 2 at lower hours and finally to the reservation occupation
when A < A2. The relationship between output and hours, and thus between
earnings and hours, is nonlinear (convex). On a per unit time basis the employee
receives more in occupation 1 than 2 and more in occupation 2 than in the reservation
occupation, r.
In the framework, the position with the highest slope is also the one with the
highest penalty with regard to reduced hours. Rather than stay in that position, an
employee who wants lower hours will shift to one that has a lower penalty but
also a lower slope. If the level of hours that the worker wants is yet lower, then the
worker will take the reservation job, which involves complete linearity with respect
to hours.


---Economics-2014-0-17.txt---
The point of the framework is to emphasize that certain occupations impose heavy
penalties on employees who want fewer hours and more flexible employment. The
lower remuneration can result in shifts to an entirely different occupation or to a
different position within an occupational hierarchy or to being out of the labor force
altogether.
Illustrations of the framework will be useful. Lawyers, for example, constitute
an occupational group, certainly one professional degree group. But an individual
with a law degree can be partner in a large law firm in which there is a premium for
working long and continuous hours. The same lawyer could, instead, be employed
as general counsel and work fewer and more flexible hours. Finally, the lawyer can
work in a small firm that allows short and discontinuous hours at no penalty. These
can be thought of as position 1 , position 2, and the reservation position in the framework.
The remuneration of these lawyers, all of whom have the same formal education,
would map out a nonlinear (convex) relationship of total earnings with respect
to hours or to the flexibility of hours.27
Pharmacy, on the other hand, has nearly linear earnings with respect to time
worked. Pharmacists who work more hours earn more, linearly. Those who are
in managerial positions in a pharmacy earn more chiefly because they work more
hours. Those who work part-time get paid less in a linear fashion.28
The explanation just provided for differences across occupations is more a part of
personnel economics than human capital theory because the underlying notions are
those of compensating differentials. Differences in pay arise because of productivity
differences in the workplace, not because of inherent differences in human capital
across workers. Some workers want the amenity of flexibility or of lower hours and
some firms may find it cheaper to provide.
The framework just outlined can be viewed as the micro-foundations of a compensating
differentials model.29 Individuals place different values on the amenity
"temporal flexibility," and firms or sectors face different costs in providing the amenity.
The framework gives reasons why there are different costs and how they might
change.30
IV. Occupational Differences from 0*Net Characteristics
Do the notions of the framework have explanatory power regarding the estimated
gender gaps for the 95 occupations previously identified and classified by group? To
explore the relationship between the residual gender earning gap and occupational
features I have used detailed occupation descriptions from 0*Net online.31
0*Net lists hundreds of separate characteristics grouped in seven categories. The
two categories most relevant for the issues at hand are "work context" (57 characteristics)
and "work activities" (41 characteristics). Five characteristics seem most


---Economics-2014-0-18.txt---

relevant to features of the model and are listed in the notes to Table 2.32 These
characteristics reflect time pressure, the need for workers to be around at particular
times, the flexibility of the occupation with regard to scheduling, the groups and
workers the employee must regularly keep in touch with, and the degree to which
the worker has close substitutes.
Each of the 0*Net characteristics has been normalized to have a mean of zero
and a standard deviation of one. I group the technology and science occupations
together and compare the values of the five characteristics with those for occupations
in business, health, and law, the largest and also the highest paying of the
"other" occupations.
Because there are about twice as many 0*Net occupations than Census occupations,
the first task was to match occupations across the two sources. In most cases
the difference was simply that 0*Net occupations are cross-referenced by industry.


---Economics-2014-0-19.txt---
The 0*Net characteristic levels were then weighted by the relative number of individuals
in the 0*Net occupations to get the characteristic values for the Census
occupations, for which the residual gender gaps had been computed.
As can be seen in Table 2, technology and science occupations score far below
the others on each of the five measures and in some cases the differences are almost
one standard deviation lower. That is to say, in comparison with business occupations
those in technology and science have far greater time flexibility, fewer client
and worker contacts, fewer working relationships with others, more independence
in determining tasks, and more specific projects with less discretion over them. Each
of these characteristics should produce a more linear relationship between hours
and earnings and the greater linearity should produce a lower residual difference in
earnings by sex.
The characteristics help differentiate the business from the technology and science
occupations rather well. They do not always capture differences between the
health professions and others. For one, they do not capture the time demands among
the self-employed and many in the higher paid health occupations (e.g., dentist,
podiatrist, physician, and veterinarian) have substantial rates of ownership. But they
do pick up the fact that most health professionals have considerable contact with
clients, have enormous discretion, and make decisions affecting the lives of others.33
Within the "other" category, lawyers are clearly at the high end of the characteristics
with considerable contact, time pressure, structure, and discretion.
The scatter plot of the simple mean of the 0*Net characteristics for each of the
95 high-income occupations against the mean (adjusted) gender earning gap for
each occupation among college graduates (full-time, year-round workers) is given
in Figure 5. The relationship is clearly negative with a correlation coefficient of
-0.463. A higher value for the characteristics is associated with a lower ratio of
(adjusted) female to male earnings (a larger negative value for the log of the gender
gap). In addition, the characteristics also pick up some of the within group variance.
The relationship is strongest for time pressure, contact with others, and freedom to
make decisions, but is also reasonable for establishing and maintaining interpersonal
relationships and structured versus unstructured work.
V. Evidence on Nonlinear Pay and the Gender Gap in Earnings
I have thus far established that the gender gap in pay is small at the start of
employment but greatly increases with age (even correcting for hours and weeks in
a national sample) and that it significantly differs by occupation. I have also shown
using the 0*Net data that characteristics of work settings are associated with the
(adjusted) gender gap in pay such that work environments that require more interactions
or have more time pressure, for example, are those with larger gender earning
gaps.
Another hint at what must be in the last chapter can be gleaned by adding a
(log hours X occupation) interaction to the regression containing the occupation


---Economics-2014-0-20.txt---

and log hours main effects. The interaction of log hours and occupation allows the
relationship between hours and earnings to differ for each occupation. The computed
elasticity of annual income with respect to weekly hours for each occupation
is graphed in Figure 3 against the residual gender pay gap for college graduates from
Figure 2, part B.34 There is a clear negative association between the residual gender
earnings gap and the elasticity of annual earnings with respect to weekly hours.
Occupations with higher elasticities have more negative log earnings gender gaps.
The largest elasticities are for business occupations and the smallest are for technology,
science and health occupations. In fact, almost half of the business occupations
have a computed elasticity that exceeds one, as does law. The business
occupation with the lowest elasticity is that of financial examiner, an occupation
often found in the federal and state government settings. Only one of the science
and technology occupations has an elasticity of earnings with respect to hours that
exceeds one and it is that of actuary.
As I will later demonstrate using data on occupations in business and law, the
impact of hours on the gender gap is large and goes far to explain much of the gender



---Economics-2014-0-21.txt---
earnings gap.35 Individuals who work long hours in these occupations receive a disproportionate
increase in earnings. That is, the elasticity of earnings with respect to
hours worked is greater than one.
In previous work, Katz and I (2008b) demonstrated that among Harvard College
graduates, the penalty to time out of the labor market differs greatly by occupation.
Among those who received their BAs around 1990, a 10 percent hiatus in employment
time 15 years after the BA, thus amounting to an 18-month break, was associated
with a decrease in earnings of 41 percent for those with an MBA, 29 percent
for those with a JD or a PhD, and 15 percent for those with an MD.36 In addition,
the reduction in earnings from time off for MDs was linear in lost experience but
was discrete (nonlinear) for MBAs. Any time off for MBAs is heavily penalized. We
also found that MDs and PhDs took the shortest non-work spells after a birth and
MBAs took the longest. That is, those with the greatest penalty to time out also took
the most time out, largely because their jobs did not enable shorter or more flexible
schedules.
In this section I expand on these findings and explore the widening gender gap
in pay with age and differences in the gender gap by occupation using data sets
specific to occupations and degrees. I will demonstrate that some occupations have
high penalties for even small amounts of time out of the labor force and have nonlinear
earnings with respect to hours worked. Other occupations, however, have small
penalties for time out and almost linear earnings with respect to hours worked. In
the first group of occupations are individuals who have earned an MBA or a JD. In
the second group - the occupations with lower penalties for time out and the more
linear ones - is one in the health sector (pharmacy).
The data sets I use are for fairly uniform groups of men and women who have
received the same advanced degree or work in the same occupation. The information
on job experience and time worked is highly detailed. The gender gap in annual earnings
for the JDs and MBAs, although large by year 15, is almost entirely explained
by various factors, such as hours worked, time out of the labor force, and years spent
in part-time employment. Small differences in time away or in hours translate into
large differences in pay. Nonlinearities in pay with respect to time worked can be
seen. For the pharmacists, however, hours worked is also of importance in explaining
gender differences in pay but earnings are fairly linear in time worked and time
out of the labor force is of less importance to contemporaneous pay. In fact, because
part-time work is prevalent in pharmacy, women do not take off much time.
A. Business (MBA): Nonlinear Occupations
At the start of their careers, earnings by gender are almost identical among MBAs
graduating from the University of Chicago Booth School from 1990 to 2006.37 But


---Economics-2014-0-22.txt---
after five years, a 30 log point difference in annual earnings develops and at 10 to
16 years after the MBA, the gender gap in earnings grows to 60 log points (that is,
women earn 55 percent what men do). Three factors explain 84 percent of the gap.
Training prior to MBA receipt, (e.g., finance courses, GPA) accounts for 24 percent.
Career interruptions and job experience account for 30 percent, and differences in
weekly hours are the remaining 30 percent. Importantly, about two-thirds of the total
penalty from job interruptions is due to taking any time out.
At 10 to 16 years from MBA receipt, 23 percent of University of Chicago Booth
School MBA women who are in the labor force work part-time and, interestingly,
more than half of those working part-time employ themselves. Around 17 percent
are not currently employed and 60 percent work full-time (5 1 percent do of those
with children). Cumulative time not working is about one year for all women 10 to
16 years after the MBA.
Not surprisingly, children are the main contributors to women's labor supply
changes. Women with children work 24 percent fewer hours per week than men or
than women without children. The impact of children on female labor supply differs
strongly by spousal income. MBA moms with high-earning spouses have labor
force rates that are 18.5 percentage points lower than those with lesser-earnings
spouses.38 They work 19 percent fewer hours per week (when working) than those
with spouses below the high-income level. The impact of higher income husbands
may be a pure income effect but it more likely results from a combination of an
income and a substitution effect in which the family requires some parental home
time and the high-flyer husband offers little.39
Another important result is that the impact of a birth on labor supply grows over
time in an individual, fixed-effects estimation. A year after a first birth, women's
hours, conditional on working, are reduced by 17 percent and their participation by
13 percentage points. But three to four years later, hours decline by 24 percent and
participation by 18 percentage points. Some MBA moms try to stay in the fast lane
but ultimately find it is unworkable. The increased impact years after the first birth,
moreover, is not due to the effect of additional births.
Part-time work in the corporate sector is uncommon and part-timers are often
self-employed (more than half are at 10 to 16 years out). Differences in career interruptions
and hours worked by sex are not large, but the corporate and financial sectors
impose heavy penalties on deviation from the norm. Some female MBAs with
children, especially those with high earning husbands, find the trade-offs too steep
and leave or engage in self-employment.
In sum, the appeal of an MBA for women is large - incomes are substantial even
if they are far lower than those of their male peers. But some women with children
find the inflexibility of the work insurmountable.


---Economics-2014-0-23.txt---
B. Law ( JD ): Nonlinear Occupations
The gender gap in earnings between male and female JDs, graduating from the
University of Michigan Law School from 1982 to 1991, is nil at the start of employment.
The gap is small and insignificant at year 5, after controlling for hours, weeks
and time off, as can be seen in Table 3, columns 1 to 3.40 But as in the case of the
MBAs the gap balloons to around 55 log points by year 15 in a longitudinal sample
40The University of Michigan Alumni Survey Research Dataset is used, which includes alumni surveys from  1967 through 2006 for persons graduating from 1952 to 2001 together with administrative data on each alumnus.  The surveys were sent to classes 5, 15, 25, 35, and 45 years after receiving their JD. Because response rates for the  cross-section data are high (in the 60 percent range), surveys were later linked, where possible, to create a longitu-  dinal dataset. The information used here is from the longitudinal data linking individuals from graduation to years  5 and 15. See Wood, Corcoran, and Courant (1993) for similar work using a much earlier form of the cross-section  data and Noonan, Corcoran, and Courant (2005) for work that uses the more recent longitudinal samples.


---Economics-2014-0-24.txt---
(column 4). The remaining gap at year 15 is reduced to around 22 log points when
time worked during the year is included and to 13 log points once work absences
and job tenure are added (columns 5 and 6).41
Of great importance with regard to the issues raised here is that annual earnings
are clearly nonlinear (convex) with respect to hours in year 15 but not in year 5. At
year 15 the coefficient on log hours in the log earnings regression is significantly
greater than one and that finding is robust to various specifications. In the column 5
specification with law school performance and weeks, the coefficient on log hours
is 1.34. It drops to 1.162 when job tenure, years out of the labor force, and years in
part-time employment are added. But the precise position chosen and thus the slope
of earnings with respect to weekly hours are determined by factors (e.g., children)
that in turn influence job interruptions and prior part-time work. The framework
necessitates a homogeneous group of individuals by training and those are given
here by the precise law school, performance in law school, completion of the JD,
and time since degree (15 years).
Because those who work in law firms usually report their hourly billing rate or fee
(about 90 percent do), columns 7 and 8 also include the relationship between hours
and the hourly fee reported. That, too, displays nonlinearity (convexity). The more
hours worked, the higher the hourly fee reported.
The nonlinearity of annual earnings with respect to hours worked and the relationship
between hourly earnings and hours are graphed in Figure 6 together with
characteristics of the JDs in each of four hour-intervals used (10-34, 35-44, 45-54,
and 55+ hours). The annual earnings graph bears a striking resemblance to the representation
of the framework in Figure 4. The nonlinearity of annual earnings with
respect to hours worked is clear.
The fraction female at 15 years is 0.288, but the fraction female decreases as
hours increase from 0.826 for the 0 to 34 hours group to 0. 182 for the 55 hours plus
group. The fraction of women who have children at 15 years out also decreases as
hours increase from 0.852 for the lowest hours bin to 0.536 for the highest. As hours
worked increase so does firm size and fraction partner, while the incidence of solo
practice decreases. Of some interest with respect to why nonlinearities in pay arise
with respect to hours worked, among JDs who work in a law firm twice the fraction
of time by the average lawyer is spent representing a Fortune 500 company in
the highest hours bin than in the smallest hours bin. Similarly, the fraction of time
representing "rich" people increases substantially (from about 0.025 to 0.09) when
a lawyer shifts from working part-time to full-time.
As in the case of MB As, the reason for the reduction in hours of work at 15 years
out is largely due to the arrival of children. And also similar to the MBA case is
that the decrease in participation is due to an interaction between children and the
income of the spouse. About 16.5 percent of JD women, and 21 percent of those
with children of any age, are not in the labor force by year 15.
Spousal income is an important determinant of who stays and who leaves employment
at year 15. JD women with children who are married to men in the upper 30
percent of the earnings distribution (more than $200K per year, in US$2007) have


---Economics-2014-0-25.txt---
lower participation rates than JD women married to lower-income husbands or who
do not have children but are married to a high-income husband. Using the highincome
cutoff for the husbands of female JDs reveals that 21.6 percent of those with
children are not in the labor force at 15 years but that 10.4 percent are not in the


---Economics-2014-0-26.txt---
labor force for those with lower income husbands.42 There are, however, almost no
differences among those with no children. Almost none of those women, independent
of the income of their husbands, is out of the labor force.
Leaving the labor force for women with a JD appears to involve an interaction of
spousal income and the presence of children. The reasons would seem the same as
offered for the MBAs. Children require a modicum of parental time, high-income
husbands provide little of it, and part-time work for JDs is insufficiently remunerative
for some to remain employed.
C. Pharmacy: A Linear Occupation
The occupation of pharmacist is an excellent example of one that has fairly linear
earnings with respect to hours worked and a negligible penalty to time out of the
labor force. Managers of pharmacies get paid more because they work more hours.
Female pharmacists with children get paid less because they work fewer hours.
Pharmacists, particularly women, often work part-time. But there is no part-time
penalty.43
Pharmacy is a high income occupation - the eighth highest for men and third
highest for women - that, in recent decades, has required a specialized six-year
combined BS-doctoral degree. I will briefly summarize the findings from a study of
the pharmacy profession by Goldin and Katz (2013) that uses, primarily, data from
the National Pharmacist Workforce Surveys for 2000, 2004, and 2009.
Most pharmacists today work for non-independent retailers, mainly large chains,
or in hospitals - about 75 percent do. But four decades ago around 25 percent were
employed in these sectors. Self-ownership and employment by independent pharmacies
declined greatly in the interim.
At the same time, women have increased their numbers in the profession. They
are now about 55 percent of all active pharmacists and 65 percent of new hires.
Women were always a reasonable fraction of pharmacists. Before the large increase
in retail chain employment, women were often the part-time assistants of male pharmacists
who managed their own pharmacies.
Today the occupation has among the lowest gender earnings gaps among highearning
occupations. The (unadjusted) ratio of earnings for female to male full-time,
full-year pharmacists is 0.85 whereas it was 0.60 in 1970. The hours-adjusted ratio
is from 0.93 to 0.95.44
Several changes in the pharmacy profession have been responsible for the increase
of female to male earnings. The first is the decrease in self-ownership and the rise
of large corporation and hospital employment. As corporate ownership and hospital
employment increased, the portion of earnings that came from self-employment
decreased. The ratio of the (time-adjusted) earnings of female to male pharmacists,

---Economics-2014-0-27.txt---
in consequence, increased as the rents from ownership decreased and because men
were disproportionately the owners.
The second change involves decreased costs to flexible employment in pharmacy.
Pharmacists have become better substitutes for each other with the increased standardization
of procedures and drugs. The extensive use of computer systems that
track clients across pharmacies, insurance companies, and physicians mean that any
licensed pharmacist knows a client's needs as well as any other. If a pharmacist is
assisting a customer and takes a break, another can seamlessly step in. In consequence,
there is little change in productivity for short-hour workers and for those
with labor force breaks. Other factors mentioned in the 0*Net section are also of
importance. For example, there is less need for interdependent teams in pharmacy
and for extensive contact with other employees.45
Female pharmacists have fairly high labor force participation rates and only a
small fraction have substantial interruptions from employment. Rather than taking
off time, female pharmacists with children go on part-time schedules. In fact, more
than 40 percent of female pharmacists with children work part-time from the time
they are in their early thirties to about 50 years old. Male pharmacists work around
45 hours a week, about nine hours more than the average female pharmacist.
The position of pharmacist became among the most egalitarian of all professions
today. The facts in Goldin and Katz (2013) are consistent with the labor market
effects of changes in technology and in the structure of the industry. They are less
consistent with change stemming from an increase in the demand for family-friendly
workplace amenities. In addition, the changes do not appear to have resulted from
legislation or anti-discrimination policy or licensing requirements or regulations
specific to the pharmacy profession. Rather, a host of structural changes outside the
realm of the labor market (e.g., increased economies of scale in pharmacies, standardization
of drugs, computer use, linked records through insurers) increased the
demand for pharmacists and reorganized work in ways that have made pharmacy a
more family-friendly and female-welcoming profession.
VI. What the Last Chapter Must Contain
The reasoning of this essay is as follows. A gender gap in earnings exists today
that greatly expands with age, to some point, and differs significantly by occupation.
The gap is much lower than it had once been and the decline has been largely due to
an increase in the productive human capital of women relative to men. Education at
all levels increased for women relative to men and the fields that women pursue in
college and beyond shifted to the more remunerative and career-oriented ones. Job
experience of women also expanded with increased labor force participation. The
portion of the difference in earnings by gender that was once due to differences in
productive characteristics has largely been eliminated.
What, then, is the cause of the remaining pay gap? Quite simply the gap exists
because hours of work in many occupations are worth more when given at particular
moments and when the hours are more continuous. That is, in many occupations


---Economics-2014-0-28.txt---
earnings have a nonlinear relationship with respect to hours. A flexible schedule
often comes at a high price, particularly in the corporate, financial, and legal worlds.
A compensating differentials model explains wage differences by the costs of
flexibility. The framework developed here shows why there are higher or lower costs
of time flexibility and the underlying causes of nonlinearity of earnings with respect
to time worked. Much has to do with the presence of good substitutes for individual
workers when there are sufficiently low transactions costs of relaying information.
Evidence from 0*Net on occupational characteristics demonstrates that certain
features of occupations that create time demands and reduce the degree of substitution
across workers are associated with larger gender earnings gaps.
Data for MBAs and JDs shows large increases in gender pay gaps with time since
graduation and also reveals the relationship between the increasing gender pay gap
and the desire for time flexibility due to the arrival of children. Lower hours mean
lower earnings in a nonlinear (convex) fashion. Lower potential earnings, particularly
among those with higher-earning spouses, often means lower labor force participation.
Pharmacists, on the other hand, have pay that is more linear with respect
to hours of work. Female pharmacists with children often work part-time and remain
in the labor force rather than exiting.
What must be in this chapter for it to be the last? The last chapter must be concerned
with how worker time is allocated, used and remunerated and it must involve
a reduction in the dependence of remuneration on particular segments of time. It
must involve greater independence and autonomy for certain types of workers and
the ability of workers to substitute seamlessly for each other. Flexibility at work has
become a prized benefit but flexibility is of less value if it comes at a high price in
terms of earnings. The various types of temporal flexibility require changes in the
structure of work so that their cost is reduced.
There are many occupations and sectors that have moved in the direction of less
costly flexibility. Firms in many sectors, including healthcare, retail sales, banking,
brokerage, and real estate, are making their employees better substitutes for each other
and trying to convince their clients of that.46 When clients perceive there is a greater
degree of substitutability among workers, a more linear payment schedule emerges.
Pharmacists are now better substitutes for each other than they once were and
their earnings are fairly linear with regard to time worked. Larger scale in healthcare
has enabled teamwork that has freed physicians from irregular and long hours. Most
small veterinary practices no longer have weekend, night, and emergency hours
and, instead, have clients use the increasing number of large regional veterinary
hospitals. Self-employment has declined in a large number of professions the past
several decades including dentists, lawyers, optometrists, pharmacists, physicians,
and veterinarians. The decline has produced a reduction in the premium to long and
unpredictable hours.
Some changes have occurred organically, often due to economies of scale (as in
the cases of physicians, pharmacists and veterinarians), some changes have been
prompted by employee pressure (as in the case of various physician specialties such
as pediatricians), and other changes have occurred because firms want to reduce


---Economics-2014-0-29.txt---
labor costs. Not all positions can be changed. There will always be 24/7 positions
with on-call, all-the-time employees and managers, including many CEOs, trial lawyers,
merger-and-acquisition bankers, surgeons, and the US Secretary of State. But,
that said, the list of positions that can be changed is considerable.
What the last chapter must contain for gender equality is not a zero sum game
in which women gain and men lose. This matter is not just a woman's issue. Many
workers will benefit from greater flexibility, although those who do not value the
amenity will likely lose from its lower price. The rapidly growing sectors of the
economy and newer industries and occupations, such as those in health and information
technologies, appear to be moving in the direction of more flexibility and
greater linearity of earnings with respect to time worked. The last chapter needs
other sectors to follow their lead.


---Economics-2015-0-02.txt---
I. Bargaining and Climate Coalitions
A. Free-riding and the Westphalian System
Subject to many deep uncertainties, scientists and economists have developed an
extensive understanding of the science, technologies, and policies involved in climate
change and reducing emissions. Much analysis of the impact of national policies such
as cap-and-trade or carbon taxes, along with regulatory options, has been undertaken.
Notwithstanding this progress, it has up to now proven difficult to induce countries
to join in an international agreement with significant reductions in emissions.
The fundamental reason is the strong incentives for free-riding in current
international climate agreements. Free-riding occurs when a party receives the
benefits of a public good without contributing to the costs. In the case of the international
climate-change policy, countries have an incentive to rely on the emissions
reductions of others without taking proportionate domestic abatement. To this is
added temporal free-riding when the present generation benefits from enjoying
the consumption benefits of high carbon emissions, while future generations pay
for those emissions in lower consumption or a degraded environment. The result


---Economics-2015-0-03.txt---
of free-riding is the failure of the only significant international climate treaty, the
Kyoto Protocol, and the difficulties of forging effective follow-up regimes.
While free-riding is pervasive, it is particularly difficult to overcome for global public
goods. Global public goods differ from national market failures because no mechanisms
- either market or governmental - can deal with them effectively. Arrangements
to secure an international climate treaty are hampered by the Westphalian dilemma.
The 1648 Treaty of Westphalia established the central principles of modern international
law. First, nations are sovereign and have the fundamental right of political
self-determination; second, states are legally equal; and third, states are free to manage
their internal affairs without the intervention of other states. The current Westphalian
system requires that countries consent to joining international agreements, and all
agreements are therefore essentially voluntary (Treaty of Vienna 1969, article 34).
B. Clubs as a Mechanism to Overcome Free-riding
Notwithstanding the Westphalian dilemma, nations have overcome many transnational
conflicts and spillovers through international agreements. There are over
200,000 UN-registered treaties and actions, which are presumptive attempts to
improve the participants' welfare. Countries enter into agreements because joint
action can take into account the spillover effects among the participants.
How have countries overcome the tendency toward free-riding associated with
the Westphalian system? Consider the many important international agreements in
international trade and finance as well as alliances that have reduced the lethality of
interstate military conflicts. These have often been accomplished through the mechanism
of "clubs." A club is a voluntary group deriving mutual benefits from sharing
the costs of producing an activity that has public-good characteristics. The gains
from a successful club are sufficiently large that members will pay dues and adhere
to club rules in order to gain the benefits of membership.
The theory of clubs is a little-known but important corner of the social sciences.
(For an early analysis, see Buchanan 1965, while for a fine survey, see Sandler and
Tschirhart 1980.) The major conditions for a successful club include the following:
(i) that there is a public-good-type resource that can be shared (whether the benefits
from a military alliance or the enjoyment of a golf course); (ii) that the cooperative
arrangement, including the dues, is beneficial for each of the members; (iii) that nonmembers
can be excluded or penalized at relatively low cost to members; and (iv) that
the membership is stable in the sense that no one wants to leave. For the current international-
trade system, the advantages are the access to other countries' markets with
low trade barriers. For military alliances, the benefits are peace and survival. In all
cases, countries must contribute dues - these being low trade barriers for trade or burden
sharing in defense treaties. If we look at successful international clubs, we might
see the seeds of an effective international system to deal with climate change.
The organization of this paper is as follows. After a sketch of the proposal, I begin
with a discussion of the issues of free-riding and previous analyses of potential solutions.
I examine potential approaches to internalizing the transnational spillovers
and conclude that a Climate Club with penalties for nonmembers is the most fruitful
mechanism. The following sections develop a model of coalition formation with
climate economics (the Coalition-DICE or C-DICE model) and show the results of


---Economics-2015-0-04.txt---
illustrative calculations. The bottom line - that clubs with penalties or sanctions on
nonparticipants can support a strong international climate agreement - is summarized
at the end of the paper.
C. A Sketch of the Climate Club
The idea of a Climate Club should be viewed as an idealized solution of the
free-riding problem that prevents the efficient provision of global public goods. Like
free trade or physics in a vacuum, it will never exist in its pure form. Rather, it is a
blueprint that can be used to understand the basic forces at work and sketch a system
that can overcome free-riding.
Here is a brief description of the proposed Climate Club: the club is an agreement by
participating countries to undertake harmonized emissions reductions. The agreement
envisioned here centers on an "international target carbon price" that is the focal provision
of an international agreement. For example, countries might agree that each country
will implement policies that produce a minimum domestic carbon price of $25 per
ton of carbon dioxide (C02). Countries could meet the international target price requirement
using whatever mechanism they choose - carbon tax, cap-and-trade, or a hybrid.
A key part of the club mechanism (and the major difference from all current proposals)
is that nonparticipants are penalized. The penalty analyzed here is uniform
percentage tariffs on the imports of nonparticipants into the club region. Calculations
suggest that a relatively low tariff rate will induce high participation as long as the
international target carbon price is up to $50 per ton.
An important aspect of the club is that it creates a strategic situation in which
countries acting in their self-interest will choose to enter the club and undertake high
levels of emissions reductions because of the structure of the incentives. The balance
of this study examines the club structure more carefully and provides an empirical
model to calculate its effectiveness.
II. Background on International Agreements on Climate Change
A. Basic Free-riding Equilibrium
There is a large literature on the strategic aspects of international environmental
agreements, including those focused on climate change. One important strand is the
analytical work on global public goods. The clear message is that without special
features the outcome will be a prisoners' dilemma or tragedy of the commons in
which there is too little abatement. This point is illustrated with a simple model that
will form the backbone of the empirical model below.
I begin by analyzing the costs and benefits of national climate policies in a
noncooperative (NC) framework (Nash 1950). In the NC framework, countries act
individually and are neither rewarded nor penalized by other countries for their policies.
The analysis assumes that countries maximize their national economic welfare
and ignores partisan, ideological, myopic, and other nonoptimizing behaviors.
While history is full of wooden-headed actions of countries and their leaders, as
well as policies that are farsighted and attend to global welfare, attempting to incorporate
these features is beyond the scope of this study of climate regimes.


---Economics-2015-0-05.txt---
B. Noncooperative Equilibrium in a One-shot Decision
Begin by assuming that countries choose their policies once and for all in a single
decision. I take a highly stylized structure, but the most complex models extant have
virtually identical results.
For this example, I assume that the emissions-intensities (a) and the damage-output
ratios are identical for all countries and that countries only differ in their sizes.
In what follows, W = total economic welfare, A = abatement cost, D = damages,
Q = output, E = actual emissions, E = uncontrolled emissions, and fj, = emissions
control rate [= (E - E)/E]. A key variable is the social cost of carbon (SCC),
which is the marginal damage from a unit of emissions. The global SCC is denoted
by 7, while 6 is the country share of world output and other variables. This first
analysis excludes trade.
The basic identity for country i is that welfare equals output minus abatement
cost minus damages. Abatement costs are assumed to be quadratic in the emissions
reduction rate, A, = aßjQt = a¡i}diQw, where a is the identical abatement-cost
parameter and Qw is world output. Damages are proportional to global emissions.
All these imply for region i:
(1) = e, - ^ - D¡ = e,Qw - aß}etQw - 1ei(Ei + e Ejì-
Mi
The potential for free riding occurs because most of the damages originate outside
the country. This is captured in the last term of equation (1), which in practice means
that for all countries the preponderance of damages originate outside, while the
preponderance of damages caused by a country's emissions falls on other countries.
Maximizing each country's welfare in a one-shot game, assuming no cooperation
or strategic interactions, yields (as shown in the online Appendix) the noncooperative
emissions-control rate and domestic carbon price (rf0):
(2) = 6>,[7cr/2a]
(3) rfc = en.
The most intuitive result shown in (3) is that a country's noncooperative carbon
price is equal to the country share of output times the global social cost of
carbon. A less intuitive result in (2) is that a country's noncooperative control rate
( //fc) is proportional to the country share of world output, to the global SCC, to
the emissions-output ratio, and inverse to the abatement-cost parameter. Equation
(3) survives alternative specifications of the abatement-cost function, while (2) is
sensitive to parameters such as the exponent in the cost function.
Under the simplified assumptions, calculate the global average NC control rate
and carbon price as functions of the cooperative levels ( ~ßc and 7e) ;



---Economics-2015-0-06.txt---
In these equations, #(0)=^/ Herfindahl index of country size.
Equations (4) and (5) show the basic free-riding equilibrium for a global public
good with the simplified structure. The globally averaged noncooperative carbon price
and control rate are equal to the Herfindahl index times the cooperative values. For
example, if there are ten equally sized countries, the Herfindahl index is 10 percent, and
the global carbon tax and emissions-control rates are 10 percent of the efficient levels.
The Herfindahl index for country gross domestic products (GDPs) is about 12 percent,
indicating that (when emissions-intensities and damage ratios are equal for
each country) the noncooperative control rate and carbon price are about 12 percent
of the cooperative values. This figure is close to calculations that have been made
in more complete models (see Nordhaus and Yang 1996; Nordhaus 2010; Bosetti et
al. 2012). For example in the multiperiod RICE-2010 model with 12 regions, the
noncooperative price is estimated to be is 1 1 percent of the efficient price (Nordhaus
2010, supplemental materials).
C. Outcomes with Repeated Decisions
A more complete treatment of country interactions in climate-change policy
views interactions in a dynamic framework with decisions over time. The standard
analysis uses the framework of a repeated prisoners' dilemma (RPD) game. For
simplicity, assume that the structure above is repeated every few years with identical
parameters. One equilibrium of a RPD is just the iterated inefficient one-shot equilibrium
with minimal abatement as described above. However, because players can
reward and punish other players for good and bad behavior, RPD games generally
have multiple equilibria; these might include more efficient outcomes if country
discount rates are low (these being the generalized results of various folk theorems).
The efficient RPD equilibrium with large numbers of countries will be hampered by
free-riding and inability to construct renegotiation-proof strategies in situations with
large number of agents.
The strategic significance of the analysis of NC behavior is threefold. First, the
overall level of abatement in the noncooperative equilibrium will be much lower
than in the efficient (cooperative) strategy. A second and less evident point is that
countries will have strong incentives to free-ride by not participating in strong
climate-change agreements. Finally, the difficulty of escaping from a low-level,
noncooperative equilibrium is amplified by the intertemporal trade-off because
the current generation pays for the abatement while future generations are the
beneficiaries of lower damages. But to a first approximation, the noncooperative
analysis in this section describes international climate policy as of 2015.
III. Climate Coalitions and International Environmental Treaties
A. Key Definitions on Sanctions and Coalitions
Might coalitions of countries form cooperative arrangements or treaties that
improve on noncooperative arrangements? Questions involving the formation,
value, and stability of coalitions have a long history in game theory, oligopoly
theory, as well as in environmental economics. In this section, I analyze coalitions


---Economics-2015-0-07.txt---
without external penalties, that is, ones that have self-contained payoffs and cannot
be enforced by third parties or be linked to other arrangements.
Begin with some definitions. The formal difference between "external" and "internal"
penalties is the following. If countries are playing a repeated game, then internal
penalties maintain the payoff structure of the game, but countries can penalize or
reward others by selecting different combinations of strategies. Tit-for-tat is a game
with internal penalties because it has a reward structure given by the payoffs of the
stage games. In the end, however, the rewards must be some combination of the payoffs
of the original game. By contrast, external penalties change the payoff structure of the
game. A standard external penalty comes when a player imposes a sanction that derives
from a trading relationship that is unconnected to the payoffs of the original game. For
example, in a treaty to preserve whales, a player might punish an uncooperative party
by imposing a duty on the imports of related products. The tariffs are unrelated to the
public-goods nature of the decline of the whale population and are therefore external.
Before turning to the analysis of coalitions, it will be useful to distinguish between
"bottom-up" and "top-down" coalitions. The standard approach in environmental
economics, reviewed in the next section, focuses on a bottom-up approach in which
coalitions optimize their own self-interest and evolve into larger or smaller coalitions.
Regional trade agreements are examples of this approach.
The Climate Club approach is instead a top-down approach. Here, the regime is
optimized to attract large numbers of participants and attain high levels of abatement,
and then countries decide whether or not to join. The Bretton Woods institutions
such as the International Monetary Fund or the World Trade Organization are
examples of the top-down model.
B. Bottom-up Coalitions and the Small Coalition Paradox
In the context of climate change, coalitions of countries can form treaties that
potentially improve the welfare of their members by taking concerted action. If several
countries maximize their joint welfare, the optimized level of abatement will rise
relative to the noncooperative equilibrium because more countries will benefit. In
the algebraic example described above, the coalition's optimal control rate shown in
equation (2) will equal the global optimum times the coalition's share of world output.
As the coalition increases to include all countries, the global level of abatement
will tend toward the efficient rate. This result might form the basis for hopes that
arrangements like the Kyoto Protocol will lead to deep emissions reductions.
In fact, theoretical and empirical studies indicate that bottom-up coalitions for cartels
and global public goods tend to be small, fragile, and unstable. Work on coalition stability
by Hart and Kurz (1983) found that coalitions are generally not stable, and their
structure will depend upon the structure of the payoffs and the stability concept. Studies
of the structure of cartels in oligopoly theory (see, e.g., D'Aspremont et al. 1983 and
Donsimoni, Economides, and Polemarchakis 1986) found that cartels are likely to be
small, unstable, or of vanishingly small importance as the number of firms grows.
Studies in environmental economics and climate change find virtually universally
that coalitions tend to be either small or shallow, a result I will call the "small
coalition paradox." The paradigm for understanding the small coalition paradox is
well discussed in Barrett's (2003) book on international environmental agreements.


---Economics-2015-0-08.txt---
His analysis emphasizes credible or "self-enforcing" treaties (Barrett 1994). These
are ones that combine individual rationality (for each player individually) and collective
rationality (for all players together). This concept is weaker than the concept
of coalition stability discussed later, which adds rationality for each subset of the
players. Barrett emphasizes the difficulties of reaching agreements on global public
goods with large numbers of participants because of free-riding. Similar to the results
for cartels, Barrett and others find that stable climate coalitions tend to have few
members; therefore, as the number of countries rises, the fraction of global emissions
covered by the agreement declines. He further argues, based on a comprehensive
review of existing treaties, that there are very few treaties for global public goods that
succeed in inducing countries to increase their investments significantly above the
noncooperative levels. Moreover, the ones that do succeed include external penalties.
How can we understand the small coalition paradox? Here is the intuition for
climate change: clearly, two countries can improve their welfare by combining and
raising their carbon price to the level that equals the sum of their SCCs. Either country
is worse off by dropping out. The 2014 agreement between China and the United
States to join forces in climate policy might be interpreted as an example of a small
bottom-up coalition.
Does it follow that, by increasing the number of countries in the treaty, this process
would accumulate into a grand coalition of all countries with efficient abatement? That
conclusion is generally wrong. The problem arises because, as more countries join, the
cooperative carbon price becomes ever higher, and ever further from the NC price. The
discrepancy gives incentives for individual countries to defect. When a country defects
from an agreement with m countries, the remainder coalition (of m - 1 countries)
would reoptimize its levels of abatement. The revised levels of abatement would still
be well above the NC levels for the remainder coalition, while the defector free-rides
on the abatement of the remainder coalition. The exact size of the coalitions would
depend upon the cost and damage structure as well as the number of countries.
The online Appendix provides a simple analysis of the bottom-up coalition equilibrium
for identical countries with the cost and damage structure shown in equations
(l)-(5). The only stable coalitions have two or three countries. (For simplicity,
assume the lower number holds in the case of ties.) The size of the stable coalition is
independent of the number of countries, the social cost of carbon, output, emissions,
and the emissions intensity. If there are ten identical countries, there will be five
coalitions of two countries each. The global average carbon price is twice that of the
NC equilibrium. This result is clear because each country-pair has a joint SCC that
is the sum of the two countries' SCCs. The globally averaged carbon price will be
one-fifth of the efficient level. With countries of different sizes but equal intensities,
countries will group together in stable coalitions of size two, with the countries of
similar sizes grouped together in pairs (i.e., largest with second-largest, and so on).
The key result is that bottom-up coalitions perform only slightly better than the
noncooperative equilibrium.
C. Modeling Results for Bottom-up Coalitions
The coalition theories described above generally use highly stylized structures
and assumptions, so it is useful to examine empirical models of climate-policy


---Economics-2015-0-09.txt---
coalitions with more realistic assumptions. Several empirical studies have examined
the structure of coalitions or international agreements using a variety of alternative
cooperative structures and coalition assumptions. A brief description of key studies
is contained in the online Appendix.
The central results of existing studies reproduce the finding of the small coalition
paradox. Without penalties on nonparticipants, stable coalitions tend to be small and
have emissions reductions that are close to the noncooperative level. In addition,
many studies find that coalitions tend to be unstable, particularly if transfers among
regions are included.
IV. Sanctions on Nonparticipants to Promote an Effective Climate Club
As noted above, the syndrome of free-riding along with the international norm
of voluntary participation appears to doom international climate agreements like the
Kyoto Protocol. The suggestion in this paper is that a club structure - where external
sanctions are imposed on nonmembers - will be necessary to induce effective agreements.
I analyze in depth a specific model of sanctions (tariffs on nonparticipants),
but the model illustrates the more general point that external sanctions are necessary
to promote participation in effective agreements to provide global public goods.
A. Stable Coalitions
While it is easy to design potential international climate agreements, the reality
is that it is difficult to construct ones that are effective and stable. Effective means
abatement approaching the global optimum. The concept of stability used here is
denoted as a coalition Nash equilibrium . Under this definition, a coalition is stable
if no group (sub-coalition) among the countries can improve its welfare by changing
its status. That is, it combines individual rationality (for each player individually),
collective rationality (for all players together), and coalition rationality (for each
subset of the players). This is a natural extension of a Nash equilibrium, which
applies to single countries. The concept is widely used in different fields and was
originally called strong equilibrium in Aumann (1959); also see Bernheim, Peleg,
and Whinston (1987). The term coalition Nash is more intuitive and is used here.
The small coalition paradox motivates the current approach. The goal here is
to find a structure that is stable and effective for a wide variety of country preferences,
technologies, and strategies. The most appealing structure is one that does
not depend on sophisticated and fragile repeated-game strategies and instead has
an efficient equilibrium for every period (in the stage games) in a repeated game.
I therefore focus on one-shot games that have efficient and unique equilibria. If
these are then turned into a repeated game, each of the one-shot games will be a
sub-game-perfect coalition Nash equilibrium, and the repeated game will have an
efficient coalition-Nash equilibrium.
B. Transfers Undermine Coalition Stability
The present study assumes that there is no sharing of the gains from cooperation
among members of the coalition. In some cases, particularly those with asymmetric


---Economics-2015-0-10.txt---
regions, allowing transfers may allow a more efficient treaty (see Barrett 2003,
ch. 13). However, allowing transfers also increases the dimensionality of the strategy
space and may increase the potential for coalition instability.
Before discussing the strategic issues, a practical exception must be made for
poor countries. We can hardly expect low-income countries struggling to provide
clean water or engaged in civil conflict to make the same commitment as rich countries.
So there might be a threshold for participation in terms of per capita income.
But once countries graduate into the middle-income group, they would assume the
obligations of club membership.
What happens if surplus-sharing is included as part of country strategies? If there
are no sharing constraints, then coalition instability is inevitable in what might be
called the stab-in-the-back syndrome. This can be seen in the case of three regions.
Suppose that a cooperative agreement of the three regions has a surplus of 300 units,
and agreements require a majority of countries. A first agreement might divide the
surplus equally among the three regions as proposal A = (100, 100, 100). However,
a coalition of the first two countries could propose another allocation as proposal
B = ( 1 10, 1 10, 80), which would lead the first two countries to defect from proposal
A to B. A little reflection will show that there is no stable coalition if the surplus can
be divided arbitrarily. (For examples of how different sharing and voting rules lead
to instability, see Meyerson 1991, ch. 9.)
One difficulty with the use of differentiated emissions targets in the Kyoto Protocol
was its stab-in-the-back instability. The initial allocation of permits across countries
is a zero-sum distribution. It can generate the same instability as the example of
the negotiation over the division of the surplus. One of the attractive features of a
regime that focuses on carbon prices is that it can operate as a single-dimensional
choice and thereby avoid stab-in-the-back instability. 1 A study of climate regimes
by Weikard, Finus, and Altamirano-Cabrera (2006) confirms the potential for instability
in climate agreements with transfers (see the online Appendix).
C. Introducing Sanctions on Nonparticipants
Both theory and history suggest that some form of sanctions on nonparticipants is
required to induce countries to participate in agreements with high levels of abatement.
It will be useful to define "sanctions" or "penalties" carefully. In their landmark
study of sanctions, Hufbauer, Schott, and Elliot (1990) define sanctions as
governmental withdrawal, or threat of withdrawal, of customary trade or financial
relationships. A key aspect of the sanctions analyzed here is that they benefit senders
and harm receivers. This pattern contrasts with most cases analyzed by Hufbauer,
Schott, and Elliot, whose studies show that sanctions usually impose costs on senders
as well as receivers and thereby raise issues of incentive-compatibility.
The major potential instrument is sanctions on international trade. Whether and
how to use international trade in connection with a climate treaty involves many
issues - economic, environmental, legal, and diplomatic. I will emphasize the


---Economics-2015-0-11.txt---
economic and strategic aspects and leave other aspects to specialists in those areas
(see Bordoff 2009 and Brainard and Sorkin 2009).
Two approaches to trade sanctions might be considered. A first approach, called
carbon duties, would put tariffs on imports of nonparticipants in relation to the carbon
content of imports. A second approach, called uniform penalty tariffs, would
apply uniform percentage tariffs to all imports from nonparticipating countries. I
discuss each of these in turn.
The central question addressed in this analysis is whether a club design which
incorporates penalty tariffs on nonparticipants can produce a stable equilibrium or
coalition that significantly improves on the noncooperative equilibrium.
D. Carbon Duties
A first approach called carbon duties - commonly proposed among scholars who
have advocated penalties - would put tariffs on goods imported from nonparticipants
in relation to the goods' carbon content. (These are also known as countervailing
duties, but I will use the more descriptive term here.) Under this approach, imports
from nonparticipants into a country would be taxed at the border by an amount that
would be equal to the domestic price of carbon (or perhaps by an agreed-upon international
target carbon price) times the carbon content of the import. Alternatively,
under a cap-and-trade regime, the requirement might be that importers purchase
emissions allowances to cover the carbon content of imports.
The technique of carbon duties is commonly used when countries violate their trade
agreements, and is also included in several international environmental agreements
(see Barrett 2003 for an extensive history). The purposes of carbon duties are to reduce
leakage, to level the competitive playing field, and to reduce emissions. Increased
participation - which is emphasized here - is usually not included as a goal of the
sanctions. See Frankel (2009) for a review of proposals and their relation to trade law.
Studies of carbon duties indicate they are complicated to design, have limited coverage,
and do little to induce participation. As an example, consider C02 emissions
from US coal-fired electricity generation, which is a major source of emissions. Since
the United States exports less than 1 percent of its electricity generation, the effect
of carbon duties here would be negligible. Modeling studies confirm the intuition
about the limited effect of the carbon-duties mechanism. For example, McKibbin
and Wilcoxen (2009) study the effects of carbon duties for the United States and the
European Union. They find that the proposal would be complex to implement and
would have little effect on emissions. Estimates of this approach using the C-DICE
model described below also indicate that carbon duties have limited effectiveness in
promoting deep abatement (see the online Appendix for more details).
E. Uniform Tariff Mechanisms
Given the complexity of carbon duties, I propose and analyze an alternative and simpler
approach: a uniform percentage tariff. Under this approach, participating countries
would levy a uniform percentage tariff (perhaps 2 percent) on all imports from nonparticipants.
This mechanism has the advantage of simplicity and transparency, although it
does not relate the tariff specifically to the carbon content of imports.


---Economics-2015-0-12.txt---
While the uniform tariff appears to be less targeted than carbon duties, it has a
different purpose. It is primarily designed to increase participation, not to reduce
leakage or improve competitiveness. The rationale is that nonparticipants are damaging
other countries because of their total emissions of greenhouse gases, not only
from those embodied in traded goods.
One objection to this approach is that a tariff on all imports is a major departure
from the approaches authorized under national and international law. It would appear
to collide with current treaties by imposing tariffs on processes and production methods
(PPMs), or on how goods are domestically produced, but that is of course the purpose
of the penalty tariffs. It also departs from the principle of proportionality in having a
binary "in or out" nature of the sanctions. However, the binary feature is central to having
countries focus on two possible policies, and including proportionate tariffs would
lead to a different set of equilibria. While there may be ambiguities as to whether some
esoteric exceptions can be used to justify the system of uniform, nonproportionate tariffs,
trying to shoe-horn the proposed uniform-tariff mechanism into current law seems
ill advised because it would raise questions of legitimacy and durability.
For these reasons, an important aspect of the proposal will be a set of "climate
amendments" to international-trade law, both internationally and domestically.
The climate amendments would explicitly allow uniform tariffs on nonparticipants
within the confines of a climate treaty; it would also prohibit retaliation against
countries who invoke the mechanism. Requiring such amendments would emphasize
that climate change is an especially grave threat, and that this approach should
not be used for every worthy initiative.
F. Tariffs as Internalization Devices
We can interpret penalty tariffs as devices to internalize transnational externalities.
Nations incur but a small fraction of the damages from climate change
domestically - less than 10 percent of global costs on average. Just as taxes or regulations
are needed to correct externalities within nations, some analogous mechanism
is needed for global public goods.
Tariffs on the trade of nonparticipants are a reasonable and realistic tool for internalizing
the transnational externality. How well-targeted are penalty tariffs? Using
the C-DICE model (which is described in the next section), I have examined the
external effects of emissions of each region along with the impacts of the penalty
tariff, and the results are shown in Figure 1.
Here are the calculations. I began with a $25 per ton C02 global social cost of
carbon. I then calculate each region's external SCC. This equals the global SCC
minus the national SCC. In all cases, the external SCC is close to the global SCC.
For example, when the global SCC is $25 per ton, the estimated US external SCC
is $21 per ton. Multiplying the region's external SCC by the difference between the
cooperative and noncooperative emissions provides the externality, shown as the left
bar in Figure 1. In this example, when the United States decides not to participate, it
increases its annual emissions by about 800 million tons, and this produces $16 billion
of additional external damages.
I then calculate the cost from the penalty tariff that a country incurs by not participating
in the Climate Club. The calculation labeled "Cost of out" shows the cost


---Economics-2015-0-13.txt---
of leaving the club when all other countries are in. For example, the United States
has a welfare loss of $10 billion when it does not participate and the penalty tariff is
2 percent. (All figures are per year at 2011 incomes and prices.) This cost is below
the external damages of $16 billion. Additionally, the figure shows the "Benefit of
in," which is the benefit of forming a club of 1, when the country is the only member
of the club. For the United States, the benefit of in is $23 billion.
For all regions, the sum of the transnational externalities is $124 billion. The
sum of the costs of out (clubs of 14) of all 15 regions is $102 billion, while the sum
of benefits of in (clubs of 1) is $98 billion. Online Appendix Table B-7 shows the
results for all regions.
The calculations provide a surprising result. They indicate that a penalty tariff provides
incentives that are reasonably well targeted to the transnational externalities.
The penalty always has the correct sign, and the size of the penalty is the right order
of magnitude with a 2 percent tariff. However, because of different trade and emissions
patterns, the externality and the trade penalty are imperfectly aligned. Note
that the tariff effect changes with club size, so the internalization effect is variable.
But on the whole, an appropriate tariff appears to be remarkably well-calibrated to
the C02 externality.
G. Tariffs as Sanctions for Global Public Goods
Two simple but critical points concern the role of tariffs as sanctions. To begin
with, they play the role of external penalties. This can be most easily seen in repeated


---Economics-2015-0-14.txt---
prisoners' dilemma (RPD) games. In RPD games with a large number of agents,
it is difficult to design an incentive-compatible internal sanction against defectors
because the punishments punish the punishers. Like a doomsday device, they are
unattractive, particularly when applied against players who make small contributions
to the public good. Part of the problem is that the penalties are internal to the
game and cannot be linked to some larger set of payoffs. The power of tariffs is that
they are external to the RPD game (i.e., they are part of a different game). Because
participation in the trade system has such large benefits to countries, these benefits
can be used to induce participation in the climate game.
A second critical feature of tariff-sanctions is that they are incentive-compatible.
Many sanctions have the disadvantage that they penalize the penalizer. For example,
if Europe puts sanctions on Russian energy companies, this is likely to raise energy
prices in Europe, hurt European consumers, and therefore have costs on Europe as
well as Russia. Similarly, other sanctions such as a "grim strategy" (which dissolves
the agreement completely if one country violates it) or analogous punishments in
«-person RPD games can only support a cooperative equilibrium for a few countries
(as is discussed in the section on the small coalition paradox). By contrast, the
tariff-sanction mechanism analyzed here (i) imposes costs on the nonparticipating
country but (ii) benefits participants that levy the penalty tariffs. Moreover, because
tariffs apply bilaterally, they can support an efficient equilibrium for global public
goods for a large number of countries as long as the optimal-tariff effect operates.
Figure 1 shows numerically how the tariff-sanction imposes costs and conveys benefits
in a manner that aligns sanctions with external effects.
H. Prices or Quantities?
The Climate Club discussed here focuses on carbon prices rather than emissions
reductions as the central organizing principle for an international agreement. While
at an abstract level either approach can be used, a review of both theory and history
suggests that use of prices is a more promising approach.
Quantitative targets in the form of tradable emissions limits have failed in the
case of the Kyoto Protocol, have shown excessive price volatility, lose precious
governmental revenues, and have not lived up to their promise of equalizing prices
in different regions. Moreover, as emphasized by Weitzman (2014), prices serve
as a simpler instrument for international negotiations because they have a single
dimension, whereas emissions reductions have the dimensionality of the number of
regions. To the extent that carbon-price targets lead to carbon taxes, the administrative
aspects of taxes are better understood around the world than marketable emissions
allowances, and they are less prone to corruption. This discussion is clearly
just a sketch, but it provides some of the reasons for preferring price over quantity
targets as part of an international climate regime. (For an extended discussion of the
relative merits of prices and quantities, see Nordhaus 2013.)
I. How to Get Started?
An important question is, how would a top-down Climate Club get started?
Who would define the regime? Would it begin with a grand Bretton-Woods-type


---Economics-2015-0-15.txt---
conference? Or would it evolve from a small number of countries who see the logic,
define a regime, and then invite other countries to join?
There are no clear answers to these questions. International organizations evolve in
unpredictable ways. Sometimes, it takes repeated failures before a successful model
is developed. The histories of the gold and dollar standards, cholera conventions, the
WTO, the European Union, and the Internet all emphasize the unpredictability in the
development of international regimes (for some histories, see Cooper et al. 1989). The
destination of a Climate Club is clear, but there are many roads that will get there.
V. Modeling Coalition Formation: The Coalition-DICE (C-DICE) Model
A. Description of the Model and Sources
Economic analysis can describe the basic structure of a Climate Club. However
detailed empirical modeling is necessary to determine the effectiveness of different
regimes in the context of actual emissions, damages, climate change, and trade
structures. For this purpose, I next describe a climate-economic model that examines
coalition formation: the C-DICE model (Coalition DICE, or Coalition Dynamic
Integrated model of Climate and the Economy). It is a static version of the multiregional
DICE-RICE model (Nordhaus 2010). While the framework is similar to standard
economic integrated assessment models (IAMs), the purpose is different. The
C-DICE model is designed to determine whether or not countries join a coalition
of high-abatement countries, and to find stable coalitions, rather than to look for an
optimal choice of climate policies or map out emissions trajectories.
The current version has 15 regions, including the largest countries and aggregates
of the balance of the countries. The regions are the US, EU, China, India,
Russian Federation, Japan, Canada, South Africa, Brazil, Mideast and North Africa,
Eurasia, Latin America, tropical Africa, middle-income Asia, and the ROW (rest
of the world). The model includes exogenous output, baseline C02 emissions, and
a baseline trade matrix for the 15 regions. Countries produce a single composite
commodity, and C02 emissions are a negative externality of production. Regions
can reduce emissions by undertaking costly abatement.
The marginal damages of emissions (social cost of carbon (SCC)) are assumed
to be constant. This is reasonably accurate for small time periods because emissions
are a flow, damages are a function of the stock, and the flow-stock ratio is small. The
fact that the SCC is little affected by abatement levels is shown in Nordhaus (2014,
Table 1), where there is virtually no difference in the SCC between the optimal and
baseline policies.
The damage estimates are drawn from a recent comparison of the social cost of
carbon (Nordhaus 2014). That study found a central estimate for the global SCC of
$24 per ton C02 in 201 1 US$ for 2020 emissions. However, estimates from other
studies range from $10 to as high as $100 per ton C02 for alternative goals and discount
rates. I therefore use a range of $12.5-$100 per ton C02 for the global SCC.
Estimates of national SCCs have proven difficult to determine because of sparse
evidence outside high-income regions. Online Appendix Table B-l shows the substantial
differences in national SCCs in three integrated assessment models, the RICE,
FUND, and PAGE models. Note that the conceptual basis of the national SCCs used


---Economics-2015-0-16.txt---
here is the calculations made by nations - using their national values, analyses, and
discount rates. Country estimates may differ from those of modelers using uniform
methods and low discount rates. For the central estimates, it is assumed that national
SCCs are proportional to national GDPs. This assumption is primarily for simplicity
and transparency but also because the national estimates are so poorly determined.
However, sensitivity analyses discussed below and in the online Appendix indicate
that alternative estimates lead to identical results on participation.
The abatement costs combine global estimates from the DICE-2013R model
with detailed regional estimates from an engineering model by McKinsey Company
(2009). Abatement costs are largely determined by the carbon-intensity of a region,
which are relatively reliable data. Aside from carbon-intensity, the differences among
regions are largely technological and sectoral as analyzed by McKinsey's study.
One major new feature is to include the effects of international trade and tariffs
on the economic welfare of each region. For both computational and empirical
reasons, the model employs a reduced-form tariff-impact function. This function
represents the impact of changes in the average tariff rate of each country on each
other country. As an example, the model estimates that if the United States imposes
a uniform additional tariff of 1 percent on Chinese imports, US net national income
rises by 0.100 percent and China's net national income falls by 0.018 percent.
Estimates from the optimal-tariff literature indicate that countries have net benefits
if they impose small uniform tariffs on other countries. Similarly, all countries
suffer economic losses if they are the targets of uniform tariffs levied by other countries.
I assume that the tariff function is quadratic with a maximum at the optimal
tariff rates. The numerical parameters of the reduced-form tariff-impact function are
derived from a model developed and provided by Ralph Ossa (2014). Details are
provided in the online Appendix.
Macroeconomic and emissions data are taken from standard sources. GDP and
population are from the World Bank. CO2 emissions are from the Carbon Dioxide
Information Action Center (CDIAC 2014). Note that I include all industrial C02 emissions
but exclude land-use emissions as well as non-C02 greenhouse gas emissions or
other sources of climate change. The interregional trade data are based on data from
the United Nations Conference on Trade and Development (UNCTAD 2014).
The model considers only one period, centered on 2011. It can be interpreted
as a game with a single long period or as a repeated game with a constant payoff
structure. As discussed above, the purpose is to find an efficient solution to the stage
game that will also be an equilibrium of the repeated game.
Countries are assumed to maximize their perceived national self-interests, and the
welfare of the rest of the world is not counted in their interests. Their estimates may
turn out to be right or wrong, but they are the basis of treaty negotiations. To avoid
stab-in-the-back instability, I assume that there are no side payments among countries.
Treaties are assumed to be stable in the sense of being coalition Nash equilibria,
which means that they are stable as compared to all alternative sub-coalitions.
B. Gains and Losses from Participation
The noncooperative (NC) equilibrium is the starting point in international relations.
Consider the decision of a single country whether to participate in a Climate


---Economics-2015-0-17.txt---
Club. Participation requires countries to have a domestic carbon price at least as
high as the minimum international target carbon price. The choice of climate policies
is simple. A nonparticipant will choose the low NC carbon price because that
maximizes national welfare for nonparticipants. Similarly, a participant will choose
the higher international target carbon price to meet its obligations because that maximizes
its economic welfare conditional on participation.
In considering whether or not to participate in the high-abatement cooperative
regime, countries face two sets of costs. The first cost is the additional abatement
cost (net of reduced damages) of participation. The additional abatement costs are
greater than the reduced damages. This fact shows immediately why countries will
not voluntarily depart from the NC equilibrium without some further inducements
to participate.
The second impact of the decision on participation is due to trade impacts. The
present study analyzes a uniform tariff on all goods and services imposed by participants
on the imports from nonparticipants into the Climate Club. Figure 2 shows the
basic structure of the tariff arrangements. As shown in the two cells on the left, the
Club treaty authorizes penalty tariffs on nonparticipants into the Club region, with no
penalty tariffs on intra-Club trade. The two cells on the right indicate that there are
no tariffs, which assumes no reaction or retaliation of non-Club members to the Club.
VI. Algorithmic Issues
Finding the equilibrium coalition, as well as determining stability and uniqueness,
is computationally demanding. Consider a global Climate Club with n regions.
The payoffs are functions of the parameters of the game, including output, emissions,
damages, the trade technology, and the tariff penalty function. In addition, the
payoffs depend up the participation of each of the other players.
In the most general version, discussed above in the section on bottom-up coalitions,
there may be multiple coalitions (i.e., regional groupings). This outcome
is seen in trade associations and military alliances formed on the basis of costs,
location, and ideologies. In the case of multiple coalitions, there will be on the order
of n! possible coalitions. For our study, with 15 regions and multiple regimes, that
would consist of about 1012 coalitions and would be computationally infeasible.
However, in the case of global climate change, it is more natural to consider
a situation where countries decide whether to join a single global climate treaty.
Assuming a single coalition has the computational advantage that it limits the number
of potential coalitions to 2" (or 32,768) coalitions, which can easily be calculated.
The problem is combinatorial in nature, and its solution is thought to be in the
class of NP-hard problems (Wooldridge and Dunne 2004) .There appears to be no
efficient algorithm for calculating stable coalitions (Rahwan 2007). In principle, we
would need to take each of the 2" coalitions and determine whether they are stable
against all the other 2" - 1 coalitions, which requires about 22" « 109 comparisons.
While this is computationally feasible, it is unnecessarily burdensome, particularly
for model construction and comparison of regimes.
I therefore settled on an evolutionary algorithm to find stable coalitions. This is
similar to a genetic algorithm except that it considers mutations of all elements rather
than just local searches. This proceeds in the following steps: (i) Start with an initial


---Economics-2015-0-18.txt---
Notes: The matrix shows the structure of penalties in the Climate Club. For example, the lower left cell indicates  that when exporting countries are nonparticipants and importing countries are participants, the trade of exporters is  penalized. In all other cases, there are no penalties.
coalition and calculate the outcomes and net benefits. Denote these the initial "base
coalition" and "base outcomes." (ii) Randomly generate a "change coalition" of a set of
m regions from the n regions. Assume that each of the m regions changes its participation
from out to in or from in to out. (iii) Construct a new test pattern of participations,
substituting the new participation status of the change coalition for its participation in
the base coalition, (iv) Calculate the test net benefits of the new test participation for
each region, (v) If the test net benefits are Pareto improving for the change coalition,
substitute the test participation pattern and other outcomes for the prior base outcomes
to get new base participation and outcomes. Note that while the results of the test coalition
will be Pareto improving for the change coalition in the new outcomes, it may
not improve the welfare for the balance of regions, (vi) Go back and restart from (ii) to
generate a new random change coalition and then go through steps (iii)-(v). (vii) The
procedure stops either when (a) the process cycles (a coalition structure repeats), or
(b) no other coalition is able to overturn the existing base coalition.
Note that the termination in (viib) cannot be determined with certainty because
of the probabilistic nature of the algorithm. However, because the change coalition
is randomly selected, in the worst case the likelihood of there being an overturning
coalition that has not been found is no more than (1 - 2~n)m after m iterations.
Experiments indicate that stable coalitions are usually found within 100 iterations.
I examined up to 50,000 iterations and random starting coalitions to test stability.
While this algorithm might potentially be improved with bounding refinements, the
flexibility of the evolutionary algorithm for finding stable coalitions suggests it is
adequate. Further details are provided in the online Appendix.
VII. Results
A. A First Example
Before diving into the results, it will be useful to present a numerical example.
Assume that the international target carbon price is $25 per ton; that the penalty
tariff rate is 4 percent; that all high-income countries participate; and that the United
States is considering whether to participate. The numbers are shown in Table 1 .


---Economics-2015-0-19.txt---
All figures in this study apply to annual output and prices for 201 1 in 201 1 US$.
The figures are often provided with two or three significant digits, but this is for
presentational purposes and should be interpreted in the context of the uncertainties
inherent in modeling as well as the results of the sensitivity analyses discussed below.
First consider a Kyoto-type regime, with no sanctions when countries do not
participate, which is the first line in Table 1 . If the United States does not participate,
it expends $0.3 billion per year for abatement and has reduced damages (from
all countries' abatement relative to zero abatement) of $7.3 billion per year. Net
climate-related benefits are $7.0 billion per year. In the no-sanctions regime, if the
US participates and sets a domestic carbon price of $25 per ton, it expends $1 1 .9 billion
annually in abatement and has reduced damages of $10.7 billion per year, for
net climate-related benefits of -$1.2 billion annually. So without sanctions, the best
national strategy is not to participate, with an annual net advantage of $8.2 billion.
However, with a 4 percent penalty tariff on nonparticipants, the numbers change
dramatically. Here, the US has trade impacts of -$15.6 billion per year if it does
not participate. This comes primarily from the terms of trade losses induced by
tariffs on the US imposed by participants. If the US does participate, it has positive
trade impacts of $36.7 billion per year because it levies tariffs on the remaining
nonparticipants.
Taking the sum of climate-related gains and trade benefits with the 4 percent
penalty tariff, the US would have a positive impact of $35.5 billion per year as a
participant. By contrast, the US would have an annual impact of -$8.6 billion as a
nonparticipant. The US would have an incentive of a net gain $44.1 billion per year
to join the agreement taking account only of its own national economic benefit. In
this example, it is not even a close call on whether to participate.
The point of this simple example is to show that nations acting in their self-interest
would join a high-income club with a 4 percent tariff but would not join such a club
with a zero penalty tariff.
B. Basics of the Simulations
The central analysis undertaken here examines 44 different regimes for the Climate
Club. A regime in the following is defined as a combination of target carbon price
and tariff rate. The regimes analyzed here involve four different international target
carbon prices and 1 1 different tariff rates. The carbon prices are $12.5, $25, $50, and
$100 per ton of C02. While other values have been used in the literature, this spans


---Economics-2015-0-20.txt---
the range of common targets, as discussed above. The tariff rates range from 0 percent
(no penalty) to 10 percent in steps of 1 percent. The upper end is chosen as one that
would begin to place a serious burden on both the trade and the enforcement systems.
For each of the calculations, I started with a base set of participants and then used
the evolutionary algorithm to find a stable coalition (if one exists), with multiple
restarts and two different platforms to test stability. The results were sensible in all
cases and will be discussed below. This paper presents the results primarily in graphical
form. The numbers underlying the figures are contained in the online Appendix.
C. Results for Stability and Participation
The first remarkable result is that virtually every regime produces a stable coalition.
(There are 6 unstable regimes out of 44. Results for these are averages of
quasi-stable coalitions as explained below.) From a theoretical point of view, there is
no obvious reason why the nonlinearities of participation would not lead to multiple
quasi-stable coalitions. The intuition is that the trade sanctions are powerful enough
to push countries into nonparticipation or participation.
The second question is whether the penalty structure is sufficient to induce participation.
In other words, how many of the 15 regions participate in the Climate
Club? Figure 3 shows the number of participating regions for different tariff rates
and different target carbon prices. The bars are arrayed from left to right by increasing
tariff rates.
The results are straightforward. No country joins the Climate Club without trade
sanctions (i.e., at a zero tariff rate). This key result confirms theory and observation.
For low target carbon prices, all or most countries join even for very low tariff rates.
For target carbon prices of $50 and $100 per ton, high penalty tariffs are required
to induce participation. With a $100 per ton target, full participation is not attained
even with the highest tested tariff rates. The participation rate rises monotonically
with the penalty tariff rate.
D. Results for Actual Carbon Prices and Abatement
The next question is the success of different arrangements in inducing abatement.
Figure 4 shows the level of the globally averaged carbon price for different regimes.
The results here are similar to those for participation but in effect weight the results
by region size.
For target carbon prices of $12.5 and $25, the treaty attains the goal of having the
global carbon price equal the target price (which is equal to the global SCC) even
at low tariff rates. For a $50 target carbon price, the target carbon price is almost
reached with a 5 percent tariff.
For a carbon price target of $100, the regime achieves no gain over a regime with
a target price of $50 until the highest tariff rate. Indeed, at medium tariff rates, we
see a Laffer-curve result as the actual global carbon price is lower with the $100
target than with the $50 target. The reason is that abatement is so costly in the $100
regime that most countries choose to accept the trade penalties. This then leads to
a low participation rate and a low actual penalty on nonparticipants because so few
countries are in the Club.


---Economics-2015-0-21.txt---



---Economics-2015-0-22.txt---
of income and emissions. To attain higher reduction rates (such as zero emissions)
would require improvements in technology so that it becomes economical to attain
these higher reduction rates at lower costs.
E. Economic Gains from the Climate Club
What are the economic gains from the Climate Club? The Club is designed to
increase economic welfare by overcoming free-riding. Figure 5 shows the net economic
gains for different regimes, while Figure 6 shows the regime efficiency as
measured by the percentage of the cooperative gains that are realized.
First examining Figure 5, it is clear that the gains to cooperation are substantial.
Taking as an example the case of $50 per ton of C02, the income gain from
noncooperative actions is $63 billion per year. The most successful cooperation
regimes have gains of $312 billion per year. (Again, all are scaled to 2011 output
and prices.)
Figure 6 shows the extent to which different regimes succeed in achieving the
potential gains from cooperation. At benchmark levels of $12.5 and $25 per ton,
the regime captures all of the potential gains for tariff rates of 3 percent or more.
Similarly, at the $50 per ton rate, the Club achieves virtually all the potential gains
with tariff rates of 5 percent or more. However, for the highest target carbon price,
the regime gets very little of the potential gains except at the highest tariff rates.
F. Trade Inefficiencies
How large are the trade costs relative to the climate gains? Note to begin with
that there are no trade losses with full participation because there are no sanctions.
However, with partial participation, there will be efficiency losses because of the tariffs.
Consider a regime with a low tariff and a low target carbon price, for example,
a 1 percent tariff and a $25 per ton target carbon price. Here, there are six nonparticipants.
The gains from the regime are $34 billion while the trade inefficiencies
are $0.4 billion. At the other extreme, consider a $50 target price with a tariff of
6 percent. For this regime, there are only two nonparticipants. The gains from the
club are $228 billion, while the trade inefficiencies are $0.7 billion. In all cases, the
gains from cooperation far outweigh the trade losses.
G. To Join or Not to Join?
An interesting question is to determine which countries join and which stay out
of the Climate Club. On first principles, the joiners are those with low abatement
costs, low carbon-intensity, high damages, and high trade shares. Table 2 shows the
percentage of the cases where a specific region participates.
A related question is, who gains and who loses from the Climate Club? The
answer depends upon the regime that is chosen. Figure 7 shows the net gains and
losses for four different sets of parameters for seven major regions. All major
regions gain from the club relative to the noncooperative outcome. In the entire set
of 40 regimes and 15 regions, there are 69 (12 percent) cases where countries lose
relative to the noncooperative regime.


---Economics-2015-0-23.txt---


---Economics-2015-0-24.txt---



---Economics-2015-0-25.txt---
H. The Kyoto Protocol as a Failed Regime
One test of the approach used here is to examine the stability of the Kyoto Protocol.
This agreement included at the outset a substantial fraction of global emissions and
would, if it had broadened and deepened, have made a substantial contribution to
slowing the growth of emissions. However, it failed to gain new adherents, and some
of the members with binding commitments, particularly the US, dropped out.
Conceptually, the Kyoto Protocol was a climate club with no sanctions. To test
its coalition stability, I formed an initial club with only the original Annex I Kyoto
Protocol countries having binding emissions commitments with no penalties for
noncompliance (0 percent tariff). Starting with the original Kyoto coalition status, I
tested for coalition stability as described above.
All of the simulations collapsed to the noncooperative equilibrium. (See
Figure B-5 in the online Appendix for the simulations.) This might not be surprising
in light of the analysis above. However, recall that the analytical models
assume much more environmental and economic homogeneity than is seen in reality.
Perhaps some combination of damages, abatement costs, and carbon intensities
might lead to limited cooperation. However, for the modeling structure used here,
the Kyoto Protocol could not survive.
So the conclusion from this simple test is that the Kyoto Protocol was doomed
from the start. It did not contain sufficient economic glue to hold a cooperative coalition
together.
I. Regional Choice among Regimes
The present analysis focuses on the design of a Climate Club and the extent to
which different club designs succeed in inducing efficient participation and abatement.
In reality, treaties do not spring full-grown but emerge from a complicated diplomatic
process. The key steps are negotiation, ratification, implementation, and renegotiation.
The present study focuses on negotiation and assumes that once treaties are negotiated,
they are ratified and implemented (so there are no "cheap talk" negotiations).
Negotiations take place in two parts. The first stage is treaty design, while the second
is the decision whether to participate. For the Kyoto Protocol, the United States was
deeply involved in treaty design but did not ratify the treaty. The last section explains
the US nonparticipation and the eventual collapse of the Kyoto Protocol as the failure
to design a treaty that would lead to widespread participation and renewal.
Turn next to the issue of treaty design. Suppose that climate negotiations consider
the different Climate Club regimes analyzed above. Which of the possible regimes
would be chosen? Consider these questions for a single case where the global SCC
is $25 per ton C02 and where the penalty tariff rate is 5 percent. Individual countries
have their own SCCs (say that the US SCC is $4 and India's is $2), as well as their
national abatement cost functions. If countries are just scaled-up or scaled-down
replicas, all would prefer a $25 per ton target carbon price. In reality, countries
differ, so their preferred target prices will differ. Countries with high damages will
prefer a high target carbon price because they will benefit from higher global abatement;
countries with high abatement costs will prefer a low target price because that
will reduce their abatement costs. The analog for health clubs is that people who


---Economics-2015-0-26.txt---
desire minimal facilities want low dues, while those who prefer extensive coverage
choose more elaborate facilities with higher dues.
Let's take an experiment where we ask each country for its preferred international
target carbon price (always keeping the global SCC at $25 and its regional distribution
unchanged). Using the C-DICE model, calculate the equilibrium coalition for
each treaty price between $0 and $200 per ton. Then, examining the welfare effects
for each region, ask which treaty carbon price is optimal for the region. For the
example chosen, the impacts on net regional incomes are roughly quadratic for low
prices because all regions participate. Above $34 per ton, countries begin to drop
out, so the calculations become clouded by the effects of participation.
Figure 8 and Table 3 show the distribution of preferred target carbon prices for
regions where the global SCC is $25. The curves show on the vertical axis the fraction
of regions that would prefer an international target carbon price at or below the target
price on the horizontal axis. The noncooperative regime is shown at the upper left
with the circle marked "NC." The curve to the left marked "preferred" shows the distribution
of regional preferred rates (the distribution of first choices). The line marked
"breakeven" shows the distribution of prices at which the country would be indifferent
between the target price and a zero price. The breakeven is close to twice the preferred.
The median preferred international target carbon price using GDP weights is $28
per ton, which is slightly above the global SCC. The median breakeven carbon price is
$48 per ton. An important finding is that all regions prefer a weak regime to the noncooperative
regime. Even the least enthusiastic region (South Africa) would prefer a
target price of $18 per ton to the $3 per ton NC equilibrium. Where the negotiations
would actually settle is an important question beyond the scope of the present study.
J. Unstable Regimes
Of the 44 regimes, 6 displayed coalitional instability, and these can be easily understood.
For example, 3 came with a $50 international target carbon price and low tariff
rates. For example, with a tariff rate of 2 percent, the solution cycled around among a
small number of quasi-stable coalitions with an average of 2.9 participants. The other
instabilities came with the $100 per ton target price and high tariff rates. For example,
with $100 per ton target price and a penalty tariff of 9 percent, the coalitions cycled
with an average number of participations of 3.9 regions.
The instabilities arise because the gains from participation are close to equal in
these different midsized coalitions. Hence, the solution cycles among quasi-stable
coalitions as each outbids the others. None of the regimes degenerates to the
noncooperative equilibrium. Rather, they cycle among similar numbers of participants
and levels of abatement.
Another potential source of instability would arise if the damage function has a catastrophic
threshold (which has not been modeled in the C-DICE model framework).
In the limit, assume that if emissions pass some quantity (below the emissions in
the NC equilibrium), then damages for each region are unlimited. There will be
multiple combinations of abatement by different regions that can stay under the catastrophic
threshold. It might be stable to a single country leaving, but would not be
stable to multiple countries entering and leaving. This example suggests that highly
nonlinear damages open up a different set of issues for regime design.


---Economics-2015-0-27.txt---
K. Sensitivity Analysis
How sensitive are the results to alternative parameters? The sensitivity analyses are
presented in detail in the online Appendix, and the results are summarized here. I examine
the impact of three different sets of parameters. The first is alternative estimates of
the regional distribution of the global SCC. There are virtually no impacts of this sensitivity
test on the participation rate or on the actual global carbon price for any of the
regimes. The second sensitivity test is for the parameter of the abatement-cost function,
which is varied by a factor of 3. The results showed considerable sensitivity, especially
for global SCC of $50 and $100. The optimal tariff was varied over a range of a factor
of 6. This had virtually no impact on the outcomes. The main variable that affects the
outcome is the global social cost of carbon, as shown in the figures and tables.
Those familiar with the literature on climate-change economics will wonder what
happened to the discount rate, which is critical in virtually all areas. The answer is
that the discount rate will primarily affect the global and national SCCs, but has
little effect on the outcomes conditional on the SCCs. For example, a lower discount
rate will raise the estimated global SCC, perhaps from $12.5 to $25. This will
lead to a higher target carbon price, higher emissions reductions, and lower annual


---Economics-2015-0-28.txt---
damages. There will be second-order effects through cost-of-capital factors, GDP,
and other economic variables. But a changed discount rate will affect the outcome
primarily through changes in the SCC.
VIII. Conclusion
The present study analyzes the syndrome of free-riding in climate agreements such
as the Kyoto Protocol and considers potential structures for overcoming free-riding.
This concluding section summarizes the basic approach and conclusions.
A. The Climate Club
The structure of climate change as a global public good makes it particularly
susceptible to free-riding. The costs of abatement are national, while the benefits are


---Economics-2015-0-29.txt---
global and independent of where emissions take place. An additional complication
is that the abatement costs are paid today while most of the benefits of abatement
come in the distant future. The present study shows, in a stylized model of costs and
damages, that the global noncooperative carbon price and abatement rate are proportional
to the Herfindahl index of country size. This implies, given realistic data,
that the global noncooperative carbon price and control rate will be in the order of
one-tenth of the efficient cooperative levels.
Next consider possible mechanisms to combat free-riding and focus on a Climate
Club. It is generally assumed that the most effective approach will be to impose trade
sanctions on nonparticipants, and this is the route followed here. Most trade sanctions
rely on duties on carbon-intensive goods. For strategic, economic, and technical reasons,
this paper instead considers penalties that take the form of uniform ad valorem
tariffs levied by club participants on nonparticipants. In the analysis, the tariff rates
vary from 0 percent to 10 percent. It is further assumed that a climate treaty will
amend trade rules so that a penalty tariff conforms with international trade law and
retaliation by nonparticipants is prohibited.
This study assumes that countries adopt an international carbon-price target rather
than a quantity target as the policy instrument. The assumed target price ranges from
$12.5-$ 100 per ton C02. In the experiments, the international target carbon price is
always set equal to the global social cost of carbon.
Individual countries are assumed to adopt climate policies that maximize their
national economic welfare. Welfare equals standard income less damages less
abatement costs less the costs of trade sanctions. I assume a one-shot static game,
but this can be interpreted as the stage game of a repeated game. The equilibrium,
described as a coalition Nash equilibrium, is a coalition of countries that is stable
against any combination of joiners and defectors. The equilibrium is calculated by
an evolutionary algorithm that tests each coalition against a random collection of
countries that can defect and join.
The study introduces a new approach called the C-DICE model (Coalition
DICE, or Coalition Dynamic Integrated Model of Climate and the Economy). It is
a 15-region model with abatement, damages, international trade, and the economic
impacts of tariffs. Using an evolutionary algorithm, the model can be used to find
stable coalition Nash equilibria.
B. Qualifications
I begin with qualifications on the results that relate to the data and structural
parameters. The data on output, C02 emissions, and trade are relatively well measured.
The global SCC is uncertain but can be varied as shown in the different
experiments. The national SCCs are also uncertain, but since they are all small
relative to the global SCC, their exact magnitudes are not critical for the findings.
Other structural uncertainties relate to the abatement cost function and the optimal
tariff rate.
A related question is whether a trade-penalty-plus-carbon-price regime can operate
in the future with the rising carbon prices that are generally associated with an
efficient climate-change program. Answering this question requires a multiperiod
coalition model and is on the agenda for future research.


---Economics-2015-0-30.txt---
These results are presented in the spirit of an extended example used to clarify the
free-riding in international agreements rather than as a specific proposal for a climate
treaty. A Climate Club of the kind analyzed here raises central issues about the
purpose of the global trading system, about the goals for slowing climate change,
about the justice of a system that puts all countries on the same footing, and about
how countries would actually negotiate such a regime. The dangers to the world
trading system of such a proposal are so important that they must be reiterated.
Today's open trading system is the result of decades of negotiations to combat protectionism.
It has undoubtedly produced large gains to living standards around the
world. A regime that ties a climate-change agreement to the trading system should
be embraced only if the benefits to slowing climate change are clear and the dangers
to the trading system are worth the benefits.
C. Results
One major result is to confirm that a regime without trade sanctions will dissipate
to the low-abatement, noncooperative (NC) equilibrium. This is true starting from
a random selection of participating countries. More interestingly, starting from the
Kyoto coalition (Annex I countries as defined by the Kyoto treaty) with no sanctions,
the coalition always degenerates to the NC structure with minimal abatement.
A surprising result is that the Climate Club structure generates stable coalitions
for virtually all sets of parameters. A few regimes produce quasi-stable coalitions
with similar numbers of participants.
A next set of results concerns the impact of different Climate Club parameters
on the participation structure. The participation rate and the average global carbon
price rise with the tariff rate. For the lowest target carbon prices ($12.5 and $25 per
ton), full participation and efficiency are achieved with relatively low tariffs (2 percent
or more). However, as the target carbon price rises, it becomes increasingly
difficult to attain the cooperative equilibrium. For a $50 per ton target carbon price,
the Club can attain 90+ percent efficiency with a tariff rate of 5 percent or more.
However, for a target carbon price of $100 per ton, it is difficult to induce more than
the noncooperative level of abatement.
Why is it so difficult to attain efficient abatement with high social costs of
carbon even with high penalty tariffs? The reason is that the gap between the
cooperative and the noncooperative equilibrium rises sharply as the global SCC
increases. Take the case of a large country like China or the United States. For
these countries the national SCC might be 10 percent of the global SCC. For a
global SCC and target price of $25 per ton, participation would require increasing
the domestic carbon price from $2.5 to $25, while a global SCC of $100
would require increasing from $10 to $100. Because abatement costs are sharply
increasing in the target carbon price, this implies that the costs of cooperation
become much larger as the target carbon price rises. On the other hand, the costs
of trade penalties associated with nonparticipation are independent of the global
SCC. So the national cost-benefit trade-off tilts toward nonparticipation as the
international target carbon price rises.
Next examine the patterns of gains and losses. Here, measure the impact relative
to the noncooperative equilibrium. Note as well that these results assume no


---Economics-2015-0-31.txt---
transfers among countries. The benefits are widely distributed among countries. The
only regions showing losses across several regimes are Eurasia and South Africa;
however, the losses are small relative to gains for other regions. There are no regimes
with aggregate losses.
Look at the distribution of gains and losses to determine whether a Climate Club
would be attractive to most countries relative to existing arrangements. All regions
would prefer a regime with penalties and modest carbon prices to a regime with
no penalties. Paradoxically, this is the case even for countries that do not participate.
The reason is that the gains from strong mitigation measures of participants
outweigh the losses from the tariffs for nonparticipants as long as the tariff rate is
not too high. This powerful result indicates that a regime with sanctions should be
attractive to most regions.
D. Bottom Line
Here is the bottom line: the present study finds that without sanctions there is
no stable climate coalition other than the noncooperative, low-abatement coalition.
This conclusion is soundly based on public-goods theory, on C-DICE model simulations,
on the history of international agreements, and on the experience of the
Kyoto Protocol.
The analysis shows how an international climate treaty that combines target carbon
pricing and trade sanctions can induce substantial abatement. The modeling
results indicate that modest trade penalties on nonparticipants can induce a coalition
that approaches the optimal level of abatement as long as the target carbon price is up
to $50 per ton at current income and emission levels. The attractiveness of a Climate
Club must be judged relative to the current approaches, where international climate
treaties are essentially voluntary and have little prospect of slowing climate change.


---Economics-2016-0-02.txt---
In recent years there has been growing interest in the mixture of psychology
and economics that has come to be known as "behavioral economics." As is true
with many seemingly overnight success stories, this one has been brewing for quite
a while. My first paper on the subject was published in 1980, hot on the heels of
Kahneman and Tversky's (1979) blockbuster on prospect theory, and there were
earlier forerunners, most notably Simon (1955, 1957) and Katona (1951, 1953).
The rise of behavioral economics is sometimes characterized as a kind of
paradigm-shifting revolution within economics, but I think that is a misreading of
the history of economic thought. It would be more accurate to say that the methodology
of behavioral economics returns economic thinking to the way it began, with
Adam Smith, and continued through the time of Irving Fisher and John Maynard
Keynes in the 1930s.
In spite of this early tradition within the field, the behavioral approach to economics
met with considerable resistance within the profession until relatively
recently. In this essay I begin by documenting some of the historical precedents
for utilizing a psychologically realistic depiction of the representative agent. I then
turn to a discussion of the many arguments that have been put forward in favor of
retaining the idealized model of Homo economicus even in the face of apparently
contradictory evidence. I argue that such arguments have been refuted, both theoretically
and empirically, including in the realm where we might expect rationality
to abound: the financial markets. As such, it is time to move on to a more constructive
approach.
On the theory side, the basic problem is that we are relying on one theory to
accomplish two rather different goals, namely to characterize optimal behavior
and to predict actual behavior. We should not abandon the first type of theories as
they are essential building blocks for any kind of economic analysis, but we must
augment them with additional descriptive theories that are derived from data rather
than axioms.
As for empirical work, the behavioral approach offers the opportunity to develop
better models of economic behavior by incorporating insights from other social science
disciplines. To illustrate this more constructive approach, I focus on one strong


---Economics-2016-0-03.txt---
prediction made by the traditional model, namely that there is a set of factors that
will have no effect on economic behavior. I refer to these as supposedly irrelevant
factors or SIFs. Contrary to the predictions of traditional theory, SIFs matter; in
fact, in some situations the single most important determinant of behavior is a SIF.
Finally, I turn to the future. Spoiler alert: I predict that behavioral economics will
eventually disappear.
I. The Historical Roots of Behavioral Economics
As Simon (1987, p. 612) noted, the term "behavioral economics" is a bit odd. "The
phrase 'behavioral economics' appears to be a pleonasm. What 'non-behavioral'
economics can we contrast with it? The answer to this question is found in the
specific assumptions about human behavior that are made in neoclassical economic
theory." These assumptions are familiar to all students of economic theory.
(i) Agents have well-defined preferences and unbiased beliefs and expectations.
(ii) They make optimal choices based on these beliefs and preferences. This in turn
implies that agents have infinite cognitive abilities (or, put another way, are as smart
as the smartest economist) and infinite willpower since they choose what is best,
not what is momentarily tempting, (iii) Although they may act altruistically, especially
toward close friends and family, their primary motivation is self-interest. It is
these assumptions that define Homo economicus, or as I like to call them, Econs.
Behavioral economics simply replaces Econs with Homo sapiens, otherwise known
as Humans.
To many economists these assumptions, along with the concept of "equilibrium,"
effectively define their discipline; that is, they study Econs in an abstract economy
rather than Humans in the real one. But such was not always the case. Indeed, Ashraf,
Camerer, and Loewenstein (2005) convincingly document that Adam Smith, often
considered the founder of economics as a discipline, was a bona fide behavioral
economist. Consider just three of the most important concepts of behavioral economics:
overconfidence, loss aversion, and self-control. On overconfidence Smith
(1776, p. 1) commented on "the over-weening conceit which the greater part of men
have of their own abilities" that leads them to overestimate their chance of success.
On the concept of loss aversion Smith (1759, p. 176-177) noted that "Pain ... is,
in almost all cases, a more pungent sensation than the opposite and correspondent
pleasure." As for self-control, and what we now call "present bias," Smith (1759, p.
273) had this to say: "The pleasure which we are to enjoy ten years hence, interests
us so little in comparison with that which we may enjoy today." George Stigler was
fond of saying that there was nothing new in economics, it had all been said by
Adam Smith. It turns out that was true for behavioral economics as well.
But Adam Smith was far from the only early economist who had good intuitions
about human behavior. Many who followed Smith, shared his views about time discounting.
For example, Pigou (1920, p. 21) famously wrote that "Our telescopic faculty
is defective and ... we therefore see future pleasures, as it were, on a diminished
scale." Similarly Fisher (1930, p. 82), who offered the first truly modern economic
theory of intertemporal choice, did not think it was a good description of behavior.
He offered many colorful stories to support this skepticism: "This is illustrated by the
story of the farmer who would never mend his leaky roof. When it rained, he could not


---Economics-2016-0-04.txt---
stop the leak, and when it did not rain, there was no leak to be stopped!" Keynes (1936,
p. 154) anticipated much of what is now called behavioral finance in the General
Theory. For example, he observed that "Day-to-day fluctuations in the profits of existing
investments, which are obviously of an ephemeral and non-significant character,
tend to have an altogether excessive, and even absurd, influence on the market."
Many economists even thought that psychology (then still in its infancy) should
play an important role in economics. Pareto (1906, p. 21) wrote that "The foundation
of political economy, and, in general of every social science, is evidently psychology.
A day may come when we shall be able to decide the laws of social science
from the principles of psychology." John Maurice Clark (1918, p. 4), the son of John
Bates Clark, went further. "The economist may attempt to ignore psychology, but it
is sheer impossibility for him to ignore human nature ... If the economist borrows
his conception of man from the psychologist, his constructive work may have some
chance of remaining purely economic in character. But if he does not, he will not
thereby avoid psychology. Rather, he will force himself to make his own, and it will
be bad psychology."
It has been nearly 100 years since Clark wrote those words but they still ring
true, and behavioral economists have been taking Clark's advice, which is to borrow
some good psychology rather than invent bad psychology. Why did this common
sense suggestion fail to gain much traction for so long?
II. Explainawaytions
In the process of making economics more mathematically rigorous after World
War II, the economics profession appears to have lost its good intuition about human
behavior. Defective telescopic facilities were replaced with time-consistent exponential
discounting. Over-weening conceits were replaced by rational expectations.
And ephemeral shifts in animal spirits were replaced by the efficient market hypothesis.
Economics textbooks no longer had any Humans. How did this happen?
I believe that the most plausible explanation is that models of rational behavior
became standard because they were the easiest to solve. This conjecture is not meant
as a put-down. One begins learning physics by studying the behavior of objects in
a vacuum; atmosphere can be added later. But physicists never denied the existence
or importance of air; instead they worked harder and built more complicated models.
For many years, economists reacted to questions about the realism of the basic
model by doing the equivalent of either denying the existence of air, or by claiming
that it just didn't matter all that much. Matthew Rabin has dubbed these defensive
reactions as "explainawaytions."1
Let's be blunt. The model of human behavior based on the premise that people
optimize is and has always been highly implausible. For one thing, the model does
not take into consideration the degree of difficulty of the problem that agents are
assumed to be "solving." Consider two games: tic-tac-toe and chess. A reasonably
bright first grader can learn to play the optimal strategy in tic-tac-toe, and so a
model that assumes players choose optimally in this game will be a pretty good


---Economics-2016-0-05.txt---
approximation of actual behavior for bright children and sober adults. Chess, on the
other hand, is quite a different matter. Most of us play chess terribly and would have
no chance of beating a free program on our smartphones, much less a grandmaster.
So, it makes no sense to assume that the representative agent plays chess as well as
tic-tac-toe. But that is essentially what we assume in economics.
When we assume that agents maximize utility (or profits) we do not condition
that assumption on task difficulty. We assume that people are equally good at deciding
how many eggs to buy for breakfast and solving for the right amount to save for
retirement. That assumption is, on the face of it, preposterous. So why has it stuck?
There has been a litany of explainawaytions.
A. Aí If
Grumblings within the profession about the so-called "marginalist revolution"
were present in the 1940s, and this journal published several articles debating the
realism of the theory that firms set output and hire workers by calculating the point at
which marginal cost equals marginal revenue. One of the participants in this debate
was Richard Lester of Princeton who had the temerity to ask the owners of business
firms how they actually made such decisions. Whatever firms were doing did not
seem to be captured by the term "equating at the margin," and Lester (1946, p. 81)
ended his paper this way: "This paper raises grave doubts as to the validity of conventional
marginal theory and the assumptions on which it rests." Machlup (1946)
took up the defense of the traditional theory and argued that even if firm owners did
not know how to calculate marginal costs and revenues, they would make decisions
that would closely approximate such choices using their intuitions.
Machlup's defense was refined and polished by Friedman (1953, p. 21) in his
famous essay "The Methodology of Positive Economics." Friedman brushed aside
questions about the realism of assumptions and argued that instead theories should
be judged based on their ability to predict behavior. He proposed what is now a
well-known analogy about an expert billiard player: "excellent predictions would
be yielded by the hypothesis that the billiard player made his shots as if he knew
the complicated mathematical formulas that would give the optimum directions of
travel, could estimate by eye the angles, etc., describing the location of the balls,
could make lightening calculations from the formulas, and could then make the balls
travel in the direction indicated by the formulas. Our confidence in this hypothesis is
not based on the belief that billiard players, even expert ones, can or do go through
the process described; it derives rather from the belief that, unless in some way or
other they were capable of reaching essentially the same result, they would not in
fact be expert billiard players."
Friedman had a well-deserved reputation as a brilliant communicator and
debater, and those skills are on full display in this passage. Using the mere
two-word phrase "as if," Friedman essentially ended the debate about the realism
of assumptions in economics. But given proper scrutiny, we can see that this passage
is simply a verbal sleight of hand. First of all, it is no accident that Friedman
chooses to discuss an expert billiard player. The behavior of an expert in many
activities may indeed be well captured by a model that assumes optimal behavior.
But what about non-experts? Isn't economic theory supposed to be a theory about


---Economics-2016-0-06.txt---
the behavior of all economic agents, not just experts? The life-cycle hypothesis
is intended to be a theory of how the typical citizen saves for retirement, not just
those with MBAs.
There is another problem with Friedman's defense, which is that even experts
are unable to optimize when the problems are difficult. To illustrate, let's return to
the game of chess. Since chess has no stochastic elements, it has long been known
that if both players optimize then one of the players (either the one who goes first
or second) must have a winning strategy, or neither of them do and the game will
lead to a draw. However, unlike checkers, which has been "solved" (if both players
optimize the game is a draw) chess matches do not yield predictable outcomes even
in matches between grandmasters. Sometimes white (first player) wins, less often
black wins, and there are many draws. This proves that even the best chess players
in the world do not maximize. Of course one can argue that chess is a hard game,
which is true. But, many economic decisions are difficult as well.
A second line of defense is to concede that we don't all do everything like experts
but argue that, if our errors are randomly distributed with mean zero, then they will
wash out in the aggregate, leaving the predictions of the model unbiased on average.
This was often the reaction to Simon's (1955) suggestion that people "satisfice"
(meaning grope for a satisfactory solution rather than solve for an optimal one). If
the choices of a satisficer are not systematically different from an optimizer, then
the models lead to identical average predictions (though satisficers will have more
noise). This line of argument was refuted by the seminal work of Daniel Kahneman
and Amos Tversky in the 1970s.
In a brilliant series of experiments on what psychologists refer to as "judgment"
and what economists might call "expectations" or "beliefs," Tversky and Kahneman
(1974) showed that humans make judgments that are systematically biased.
Furthermore, these errors were predictable based on a theory of human cognition.
Kahneman and Tversky's hypothesis was that people often make judgments using
some kind of rule of thumb or heuristic. An example is the "availability heuristic"
in which people estimate the frequency of some event by the ease with which they
can recall instances of that event. Using this heuristic is perfectly sensible since
frequency and ease of recall are generally positively correlated. However, use of
the heuristic will lead to predictable errors in those situations where frequency and
ease of recall diverge. For example, when asked to estimate the ratio of gun deaths
by homicide to gun deaths by suicide in the United States, most people think homicide
gun deaths are more common, whereas there are in fact nearly twice as many
gun-inflicted suicides as homicides. These are expectations that are not close to
being "as if' rational - they are predictably biased.
Kahneman and Tversky's second influential line of research was on decision
making. In particular, in 1979 they published their paper on prospect theory, which
was proposed as a "descriptive" (or what Milton Friedman would have called "positive"
) model of decision making under uncertainty. Prospect theory was intended
to be a descriptive alternative to von Neumann and Morgenstern's (1947) expected
utility theory, which is rightly considered by most economists to characterize how
a rational agent should make risky choices. Kahneman and Tversky's research documented
numerous choices that violate any sensible definition of rational. This pair
of problems posed to different groups of subjects offers a good illustration.


---Economics-2016-0-07.txt---
Problem 1. - Imagine that you face the following pair of concurrent decisions.
First examine both decisions, and then indicate the options you prefer.
Decision (i) Choose between:
A. A sure gain of $240 [84%]
B . 25 % chance to gain $ 1 ,000 and
75% chance to gain or lose nothing [16%]
Decision (ii) Choose between:
C. A sure loss of $750 [13%]
D. A 75% chance to lose $1,000 and a
25% chance to lose nothing [87%]
The numbers in brackets indicate the percentage of subjects that chose that option.
We observe a pattern that was frequently displayed: subjects were risk averse in the
domain of gains but risk seeking in the domain of losses. It is not immediately obvious
that there is anything particularly disturbing about these choices; that is, until
one studies the following problem.
Problem 2. - Choose between:
E. 25% chance to win $240
and 75% chance to lose $760 [0%]
F. 25% chance to win $250
and 75% chance to lose $750 [100%]
Inspection reveals that although Problem 2 is worded differently, its choices are formally
identical to those in Problem 1. The difference is that some simple arithmetic
has been performed for the subjects. Once these calculations are made it becomes
clear to every subject that option F dominates option E, and everyone chooses
accordingly. The difficulty, of course, is that option E, which no one selects, is made
up of the combination of options A and D, both of which were chosen by a large
majority of subjects, while option F, which everyone selects, is a combination of B
and C, options that were highly unpopular in Problem 1. Thus this pair of problems
illustrates two findings that are embarrassing to rational choice adherents. First, subjects'
answers depend on the way a problem is worded or "framed," behavior that is
inconsistent with almost any formal model. Second, by utilizing clever framing, a
majority of subjects can be induced to select a pair of options that are dominated by
another pair. Once again, this behavior does not seem consistent with the idea that
people are choosing as «/they are rational.
B. Experiments, Incentives, and Learning
A second class of explainawaytions emerged in the 1980s, in part as a reaction to
the findings of Kahneman and Tversky and an early paper of mine (Thaler 1980).
These retorts, usually delivered orally in workshops and conference presentations
rather than in print,2 were intended to be justifications for continuing business as


---Economics-2016-0-08.txt---
usual. Some of the critiques were aimed at the empirical methods used in these
early papers, namely hypothetical survey questions such as problems 1 and 2 above.
Economists have never been very impressed by such data because the subjects have
nothing on the line. Furthermore, typically these questions were just asked once,
so many argued that they were not a good indication of what people would do in
real-life situations in which they had an opportunity to learn from prior mistakes. So
the critique was two-fold. First, if you raise the stakes people will take the questions
more seriously and choose in a manner more consistent with optimization. Second,
if given a chance to learn, people will get it right. Often the same person would make
both of these critiques, thinking that they reinforced one another.
Of course there is no doubt that the ability to practice improves performance in
most tasks. No one plays well in his first game of chess, or billiards for that matter.
And most people eventually become at least competent at highly complex tasks such
as riding a bike or running down a flight of stairs. Similarly, the notion that people
will pay more attention when the stakes go up is intuitively appealing. Certainly we
pay more attention when buying a car than when deciding what to order for lunch.
But rather than these two arguments working together, they actually go in opposite
directions. The reason this is so is that, as a rule, the higher the stakes, the less often
we get to do something.
Consider the following list of economic activities: deciding how much milk to
buy at the grocery store, choosing a sweater, buying a car, buying a home, selecting
a career, choosing a spouse, saving for retirement. Most households have mastered
the art of milk inventory management through trial and error. Buy too much and it
spoils, buy too little and you have to make an extra trip to the convenience store.
But if households do this (say) twice a week, eventually they figure it out, at least
until the children move out of the house or switch to beer. Few of us buy cars often
enough to get very good at it, and the really big decisions like careers, marriages,
and retirement saving give very little room for learning. So critics can't have it both
ways. Either the real world is mostly high stakes or it offers myriad opportunities to
learn - not both.
Even in domains where there are multiple opportunities to learn, people may not
make the best of those situations. Daniel Kahneman and I ran an experiment years
ago that illustrates this point. (We never published the results so the details will be
sketchy.) Subjects were given forms that looked something like this:
Heads: 1 2345 ... 18 19 20
Tails: 1 2345 ... 18 1920
They were then shown two large manila envelopes that were labeled Heads and
Tails and were shown that each envelope contained 20 poker chips numbered from
1 to 20. The experimenter said he would first flip a coin and then, depending on the
outcome, choose a poker chip from the respective envelope. Subjects were allowed
to circle five numbers on their form, dividing their choices as they wished between
the heads and tails rows. When the experimenter selected a chip and announced the
result, for example "Heads, 17" any subject who had circled the winning coin face
and number would win some money. Specifically, if the chip came from the Heads


---Economics-2016-0-09.txt---
envelope winners would be paid $2, but if the chip came from the Tails envelope
they would win $3.
Of course the optimal strategy in this game is to only circle numbers in the Tails
row since those have a 50 percent higher expected payoff, but this strategy was not
obvious to everyone. About half the subjects (MBA students at a top university)
adopted the correct strategy of circling only Tails, but the rest used what might
called an "inept mixed strategy," dividing their choices between Heads and Tails,
with the most common allocation being three Tails and two Heads, matching the
ratio of the payoffs.3
The question that Kahneman and I were most interested in, however, was not
these initial choices. This was an experiment about learning. So we had the subjects
repeat the same task nine more times. Each time the subjects got feedback about the
outcome of the coin toss and the number drawn, and the winning guessers were paid
in cash immediately in plain view of the other subjects. Try to guess the results as a
thought experiment.
Of the subjects that did not figure out the "all Tails" strategy immediately, how
many learned to use that strategy over the course of the nine additional trials? The
answer is one. One subject switched at some point to an all Tails strategy, but that
subject was offset by another subject who had circled only Tails on the first trial, but
then switched to the inept mixed strategy at some point during the "learning" phase.
It is instructive to consider why there was essentially no learning in this experiment.
We know from psychology that learning takes place when there is useful,
immediate feedback. When learning to drive we quickly see how much pressure to
use on the accelerator and brake pedals in order to start and stop smoothly. In the
experiment, however, the subjects were first told the outcome of the coin flip, then
the number drawn. Obviously, about half the time the coin came up Heads, and
those who were including Heads in their portfolio were pleased to be still in the
game (if only for another few seconds). Furthermore, every time that someone won
some money from a Heads outcome, there was some reinforcement for continuing
to include some of that "strategy" in the portfolio.
The general point is that learning can be difficult even in a very simple environment.
Those who teach an introductory course in economics know that many
of the first principles that are basic to rational choice models (such as the notion
of opportunity costs) are by no means intuitively obvious to the students. But our
models assume they can understand much more difficult concepts such as backward
induction.
As for the argument that people will do better in experimental tasks if the stakes
are raised, there is little or no evidence to support this hypothesis. The first empirical
test of this idea was conducted by David Grether and Charles Plott (1979) in the
context of an investigation of the "preference reversal phenomenon," discovered by
psychologists Sarah Lichtenstein and Paul Slovic (1971). Lichtenstein and Slovic
presented subjects with two gambles, one a near sure thing they called the p-bet (for
high probability) such as a 35/36 chance to win $10, the other more risky called the
$-bet, such as an 1 1 /36 chance to win $30, a higher potential payoff. Subjects were


---Economics-2016-0-10.txt---
asked to value each bet by naming the lowest price at which they would sell it if
they owned it, and also to choose which of the bets they would rather have. The term
"preference reversal" emerged from the fact that of those who preferred the p-bet,
a majority reported a higher selling price for the $-bet, implying that they valued it
more than the p-bet.
Grether and Plott (1979) were perplexed by this finding and set out to determine
which mistake the psychologists must have made to obtain such an obviously
wrong result. Since the original study was based on hypothetical questions, one of
the hypotheses Grether and Plott investigated was whether the preference reversals
would disappear if the bets were played for real money. (They favored this hypothesis
in spite of the fact that Lichtenstein and Slovic (1973) had already replicated
their findings for real money on the floor of a Las Vegas casino.) What Grether
and Plott found surprised them. Raising the stakes did have the intended effect of
inducing the subjects to pay more attention to their choices (so noise was reduced)
but preference reversals did not thereby vanish; rather, their frequency went up! In
the nearly 40 years since Grether and Plott's seminal paper, I do not know of any
findings of "cognitive errors" that were discovered and replicated with hypothetical
questions but then vanished as soon as significant stakes were introduced.
C. The Invisible Handwave
There is a variation on the "if there is enough money at stake people will behave
like Econs" story that is a bit more complicated. In this version markets replace the
enlightening role of money. The idea is that when agents interact in a market environment,
any tendencies to misbehave will be vanquished. I call this argument the
"invisible handwave" because there is a vague allusion to Adam Smith embedded
in there somewhere, and I claim that it is impossible to complete the argument with
both hands remaining still.
Suppose, for example, that Homer falls prey to the "sunk cost fallacy" and always
finishes whatever is put on his plate for dinner, since he doesn't like to waste money.
An invisible handwaver might say, fine, he can do that at home, but when Homer
engages in markets, such misbehaving will be eliminated. Which raises the question:
how exactly does this occur? If Homer goes to a restaurant and finishes a rich dessert
"because he paid for it" all that happens to him is that he gets a bit chubbier.
Competition does not solve the problem because there is no market for restaurants that
whisk the food away from customers as soon as they have eaten more than X calories.
Indeed, thinking that markets will eradicate aberrant behavior shows a failure to
understand how markets work. Let's consider two possible strategies firms might
adopt in the face of consumers making errors. Firms could try to teach them about
the costs of their errors or could devise a strategy to exploit the error to make higher
profits. The latter strategy will almost always be more profitable. As a rule it is easier
to cater to biases than to eradicate them. DellaVigna and Malmendier (2006)
provide an instructive example in their article "Paying Not to Go to the Gym." The
authors study the usage of customers of three gyms that offer members the choice of
paying $70 a month for unlimited usage, or a package of 10 entry tickets for $100.
They find that the members paying the monthly fee go to the gym an average of 4.3
times per month, implying an average cost of over $17 per visit.


---Economics-2016-0-11.txt---
Obviously the typical monthly members have an arbitrage opportunity available.
Why pay $17 a visit when they could be paying $10? One possible explanation
for this behavior is that customers understand that they are affected by sunk costs
(whether or not they realize it is a fallacy) and are strategically using the membership
fee as a (rather ineffective) commitment device to try to induce more frequent gym
usage. Let's suppose that explanation is correct. What could a competing gym do to
both make more money and reduce or eliminate the less than fully rational behavior
of their clients? It would certainly not be a great strategy to explain to customers
that they could save a lot of money by switching to the 10-ticket package. Not only
would the gym be losing money on a per visit basis, but they would also forego the
payments from infrequent gym users who procrastinate about quitting. The average
person who quits has not been to the gym in 2.3 months. So if competing gyms
can't make money by turning them into Econs, who can? I suppose Della Vigna and
Malmendier could have started a service convincing people to switch to paying by
the visit, but I think they made a wise career choice in selecting academia over personal
finance consulting.
The same analysis applies to the recent financial crisis. Many homeowners took
out mortgages with initial low "teaser rates." Once the rates reset, some homeowners
found they were unable to pay their mortgage payments unless home prices continued
to go up and mortgage refinancing remained available at low interest. The mortgage
lenders who initiated such mortgages and then immediately sold the loans to
be securitized made lots of money while it lasted, but the subsequent financial crisis
was painful to nearly everyone. Let's assume that at least some of these mortgage
borrowers were fooled by fast-talking mortgage brokers.4 How would the market
solve this problem? No one has ever gotten rich convincing people not to take out
unwise mortgages.
Similarly if people fail to follow the dictates of the life-cycle hypothesis and fail
to save adequately for retirement, how is the market going to help them? Yes, there
are firms selling mutual funds but they are competing with other firms selling fast
cars, big screen televisions, and exotic vacations. Who is going to win that battle?
The bottom line is there is no magic market potion that miraculously turns Humans
into Econs; in fact, the opposite pattern is more likely to occur, namely that markets
will exacerbate behavioral biases by catering to those preferences.
The conclusion one should reach from this section is that the explainawaytions
are not a good excuse to presume that agents will behave as if they were Econs.
Instead we need to follow Milton Friedman's advice and evaluate theories based on
the quality of their predictions, and, if necessary, modify some of our theories.
III. Financial Markets3
A good place to start in an evaluation of the potential importance of
less-than-fully-rational agents is financial markets. I say this because financial markets
have the features that should make it hardest to find evidence of misbehavior.


---Economics-2016-0-12.txt---
These markets have low transaction costs, high stakes, lots of competition (except
perhaps in some banking sectors) and crucially, the ability to sell short. It is short
selling that allows for the possibility that even if most investors are fools, the activities
of "smart money" arbitrageurs can assure that markets behave "as if' everyone
were smart. This is the intellectual underpinning of the efficient market hypothesis
(EMH).
The efficient market hypothesis really has two distinct components. The first,
what I call the "no free lunch" provision, is that it is not possible to "beat the market"
on a properly risk-adjusted basis. There is an enormous literature devoted to testing
this hypothesis, with many arguments on each side. The difficulty in evaluating
competing claims is in agreeing on the way to account for risk. For example, there is
widespread agreement in the literature that a strategy of buying "value stocks," for
example those with low ratios of price to earnings or book value, earns higher returns
than buying "growth stocks," which have high price-earnings ratios. However, there
is a debate about the explanation for these excess returns. Behavioralists (for example
De Bondt and Thaler 1985, 1987; Lakonishok, Shleifer, and Vishny 1994) argue
that the excess returns reflect mispricing of some sort. On the other side, efficient
market advocates such as Fama and French (1993) argue that the high returns to
value stocks occur because those stocks are risky. Although it would not be right to
say that this argument has been settled to everyone's satisfaction, I do think that no
one has been able to identify a specific way in which value stocks are riskier than
growth stocks. (For example, value stocks tend to have lower betas, the traditional
measure of risk in the Capital Asset Pricing Model.) Still, while academics debate
about the correct interpretation of these empirical results, one important fact first
documented in Jensen's (1968) PhD thesis remains true: the active mutual fund
industry on average does not beat the market.
So from the point of view of an investor, this aspect of the efficient market hypothesis
can safely be considered to be at least approximately true. Nevertheless, it is
important not to misinterpret this finding. The lack of predictability in stock market
returns does not imply that stock market prices are "correct." This is the second
aspect of the EMH, what I call the "price is right" component. The inference that
unpredictability implies rational prices is what Shiller (1984, p. 459) once called
"one of the most remarkable errors in the history of economic thought." It is an error
because just as the path of a toddler running around on a playground might be completely
unpredictable, the path is also not likely to be the result of maximizing some
well-formed objective function (other than having fun).
The price is right component of the EMH is, in my opinion, by far the more
important of the two ingredients of the theory. It is important because if prices are
"wrong" then capital markets are not doing an efficient job of allocating resources.6
The problem has been to come up with a convincing test of this part of the theory
because the intrinsic value of a security is normally unknowable. If the price of
Apple Inc. were too high or too low, how would we know? It turns out that there are
classes of assets for which we can say something definitive, namely those for which
we can use the law of one price as a test. Although we don't know the rational price


---Economics-2016-0-13.txt---
of Apple, we can say for sure that odd-numbered share certificates (if such things
still exist) should sell for the same price as even-numbered shares. I have explored
several such examples in work with Owen Lamont,7 and he recently told me about
another one that I will describe here.
One type of security that has provided a fruitful source of tests of the law of one
price is closed-end mutual funds. Unlike their open-ended cousins, which accept
new investments that are valued at the net asset value of the securities held by the
fund, and then redeem withdrawals the same way, closed-end funds are, as their
name suggests, closed to new investors. Rather, when the fund starts, a certain
amount of money is raised and invested, and then the shares in the fund trade on
organized markets such as the New York Stock Exchange. The curious fact about
closed-end funds, noted early on by Graham (1949) among others, is that the price
of the shares is not always equal to the net asset value of the underlying securities.
Funds typically sell at discounts of 10-15 percent, but sometimes sell at substantial
premia. This is the story of one such fund.
The particular fund I want to highlight here happens to have the ticker symbol
CUBA. Founded in 1994, its official name is the Herzfeld Caribbean Basin Fund,
which has 69 percent of its holdings in US stocks with the rest in foreign stocks,
chiefly Mexican. It gave itself the ticker "CUBA" despite the fact that it owns no
Cuban securities nor has it been legal for any US company to do business in Cuba
since 1960 (although that may change at some point). The legal proviso, plus the
fact that there are no traded securities in Cuba, means that the fund has no financial
interest in the country with which it shares a name. Historically, the CUBA fund
traded at a 10-15 percent discount to Net Asset Value.
Figure 1 plots both the share price and net asset value for the CUBA fund for a
time period beginning in September 2014. For the first few months we can see that
the share price is trading in the normal 10-15 percent discount range. Then something
abruptly happens on December 18, 2014. Although the net asset value of the
fund barely moves, the price of the shares jumped to a 70 percent premium. Whereas
it had previously been possible to buy $100 worth of Caribbean assets for just $90,
the next day those assets cost $170! As readers have probably guessed, this price
jump coincided with President Obama's announcement of his intention to relax the
United States' diplomatic relations with Cuba. Although the value of the assets in
the fund remained stable, the substantial premium lasted for several months, finally
disappearing about a year later.
This example and others like it show that prices can diverge significantly from
intrinsic value, even when intrinsic value is easily measured and reported daily. What
then should we think about broader market indices? Can they also get out of whack?
Certainly, the run-up of technology stocks in the late 1990s looked like a bubble at
the time, with stocks selling for very high multiples of earnings (or sales for those
without profits), and it was followed by a decline in prices of more than two thirds
in the NASDAQ index. We experienced a similar pattern in the housing boom in the
mid 2000s, especially in some cities such as Las Vegas and Phoenix. Prices sharply
diverged from their long-term trend of selling for roughly 20 times rental prices,


---Economics-2016-0-14.txt---
Figure 1. Price and Net Asset Value for CUBA Fund
Note: On December 18, 2014, President Obama announced he was going to lift several restrictions against Cuba.
Source: Bloomberg
only to fall back to the long-term trend. Because of the various forms of leverage
involved, this rise and fall in prices helped create the global Great Recession.
The difference between the CUBA example and these much larger bubbles is that
it is impossible to prove that prices in the latter were ever wrong. There is no clear
smoking gun. But it certainly feels like asset prices can diverge significantly from
fundamental value. Perhaps we should adopt the definition of market efficiency proposed
by Fischer Black (1986) in his presidential address to the American Finance
Association, which had the intriguing one word title "Noise." Black (1986, p. 553)
says "we might define an efficient market as one in which price is within a factor of
two of value, i.e., the price is more than half of value and less than twice value. The
factor of two is arbitrary, of course. Intuitively, though, it seems reasonable to me,
in light of sources of uncertainty about value and the strength of the forces tending
to cause price to return to value. By this definition, I think almost all markets are
efficient almost all the time. 'Almost all' means at least 90 percent."
One can quibble over various aspects of Black's definition but it seems about
right to me, and had Black lived to see the tech bubble of the 90s he might have
revised his number up to three. I would like to make two points about this. The first
is that the efficient market hypothesis has been a highly useful, indeed essential
concept in the history of research on financial markets. In fact, without the EMH
there would have been no benchmark with which to compare anomalous findings.
The only danger created by the concept of the EMH is if people, especially policymakers,
consider it to be true. If policymakers think that bubbles are impossible,
then they may fail to take appropriate steps to dampen them. For example, I think it
would have been appropriate to raise mortgage-lending requirements in cities where
price to rental ratios seemed most frothy. Instead, this was a period in which lending
requirements were unusually lax.


---Economics-2016-0-15.txt---
There is a broader point to make. For lots of reasons we might expect that financial
markets are the most efficient of all markets. They are the only markets where
it is generally possible to cheaply sell short, an essential feature if we expect prices
to be "right." Yet if financial markets can be off by a factor of two, how much confidence
should we have that prices in other markets are good measures of value,
where there are no realistic arbitrage opportunities?
To give just one example, consider labor markets. There has been considerable
attention paid in recent years to the growing inequality in incomes and wealth
around the world (Piketty 2014; Atkinson, Piketty, and Saez 2011). Although there
has been much debate about the cause of this trend, most of the discussion within
economics is based on the presumption that differences in income reflect differences
in productivity. Is that presumption warranted? If stock prices can be off by a factor
of two, might not that be true for workers, from hamburger flippers to CEOs?
There is reason for skepticism about that presumption from the bottom to the
top of the income ladder. At the lower end of the wage distribution there has been
a long literature begun by Slichter (1950) documenting odd inter-industry wage
differentials. Simply put, some industries pay more than others, and this applies to
clerical workers and janitors as well as higher paid executives. Important papers by
Krueger and Summers (1988) and Dickens and Katz (1986) reignited this literature
summarized in Thaler (1989). Card, Heining, and Kline (2013) have recently documented
similar findings in Germany using panel data that allow for individual fixed
effects. They find that when workers move from a bottom quartile paying industry
to a top quartile industry their wages jump, and the opposite thing happens when
workers move from a high paying industry to a low one. It seems implausible that
these workers become significantly more or less productive simply by changing
industries.
At the other end of the spectrum, the ratio of CEO pay to that of the average
worker has skyrocketed in the past few decades. In 1965 for large firms based in the
United States this ratio was 20; by 2014 it was over 300, more than twice the ratio
in any other country (Mishel and Davis 2015). Of course some economists argue
that this rise simply reflects the growing productivity of the CEOs (e.g., Kaplan,
Klebanov, and Sorensen 2012) but how confident should we be in this assessment?
CEO pay is usually set by the compensation committees of boards of directors that
rely on consultants who base their recommendations in part on the pay of other
CEOs. This kind of recursive, self-fulfilling process is not one that generates high
confidence that pay and performance are highly correlated. Of course there is no
way to settle this argument. Rather, I just want to repeat my question. If stock prices
can be off by a factor of two, why should we be confident that other markets do not
diverge by that much, or more?
IV. One Theory, Two Tasks
The conclusion I reach from research in behavioral finance is that even these most
efficient of markets often lead to empirical results that are inconsistent with theories
based on rational investors making choices in markets with tiny transaction costs. In
other words, the results we obtain are not consistent with the hypothesis that investors
behave "as if' they were rational. And there should be even greater suspicion


---Economics-2016-0-16.txt---
that such models will make good predictions in other markets where arbitrage is
impossible. So what should happen to economic theory?
The problem is that we are asking our theories to do two different tasks. The first
is to solve for optimal solutions to problems, the other is to describe how Humans
actually choose. Of course in a world consisting only of Econs there would be no
need for two different kinds of models. Economic agents would have the courtesy to
make the optimal choices that the model determines are best (at least on average).
But we are far from that world: We Humans struggle both to determine what the best
choice would be and then to have enough willpower to implement that choice, especially
if it requires delay of gratification. So we need descriptive economic theories.
The first and most successful such theory is Kahneman and Tverksy's (1979)
prospect theory, which has had an enormous impact on both economics and social
science more generally.8 Beyond the insights of the model itself, prospect theory
provides a template for the new class of theories we need. Expected utility theory
remains the gold standard for how decisions should be made in the face of risk.
Prospect theory is meant to be a complement to expected utility theory, which tell us
how people actually make such choices. Using one theory for both purposes makes
no more sense then using a hammer both to pound nails and to apply paint.
Some economists might think that without optimization there can be no theory,
but in a cogent essay Arrow (1986) rejected this idea. "Let me dismiss a point of
view that is perhaps not always articulated but seems implicit in many writings. It
seems to be asserted that a theory of the economy must be based on rationality, as a
matter of principle. Otherwise there can be no theory." Arrow noted that there could
be many rigorous, formal theories based on behavior that economists would not be
willing to call rational. He also pointed out the inconsistency of an economic theorist
who toils for months to derive the optimal solution to some complex economic
problem and then blithely assumes that the agents in his model behave as if they
are naturally capable of solving the same problem. "We have the curious situation
that scientific analysis imputes scientific behavior to its subjects. This need not be a
contradiction, but it does seem to lead to an infinite regress."
This is not the place, and I am not the person, to present a detailed roadmap
of what a behavioral approach to economic theory should be, but perhaps a few
brief thoughts are appropriate. The first is that behavioral economic theories (or
any descriptive theories) must abandon the inductive reasoning that is the core of
neoclassical theories and instead adopt a deductive approach in which hypotheses
and assumptions are based on observations about human behavior. In other words,
behavioral economic theory must be evidence-based theory. The evidence upon
which these theories can be based can come from psychology or other social sciences
or it can be homemade. Some might worry about basing theories on empirical
observation, but this methodology has a rich tradition in science. The Copernican
revolution, which placed the sun at the center of our solar system rather than the
earth, was based on data regarding the movement of the planets, not on some first
principles.


---Economics-2016-0-17.txt---
A second general point is that we should not expect some new grand behavioral
theory to emerge to replace the neoclassical paradigm. We already have a grand
theory and it does a really good job of characterizing how optimal choices and
equilibrium concepts work. Behavioral theories will be more like engineering, a
set of practical enhancements that lead to better predictions about behavior. So far,
most of these behavioral enhancements focus on two broad topics: preferences and
beliefs.
A. Behavioral Preferences
Prospect theory is a good illustration of a model based on assumptions about preferences
that differ from the ones used to derive expected utility theory. Specifically,
most of prospect theory's predictive power comes from three crucial assumptions
about preferences. First, utility is derived from changes in wealth relative to some
reference point, rather than levels of wealth, as is usually assumed in theories
based on expected utility.9 Second, the "value function" which translates perceived
changes in wealth into utility, has a kink at the origin, with losses weighed more
heavily than gains - i.e., "loss aversion." Third, decision weights are a function of
probabilities n(p) where II(p) 7^ p. These aspects of the theory were inferred from
studying the choices subjects made when asked to choose between gambles.
Two other research streams have been based on models of preferences. The first
topic is intertemporal choice. As revealed by the quotations from Smith, Pigou, and
Fisher mentioned earlier in this essay, economists have long worried that people
display what we now call "present biased" preferences, meaning that the discount
rate between "now" and "later" is much higher than between "later" and "even
later." Such preferences can lead to time-inconsistent behavior since we expect to be
patient in choosing between a smaller reward in a year and a larger reward in a year
plus a week, but when the year passes and the smaller reward is available "now," we
submit to temptation. If people realize they have such preferences, they may choose
to commit themselves now to choosing the larger delayed reward, a strategy they
will later regret (at least for a week or so).
Two kinds of models have been proposed to deal with these aberrant preferences.
One is based on a two-self (or "two-system") approach that is meant to capture
the inherent conflict that defines self-control problems. In the version of this type
of model that Hersh Shefrin and I favor (Thaler and Shefrin 1981) individuals are
assumed to have a long-sighted "planner" and myopic "doer" that interact in a
model similar to agency models of organizations. Schelling (1984) and Fudenberg
and Levine (2006) also proposed two-self models to characterize this behavior.
Although these two-self models provide more psychological texture, they have
not been as popular among economic theorists as the simpler and more tractable
"beta-delta" model originally proposed by Strotz ( 1955) and then refined by Laibson


---Economics-2016-0-18.txt---
(1997) and O'Donoghue and Rabin (1999). In these models delta is the standard
exponential discount rate and beta measures short-term impatience. The standard
model is just the special case in which beta is 1 .0. The beta-delta model is a good
example of what Rabin (2013) calls PEEMs, which stands for "portable extensions
of existing models." The ease with which economists can incorporate such models
into an otherwise standard analysis has obvious appeal.
Along with intertemporal choice, the important aspect of preferences that has
received a lot of attention from behavioral economic theorists is "other-regarding
preferences." These models were all stimulated by empirical findings showing that
humans are not completely selfish, even to strangers. For example, in one-shot prisoners'
dilemma games about 40-50 percent of subjects cooperate, both in laboratory
experiments and even in a game show environment where the stakes are over
£10,000 (van den Assem, van Dolder, and Thaler 2012). Similarly, people cooperate
in public goods environments when the rational selfish strategy is to give nothing.
The most prominent models in this space are by Rabin ( 1993) and Fehr and Schmidt
(1999). The easiest way to summarize this literature is to say that Humans are nicer
and more mannerly than Econs. Specifically, their first instinct is to cooperate as
long as they expect others to do likewise.
B. Behavioral Beliefs
When people make choices they do so based on a set of expectations about the
consequences of their choices and the many exogenous factors that can determine
how the future will evolve. Traditionally, economists assume that such beliefs are
unbiased. Although the rational expectations hypothesis as first formulated by Muth
(1961) and elaborated upon by Lucas (1976) and many others is often considered
to be a specific approach to economic modeling, especially in macroeconomics, I
think it is fair to say that the essential idea is entirely mainstream. The assumption
of rational expectations makes explicit an idea that is commonplace in economic
theory, namely that agents act as if they understood the model (and state-of-the-art
econometrics techniques as well). Whether this assumption is empirically valid is
another question.
Explicit tests of rational expectations per se are uncommon because we rarely
observe or elicit actual expectations data. When we do, we often find that actual
expectations diverge from what would reasonably be called rational. For example,
Case, Shiller, and Thompson (2012) find that homeowners during the period
of rapidly rising prices from 2000-2005 expected home prices to continue to rise
at double-digit rates for the next decade. While one can't prove such expectations
were irrational, they certainly seem excessively optimistic, both ex ante and ex post.
Furthermore, in this domain and in many others, expectations seem to rely too much
on extrapolation of recent trends. To a first approximation, people expect that what
goes up will continue to go up.
We also see violations of rational expectations in the predictions of stock market
returns by chief financial officers studied by Ben-David, Graham, and Harvey
(2013). The CFOs were asked to predict one-year rates of return on the S&P 500
and also give 80 percent confidence limits. Perhaps unsurprisingly, the CFOs had
essentially no ability to predict returns in the stock market. What is more disturbing


---Economics-2016-0-19.txt---
is that they had no self-awareness of their lack of predictive skills. If the CFOs
had well-calibrated forecasts the actual stock-market return would fall between
their high and low estimate 80 percent of the time. Instead, their ranges included
the actual outcome for just 36 percent of the forecasts recorded over a ten-year
period. This is quite similar to the overconfidence observed in dozens of laboratory
studies.
Overconfidence and excessive extrapolation are just two examples of biased
beliefs that have been documented by psychologists studying human judgment.
This literature began with the original three heuristics studied by Kahneman and
Tversky - availability, representativeness, and anchoring and adjustment - but
many others have been investigated and documented since then: hindsight bias,
projection bias, excessive attention to whatever feature of the environment is most
salient, etc. For each of these biases and many more, economists have created
descriptive models to try to make the implications of the biases more specific and
rigorous.
The fact that there is a long list of biases is both a blessing and a curse. The
blessing is that there are a multitude of interesting ways in which human judgment
diverges from rational expectations, each of which offers the possibility of providing
useful insights into economic behavior. The curse is that the length of the list seems
to offer theorists a dangerously large number of degrees of freedom. Although I do
not dismiss this latter risk out of hand, I think good scientific practices can mitigate
this degrees-of-freedom risk.
The most important thing to remember is that all these biases have empirical support,
and many of the laboratory findings have subsequently been replicated in the
field. Thus some discipline has already been imposed: behavioral economists can
draw on a long list of potential explanatory factors, but for each there is at least some
evidence that the factor is real. Compare this with the degrees of freedom available
in traditional rationality-based models. For example, consider the all-purpose fudge
factor: transaction costs. In the abstract such costs can explain many anomalies,
but unless those costs can be measured the use of the concept is undisciplined. If
we limit ourselves to variables that have an empirical basis, all of economics will
become more disciplined.
Of course I do not mean to suggest that behavioral economic theory is a finished
product. The field is new and growing rapidly. One goal should be to devise theories
that are not just portable extensions of existing models but also testable extensions.
I will leave it to Rabin to decide where to insert the letter T into his PEEM acronym.
V. Supposedly Irrelevant Factors
It is rare that economic theory makes predictions about magnitudes. Mostly theories
make predictions about the sign of an effect. Demand curves slope down; supply
curves slope up. When a clever theorist is able to extract a more precise prediction
from the theory, things can get interesting. The equity premium puzzle is a case in
point. The first-order prediction that stocks are riskier than bonds and so should
earn a higher rate of return is resoundingly supported by the historical data. But
Mehra and Prescott (1985) showed that the standard model cannot simultaneously
explain the low historical risk-free rate and an equity premium in the neighborhood


---Economics-2016-0-20.txt---
of 6 percent - the largest value they could justify was 0.35 percent. As a result of
this calibration exercise a long and interesting literature ensued.
Although such examples of predictions about magnitude are uncommon, economic
theory does make some rather precise predictions about effect sizes, namely
for variables that should have no effect at all on behavior. For example the following
things should not matter: the framing of a problem, the order in which options are
displayed, the salience of one option over another, the presence of a prior sunk cost
(or gain), whether the customer at a restaurant can see the dessert options when
choosing whether to stick to the planned diet, and so forth. I call these, and a multitude
of other possible variables that can and do influence choices, "supposedly
irrelevant factors" or SIFs. One of the most important ways in which behavioral economics
can enrich economic analyses is by pointing out the SIFs that matter most.
One domain in which the potential importance of SIFs has been best documented
is retirement saving. In a standard life-cycle model Econs compute their optimal
consumption path and then implement a plan of saving, investing, and eventually
dis-saving that maximizes lifetime utility, fully incorporating proper actuarial probabilities
of mortality, rates for husband and wife as well as risks of divorce, illness,
and so forth. This is a problem that makes playing world-class chess seem easy.
Chess has neither uncertainty nor self-control problems to muck up the works. So
it should not be surprising that many Humans have trouble dealing with retirement
saving in a defined-contribution world in which they have to make all the decisions
themselves. However, it has been possible to help people with this daunting task
with the aid of some SIFs.
The first SIF that has been important in helping people to save for retirement is
the intelligent use of the default option. In a world of Econs, especially when the
stakes are as high as they are for retirement saving, it should not matter whether
someone gets signed up for the plan unless he opts out or is excluded from the
plan unless he opts in. The cost of ticking a box and filling out a form must be tiny
compared to the benefits of receiving a company match and tax-free accumulations
for decades. Nevertheless, changing the default has had an enormous impact on the
utilization rates of 401(k) plans.
The first paper to document this effect was Madrian and Shea (2001) using data
from a company that had adopted what is now called "automatic enrollment" in
1999. Previously, to join the 401(k) plan employees had to fill in some forms, and
if they failed to do so, they were not enrolled. Madrian and Shea compared the
enrollment rates for new employees in 1998 under the old "opt in" regime to those in
1999 where employees had to opt out if they did not want to join. Before automatic
enrollment, only 49 percent of employees joined the plan within their first year of
employment; after the switch to automatic enrollment, 86 percent of the employees
were enrolled in their first year. Supposedly irrelevant indeed! By now automatic
enrollment is widespread. More than half of large US employers are using the concept
and the United Kingdom is in the process of rolling out a national defined contribution
savings plan with this feature. Most plans, including the national UK plan,
find that opt out rates are around 10 percent.
One problem with automatic enrollment is that many plans initially enroll
employees at a low savings rate; in the United States it is often just 3 percent of pay.
As Madrian and Shea pointed out in their initial paper, such a low initial default


---Economics-2016-0-21.txt---
savings rate can have the unintended consequence of reducing the savings of those
who, lacking a default, would have chosen to save more. As one solution to this
problem, and more generally as a way to nudge employees to increase their savings
rates, Shlomo Benartzi and I (Thaler and Benartzi 2004) introduced a plan we called
"Save More Tomorrow." Under this plan, workers are offered the option to increase
their savings rate starting at some later date, ideally when they get their next raise.
Once an employee enrolls in the plan, her savings rate keeps increasing until she
reaches some cap or opts out.
Notice that Save More Tomorrow is just a collection of SIFs. It should not matter
that the savings rate is increased in a few months rather than now, nor that the
increases are linked to pay increases, nor that the default is to stay in the plan, but of
course all these features help. Putting off the increase in saving to the future helps
those who are present biased; linking to increases in pay mitigates loss aversion; and
making staying in the plan the default puts status quo bias to good use. In the first plan
Benartzi and I studied (Thaler and Benartzi 2004), savings rates more than tripled in
three years. In a recent paper (Benartzi and Thaler 2013) we estimated that automatic
escalation (the generic term for Save More Tomorrow, in which savings increases are
not always linked to pay increases) had boosted annual savings by $7.4 billion.
One worry about such programs has been that the increases we observe in retirement
savings produced by automatic enrollment and Save More Tomorrow might
be offset by reductions in savings (or increases in borrowing) in other accounts.
However, there was no dataset in the United States that allowed anyone to test this
hypothesis. Fortunately, such data do exist in Denmark, which, because of a history
of having a wealth tax, has long kept good data on household wealth. A recent paper
by Chetty et al. (2014) has made use of these data to answer this question.
The method Chetty et al. (2014) use is to see what happens to savings rates when
an employee moves jobs to an employer with a more generous retirement savings
plan. Using panel data with 41 million person-year observations the authors study
three kinds of savings: employer contributions to tax-sheltered pensions, employee
contributions to those pensions, and employee savings in taxable accounts. Their
research strategy is to study those employees who have been saving a positive
amount on their own and then switch to a firm whose contributions are at least 3
percentage points higher. On average these workers receive an increase in pension
contributions of 5.64 percent of labor income. Do workers contribute less to compensate
for this change in their employers generosity? Yes, but just by 0.56 percentage
points. And saving in taxable accounts is essentially unchanged.
Chetty et al. (2014) also make use of a change in tax policy that occurred
during the period for which they have data. This natural experiment allowed them
to compare the effectiveness of the tax subsidy given to pension contributions in
encouraging retirement savings relative to the effects of design features such as the
employer contribution. The change in the law they exploit was a reduction in the
subsidy given to retirement saving for roughly the top quintile of the income distribution.
Even among this relatively affluent group, the vast majority did not react
at all to the change in the subsidy - they were "passive savers." About 20 percent
of this segment did react and eliminated all their contributions to the tax-sheltered
plans, but they did not spend that money; they just shifted it to taxable savings
vehicles. This leads to a remarkable conclusion. Each $1 of tax expenditure on


---Economics-2016-0-22.txt---
retirement savings only produced a penny in increased savings. What determines
savings rates is not tax policy but the design features of the employer pension plans,
i.e., SIFs.
There are many other examples of the potential power of behavioral factors in policy
analysis but summarizing them would be a waste of time. I cannot possibly do a
better job of that than Raj Chetty (2015) did last year in his Ely lecture: "Behavioral
Economics and Public Policy: A Pragmatic Perspective." I completely endorse his
view that the best way to proceed is to stop arguing about theoretical principles and
just get down to work figuring out the best way of understanding the world.
VI. Conclusion
There is one central theme of this essay: it is time to fully embrace what I would
call evidence-based economics. This should not be a hard sell. Economists use
the most sophisticated statistical techniques of any social science, have access to
increasingly large and rich dataseis, and have embraced numerous new methods from
experiments (both lab and field) to brain imaging to machine learning. Furthermore,
economics has become an increasingly empirical discipline. Hamermesh (2013)
finds that the percentage of "theory" papers in top economics journals has fallen
from 50.7 percent in 1963 to 19.1 percent in 201 1. We are undeniably an empirical
discipline - so let's embrace that.
Viewed in this context, behavioral economics is simply one part of the growing
importance of empirical work in economics. There is nothing unique about incorporating
psychological factors such as framing, self-control, and fairness into economics
analyses. If such factors help us understand the world better and improve
predictions about behavior, then why wouldn't we use them just like we would use
any other new source of data such as web searches or genetic markers?
In this sense I think it is time to stop thinking about behavioral economics as
some kind of revolution. Rather, behavioral economics should be considered simply
a return to the kind of open-minded, intuitively motivated discipline that was
invented by Adam Smith and augmented by increasingly powerful statistical tools
and dataseis. This evidence-based discipline will still be theoretically grounded, but
not in such a way that restricts our attention to only those factors that can be derived
from our traditional normative traditions. Indeed, my sense is that we are at the
beginning of a new wave of theoretical developments made possible simply by turning
our attention to the study of Humans rather than Econs.
If economics does develop along these lines the term "behavioral economics"
will eventually disappear from our lexicon. All economics will be as behavioral as
the topic requires, and as a result, we will have an approach to economics that yields
a higher R2.


---Economics-2017-0-03.txt---
By narrative economics, I mean the study of the spread and dynamics of popular
narratives, the stories, particularly those of human interest and emotion, and how
these change through time, to understand economic fluctuations. A recession, for
example, is a time when many people have decided to spend less, to make do for
now with that old furniture instead of buying new, or to postpone starting a new
business, to postpone hiring new help in an existing business, or to express support
for fiscally conservative government. They might make any of these decisions in
reaction to the recession itself (that's feedback), but to understand why a recession
even started, we need more than a theory of feedback. We have to consider the possibility
that sometimes the dominant reason why a recession is severe is related to
the prevalence and vividness of certain stories, not the purely economic feedback or
multipliers that economists love to model.
The field of economics should be expanded to include serious quantitative study
of changing popular narratives. To my knowledge, there has been no controlled
experiment to prove the importance of changing narratives in causing economic


---Economics-2017-0-04.txt---
fluctuations. We cannot easily prove that any association between changing narratives
and economic outcomes is not all reverse causality, from the outcomes to the
narratives. But there have been true controlled experiments showing that people
respond strongly to narratives, in the fields of marketing (Escalas 2007); journalism
(Machill, Köhler, and Waldhauser 2007 ); education (McQuiggan et al. 2008);
health interventions (Slater et al. 2003); and philanthropy (Weber et al. 2006).
My goal in this paper is to describe what we know about narratives and the penchant
of the human mind to be engaged by them, to consider reasons to expect
that narratives might well be thought of as important, largely exogenous shocks to
the aggregate economy. This address extends some earlier work I have done with
George Akerlof (Akerlof and Shiller 2009, 2015) and some of my own earlier work
going back decades (Shiller 1984), but develops the analysis and captures a much
broader relevant literature.
Of course, almost nothing beyond spots on the sun is truly exogenous in economics,
but new narratives may be regarded often as causative innovations, since each
narrative originates in the mind of a single individual (or a collaboration among a
few). Mokyr (2016) calls such an individual a "cultural entrepreneur," and traces
the concept back to Hume (1742) who wrote that "what depends on a few persons
is, in great measure, to be ascribed to chance, or secret and unknown causes; what
arises from a great number may often be accounted for by determinate and known
causes."1
I will present here some thoughts on these effects of a "few persons" and offer
a class of mathematical models for some of these determinate and known causes
of the path of narratives, quantifying the dynamics of narratives, and will consider
how our understanding can be enhanced of major economic events: the Depression
of 1920-1921, the Great Depression of the 1930s, the Great Recession of 2007-
2009, and our present time right after our narrative-filled 2016 US presidential
election.
I use the term narrative to mean a simple story or easily expressed explanation
of events that many people want to bring up in conversation or on news or social
media because it can be used to stimulate the concerns or emotions of others, and/
or because it appears to advance self-interest. To be stimulating, it usually has some
human interest either direct or implied. As I (and many others) use the term, a narrative
is a gem for conversation, and may take the form of an extraordinary or heroic
tale or even a joke. It is not generally a researched story, and may have glaring
holes, as in urban legends. The form of the narrative varies through time and across
tellings, but maintains a core contagious element, in the forms that are successful
in spreading. Why an element is contagious, when it may even "go viral," may be
hard to understand, unless we reflect carefully on the reason people like to spread
the narrative. Mutations in narratives spring up randomly, just as in organisms in
evolutionary biology, and when they are contagious, the mutated narratives generate
seemingly unpredictable changes in the economy.
Narratives can be based on varying degrees of truth. Wishful thinking may
enhance contagion (Bénabou 2013). The impact of nonfactual narratives might


---Economics-2017-0-05.txt---
be somewhat greater in today's world than in decades past, since established news
media are in upheaval after the relatively recent advent of modem information technology
and social media. But in past decades we can also observe that narratives
with no factual basis were widely disseminated and believed. For instance, until
modern times, it was asserted that women were not capable of learning men's occupations
(Goldin 2014). Similarly, it was argued that some racial or ethnic groups
were not really capable of integrating into civilized society (Myrdal 1974). How
people could believe these views in the past seems hard to imagine today because
we are no longer immersed in their narratives.
Disturbingly, Oxford Dictionaries in 2016 gave post-truth as "international word
of the year."2 Are narratives becoming increasingly based on false ideas? This possibility
is something we ought to try to understand better. Among normal people,
narratives are often somewhat dishonest and manipulative. In a competitive market
where competitors manipulate customers and profit margins have been competed
away to normal levels, no one company can choose not to engage in similar manipulations.
If they tried, they might be forced into bankruptcy. A phishing equilibrium
with a certain equilibrium acceptable level of dishonesty in narrative is established
(Akerlof and Shiller 2015).
I. The Role of Narratives Broadly, in the Social Sciences and the Humanities
When we as economists want to understand the most significant economic events
in our history, such as the Great Depression of the 1930s, or subsequent recessions,
or policies toward wealth and poverty, we rarely focus on the important narratives
that accompanied them. We have lagged behind other disciplines in attending to the
importance of narratives (Figure 1), and while all disciplines use narratives more
since 2010, economics (and finance) remain laggards. This is despite calls for economists'
attention to social dynamics and popular models (Shiller 1984); to a new
culturomics (Michel et al. 2005); or humanomics (McCloskey 2016); or for more
narrativeness in economics (Morson and Schapiro 2017). We see little use of enormous
databases of written word that might be used to study narratives.
Some would say that the field of history has always had an appreciation for narratives.
However, historians need also to be reminded sometimes that a deep understanding
of history requires imputing what was on the minds of those people who
made history, what their narratives were, as historian Ramsay MacMullen implored
in his book, Feelings in History: Ancient and Modern (2003). He does not literally
stress the concept of narratives: he has told me he would prefer a word conveying
"stimulus to some emotional response, and there is no such word." But he shows that
if we try to understand people's actions, we will need to replicate in ourselves as
best we can the feelings they themselves experienced, and his book describes many
of the narratives that in history communicated such feelings.
In the social sciences, the last half-century saw the blossoming of schools of
thought that emphasized study of popular narratives, a study that has been variously
named as narrative psychology (Sarbin 1986); storytelling sociology (Berger and


---Economics-2017-0-06.txt---
Quinney 2004); narrative approaches to religious studies (Ganzevoort et al. 2013);
narrative criminology (Presser and Sandberg 2015); and so on. The overriding
theme is that people typically have little or nothing to say if you ask them to explain
their objectives or philosophy of life, but they brighten up at the opportunity to
tell personal stories, which then reveal much of their values (O'Conner 2000). For
example, if one interviews in a prison, one finds that the subject tends to respond
well when asked to tell stories about other criminals, and these stories tend to convey
a sense not of amorality but of altered morality.
Consideration of narratives in economics brings us to an unfamiliar association
with literature departments in our universities. Some literary theorists have found
that certain basic structures of stories are repeated constantly, though the names and
circumstances change from story to story, perhaps suggesting that the human brain
practically has receptors for certain stories built in. Cawelti (1976) classifies what
he calls "formula stories." Propp (1984) found 31 "functions" present in all folk stories.
Tobias (1999) says that in all of fiction there are only 20 master plots: "quest,
adventure, pursuit, rescue, escape, revenge, the riddle, rivalry, underdog, temptation,
metamorphosis, transformation, maturation, love, forbidden love, sacrifice, discovery,
wretched excess, ascension, and descensión." Booker (2004) argues that there
are only seven basic plots: "overcoming the monster, rags to riches, voyage and
return, comedy, tragedy, and rebirth." According to literary theorist Klages (2006,
p. 33), structuralism in literary theory takes such efforts to list all basic stories as
"overly reductive and dehumanizing." Although dismissing these lists of basic plots,
she asserts: "structuralists believe that the mechanisms which organize units and
rules into meaningful systems come from the human mind itself." Brooks (1992,
location 74) says narratology should be concerned with "how narratives work on
us, as readers, to create models of understanding, and why we need and want such
shaping orders." Well-structured narratives, Brooks (1992, location 749) argues,
"animate the sense-making process" and fulfill a "passion for meaning" and the
study of narratives brings him to psychoanalysis.


---Economics-2017-0-07.txt---
II. Narratives as Creative, Essentially Human, Works
Predicting the success of creative works with the public is an extremely difficult
task, one that is fundamentally related to the challenge we have in predicting contagion
rates of narratives, processes that depend on many factors. We don't precisely
observe the mental and social processes that create contagion. For example, predicting
the success of motion pictures before they are released is widely known to be
quite difficult (Litman 1983). Jack Valenti, former president of the Motion Picture
Association of America, said
With all the experience, with all the creative instincts of the wisest people
in our business, no one, absolutely no one can tell you what a movie is
going to do in the marketplace... Not until the film opens in a darkened
theater and sparks fly up between the screen and the audience can you say
this film is right?
By analogy, the reason that writers pull quotes, especially paragraph-length quotes,
and display them as I have just done is often to convey a narrative, to give the reader
a historical sense of a past narrative that had impact and might again have impact on
the reader if it is repeated just as it was perfectly worded. As with jokes, a narrative
has to be delivered just right to be effective. Similarly, with music we want to hear
again and again a performance that sounds exactly right and by the perfect performer.
This is why narratives are difficult to study, and why there are limitations in textual
analysis involving word counts or n-gram counts to quantify and study them.
Since around the beginning of the twentieth century, scholars from a wide array
of disciplines began to think that narratives, stories that seem outwardly to be of
entertainment value only, are really central to human thinking and motivation. For
example, in 1938 the existentialist philosopher Jean-Paul Sartre wrote
A man is always a teller of tales, he lives surrounded by his stories and
the stories of others, he sees everything that happens to him through them;
and he tries to live his life as if he were recounting it
The tales tend to have human interest, if only suggested. By analogy, when asleep
at night, narratives appear to us in the form of dreams. We do not dream of equations
or geometric figures without some human element. Neuroscientists have described
dreaming, which involves characters, settings, and a hierarchical event structure,
as a story-telling instinct that resembles in brain activity a form of spontaneous
confabulation caused by lesions of the anterior limbic system and its subcortical
connections (Pace-Schott 2013).
Anthropologists, who conduct research on the behavior of diverse tribes around
the world, have observed a "universal" that people "use narrative to explain how
things came to be and to tell stories" (Brown 1991). Visitors to any human society
will observe people facing each other, sitting around the television - or the
campfire - together, and vocalizing, and more recently, tweeting, stories, at the
same time waiting to learn other's reactions, interested in feedback that will either


---Economics-2017-0-08.txt---
confirm or disconfirm one's thoughts. It seems that the human mind strives to reach
enduring understanding of events by forming them into a narrative that is embedded
in social interactions.
Some have suggested that it is stories that most distinguish us from animals,
and even that our species be called Homo narrans (Fisher 1984) or Homo narrator
(Gould 1994) or Homo narrativus (Ferrand and Weil 2001) depending on whose
Latin we use. Might this be a more accurate description than Homo sapiens, i.e.,
wise man? It is more flattering to think of ourselves as Homo sapiens, but not necessarily
more accurate.
It is important to note that narratives may not generally be acted upon reflectively,
since, in the words of psychologists Schank and Abelson (1977), they may be taken
as scripts. When in doubt as to how to behave in an ambiguous situation, people may
think back to narratives and adopt a role as if acting in a play they have seen before.
The narratives have the ability to produce social norms that partially govern our
activities, including our economic actions. The "prudent person rule" in finance is
one of those norms that has economic impact: fiduciaries and experts do not have the
right to act on their own judgment. They must instead mimic the "prudent person,"
and this, in effect, means following a script (Shiller 2002).
Popular narratives may have a spirit of "us versus them," a Manichean tone of
revealed evil described of others in the story. Jokes are quite often at somebody else's
expense - members of some other group. In extreme cases, they may be focused on
events as evidence of some kind of imagined conspiracy.
Of course, it is rational for people to be alert to conspiracies, since history is filled
with actual examples of them. But the human mind seems to have a built-in interest
in conspiracies, a tendency to form a personal identity and loyalty to friends built
around perceived plots of others. This disposition appears to be related to human
patterns of reciprocity, of vengeance against presumed enemies, a tendency that has
been found relevant to economic behavior (Fehr and Gächter 2000). The disposition
may be amplified individually by brain damage, into a paranoid personality disorder
that is recognized in the Diagnostic and Statistical Manual, Fifth Edition (DSM V)
of the American Psychiatric Association as afflicting 2.3 percent to 4.4 percent of
the US adult population.4
There is a daunting amount in the scholarly literature about narratives, in a number
of academic departments, and associated concepts of memetics, norms, social
epidemics, contagion of ideas. While we may never be able to explain why some
narratives "go viral" and significantly influence thinking while other narratives do
not, we would be wise to add some analysis of what people are talking about if we
are to search for the source of economic fluctuations. We economists should not just
throw up our hands and decide to ignore this vast literature. We need to understand
the narrative basis for macroeconomic fluctuations, and to think about how narrative
economics ought to be more informing of policy actions now and in the future.
Narratives are major vectors of rapid change in culture, in Zeitgeist, and ultimately
in economic behavior. Spreading narratives, often many parallel narratives


---Economics-2017-0-09.txt---
around a common theme, have been creating cultural change long before the
Internet revolution, when the appearance of the parallel phenomenon of computer
viruses, which spread by contagion from computer to computer, popularized the
virus metaphor.
Talk of the epidemic spread of narratives goes back centuries. David Hume wrote
in 1742
... when any causes beget a particular inclination or passion, at a certain
time and among a certain people, though many individuals may escape
the contagion, and be ruled by passions peculiar to themselves; yet the
multitude will certainly be seized by the common affection, and be governed
by it in all their actions ,5
McKay (1841) drew attention to the contagious spread of "extraordinary popular
delusions." Gustave Le Bon said in his book Psychologie des foules ( The Crowd):
"Ideas, sentiments, emotions, and beliefs possess in crowds a contagious power as
intense as that of microbes" (Le Bon 1895). Related terms were collective consciousness
(Durkheim 1897), collective memory (Halbwachs 1925), and memes
(Dawkins 1976). Contagion is also related to issues of identity, since an important
part of most narratives describing past events involving people is that one must
imagine oneself as another person, and thus momentarily at least feel a shared identity
with someone else (Akerlof and Kranton 201 1).
III. Dimensions of Narratives Normal and Abnormal
The psychologist Jerome Bruner, who has stressed the importance of narratives,
wrote that we should not assume that human actions are ever driven in response to
purely objective facts:
I do not believe that facts ever quite stare anybody in the face. From a
psychologist's point of view, that is not how facts behave, as we well know
from our studies of perception, memory, and thinking. Our factual worlds
are more like cabinetry carefully carpentered than like a virgin forest inadvertently
stumbled upon.6
Narratives are human constructs that are mixtures of fact and emotion and human
interest and other extraneous detail that form an impression on the human mind.
Psychiatrists and psychologists recognize that mental illness is often an extreme
form of normal behavior, or a narrow disruption of normal human mental faculties.
So we can learn about the complexities of normal human narrative brain processing
by looking at dysnarrativia, abnormal narrative phenomena. Neuroscientists Young
and Saver (2001) listed some of its varied forms: arrested narration, undernarration,
denarration, confabulation.
Schizophrenia may be regarded as another brain anomaly related to narrative
problems (Gaser et al. 2004). Schizophrenia has aspects of being a disorder of
narrative, as it often involves the hearing of imaginary voices delivering a fantastic,


---Economics-2017-0-10.txt---
and disordered, narrative (Saavedra, Cubero, and Crawford 2009). Hearing voices
as a symptom of schizophrenia is correlated with volume deficits in specific brain
areas (Gaser et al. 2004). The narrative disruption found in autism spectrum disorder
also is related to brain anomalies (Losh and Gordon 2014; Pierce et al. 2001).
Narrative psychology is also related to the psychologists' concept of framing
(Kahneman and Tversky 2000; Thaler 2015, 2016). If we can create an amusing
story that will get retold, it can establish a point of view, a reference point, which
will have influence on decisions. It is also related to the Kahneman and Tversky
(1973) representativeness heuristic, whereby people form their expectations based
on similarity of circumstance to some idealized story or model, and tend to neglect
base-rate probabilities.
Psychologists have noted an affect heuristic, whereby people who are experiencing
strong emotions, such as fear, tend to extend their feelings to unrelated happenings
(Slovic et al. 2007). Sometimes people note strong emotions or fears about
possibilities that they know logically are not real, suggesting that the brain has multiple
systems for assessing risk, and the "risk as feelings" hypothesis that some
primitive brain system more connected to palpable emotions has its own heuristic
for assessing risk (Loewenstein et al. 2001.)
In joint work with Goetzmann and Kim (2016), using data from a questionnaire
survey I have been conducting with institutional investors and high-income
Americans since 1989, we found that these people generally have exaggerated
assessments of the risk of a stock market crash, and that these assessments are influenced
by the news stories, especially front page stories, that they read. One intriguing
finding was that an event such as an earthquake could influence estimations of the
likelihood of a stock market crash. The respondents in our survey gave statistically
significantly higher probabilities to a stock market crash if there had been an earthquake
within 30 miles of their zip code within 30 days, triggering the affect heuristic.
It seems reasonable to hypothesize that local earthquakes start local narratives
with negative emotional valence. Analogous evidence has been found of seemingly
irrelevant events with narrative potential having effects on economic or political
outcomes: the effect of World Cup outcomes on economic confidence (Dohmen et
al. 2006) and of background music in advertisements on viewers (Boltz, Schulkind,
and Kantra 1991).
The news media, for which the survival of any one organization is never assured
in a competitive news marketplace, must become adept at managing the news to
make narratives work in their favor. Moreover, news media embellish the emphasized
news with human interest stories. Given the large and often dramatic degree
of news media coverage of disasters - natural and man-made - and of crimes and
human interest stories, it seems clear that news media believe that covering such
events will result in increased sales and attention for their news products. However,
little attention has been given to the impact of such news stories on other stories in
the same publication.
IV. Epidemic Models
The Kermack-McKendrick (1927) mathematical theory of disease epidemics
marked a revolution in medical thinking because it gave a realistic framework for


---Economics-2017-0-11.txt---
understanding the all-important dynamics of infectious diseases. Their simplest
model divided the population into three compartments: susceptibles, infectives, and
recovereds, hence it is called an SIR model or compartmental model; S is the number
of susceptibles, people who have not had the disease and are vulnerable; I is the
number of infectives, people who have the disease and are actively spreading it; R
is the number of recovereds, who have had the disease and gotten over it and are no
longer capable of catching the disease again or spreading it. The total population
N = S + I + Ris assumed constant.
The key idea of the Kermack-McKendrick mathematical theory of disease epidemics
was that in a thoroughly mixing population the rate of increase of infectives
in a disease epidemic is equal to a constant contagion rate c > 0 times the product
of the number of susceptibles S and the number of infectives I minus a constant
recovery rate r > 0 times the number of infectives. Each time a susceptible meets
an infective there is a chance of infection. The number of such meetings per unit
of time depends on the number of susceptible-infective pairs in the population. The
recovery from the disease is assumed for simplicity to occur in an exponential decay
fashion, instead of the more usual notion of a relatively fixed timetable for the course
of the disease. The three-equation Kermack-McKendrick SIR model is
= -cSI'
dt
% = cSI-ri,
TT dt = rL
TT dt
The same SIR model can of course be used to describe the word-of-mouth transmission
of an idea. Here, the contagion rate is the fraction of the time in an encounter
that an infective, a person interested in and accepting of a story, effectively
convinces the susceptible enough of the story to spread it further. Many encounters
may be needed before a particular person is infected. The removal rate might be
described as the rate of forgetting, of simple decay of memories, but there is also
cue-dependent forgetting. This removal also occurs as the repertory of other current
stories evolves away from this story, so that there are declining cues for the memory;
this story seems less connected, less apt, or even superficially contrary to current
theories and prejudices. It might be plausible to suppose, as the model does, that
contagion rates and removal rates are both constant through time, if they are intrinsic
to the narrative. Inaccurate retellings of the narrative that leave out its essential
interest value, because of transmission error, just don't survive; the parameter c
refers to successful spread of the core interest value of a story.
Even though direct face-to-face communications of ideas is less important in
modern times because of the communications media, it still remains a workable
model. The core model may apply no matter how people may connect with each
other. A survey of individual investors shows that on average they talked to 20 people
about a typical investment, and only 23 percent did any analysis themselves
(Shiller and Pound 1989).


---Economics-2017-0-12.txt---
This model implies that from a small number of initial infectives, the number
infected and contagious itself follows a bell-shaped curve, rising at first, then falling.
A mutation in an old much-reduced disease may produce a single individual who
is infective, and then there will be a lag, possibly a long lag if c is small, before the
disease has infected enough people to be noticed in public. The epidemic will then
rise to a peak, and then fall and come to an end without any change in the infection
or removal rates, and before everyone is infected.
Figure 2 shows an example, implied by the three equations above, with N = 100
people, of very small transmission of infection per unit of time on any one exposure,
70 = 1, c - 0.005, r = 0.05. In this case, almost 100 percent of the population
eventually gets infected. During an actual epidemic, public attention tends to focus
on the number of infectives, seen here as a bell-shaped curve skewed to the right.
Not everyone will ever catch the disease, as some completely escape for lack of
effective encounter with an infective, and gradually it gets safer and safer for them
because the infectives are becoming more and more reduced in number as they get
over the disease and are immune to it, so there are not enough new encounters to
generate sufficient new infectives to keep the disease on the growth path. Eventually,
the infectives almost disappear, and the population consists almost entirely of susceptibles
and recovereds.
What creates a big disease that ultimately reaches a lot of people (the total ever
infected and recovered as measured by 7?^)? It is a function of the ratio c/r of the
two parameters. As time goes to infinity, the number of people who have ever had
the disease goes to a limit R ^ (called the size of the epidemic) strictly less than N. It
follows directly from the first and third equations that dS/dR - -(c/r)S and the initial
condition 70 that S = (N - 70) e ~&R and since 7^ = 0, N = we have
C_
C_ r
If we could choose c and r, we could make the size of the epidemic anything
we want between 70 and N. Higher c/r corresponds to higher size of epidemic Rœ,
regardless of the level of c or of r, while higher c itself, holding c/r constant, yields a
faster epidemic. If we define "going viral" by R ^ > N/2, then we see this happening
in this example when c/r > 0.014.
If we multiply both parameters, c and r, by any constant a, then the same equations
are satisfied by S(at), I(at), R(at). This means, depending on the two parameters
c and r, that there can be both fast and slow epidemics that look identical if the
plot is rescaled. If we also vary the ratio c/r, we can have epidemics that play out
over days and reach 95 percent of the population, or epidemics that play out over
decades and reach 95 percent of the population, or epidemics that play out over days
and reach only 5 percent of the population, or epidemics that play out over decades
and reach 5 percent of the population. But in each case, we can have hump-shaped
patterns of infectives that on rescaling look something like that in Figure 2.
The Kermack-McKendrick SIR model has formed a starting point for mathematical
models of epidemics that have, over 90 years since, produced a huge literature. The
basic compartmental model, the SIR model, has been modified to allow for gradual
loss of immunity, so that recovereds are gradually transformed into susceptibles again


---Economics-2017-0-13.txt---

(the SIRS model). This is the same as the SIR model above except that a term +sR
is added to the right-hand side of the first equation and -sR to the right-hand side of
the third equation, where s > 0 is a resusceptibility rate. In this model the infectives'
path may, depending on parameters, look similar to that in Figure 2 but approaching
a nonzero horizontal asymptote as time increases: the infectives never effectively
disappear. The SIR model can also be modified so that an encounter between a susceptible
and an infected leads to an increase in exposed E, a fourth compartment,
who become infective later (the SEIR model). The model has also been modified to
incorporate partial immunity after cure, birth of new susceptibles, the presence of
superspreaders with very high contagiousness, geographical patterns of spread, etc.
V. Applications of the Kermack-McKendrick to Social Epidemics
In applying the compartmental model to social epidemics, to epidemics of ideas,
certain changes seem natural. One thought is that the contagion rate should decline
with time, as the idea becomes gradually less exciting. Another way of modeling
approximately the same notion is due to Daley and Kendall (1964, 1965), who said
that the Kermack-McKendrick model could be altered to represent that infectious
people might tend to become uninfectious after they meet another infectious person
or a recovered person, because they then think that many people now know the
story, making it no longer new and exciting, and thus they choose not to spread the
epidemic further.
Bartholomew (1982) argued that when variations of the Kermack-McKendrick
model are applied to the spread of ideas, we should not assume that ceasing to infect
others and forgetting are the same thing. Human behavior might be influenced by an
old idea not talked about much but still remembered. This has been called "behavioral
residue" (Berger 2013).
There is now a substantial economics literature on network models: see the recent
Handbook of the Economics of Networks (Bramoullé, Galeotti, and Rogers 2016),
though there appears to be little in the way of such behavioral models (the word


---Economics-2017-0-14.txt---
narrative does not appear even once). Some of these modified SIR models involve
complex patterns of outcomes, and sometimes cycles. Geographic models of spread
are increasingly complicated by worldwide social media connections (Bailey et al.
2016).
Some SIR models (surveyed in Lamberson 2016) dispense with replacing the
idea of random mixing and choose instead a network structure. There may be strategic
decisions whether or not to allow oneself to be infected, and the fraction of
the population infected may enter into the decision (Jackson and Yariv 2005). There
are variations of models that describe individuals as adopting a practice not merely
through random infection but through rational calculations of the information
transmitted through their encounters with others (Banerjee 1992; Bikhchandani,
Hirshleifer, and Welch 1992).
Currently, there is not any one single model for social epidemics; instead, there
is a tool-bag of models for understanding epidemics depending on their circumstances.
But we can still refer to the original Kermack-McKendrick SIR model as
a metaphor for the class of dynamic models that rely on contagion and recovery.
These other models typically still take analogues of c and r as fundamental parameters,
still have the property that a slight tweak of c upward or r downward may set
in motion a chain of events that sets off an epidemic with a lag, so that on the date
the epidemic is first publicly noticed, the causes are now in fairly distant history and
hard to discern.
Even though direct face-to-face communication of ideas is less important in
modern times, because of the communications media, the Kermack-McKendrick
model still remains an important model for economics. The core model may apply
no matter what way people may connect with each other. There is a concern that
modern communications media (the press, the Internet, etc.) make the SIR model
less accurate in describing social epidemics. But the change may instead be roughly
within the framework of the model, with higher contagion rates for narratives due to
the social media automatically directing narratives to people with likely interest in
them, regardless of their geography. Moreover, marketing literature finds that direct
word-of-mouth communications still beat other forms in persuasiveness (Herr,
Kardes, and Kim 1991), and the marketing profession has responded by promoting
word-of-mouth seeding strategies and television ads that feature actors portraying
people with whom the common person can identify and simulating direct interpersonal
word of mouth. In considering the Internet and social media and the SIR
model, Zhao et al. (2013) argue for a modified SIR model where analogues to the
parameters c and r are both increased by the new media.
Bauckhage (2011) showed evidence that the SIRS variant of the KermackMcKendrick
compartmental model fits time series data reasonably well on Internet
memes from Google Insights (now Google Trends.) He looked at silly recent
Internet viruses like the "O RLY?" (Oh, really?) meme that displayed nothing more
than a picture of a cute owl with what would appear to be a puzzled facial expression.
Because the memes are largely nonsensical, we might expect them to follow a
course independent of other ideas and thus to fit the SIRS model well, as he found.
Roughly the same hump-shaped pattern of infectives was found again and again.
Michel et al. (201 1) showed evidence that mentions of famous people in books
tends to follow a hump-shaped pattern through time, a slow epidemic, something like


---Economics-2017-0-15.txt---
those observed by Bauckhage, but over decades, rather than months or years. Some
examples from economists possibly illustrate a compartmental model slow epidemic,
a hump-shaped pattern that plays out over centuries. Adam Smith (1723-1790) has
a Google Ngrams plot that did not peak until the 1880s, and has since been declining
slowly for over a century. Karl Marx (1818-1883) has a Google Ngrams plot that
did not peak until the 1970s.
Taking the Kermack-McKendrick model as an illustration or metaphor, it would
appear that this learning theory might imply that the total impact of an idea is measured
by its Rqc long after the idea has ceased to be infectious. The long-run impact
of an idea then is not testimony to its correctness, but rather to the c/r in the initial
epidemic that popularized it.
In applying the SIR model to social epidemics, we need also consider that narratives
affect the contagion rates of other narratives that are seen to be on the same
topic or that further inform. One new narrative may remind one of another that has
been lying fairly dormant. A wave of similar narrative epidemics can appear.
One must also consider that stories tend to be strategic, fine-tuned by politicians,
advertisers, or other interested parties (Akerlof and Shiller 2015). People with an
aptitude for storytelling see great fortune in monkeying with stories in an effort
to have them go viral. Falk and Tiróle (2016) call them "narrative entrepreneurs."
Glaeser (2005) refers to the "supply of stories," exemplified by hate-creating stories
crafted, produced, fashioned for political advantage. Gino, Norton, and Weber
(2016) describe concoctors of excuses as "motivated Bayesians," meaning that these
people start to believe the fake news that they generated for self-advantage.
Much of the purposeful generation of new stories is for individual profit, but
some of it is done in a patriotic attempt to support confidence and good values. The
process of perfecting stories is one of trial and error, and responsiveness to the kinds
of things that go viral. A successful story entrepreneur may have a lifetime of failed
attempts and perhaps only one or a few breakthroughs. The rare breakthroughs make
the entire enterprise worthwhile.
VI. Narrative Epidemics of Economic Theories
Let us consider as an example the narrative epidemic associated with the Laffer
Curve, a diagram created by economist Arthur Laffer in the theory of public finance.
Let's curve the curve: look at Kermack-McKendrick curves for the narrative of the
Laffer Curve. Looking at the Laffer Curve is an arbitrary choice among a vast number
of narratives, but one that can be searched on databases easily since the name
Laffer Curve appears to connect well to a single narrative with clear economic
impact. This narrative exploded into public attention in 1978 focusing on a 1974
event involving the US economist Art Laffer. In Figure 3, a plot of the frequency
of references to the Laffer Curve by year, from ProQuest News and Newspapers
and by Google Ngrams, somewhat resembles the infectives plot of the KermackMcKendrick
model in Figure 2.
For both, the curves are choppy relative to the model, due to some publication
noise, but show some of the hump shape. For news and newspapers, the frequency
of use of the term Laffer Curve initially rises strongly, from 1977 to 1978, and then
rises rapidly for three more years, peaking in 1981 . Then it falls and continues to fall


---Economics-2017-0-16.txt---

for many years, as in the model Figure 2, though it shows something of an upturn
again after 2000. For books, the curve rises more slowly, for five of six consecutive
years, from 1978 to 1983, before entering a long period of slow decline, as in
Figure 2.
The Laffer Curve owes much of its contagion to the fact that it was seen as justifying
major tax cuts. The Laffer Curve's contagion related to fundamental political
changes associated with Ronald Reagan's election as US president in 1980 with
his commitment to cutting taxes (and with Margaret Thatcher, but not the socialist
François Mitterrand elected around that time).
The Laffer Curve is an inverted U curve relating tax revenue to tax rates, a theory
that was used to justify cutting taxes on high-income people since high taxes
incentivize them to produce less. The notion that taxes might reduce the incentive
to earn income and create jobs was hardly new: the idea was expressed as long
ago as Adam Smith in the eighteenth century.7 Andrew Mellon, US Treasury
Secretary, 1921-1932, was famous for his "trickle down" economics, and, along
with US President Calvin Coolidge (1923-1929), successfully argued for reduction
of income taxes that had remained high for a while after World War I. But then the
Mellon name began to fade (outside of Carnegie-Mellon University), and the theory
lost persuasive narrative.
The story of the Laffer Curve did not go viral in 1974, the reputed date when
Laffer first introduced it. Its contagion is explained by a literary innovation that was
first published in a 1978 article in National Affairs by Jude Wanniski, editorial writer


---Economics-2017-0-17.txt---
for the Wall Street Journal. Wanniski wrote the colorful story about Laffer sharing
a steak dinner at the Two Continents in Washington, DC in 1974 with top White
House powers Dick Cheney8 and Donald Rumsfeld,9 as well as Wanniski. Laffer
drew his curve on a napkin at the restaurant table. When news about the "curve
drawn on a napkin" came out, with Wanniski's help, this story surprisingly went
viral, so much so that it is now commemorated. A napkin with the Laffer Curve can
be seen at the National Museum of American History, and a reenactment video of
the restaurant-napkin event depicting Laffer drawing his curve on a napkin is available
on Bloomberg.
Why did this story go viral? It is always hard to explain these things. Laffer
himself said after the Wanniski story exploded that he could not even remember
the event, which had taken place four years earlier.10 But Wanniski was a journalist
who sensed that he had the right elements of a good story. The key idea as Wanniski
presented it, is indeed punchy: at a zero-percent tax rate, the government collects
no revenue. At a 100 percent tax rate the government would also collect no revenue,
because people will not work if all of the income is taken. Between the two
extremes, the curve, relating tax revenue to tax rate, must have an inverted U shape.
Now, as Wanniski pointed out with fanfare in the opening line of his 1978 article,
this means that for any feasible tax revenue except one at the very top of the Laffer
Curve, there are two tax rates that will generate this U shape, one at the left with a
high-income level and low tax rates, the other at the right with a low-income level
and high tax rates.
Here is a notion of economic efficiency easy enough for anyone to understand.
Wanniski suggested, without any data, that we are on the inefficient side of the
Laffer Curve. Laffer's genius was in narratives, not data collection. The drawing of
the Laffer Curve seems to suggest that cutting taxes would produce a huge windfall
in national income. To most quantitatively inclined people unfamiliar with economics,
this explanation of economic inefficiency was a striking concept, contagious
enough to go viral, even though economists protested that we are not actually on
the inefficient declining side of the Laffer Curve (Mirowski 1982). It is apparently
impossible to capture why it is doubtful that we are on the inefficient side of the
Laffer Curve in so punchy a manner that it has the ability to stifle the epidemic.
Years later Laffer did refer broadly to the apparent effects of historic tax cuts (Laffer
2004); but in 1978 the narrative dominated. To tell the story really well one must
set the scene at the fancy restaurant, with the powerful Washington people and the
napkin.
Another fad, associated with multiple narratives, appeared around the same time
as the Laffer Curve: "Rubik's Cube." Rubik's Cube was bigger than Laffer Curve on
ProQuest News and Newspapers, but smaller than Laffer Curve on Google Ngrams.
They both show similar hump-shaped paths through time.
The Laffer Curve epidemic was followed during the Reagan presidency (198 1-
1989) by a reduction in the top US federal income tax bracket from 70 percent to


---Economics-2017-0-18.txt---
28 percent. The top-bracket US corporate profits tax rate was cut from 46 percent to
34 percent during the Reagan administration. The top US capital gains tax rate was
reduced from 28 percent to 20 percent in 1981 (though raised back to 28 percent
again in 1987 during the Reagan presidency). If the Laffer Curve epidemic had even
a minor effect on these changes, it must have had tremendous impact on measures
of output and prices.
Figure 4 shows Google Ngrams results for several other examples of economic
theories, though less appropriate for our purposes because they are not just narratives,
they are more substantively original than the Laffer Curve. Still, even important
original theories have associated narratives and might have SIR dynamics. The
IS-LM model (Hicks 1937); the multiplier-accelerator model (Samuelson 1939);
the overlapping generations model (Samuelson 1958); and the real business cycle
model (Kydland and Prescott 1982) - all show hump-shaped patterns akin to those
that can be produced by the Kermack-McKendrick model, as seen in Figure 2. In
three of the cases, the epidemic first became visible more than a decade after the
model was introduced, a phenomenon that is also explainable within the KermackMcKendrick
framework, where epidemics may go unobserved for a while after they
have just started from very small beginnings.
VII. Feedback Variations: Multipliers and Bubbles
To think of the events surrounding the explosion of the Laffer Curve as a social
epidemic is to think about feedback. The original Kahn-Keynes multiplier model
in macroeconomics (Kahn 1931, Keynes 1936), with its famous "multiple rounds
of expenditure" might be described as a sort of epidemic model but with the contagion
rate c replaced by the marginal propensity to consume (MPC), and with a zero
removal rate r. It was an appealing feedback model, on paper. Any stimulus to the
economy, call it /0 to show parallel with our epidemic model, increases someone's
income. This individual, who then spends MPC of that income, generating income
for yet another who spends MPC of that income, and so on, such that national
income rises gradually, and eventually the result is an increase of national income
of 70/(l - MPC).
In practice, however, the purely Keynesian form of contagion is limited. The
permanent income hypothesis suggests that Keynesian contagion may be very low
if people do not believe that the spurt in income will be permanent. Some research
supports low multipliers, for example Shapiro (2016) reports on research of his
own and of coauthors, based on survey questions conducted by the University of
Michigan's Survey of Consumers, of the impact of fiscal stimulus policies. The presented
counterfactuals (what would you have done differently if you did not receive
the tax rebate check) found that, at least at the time of their survey, the marginal
propensity to consume may have been quite low, between one-quarter and one-third,
suggesting a multiplier little different from one.
A different special kind of epidemic model describes speculative bubbles.
Speculative asset prices themselves would not generally be well modeled directly in
terms of this model since the model yields smooth-through-time paths. Most speculative
asset prices are nearly random walks on a day-to-day basis. The reason is
obvious: if it were possible for smart money to predict the day-to-day price changes


---Economics-2017-0-19.txt---
even reasonably well, they could become rich very fast, and they would take over
the market. There are both smart money and noise traders in the market, the former
attempting to predict the latter.11 There is a large literature on value investing that
confirms that there has been a long-term return to doing it.12
In a bubble, the contagion is altered by the public attention to price increases: rapid
price increases boost the contagion rate of popular stories justifying that increase,
heightening demand and more price increases. In a stock market bubble, these might
be stories of the companies with glamorous new technology and of the people who
created the technology. In a housing bubble, these might be stories of clever people
making a fortune flipping houses. There can also be price-to-GDP-to-price feedback,
if speculative price increases stimulate purchases and hence more increases,
price-to-corporate profits-to-price feedback, and price-to-regulatory laxity-to-price
feedback, all mediated by changing narratives (Shiller 1984, 2000, 2015).
The impact of the epidemic on the asset return would depend on the speed of the
epidemic relative to the discount rate. If the speed is very low, there would be very
little impact on short-term returns. Then the asset price changes would find little
short-term serial correlation through time, and would be approximately a random
walk over short time periods.


---Economics-2017-0-20.txt---
VIII. Narratives During the Sharpest (Though Not the Worst) US Contraction Ever,
1920 to 1921
In looking for the narrative basis of economic recessions, which might be hard
to see since narratives are not easy to measure, it would appear that we would have
the most luck looking at really big ones: 1920-1921 was the sharpest US recession
since modern statistics are available. The US Consumer Price Index switched
suddenly from inflation to deflation: between June 1920 and June 1921, during the
Depression, it fell 16 percent, the sharpest one-year deflation ever experienced in the
United States. The Index of Wholesale Prices fell much more: 45 percent over the
same time interval, its sharpest decline ever.13 The cyclically adjusted price earnings
ratio (real S&P Composite Index divided by ten-year average of real earnings,
Campbell and Shiller 1988) fell to 4.78 by December 1920, by far the lowest ratio in
the entire history of the US stock market since 1871. In contrast, note that the ratio
is currently around 28. The conventional price-earnings ratio with 12-month trailing
earnings was also extremely low, at 8.51. This was the recession that, because of its
severity, influenced the fledgling National Bureau of Economic Research (NBER)
toward a research program emphasizing the study and dating of business cycles.
Surprisingly, the online NBER Working Paper Series, almost a hundred years
later, when searched, has virtually nothing to say about what caused this spectacular
depression. Why, after all, did it happen?
Milton Friedman and Anna J. Schwartz, in their Monetary History of the United
States , have given the most influential account. According to them, the 1920-1921
contraction has a single identifiable cause: an error made by the fledgling Federal
Reserve to raise the discount rate to trim out-of-bounds inflation in 1919 caused
by their carelessly over-expansionary policy right after World War I, leading to a
necessity to take strong measures against inflation in 1920. Benjamin Strong, the
president of the New York Fed, was on a long cruise starting December 1919, and
was unable to prevent Federal Reserve Banks (which did not coordinate their policies
with each other so much back then) from raising the discount rate as much as a
full percentage point in one shot in January 1920.
The Fed was new then, having opened its doors only in 1914, and so it is not surprising
that they could have made mistakes and implemented overly strong swings
in policy. But let us not jump to conclusions about the Fed having caused the depression.
Other remarkable things were going on too. If we are going to single out significant
events for study, we should keep in mind that important events are usually the
result of the confluence of many factors, and often changing narratives are at work
in those factors.
There was a background then of horrible recent events: World War I, which ended
only 14 months before the start of the Depression; an influenza epidemic even more
deadly than the war that started during the war and was not quite over; and a series
of postwar race riots in the United States. But how did these events translate into
current narratives in 1920?


---Economics-2017-0-21.txt---
I am reminded of a book about the 1920s, Only Yesterday: An Informal History
of the 1920s, written by a popular writer, Frederick Lewis Allen, published in 1931,
which commented on the early stages of the Great Depression in its afterword.14
His history emphasizes all the little silly fads and diversions that occupied people's
attentions, and might be considered a history of silliness, except that some of the
events portrayed were ominous or deadly.
The 1920-1921 recession also began with the early stages of widespread public
fear of Communism. A computer search of news and newspapers on ProQuest
shows relatively little use of the term Communist before 1919. Certainly, Karl Marx
and Friedrich Engels wrote their Communist Manifesto long before, in 1848, and
newspapers did write about them, though much less frequently. Most people could
not be bothered to learn some abstract Communist theory if there had never been a
real Communist revolution. Marx and Engels often were seen as minor eccentrics.
But after 1918, there was a sudden jump in focus on Communism.
Allen (1931, p. 38) writes of the Big Red Scare period in America
They [ Americans ] were listening to ugly rumors of a huge radical conspiracy
against the government and institutions of the United States. They had
their ears cocked for the detonation of bombs and the tramp of Bolshevist
armies. They seriously thought - at least millions of them did, millions of
otherwise reasonable citizens - that a Red revolution might begin in the  United States the next month or next week.
Notably, there is a very dramatic story of the sudden advance of communism
after World War I, and its brutality: the murder of Czar Nicholas II and his entire
family by Communists on June 17, 1918. It is a particularly repellent story of an
internationally recognized family (readers all over the world had seen them in
photogravures in newspapers and postcards, much as we routinely see the British
Royalty today), who were asked to dress up, then were seated and executed at
the surprise appearance of a firing squad. The announcement of the Czar's death
appeared in newspapers by June 28, 1918, but not all the gory details were made
available to the public until the Report of the Inquiry at Ekaterinburg in September
1920, in the middle of the Depression. Then this story - certainly a contagious
story, and one that would attract readers - was reported in excruciating detail by
newspapers around the world. The story lives on today; it persists in diminished
form as a long, slow epidemic of the kind with low contagion rate but equally low
removal rate.
There was also an oil price shock, which generated its own narratives. US oil
prices rose over 50 percent from mid- 19 19 to the end of 1920, in the middle of the
Depression. Because consumer prices were falling then, the real price increase was
even greater, and by the end of 1920, real oil prices were at the highest level in the
twentieth century before 1979.


---Economics-2017-0-22.txt---
Some newspapers did offer a straightforward interpretation of the high oil prices:
temporary supply disruptions in Russia and Mexico because of unrest there. But,
another explanation had a higher contagion rate. High oil prices were then attributed
to strong demand at a time when automobiles and other energy-intensive devices
were proliferating despite the Depression. An article by W. W. Woods in the Los
Angeles Times in September 1920 said: "In the last two years the growing appetite
of the internal combustion engine for gasoline has been more than six times what
it was in 1901." He concluded that the nation's oil supply would be exhausted in
18 years. Moreover, coal prices were high in late 1920, and retail stores of coal for
home furnaces were reportedly exhausted as winter approached. Clayton (2015)
documents the history of this narrative, from Teddy Roosevelt's 1908 White House
conference on exhaustion of resources, to even more dire predictions by May 1920
from David White, chief geologist at the USGS, that oil production would peak
"probably within five years and possibly in three years." (In fact, new oil discoveries
quickly brought the price of oil down after 1920.)
All of these events - World War, the influenza epidemic, the race riots, the Big
Red Scare, the oil shock - were associated with hugely unsettling narratives that
could have led to a sense of economic uncertainty that might have discouraged
discretionary spending of households and slowed down hiring decisions of firms
around the world. These certainly sound like more significant potential causes than
New York Fed President Benjamin Strong's decision to take a cruise when he was
needed.
There were also more subtle narratives that might have brought on the recession.
A story was afloat in 1920 that the Consumer Price Index would eventually come
back down to its level in 1913, just before World War I. Not everyone expected
this of course, but, obviously, with such deflation expectations, many would think
one should wait to buy until prices fell, but large numbers of people waiting to buy
brought on a depression. For those with such deflation expectations, the expected
real interest rate was super high. Not a single newspaper represented by ProQuest
News and Newspapers made any reference at all to real interest rates during this
depression, even though the concept of the real interest rate had been introduced to
economists by John Bates Clark (1895). The concept of real interest rate just had
not gone viral yet (and you might say it still hasn't). But many people certainly
understood why they should postpone buying and avoid borrowing when massive
deflation is expected.
In 1913 in the United States, a retail price index, predecessor to the Consumer
Price Index (CPI), attracted great attention. The CPI that we have now began with a
base value of 10 in 1913. By 1920, the index had doubled to 20, and by mid-1921 it
had fallen to 17. The price increase between the end of the war and 1920 was widely
blamed on people who were labeled with the newly popular word "profiteer." The
Oxford English Dictionary gives first use of the word profiteer as occurring in 1912,
but its use did not take off until late in World War I and after. None of the usual
synonyms for profiteer (racketeer, exploiter, black marketer, bloodsucker, vampire)
seem to have the same meaning and association with wartime fortune building at the
expense of war heroes. The word is a play on the much older word privateer, meaning
a pirate who has the support of a hostile government. Wartime narratives spread
of customers angry at high prices chastising their milkmen and telling their butcher


---Economics-2017-0-23.txt---
they would stop eating meat altogether to spite them. But the narrative epidemic was
unfazed by the end of the war.
The popular author Henry Hazlitt wrote in 1920
Hence, we have self-righteous individuals on every corner denouncing
the outrages and robberies committed by a sordid world. The butcher is
amazed at the profiteering of the man who sells him shoes; the shoe salesman
is astounded at the effrontery of the theatre ticket speculator; the theatre
ticket speculator is staggered at the high-handedness of his landlord;
the landlord raises his hands to high heaven at the demands of his coal
man, and the coal man collapses at the prices of the butcher. 15
US Senator Arthur Capper was reported in January 1920 as saying "Profiteers
are more dangerous than Reds" and urged consumers to "boycott the profit hogs
by refusing to buy goods offered at extortionate prices."16 Perhaps the 1920-1921
recession might better be thought of as the 1920-1921 consumer boycott.
But it wasn't just anger at profiteers that curtailed consumption. The story that
consumer prices would fall dramatically, a story which had good contagion since it
was associated with the profiteer narrative, was not so much told as intimated thousands
of times during the 1920-1921 recession when newspapers heralded some
individual prices that had indeed fallen to 1913 or 1914 levels. This is typical newspaper
reporting, with writers attempting to make an otherwise marginally interesting
story engaging to general readers. The newspapers knew that at a time of deflation,
readers were responsive to such stories because to the untrained mind, it seemed
natural that once the war was over, prices would return to their old levels: very
important to someone trying to decide whether to buy a new house or car.
Waiting to buy discretionary items until the prices fell seemed an obvious strategy
to many, but doing so brought on a depression. As one observer wrote in 1920:
"The buying public knows that the war is over and has reached the point where it
refuses to pay war prices for articles. Goods do not move, for people simply will
not buy."17 There was populist anger and protests against profiteering manufacturers
and retailers. The protests took on moral dimensions: "If people determine to buy
foodstuffs or anything else only what they actually cannot do without, the working
of the inexorable law of supply and demand will operate automatically to bring
conditions to a more normal state."18 Thrift became a new virtue. This 1913 standard
was framed as a magical number. In January 1920 the Commissioner of Labor
Statistics, Royal Meeker, noted that his agency had started its retail price index in
1913 - a date many believed to be a grounding point for the price level which would
lead them to delay purchases: "The prices we kicked about in 1913 have come to be
regarded as ideal" but he said that was a mistaken ideal.
Apparently, his words did not have immediate impact, and people did indeed hold
off buying, out of both anger at the narratives of selfish profiteers and the perceived
opportunity to profit from postponing their purchases during a time of falling prices.


---Economics-2017-0-24.txt---
Both reasons had emotional resonance as the affect heuristic would predict, in the
wake of the war, the influenza epidemic, and other factors.
There we have it: a possible narrative-based unconventional explanation - or at
least partial explanation - for the Depression of 1920-1921: substantially a consumer
boycott against imagined profiteers, based on narratives that made them villains,
abetted by a sense of possible personal opportunity to postpone buying, or
sense of revenge against the profiteers by outsmarting them, in the presence of an
affect heuristic event driven by other emotion-laden narratives (connected to the
World War, the Communist revolution, the influenza epidemic, the race riots, the
Big Red Scare, and the oil shock). Betting on falling consumer prices is a speculative
bet that can come to an end just as quickly as opinions about the stock market
change, and in this case, the end came in a little over a year.
IX. Narratives during the Great Depression of the 1930s
The Great Depression (which I describe as the whole period 1929 to 1941,
including the two NBER contractions) is the most long-term severe episode of
macroeconomic malfunction in world history. And yet we have no received theory
as to why it had such magnitude and duration.
Some theories of the extreme persistence of the Great Depression not relying on
narratives seem plausible. Cole and Ohanian (2004) argued that a policy intended
to combat the Depression in the United States, embodied in the 1934 National
Industrial Recovery Act, which imposed "codes of fair competition," prolonged the
Depression. The Act made it easier for businesses to form cartels and more difficult
for them to cut wages. Although the Supreme Court declared the Act unconstitutional


---Economics-2017-0-25.txt---
in 1935, Cole and Ohanian argue that the Roosevelt administration was able to keep
the substance of the codes going anyway.
Eichengreen (1996) and Eichengreen and Temin (2000) have argued that the persistence
of the Great Depression has something to do with an unthinking national
commitment to the gold standard, as if it were some kind of God-given virtue, despite
changes in labor markets that made wages more downwardly rigid. They show that
countries which abandoned the gold standard earlier showed better recoveries.
Milton Friedman and Anna J. Schwartz, in their Monetary History of the United
States, blamed the Great Depression on the Federal Reserve, saying that it was
explained by variations in the money supply. It was this book that coined the term
"quasi-controlled experiment," anticipating the large number of papers we now have
under the rubric of "natural experiment," but the authors didn't have any such claims
at this point. Peter Temin put Friedman and Schwartz in a better perspective. The
declines in the money supply were mostly endogenous, triggered in part by a series
of bank runs, caused by the same feedback that created the Great Depression, and
Friedman and Schwartz were really saying nothing more than that the Fed would
have done better if they had offset these declines. Temin also observed that Friedman
and Schwartz indicated no substantial correspondence between the bank runs and
measures of economic activity.
Once again, I argue that most likely a multiplicity of factors, whose confluence
produced such severity, caused people to cut back substantially on their expenditures,
and these factors in turn often have the form of epidemics of narratives.
The first new narrative of the Great Depression was that of the stock market drop
on October 28, 1929. This narrative was especially powerful, in its suddenness and
severity, focusing public attention on a crash as never before in America. These were
record one-day drops. But, beyond the record size, it is hard to say what made this
crash narrative such a success. There was something timed very well about this story
that caused an immediate and lasting public reaction.
The narrative of the crash of 1929 was so strong that it persists today (see
Figure 6), though more in books than in newspapers. The epidemic actually seems
to have begun weakly in 1926, before the actual crash of 1929. In newspapers, there
have been two fast epidemics, each peaking within a year, implying very strong
short-run contagion. The first assumed massive proportions in 1929 with the record
12.8 percent one-day drop in the Dow Jones Industrial Average on October 28,
1929. The second started on October 19, 1987, when the Dow had a 22.6 percent
drop, almost doubling the October 28, 1929 drop, though falling short of the
two-day drop in 1929. It may seem odd that no other stock price movement merited
being called a crash since 1929. Newspapers are very focused on records, presumably
because they know their readers are, and 1987 was the only record one-day
drop after 1929. Folklore suggests that the stock market epidemic had extremely
high contagion in 1929.
We know there was high contagion in 1987. In my own mail questionnaire survey
in the days following October 19, 1987, 1 asked when the respondent learned of the
1987 crash. Ninety-seven percent of respondents said they learned of it that day, and
most of them by midday, meaning neither from morning news or evening news, but
presumably largely from word of mouth (Shiller 1989). The 1987 epidemic looks
far stronger than the 1929 one, but it certainly draws its strength from memories of


---Economics-2017-0-26.txt---
1929, and it may not in fact have been stronger, despite the appearance in Figure 6,
given the limitations of word counts as measures of narratives.
Where did the 1929 crash narrative get such strength? Part of the strength in 1929
seems to come from certain moralizing. The fact that the 1920s had been not only a
time of economic superabundance but also of sexual liberation - a morality viewed
negatively by some, though they were unable to make a case against it until the new
story of the stock market crash appeared.
Sermons preached on the Sunday after the crash, November 3, 2016, as reported
in newspapers the following day, took great note of the crash, and attributed it to
excesses, moral and spiritual. The sermons helped frame a narrative of a sort of
day-of-judgment on the "Roaring Twenties." (Google Ngrams shows that the term
"Roaring Twenties" was rarely used in the 1920s: the usage of that term first became
substantial in the 1930s and follows a hump-shaped pattern roughly like the infectives
plot in Figure 2, not peaking until the early 1960s.)
It helps to listen to these people's narratives from the time of the Great Depression
in their own words. R. G. D. Allen, in Only Yesterday, spoke of a more modest
countenance:
...striking alterations in the national temper and ways of American life...
One could hardly walk a block in any American City or town without
noticing some of them.19
Rita Weiman, an author and actress, described the change too, in 1932:
During those years of inflation, when we were right on the edge of a precipice
all the time, we lost our sense of perspective. We spent fabulous sums



---Economics-2017-0-27.txt---
for objects and pleasures out of all proportion to the value received. If it
cost a great deal of money, we promptly came to the conclusion that they
must be good. . . Take the matter of home entertainment. Many of us had
almost forgotten how much fun it can be to gather friends around one's
own table. Any number of us suffered from "restaurant digestion."20
Catherine Hackett, another writer in 1932, explained her view of the new morality
in the Great Depression:
In the old Boom era I could buy a jar of bath salts or an extra pair of
evening slippers without an uncomfortable consciousness of the poor
who lacked the necessities of life. I could always reflect happily on the
much-publicized day laborers who wore silk shirts and rode to their work
in Fords. Now it was different. The Joneses were considered to be callous
to human misery if they continued to give big parties and wear fine
clothes.21
Another narrative at the beginning of the Great Depression was that of a repeat
of the 1920-1921 event - no more distant in their memories than the events of
2007-2009 are to our memories today. Since many must have expected prices to
fall, as they did before during 1920-1921, they would want to delay their purchases
until the price decline was complete. Economists naturally expected the contraction
to be as short lived as in 1920-1921, which helps explain why President Hoover
and others confidently explained that it would be over soon. But the public didn't
necessarily believe President Hoover. Catherine Hackett wrote
I have read enough predictions by economists to convince me that my
guess is as good as anyone 's on the future trend of prices. A housewife
plays the falling commodity market just as an investor plays the falling
stock market ; she sits tight and waits for prices to settle before buying
anything but actual necessities. But I do not need to be an economist to
realize that if all the twenty million housewives do that , business recovery
will be indefinitely delayed.22
I have displayed this quote here as originally worded in 1932 because it illustrates
some aspects of consumer behavior then that may not be remembered and
appreciated. She finds similarity in consumer behavior to the behavior of stock
market speculators, who put their own emotional energy into forming their own
personal forecast for the individual stock prices, not trusting experts, as well as the
high contagion of narratives about such speculation. Women must have been talking
like speculators, telling stories about some smart decisions and some mistakes with
their shopping successes and failures. This must often have been about the second
moment of consumer price changes. Even if average expectations for inflation were
nonnegative, it is plausible if there was a higher contagion rate for emotionally laden
narratives about the price decline scenario, there could be significant net decrement
to consumer spending.



---Economics-2017-0-28.txt---
It is curious that economists haven't looked more at testimonies of women to
understand the consumption function. Even when, maybe especially when, prices
are rising rapidly, women must have been talking extensively about strategizing
their shopping around their hunches. Apparently then, especially as sex roles were
more strongly divided than they are today, it was men's business to play the stock
market and women's business to manage the shopping, except perhaps around 1929
when women were notoriously getting into the stock market speculation game.23
So, there were attempts again to create a moral imperative against betting on
falling prices, against women's behavior that was analogous to that of the generally
male short sellers on the stock market. The Washington, DC Chamber of
Commerce launched a campaign in 1930 with the slogan "Buy Now for Prosperity."
A "Prosperity Committee" sought the participation of clergymen of all denominations
to "preach prosperity through their pulpits" and thereby to "stimulate production,
relieving the unemployment situation."24
In 1932, the depths of the Depression, a Mrs. Charles E. Foster reportedly told a
women's group
One of the most effective weapons in the hands of American women
today is their tremendous purchasing power. We are told that they spend
eighty-five percent of the incomes of the United States. How could they
better create public opinion in favor of spending as usual than by setting
the example themselves?25
Even as it happened, the contraction was thought of popularly as the product of
some kind of feedback. In his 1933 inaugural address, President Franklin Delano
Roosevelt summed it up with the words "the only thing we have to fear is fear itself,"
describing people as responding with fear to others' fear. This quote is widely
remembered today; his fireside chat has developed into a powerful slow narrative, in
contrast to Hackett's, which does not quite describe an emotion of fear.
Roosevelt also offered moral reasons to spend. Days after his inauguration in
1933, President Roosevelt took the unusual step of addressing the nation on the
radio at a time of a massive national bank run that had necessitated shutting all
the banks down. In this "fireside chat" he explained the banking crisis and asked
people not to continue their demands on banks. He spoke to the nation as a military
commander would speak to his troops before a battle, asking for their courage and
selflessness. Roosevelt asserted, "You people must have faith. You must not be stampeded
by rumors or guesses. Let us unite in banishing fear."26 His personal request
to the nation stuck: it created a narrative to end the bank run. Money flowed into,
not out of, the banks when they were reopened. The narrative of this first fireside
chat is still with us today, and every president starting with President Ronald Reagan
has offered regular chats on the radio on Saturdays, but the narrative has not been
powerful enough, or not used well enough, to prevent recessions.


---Economics-2017-0-29.txt---
The macro storyline in the Great Depression gradually morphed into a national
revulsion against the excesses and pathological confidence of the Roaring Twenties.
Other narratives focused on the rising leftist or Communist movement that was
seen as potentially influencing future government policy far beyond the National
Industrial Recovery Act (NIRA), the New Deal institution to support fair prices.
The NIRA was perceived as only one example of the interference with business that
might come eventually.
The worst days of the Great Depression, in 1932 and 1933, were haunted by
scary narratives coming from Europe and Asia. Japan had just occupied Manchuria
in 1931. In the Soviet Union in 1932-1933, The Holodomor, the ethnic-Ukrainian
version of the Holocaust, was raging in the form of a man-made famine - Stalin's
attempt to stifle dissent, which cost millions of lives. These narratives, with hindsight
sounding like some of the worst narratives of World War II to come, were
widely circulated in the West. In January 1933, Hitler seized power in Germany and
quickly began murdering his political opponents and terrorizing Jews.
These narratives were good reasons for anxiety that might encourage many to
cancel their plans for frivolous spending, such as a larger house or a new car, in order
to feel they had sufficient savings. Some of these events also brought moral reasons
not to spend. There were many boycotts: against German and Japanese goods as
well as against goods associated with Jewish people. Germans began boycotting
Western goods. These actions must have had some economic effect.
Frederick Allen's 1931 book, Only Yesterday, seemed to get closer to understanding
the real cause of the Great Depression, in terms of the stories of the time, and
the book has motivated my thinking ever since I first read it. The book may seem
superficial in a sense: just a collection of stories with no organizing theme except in
the afterword. However, through understanding all those stories of fads and crazes,
one gets a view of what was happening in the 1920s. One reviewer of Allen's book
in 1931 explained what he learned from the book: "Of course, we change fads. That
is the essence of our changelessness - that we plunge from one craze to another with
kaleidoscopic ease."27
Contagion rates for stories of business failures, rather than inspirational stories,
were naturally high at a time when a large fraction of the population were unemployed.
Stories abounded of business people committing suicide.28
It seemed to most people in the late depression that there was an inevitable trend
toward government control of business. A May 1939 poll asked, "Do you think that
ten years from now there will be more government control of business than there is
now or less government control of business?" Fifty-six percent of the respondents
said more; 22 percent less; 8 percent neither; 14 percent had no opinion (Higgs
1997).
By the 1930s, the theory that Communist forces in America were massing forces
into an inevitable future direction for America was a story everyone knew, whether
they liked it or not. The increasing radicalization of President Roosevelt plays a part


---Economics-2017-0-30.txt---
in these stories: in 1936, speaking of the magnates of organized money, he said "I
welcome their hatred." (Higgs 1997).
The Communist conspiracy narrative, from the late 1930s, included anecdotes
about increasingly leftist labor laws stymieing business. Stories circulated of outside-
agitator unions disrupting peaceful companies whose employees were happy
with their jobs; of strangers picketing and coordinated refusals of other unions to
handle the company's products; of the existing employees forming their own union
to try to stave off the attack, and the radical National Labor Relations Board throwing
it out as a "company union."29
One of the stories that was circulating in the United States during the Great
Depression was that of Lázaro Cardenas, the president of Mexico (1934-1940). Just
as in the United States, the Depression amplified calls for socialist or Communist
solutions in Mexico, which put fear in the hearts of businesspeople. Cardenas expropriated
land from the commercial haciendas, and in 1938 nationalized the Mexican
oil industry and railroads. These actions were seen as a model for what might happen
in other countries, and indeed nationalizations in other countries did follow
Cárdenas' example, though not in the United States.
A huge rise in policy uncertainty in both the United States and the United
Kingdom was revealed by the Economic Policy Uncertainty Index of Scott R. Baker,
Nicholas Bloom, and Steven J. Davis (2016), which is based on counts of words in
online news media.
X. Narratives Leading Up to the Great Recession, 2007-2009
The 2007-2009 world financial crisis has been called the Great Recession as a
reference to the Great Depression of the 1930s. Certainly, the narrative of the Great
Depression was suddenly thrust into the national attention as never before, not since
the 1930s (see Figure 7).
The figure suggests far more attention was paid in 2009 to the Great Depression
than during the Great Depression itself, though one must be careful to understand
that people hadn't really named it the Great Depression yet as it happened. They
certainly had Depression-linked narratives, associated with words unusual to that
period, such as breadline, a word that Google Ngrams shows grew rapidly in use in
books from 1929 to 1934, and decayed fairly steadily ever since.
The interest in the Great Depression in 2009 is confirmed in Google Trends
search counts as well, though not as dramatically as in Figure 7. This does not mean
that people were suddenly more interested in Franklin Roosevelt or the New Deal.
Counts show virtually no increase in interest in these details of history. It was more
just a quick and easy way to communicate narrative: we have passed, by 2007, a
euphoric speculative immoral period like the Roaring Twenties, the stock market
and banks are collapsing in 2008 as around 1929, and now the economy might really
collapse again like that; we might even be unemployed and on the street crowding
around failed banks, yes really! End of basic narrative.

---Economics-2017-0-31.txt---
Consider a narrative-based chronology of the 2007-2009 financial crisis.
Financial crises are driven by a cadence of stories. For example, stories about bank
runs were in the nineteenth century virtually synonymous with financial crises. But
after the Great Depression bank runs were thought to be cured. The Northern Rock
bank run in 2007, the first UK bank run since 1866, brought back the old narratives
of panicked depositors forming angry crowds outside closed banks. The story led
to a skittishness internationally, and to the Washington Mutual (WaMu) bank run a
year later in the United States, and, then, the Reserve Prime Fund run a few days
after that in 2008. These events then led to the very unconventional US government
guarantee of all US money market funds for a year. Apparently, governments were
aware that they could not let the old story of a bank run go live over concern for its
effects on public anxiety.
A narrative approach to understanding the crisis might take us back further in
time. In 2001, the UK television show Property Ladder was launched. This reality
TV show which depicted individuals buying homes, fixing and prettifying them a
little, and then reselling them at a large profit, was a big success. Successful narratives
are copied with appropriate changes and launched anew in other countries.
The US TV show Flip that House attempted to replicate Property Ladder in 2005,
but it was canceled at the time of the financial crisis in 2008. Property Ladder
lasted until 2009, renaming itself Property Snakes and Ladders, until its demise
later that year.
Leading up to the Great Recession 2007-2009, and setting the stage for it, were
widespread fears of long-term job insecurity because of advancing technology.
Facebook and Gmail appeared in 2004, YouTube in 2005, Twitter in 2006, and the
iPhone in 2007. These were the prominent business stories of the time, and they may


---Economics-2017-0-32.txt---
have left the impression that the companies that were forming might not be creating
jobs for those not technologically gifted or connected with other such people.
The very name Great Recession could be interpreted as evidence of a narrative
epidemic. Naming the financial crisis after the Great Depression was not the
choice of any one individual. There had been earlier attempts to attach the name
Great Recession to preceding recessions. Otto Eckstein wrote a book entitled The
Great Recession in 1978 that attempted to attach this name to the severe 1974-1975
recession. However, the name did not stick. Again in the 1981-1982 recession this
term was used by Joseph Granville, the flamboyant analyst who stirred much talk
about his prediction of stock market drops by engaging in stunts such as appearing
at events with a trained chimpanzee.30 But again, the name did not go viral. It
was different, however, in 2007-2009. Nouriel Roubini first referred to the "Great
Recession of 2007" in late 2006, a year before the recession had started.31 But it
took several more years, until 2009, for the term to catch on and go viral.
The Great Depression of the 1930s has long been associated in the public mind
with the "Stock market crash of 1929" that preceded it. Most people appear to be
wary of attaching grandiose names to events, unless there is authority justifying
that. Lionel Robbins was successful with his book title, The Great Depression in
1934 because President Roosevelt, a generally recognized authority, had used the
words,32 suggesting the worst economic contraction ever. It is conceivable that naming
the event thus might have been a self-fulfilling prophecy in that it created worries
and uncertainty, perhaps contributing to the persistence of a depressed economy.
What is it about the 2007-2009 event that made the name Great Recession suddenly
contagious? Judging from the peak US unemployment rate, it was less severe
than the 1981-1982 recession.
Perhaps it was because the 2007-2009 event fit the most generic and ill-informed
memories of the Great Depression. People remember massive bank failures as part
of the Great Depression story, and that was a better fit, it appeared, with the events of
2007. In the 1981-1982 recession the stock market had not recently been booming
(it had never recovered from the 1974-1975 crash, which still seemed like a recent
major event in 1981), and the stock market did not fall below its 1980 value by 1982.
In contrast, 2007-2009 saw a halving of the market from very high levels. People in
1981-1982 were, as public opinion polls confirm, preoccupied with out-of-control
consumer-price inflation, and saw the events in terms of a suddenly strong central
bank effort to contain the inflation.
Psychologists Daniel Kahneman and Amos Tversky's representativeness heuristic
is the principle that people judge current events by their similarity to memories
of representative events. The Great Depression was a model that is exaggerated in
people's minds because of its legendary status. In 2007-2009 presidents and prime
ministers invoked parallels to the Great Depression to justify their requests to apply
stimulus. Did this contribute to a self-fulfilling prophecy, a mirror event to the Great


---Economics-2017-0-33.txt---
Depression, albeit not as severe? Indeed, the name says it all: the Great Recession
narrative, not quite the Great Depression.
XI. Narratives in 2017 and the Outlook for the Economy Today
The narratives from the 2007-2009 financial crisis have faded in our memories,
but are still alive and still relevant. But, for now, the US national attention has shifted
quite dramatically away from that mode of thought.
President-elect Donald J. Trump is a master of narratives. His narratives have
become highly inspirational for some, as was seen by the tens of thousands who
came to his campaign rallies. They have become a source of alarm for others.
His remarkable success in the presidential election campaign last year may be
attributed to, among other things, his public persona from his lifelong effort to promote
himself as a business genius able to make hard decisions and strike deals. An
important reason for his contagion is his reality television show, The Apprentice and
Celebrity Apprentice, which focused lavish attention on Trump. These shows were
such a success that they were copied in numerous other countries each with their
own homegrown Trump substitute, some of whom went on to achieve their own
political success, notably João Doria, elected mayor of São Paulo, Brazil, in 20 16.33
History does not suggest that even a politician as skilled as Trump can actually
control the progression of the narratives he created. The manner in which these narratives
unfold will play an important role in any economic forecast. To best predict
economic activity, we need, among other things, to watch the narratives and ask:
how will the emerging twists in the narratives affect propensity to spend, to start
unconventional new businesses, to hire new employees? In short, how will the animal
spirits be affected?
Trump's example and admonition to "think big and live large" (Trump and Mclver
2004) appears to provide inspiration to many of his admirers, and that might well
be expected, along with his stimulative tax policy, to boost consumption demand as
well as entrepreneurial zeal. Many people might take the Trump story as a script for
themselves, and thus spend freely and raise their risk tolerance.
But we also have to take into account the Trump detractors, about as many as
his admirers, who may be thinking more along the lines of the morality play of the
1930s. Predicting aggregate demand from the new Trump narratives requires some
careful attention to conflicting narratives.
Incorporating such factors into economic forecasts is not impossible. We would
benefit, however, from more research into understanding the role of narratives in the
economy.
XI. Opportunities for Researchers in Narrative Economics
Narrative economics, to the extent that it has ever been practiced by scholars,
has had a poor reputation. In part, it may be due to the fact that the relation between
narratives and economic outcomes is likely to be complex and time varying. The


---Economics-2017-0-34.txt---
impact of narratives on the economy is regularly mentioned in journalistic circles,
but without the demands of academic rigor. The impact of journalistic accounts
of narratives may have been connected to aggressive forecasts which often proved
wrong. But, the advent of big data and of better algorithms of semantic search might
bring more credibility to the field.
Research in economics is already on its way to finding better quantitative methods
to understand the impact of narratives on the economy. Textual search is a small
but expanding area in economic research. A search of the NBER Working Paper
database finds less than 100 papers with the words "textual analysis." Textual analysis
has been used by economists, for example, to document changes in party affiliation
(Kuziemko and Washington 2015); political polarization (Gentzkow, Shapiro,
and Taddy 2016); and news and speculative price movements (Roll 1988; Boudoukh
et al. 2013). But much more could be done. The historical analysis could be carried
further into databases of personal diaries, sermons, personal letters, psychiatrists'
patient notes, and social media.
There should be more serious efforts at collecting further time series data on
narratives, going beyond the passive collection of others' words, toward experiments
that reveal meaning and psychological significance. Since 1989, 1 have been
collecting some such data, on questionnaires about stock prices and home prices,
with open-ended questions that invite the respondent to write a sentence or two. The
questions are designed to stimulate the respondent to think about what is motivating
them, so that their responses can be analyzed in posterity. However, I would advocate
for there to be resources devoted to collecting data about narratives and public
reactions and understandings of narratives on a serious scale. It could be done with
focus groups and social media.
But this research still today needs improvement in tracking and quantifying narratives.
Researchers have trouble dealing with a set of often-conflicting narratives and
gradations and superposition of them. Even the simplest epidemic model shows that
no narratives reach everyone, and whom a particular narrative reaches and whom it
does not is largely random. The meanings of words depend on context and change
through time. The real meaning of a story, which accounts for its virality, may also
change through time and is hard to track in the long run.
There are serious issues of inferring causality, distinguishing between narratives
that are associated with economic behavior just because they are reporting on the
behavior, and narratives that create changes in economic behavior. These issues are
not insurmountable.34
Researchers have to grapple with issues that have troubled literary theorists.
Those theorists who, as noted above, try to list the basic stories in all of literature,
have to try to distill what it is that defines these stories, what makes them contagious.
There are so very many contagious stories at any time in history, and it is hard
to sort through them. The theorists run the risk of focusing on details of the stories
that are common just because the events are actually familiar in everyday life. They
also face the difficulty of accounting for changes through time in the list of stories.


---Economics-2017-0-35.txt---
Research in semantic information and semiotics is advancing. For example,
machine translation is already somewhat able to pick which meaning of the word is
intended by looking at context, at other adjacent words. Semantic search took a big
leap forward in 2010 when Apple, Inc. introduced its Siri function, which allows
users to verbally ask a question like "What is the longest river in South Africa?"
and receive a direct verbal answer. Semantic search is now getting well established
around the world.
Semantic search may, however, take a long time to reach the abilities of the human
mind to understand narratives. In the meantime, researchers can still be quantitative
in the study of narratives if they use multiple research assistants with explicit
instructions to read narratives and to classify and quantify them for their essential
emotional driving force. Advances in psychology, neuroscience, and artificial intelligence
may be relied upon to improve our sense of structure in narrative economics.
As research methods advance, and as more social media data accumulate, textual
analysis will be a stronger field in economics in coming years. It may allow us to
move beyond 1930s-style models of feedback, the "multiple rounds of expenditure,
" and get closer to all the kinds of feedback that really drive economic events.
And it will help us to better understand the kinds of deliberate manipulations and
deceptions we have been suffering under, and to formulate some positive economic
policies that take into account the background of narratives.


---Economics-2018-0-02.txt---
Market design is both an ancient human activity
1 and the relatively new part of
economics that strives to understand how the design of marketplaces influences the


---Economics-2018-0-03.txt---
functioning of markets.2 Armed with this understanding, economists can sometimes
help build new marketplaces or repair those that are broken.
In this essay, I’ll try to illustrate some principles of market design, point to some
questions worth further study, and show how the study of marketplaces opens new
windows through which to view markets.3 Market design has also opened up new
ways for economists to earn their livings.4
I want to emphasize the view that marketplaces—which consist of infrastructure,
rules, and customs through which information is exchanged and transactions are made—can be relatively small parts of large markets. Participants may have large strategy sets, i.e., many options available to them, beyond those available in any particular marketplace.
Because marketplaces serve many different kinds of markets, different marketplaces
have different tasks to accomplish. Practical market design that aims to design marketplaces that will be adopted, implemented, and maintained can be thought of as a kind of economic engineering, so details matter.
5 But, as with other kinds of
engineering, although each application may demand custom design, there are com-mon issues that many marketplaces have to deal with to be successful, and so there are also general lessons to be learned. And (also as with other kinds of engineering) sometimes these lessons are learned most clearly from marketplace failures, i.e., from marketplaces whose (poor) design impedes market efficiency. (So it isn’t nec-essarily the most important markets that yield the most useful lessons.)
Marketplace design (or even design economics) might sometimes be a more
descriptive name for the emerging field of market design.
6 However, although I
didn’t choose the name, I’ve been glad of the opportunity it has given me to defend the notion that “markets” come in many varieties, and are not only or even primarily commodity markets whose only job is price discovery.
Commodity markets are themselves a great invention, which allow trade to be
conducted safely with relatively anonymous counterparties, with prices doing all


---Economics-2018-0-04.txt---
the work of deciding who gets what. Commodity markets require design of both
the marketplace procedures and the commodities themselves. The Chicago Board of Trade deals in commodities like US soft red winter wheat, a much more precisely defined commodity than “wheat.” In the design of commodities, there is room for both regulation by governments and standard-setting by private sector actors: see, e.g., United States Department of Agriculture (2014), and cf. Levin and Milgrom (2010). Commodities can be traded without further inspection, and without know-ing one’s counterparty in any given transaction. So bids and asks can be directed to the market at large. It is practical to trade a relatively small set of commodities among a large set of potential participants, since only one price for each commodity needs to be established for a transaction. That is, a relatively small set of prices can clear the market among a large set of traders.
Note that some marketplace centralization is needed even to achieve the full benefits
of commoditization, both to standardize the commodities and to allow them to be efficiently traded. Centralization of trading might involve just a central location (in space or in cyberspace), with traders coming together to trade with one another directly. Or it might also involve a centralized clearinghouse of some sort, as in a centrally administered order book that keeps track of time-stamped bids, and asks and arranges transactions according to a set of marketplace rules.
A commodity market must therefore employ particular marketplace technology
and procedures, and the strategies available to participants may evolve along with the marketplace’s technologies, and their own. Open-outcry markets with gestic-ulating traders have largely been replaced by electronic trading, now often high speed algorithmic trading. These new technologies and the high-frequency trading strategies they enable may in turn make new market designs desirable (cf. Budish, Cramton, and Shim 2015).
Many markets that are not quite commodity markets also work through price
discovery, and trade among (almost) anonymous counterparties.
7 Google search
auctions, for example, determine which ads to show for each word that someone is searching for; i.e., they are an auction for eyeballs—for attention—with some eyeballs searching for more valuable things than others. Auctions for banner ads on websites may involve bids based on the cookies that reveal information about the previous web activity of the eyeballs being auctioned. The design of these auctions reflects that they have to be fast, so as not to lose the attention advertisers hope to gain with a winning bid.
8
Milgrom (2017) recounts the design of the incentive auction through which the
US government brokered the purchase and resale of spectrum licenses owned by broadcasters so that they could be repurposed. He points out that the allocation of spectrum comes with many constraints, which makes the licenses very
heterogeneous

---Economics-2018-0-05.txt---
and may even preclude the existence of impersonal equilibrium prices for individual
licenses. These complex auctions need to match bidders with the right units (and packages) of the licenses being auctioned.
9 Wilson (2002) discusses different complexities
in the markets for electricity, in which the time and place of delivery face (and impose) constraints on the electricity network.
And in many markets, you care with whom you are dealing. In matching mar -
kets, you can’t just choose what you want, even if you can afford it: you also must be chosen.
10 So prices don’t do all the work of deciding who is matched to whom:
marketplaces that serve matching markets must be able to do more than price dis-covery. Regulators take note: because matching marketplaces must do more than marketplaces for commodity markets, they may need to facilitate additional kinds of coordination, and so preserving competition in matching markets may involve different kinds of regulation.
Matchmaking is an old form of market making, but new technologies are making
new kinds of matching markets possible. For example, Airbnb, which matches travelers and hosts, is in many ways an outgrowth of internet technology, while ride-sharing marketplaces like Uber, which match passengers with nearby drivers, depend on smartphones that know the location of the passenger and of available cars.
11 The ride-sharing/taxi markets will be transformed again (likely along with
automobile ownership) when self-driving cars become common (cf. Ostrovsky and Schwarz 2018).
College admissions and labor markets are good examples of matching markets.
12
Prices—i.e., tuition and wages—are important, but they don’t decide who goes to which college or works at which job. You can’t just decide where to study or work, you also have to be admitted or hired. And candidates apply to particular colleges and employers, who in turn make offers to individuals, not to the market at large as when buying and selling wheat futures.
13


---Economics-2018-0-06.txt---
I. The Mark et for New Economists
Consider, as a familiar example, the marketplace for interviews between employers
and new PhD economists conducted at the winter meeting of the Allied Social
Sciences Associations (ASSA), organized by the American Economic Association.14
This marketplace facilitates many of the matches of departments and new PhDs for interviews.
The larger market for new economists extends beyond this marketplace for inter -
views. Not everyone chooses to participate in interviews of this sort, and even for those who do, the market extends in time and space. Earlier, there are positions to be advertised, applications to be made, and reference letters to be sent, today, often via internet clearinghouses. As interviews are being arranged, there are signals of particular interest to be transmitted, including formal signals organized by the AEA (cf. Coles et al. 2010). Later there are flyouts, and offers to be made and accepted or rejected. Later still there is a “scramble” website organized by the AEA to help organize the market for those not yet matched in the main market.
15 These marketplace
institutions were established incrementally, over time.
Before the early 1970’s, many economics departments did not formally advertise
faculty positions, even for new PhDs. Instead, positions were filled through infor -
mal, decentralized communications, with the result that there was not a clear time at which most jobs and applicants were known to be available. The AEA began publishing Job Openings for Economists around 1974, which helped make infor -
mation about positions widely known. This made it easier for many economists to apply for each position, and for each applicant to apply to many positions. This trend toward more applications (per position and per applicant) was enhanced by changes in technology from typewriters and snail mail to word processors and the internet. These changes lowered the costs, in labor, time, and postage, of crafting and sending applications. Similar changes have lowered the costs to faculty advisors or their assistants of sending letters of reference.
16 A common medium on which
most jobs were advertised in a relatively concentrated period, together with lowered costs of application, helped make the market thick, i.e., helped bring together many employers and job seekers at the same time.
Thick markets may suffer from congestion. By congestion I mean the accumulation
of more time-consuming activities than can easily be accommodated in the time available. Evaluating many job opportunities, or many applicants, is time consum-ing. Organizing interviews at the ASSA meetings helps to deal with this congestion, because many interviews can be conducted in a short time once the candidates and recruiting committees are all assembled in the same place. However, as the number of applications per position has grown, many employers cannot conduct as many


---Economics-2018-0-07.txt---
interviews as they might wish in the three days of the meeting, and must choose
from hundreds of applicants those they will interview.
Many departments must therefore make strategic decisions about whom to inter -
view. They have to consider not only which candidates they like, but also which candidates are not too likely to receive other offers they would prefer. That is, in choosing whom to interview, departments have to consider not only how likely they would want to hire the candidate, but also how likely they would succeed, since they don’t have time to interview all the candidates who they might eventually be willing to hire. So the interviewing process is congested. As a step toward easing this con-gestion, in 2006 the AEA instituted a signaling mechanism that allows candidates to send a signal of particular interest to no more than two employers.
17 Because candidates
can send only two signals, signals are a scarce resource that can convey serious potential interest.
18 And interest in signals has been steadily growing.19
Note that two very different kinds of signals are sent via various channels, throughout
the courtship between candidates and employers that ultimately leads to match-ing applicants and jobs. On the one hand are signals about applicant quality.
20 Job
market papers, for example, convey information about talents, accomplishments, and skills that signal why candidates should be interesting to employers (Spence 1973).
21 But congestion—e.g., the fact that many employers’ interview slots are in
short supply and must be carefully allocated—also makes it useful for candidates to signal that they are interested in particular jobs (Coles et al. 2010; Coles, Kushnir, and Niederle 2013). Because technology has made it easy to send many applica-tions, simply sending an application is no longer a strong signal of interest.
22


---Economics-2018-0-08.txt---
As candidates and recruiting committees depart from three intense days of inter -
views at the ASSA meetings, the market becomes much less organized. Departments
invite some candidates they interviewed to day-long “flyouts” to their campuses, to present their job talks and meet the faculty. Essentially each department operates its own marketplace where candidates come to be evaluated and compared by the fac-ulty, and each candidate turns into a traveling salesperson, bringing his or her per -
sonal marketplace on the road. This part of the market is congested. Flyouts absorb time and other resources, so there’s a limit to how many each department can host. This happens over several months, starting immediately after the ASSA meetings.
An important part of this congestion involves the timing of offers, acceptances,
and rejections. Large, highly ranked departments at wealthy universities can often schedule flyouts early and make multiple offers per position, and can thus make offers to all the candidates they wish to hire. Such offers are often effectively open-ended, allowing candidates time to see what other offers may be forthcoming. But smaller, less prosperous, and less highly ranked departments often put time limits on how long their offers will remain open. Even an offer open for two weeks (which is not too unusual), which gives the recipient some time to think and maybe to come for a second visit, may call for a decision before other offers come in, or even before other flyouts are accomplished. Congestion is one reason a department may wish to put time limits on its offers. If it expects to have to make multiple offers to fill a single position, but can only make one at a time, then making consecutive offers quickly gives it the opportunity, should the need arise, to make offers to multiple candidates before they have committed to other positions. A less beneficent reason to leave offers open for only a short time is to pressure candidates into committing before they have time to receive another offer they might prefer. For whatever reason they are made, these asynchronous offers and deadlines make the market effectively less thick, since many candidates are deprived of the opportunity to consider mul-tiple offers. Offers with short deadlines create a negative externality not just for applicants but also for other employers, since they shorten the time during which candidates remain available.
23
Although most economics departments do not make offers that candidates must
accept or reject before the ASSA meetings, some potential participants choose not to participate in the ASSA marketplace for interviews. Some employers of new economics PhDs—such as marketing departments in business schools—make offers before the ASSA meetings, and by the time of the meetings have already hired, or perhaps been turned down by economists who preferred to try their



---Economics-2018-0-09.txt---
chances in the larger, later marketplace.24 Other employers choose not to inter -
view at the meetings, but to wait and perhaps hire some talented individual who
“fell through the cracks” amidst the crowded interview schedules. So the meetings provide some services to the broader market, but the strategy sets of participants are large, and they can choose to do without those services, and not participate in that marketplace.
25
Finally, note that some marketplaces in a larger market may be repugnant, in the
sense that although they are attractive to some participants, others not only don’t wish to participate themselves but think that such marketplaces should not operate at all (Roth 2007). This kind of repugnance can occur even in the absence of easily measurable negative externalities to those who would like to eliminate such a mar -
ketplace (Ambuehl, Niederle, and Roth 2015). The kidney transplant marketplaces
I’ll discuss later have been fundamentally shaped by the repugnance, enshrined in law, toward buying and selling organs.
26 But until recently I would have been hard
pressed to name a part of the market for economists that was viewed as repugnant by other economists. Lately, however, the website Economics Job Market Rumors (EJMR
27) has achieved that distinction.
The site started as an anonymous internet discussion board focused on the job
market, with threads devoted to particular candidates, schools, subfields of econom-ics, etc., including anonymously sourced rumors about which schools were inter -
viewing, flying out, and offering jobs to which junior candidates. Over time it also featured discussions of other aspects of academic economics, and discussions of any-thing else someone wished to discuss. As has happened with many other anonymous and pseudonymous internet sites, the tone of the discussion sharply deteriorated, and many of the posts became attacks on particular individuals, often in misogynistic, homophobic, anti-Semitic, or racist language.
28 The matter became widely known
among economists after an undergraduate thesis, Wu (2017), which cataloged the
prevalence of misogynistic language and critical comments about individual women on the site, was discussed in a NY Times column.
29 (As of this writing it appears that

---Economics-2018-0-10.txt---
there has been increased effort by the (anonymous)  proprietors of EJMR to monitor
the site and remo
ve the nastiest vocabulary, perhaps with automatic filtering.)30
Reaction included a petition addressed to the AEA by the International Association
for Feminist Economics (IAFE 2017), signed by more than 1,000 economists, and a
recommendation by the AEA’s Committee on the Status of Women in the Economics Profession (CSWEP 2017) that the AEA set up an alternative site, to provide some
of the job market information available from EJMR in a less repugnant forum. The idea was that EJMR provided information that some economists were eager to have, but in an unsafe venue for women and others, and that the AEA might provide this information in a safer way. The AEA Executive Committee responded by conduct-ing a survey of department chairs about what kinds of in-progress job market infor -
mation they might be willing to share, and at the 2018 meeting it was decided to produce some non-anonymous websites for job market information in time for the 2018–2019 job market.
31,32
I’ll come back to this idea of moderating the impact of repugnant markets and
marketplaces by organizing competition for them via marketplaces that provide more acceptable alternatives when I speak about efforts to ban some markets, the black markets that arise in response, and the choices this raises about relaxing laws against markets that can’t effectively be eliminated.
To summarize, even these familiar, prosaic institutional features of the job market
for new economists illustrate some issues—like thickness, congestion, safety, and
repugnance—that often arise in the study of markets through their marketplaces, and which require attention when marketplaces are designed (Roth 2008).
33


---Economics-2018-0-11.txt---
II. Mark etplace Failures Related to Timing
Thick marketplaces that operate at a time at which transactions can be made efficiently
provide a public good to the participants, by allowing them to compare many
possible transactions. But as with any public good, there is a temptation to free ride, and thus marketplaces often need to be defended in order not to lose their thickness, or their efficient timing. For example, markets can unravel in time, a process by which offers in a labor market gradually become earlier, shorter in duration, and diffuse in time (with the consequence that participants must make decisions before important information is available, and without knowing what other opportunities might be available in the market).
34 Roth and Xing (1994) describe many unraveled
markets, and for many markets identified organizations that try—often with only mixed success—to regulate the time at which marketplaces operate, and often the timing and duration of offers, to prevent free riding and promote efficiency.
35,36
Early offers, like exploding offers, can arise in anticipation of congestion: a firm
that expects to have to make multiple offers may wish to begin early, to have more time. But it can also result from the desire to move before competitors do, and combined with exploding offers it provides a strong negative externality to those competitors (although recipients of early offers may or may not be glad to get them,
rather than waiting for potential later offers). But there is every reason to believe that unraveling results from multiple causes, because markets are multidimensional but time is one-dimensional. Offers can be made only later or earlier, and making offers later than one’s competitors does not affect them in the same way as making early offers to candidates they might have wished to hire themselves, which can lead to escalation and a race to be first.
37


---Economics-2018-0-12.txt---
Consider, for example, the markets for two of the most competitive kinds of positions
for new lawyers: clerkships for appellate judges (Avery et al. 2001, 2007),38
and associate positions in the largest law partnerships (Roth and Xing 1994 and
Roth 2012). If someone graduating from law school this year moves into one of those positions, it is very likely that the job was arranged two years in advance (formally in the case of a clerkship, somewhat informally in case of an associate position, with the formal contract being made only one year in advance).
For clerkships, the Judicial Conference of the United States has tried to change
this situation, often with temporary and partial success, at least six times since 1983. In each case they have specified a date before which judges should not make offers of clerkships. The most recent and long-lasting of these attempts began in 2002 and was officially abandoned only in 2014, although by that time it was widely under -
stood that most judges were no longer following the rules.
39
The 2002 attempt at rule-making initially specified only that offers should not
be made “earlier than the Fall of the third year of law school.” But by 2003, three precise dates (starting after Labor Day of students’ third year) were specified before which no applicant may submit nor judges receive applications and letters of ref-erence; before which interviews may not be scheduled; and before which judges may not conduct interviews and (simultaneously) extend offers. This led to a very compressed market following the time of the first allowable interviews, which often ended with exploding offers. For example, Avery et al. (2007, p.

448) quote a 2005
clerkship applicant:
I received the offer via voice mail while I was in flight to my second
interview. The judge actually left three messages. First, to make the offer. Second, to tell me that I should respond soon. Third, to rescind the offer. It was a 35-minute flight.
This compression around first interviews made some judges try to “jump the gun,”
by scheduling interviews, and making offers, before the allowed time. Avery et al.
(2007)  report that by 2005 over one-half of the judges who responded to a survey
were aware that a substantial number of judges did not adhere to the indicated dates. Nevertheless, the plan was restated each year before being officially abandoned in 2014.
During that time, a growing number of judges violated the dates by moving
sometimes only a little earlier than in previous years. Moving just a little early allows a judge to free ride on the public good provided by a thick market operat-ing in students’ third year of law school. By waiting to hire clerks until their last year of law school, judges can collect more reliable information on each student’s likely abilities.
40 But by moving just a little early, judges can seek to benefit from


---Economics-2018-0-13.txt---
this abundant information and to choose whom to interview from the large pool of
candidates, while avoiding competition from other judges. This eventually proved to be an effective form of competition, so that more and more judges moved earlier and earlier.
The judges most inclined to move early were those who could offer good positions
to the most desirable clerkship candidates, but not the very most desirable positions. In an orderly market, these candidates could expect to be able to choose among the most prestigious positions, but a judge with a slightly less attractive posi-tion might succeed in hiring top candidates—e.g., some of those who had been elected Editor in Chief of their school’s law review—if the judge could deliver an exploding offer first. (See, e.g., Judge Alex Kozinski 1991, in an appropriately titled article on his recruiting practices.) Judge Kozinski was in the West Coast Ninth Circuit, the largest in the country, but less prestigious than some of the East Coast Circuits, particularly the DC Circuit.
In contrast, judges in the most prestigious circuit court of appeals have the most
interest in having the market operate with lots of information, since then they can identify the best students, who they would have a good chance of hiring, in a thick market. But as other judges hired clerks earlier, via exploding offers, their oppor -
tunity to do this waned. The DC Circuit formally announced its abandonment of the plan in January 2013, saying that if other judges were going to go early then so would they,
41 and the plan was formally laid to rest the following year.42 The mar -
ket very quickly unraveled back into the summer after the first year of law school, when clerkship offers were once again mostly finalized. Judges with competitive clerkships made offers to candidates without the information about them that would have been available if the market operated later, and most applicants accepted the first offer they received.
But hope springs eternal: in February 2018 the DC Circuit announced its support
of a new attempt to control the timing of clerk hires (see United States Court of Appeals District of Columbia Circuit 2018). Future law clerks already in their sec-ond or third year of law school had already been hired by this time, but the courts’ plan is to defer the hiring of those who just entered law school in Fall 2017. It will have implications for hiring starting in 2019, and again in 2020, after which it will be reviewed. The plan does have a new feature: it will attempt to ban exploding offers by requiring that offers remain open for 48 hours. To my jaundiced eye it does not appear very promising—this will be the seventh such attempt to halt unraveling in this market since 1983. So I predict that either there will be more rules added in an attempt to make the market design more robust to the kinds of failures observed



---Economics-2018-0-14.txt---
in the past, or that the present plan will also fail, although perhaps only slowly as
was the case with the most recent failure.43
Another very prestigious job for new lawyers is to become an associate at a large
law firm. This part of the market for lawyers also presently sees a lot of action in the summer following students’ first year of law school, since that is when those firms hire second-year summer associates, often with exploding offers (see Roth 2012). This is a very direct channel to full-time employment after graduation. The National Association of Law Placement (NALP) reports that almost 95 percent of the sec-ond-year summer associates of all the firms they surveyed in 2016 received offers of full-time employment for the following year (98.2 percent for firms employing more than 700 lawyers), with around 85 percent of offers accepted (NALP 2017a, Table 13, p.
29
). That is, nearly all summer associates receive an offer of a full-time
position, and the vast majority accept and go to work after graduation at the firm at which they summered. So the arrangement of a second-year summer associateship in the summer after the first year is very close to the same thing as being hired for a post-graduation job, two years in advance of graduation.
44
Unlike judges, who have no flexibility on clerk salaries (which are set by
Congress), private law firms can compete vigorously with one another, and could pay associates differently from other firms, and also pay different first-year asso-ciates of their own differently. However, neither of those things happen, and for a number of years virtually all first-year associates at large firms made the same salary, of $160,000 (and most firms give uniform bonuses to associates of the same
vintage).
45 Thus, it is not the salary that determines the matching between large
firms and their summer associates, i.e., it doesn’t determine which associates work for which firms. The matching between new lawyers and big law firms is almost entirely determined by the chaotic matching process that plays out very early in law school.
Other contemporary examples of unraveled markets include employment in
private equity, recruiting for college sports (and early admissions to college more generally), Swiss apprenticeships, and some medical fellowships in specialties that have not yet adopted a centralized clearinghouse.
46
The threat of unraveling sometimes prompts action, because marketplaces need
to be maintained, and, when necessary, defended. Markets that have long operated at


---Economics-2018-0-15.txt---
a given time can lose their thickness through unraveling. For example, the American
Philosophical Society (APS) is attempting to preserve the marketplace for inter -
views provided by their Association’s Eastern Division meeting, which takes place around the time of the ASSA meetings, and serves the same function as a market-place for academic interviews. The recent growth of earlier Skype interviews has not only made that marketplace less thick, it has also moved flyouts and offers (and the time at which they must be accepted) earlier, leading to efforts by the philosophy association to stop further unraveling.
47
The design of attractive marketplaces can sometimes reverse unraveling. In medicine
and in some other markets, centralized clearinghouses have arisen partly in response to unraveling.
III. Computerized Clearinghouses f or Stable Matching: Medical Labor Markets
and School Choice
A. Medical Labor Markets
The labor markets that American medical doctors navigate in their early careers
have pioneered one approach to reversing unraveling and avoiding congestion. These marketplaces are centralized at a different point than the market for new economists. While economists have a centrally located marketplace for interviews, new doctors have decentralized interviews. But they have a centralized clearinghouse through which all offers, acceptances, and rejections are made quickly by a computerized algorithm that takes as input the preference orderings over jobs submitted by appli-cants, and over applicants submitted by employers. (Thus, as I will describe, these markets are presently experiencing congestion in the interview process, but not in the process of making offers and acceptances.)
48
Almost all new American doctors go through a centralized labor market clear -
inghouse when seeking their first positions as medical “residents” in teaching hos-pitals, and many doctors go through it again as they seek more senior positions. This became a widespread form of marketplace in medicine because the market for newly graduated physicians has essentially served as a model for other parts of the medical labor market, as many of these markets for more specialized, more senior physicians recapitulated the marketplace failures that had been encountered and overcome in the market for new doctors.
Briefly, the market for new doctors unraveled, first incrementally and then by
leaps and bounds, in the first half of the twentieth century, until by 1945 new doctors were customarily hired two years before they graduated from medical school. From 1945 through 1950 there were successful attempts to specify a time before which offers should not be made, but (as in the markets for lawyers some years later) this led to congestion, and exploding offers that gave applicants and employers little opportunity to consider multiple opportunities.



---Economics-2018-0-16.txt---
In the early 1950s, a collection of medical groups proposed and implemented a
centralized clearinghouse. Applications and interviews would go on as before, but
instead of being followed by decentralized offers and acceptances or rejections, both applicants and directors of hospital residency programs would be invited to sub-mit a rank ordering (in order of preference) of those they had interviewed. That is, applicants would submit a rank ordering of the positions to which they still wished to apply after interviewing (i.e., a first choice, second choice, etc.), and program directors (who I will call “hospitals”) would submit a rank ordering of applicants
they had interviewed and would be willing to hire.
An algorithm would process these preference lists and propose a matching of
applicants to hospitals, and the matched parties were encouraged to exchange promptly a signed contract.
49 After a little trial and error, an algorithm was settled
on that was later shown (in Roth 1984) to produce a stable matching, in the sense,
defined by Gale and Shapley (1962), that no applicant and hospital who were not matched to one another would both prefer to be matched to the other than to their proposed match, and thus form a “blocking pair.”
50 The empirical evidence is that
producing a matching that is stable in this way is important for the long-term suc-cess of this kind of clearinghouse (see, e.g., Roth 1991a, Kagel and Roth 2000). The importance of stable matchings for the success of a clearinghouse is closely related to the fact that market participants have large strategy sets that may involve actions taken outside of the marketplace. If a matching is not stable, then there are blocking pairs of doctors and hospitals not matched to one another who would both prefer to be, and it is often hard to prevent them from circumventing the marketplace and matching together.
51
But another important feature of the computerized clearinghouse was how it
eliminated congestion that arose when offers were made in a decentralized way.52
The easiest way to see this is to consider a version of the Deferred Acceptance
algorithm defined by Gale and Shapley, which is the basis for the more complicated algorithm now used. It takes as input the preference lists submitted by the applicants and hospitals, and performs the following operations (phrased here as if the indi-viduals involved were making and responding to applications, but all performed at computer speed).
•
Initially
, each applicant applies to his/her top choice hospital, and each hospital
h with q positions holds the top q applications among the acceptable applica-tions it receives, and rejects all others.



---Economics-2018-0-17.txt---
• At each subsequent step of the algorithm, an y applicant rejected at the previous
step applies to his or her most preferred acceptable hospital that hasn’t yet
rejected him/her. (If no acceptable choices remain, he/she makes no further
applications.) Each hospital holds its q most preferred acceptable applications to date, and rejects the rest.
•
The algorithm stops when no further applications are made, and the resulting match is the one in which each hospital accepts the applicants
(if any) whose
applications it is holding.
Gale and Shapley called this a deferred acceptance algorithm because the deter -
mination of which applications are accepted by each hospital is deferred until the end of the application process, i.e., until no more applications are forthcoming (either because every applicant has an application being held by a hospital, or has been rejected by all hospitals on the applicant’s preference list). They observed that, no matter what preference rank-orderings are submitted to the algorithm, the matching it produces is always stable with respect to those preferences, because any applicant who would prefer to be matched to a different hospital must have already applied to that hospital and been rejected because it could already fill all of its positions with applicants it preferred.
The DA algorithm organized in this way (with applicants applying rather than
firms making offers) makes it safe (in fact a dominant strategy) for applicants to submit rank order lists that correspond to their true preferences. Mechanisms that make it a dominant strategy to reveal true preferences are called “strategy-proof,” because participants don’t have to make strategic calculations about what others are doing, they just have to decide what they like. And although no mechanism that always produces stable matchings can be strategy-proof for hospitals employ-ing multiple residents, this proves to be less of an issue in this market than it once seemed, because it turns out that the set of stable matchings is very small, and that this and related properties of the market limit the benefits that anyone can even hope to achieve by misrepresenting their preferences.
53
In the course of the Deferred Acceptance algorithm, many offers may have to be
made. But because computers are fast, and because the algorithm doesn’t end until each hospital has had a chance to consider every application that it can get, this process isn’t congested. In particular, every participant can submit a long preference list, and be confident that they will not be forced to make a match before exploring all available alternatives.
54



---Economics-2018-0-18.txt---
One way in which it becomes apparent that this kind of marketplace for doctors,
and indeed the whole market for doctors, is part of a much larger labor market is
in the changing demographics of the medical labor force. In the 1950s, almost all graduates of American medical schools were men, and today the graduates are quite equally divided between women and men. One consequence of that change has been a steadily increasing number of couples graduating from medical school together and seeking two jobs, not one. A clearinghouse in which each individual submitted a rank order list meant to represent his or her individual preferences didn’t work for couples.
55 But in the presence of couples who submit preference lists ranking pairs
of positions, stable matches are not guaranteed to always exist (Roth 1984).
It was thus apparent to me, when I agreed in 1995 to direct a redesign of the resident
match, that market design was going to require answers to different questions than had so far been asked. Instead of being satisfied with theorems indicating that stable matchings always exist in simple matching problems, but might sometimes fail to exist when couples (or other instances of complementary preferences) were present, it was going to be important to know, for example, how often stable match-ings might fail to exist when couples are in the market.
56 The algorithm we designed
to meet the changing needs of the medical match (Roth and Peranson 1999) allowed us to show empirically not only that the set of stable matches was small, but also that it was virtually never empty. As is often the case in market design, this empir -
ical observation, which enabled the Roth-Peranson algorithm to be implemented with confidence (albeit with a check of the final stability of the outcome), preceded and eventually inspired the theoretical work that has begun to explain why stable matchings involving couples can be found in large markets (see Kojima, Pathak, and Roth 2013, and Ashlagi, Braverman, and Hassidim 2014). Today dozens of medical and healthcare marketplaces use centralized clearinghouses that employ the Roth-Peranson algorithm, and there are a number of clearinghouses that do not deal with couples and use other variants of the deferred acceptance algorithm.
57



---Economics-2018-0-19.txt---
Sometimes, establishing a clearinghouse to replace an unraveled decentralized
market involves designing the way that offers in the larger decentralized market and
in the centralized marketplace will interact.58 This was an issue in the ( re)establishment
of a centralized clearinghouse for gastroenterology fellows, to replace a market that had unraveled steadily since a previous clearinghouse had been aban-doned (Niederle and Roth 2003b) . One purpose of the match was to move hiring
of fellows later in the careers of internal medicine residents, who would become gastroenterology fellows only after completing their residency. But there was a concern that those fellowship programs that delayed making offers until the later time at which the clearinghouse would operate would be “scooped” by their com-petitors who might keep making early offers. That is, the clearinghouse was to be a marketplace in a larger market, in which early hiring might still continue, because the gastroenterology organizations did not have any ability to restrain fel-lowship directors from continuing to make early offers. What the professional soci-eties could do, however, was empower students who had accepted an early offer to change their minds and later participate in the centralized marketplace. This was sufficient to get the clearinghouse off to a successful start ( Niederle, Proctor, and
Roth 2006, 2008) .
59
The problems that cause medical markets to organize centralized clearinghouses
aren’t a thing of the past. Several medical subspecialties have recently organized clearinghouses to combat unraveling.
60 But for many medical labor markets, centralized
clearinghouses are a mature technology, and strategies for navigating the market have adapted.
In particular, when markets are unraveled, there isn’t a lot of interviewing, because
attractive applicants get early exploding offers as programs try to fill their positions before their competitors, and programs don’t need to conduct further interviews once their positions are filled. But when a centralized clearinghouse allows partici-pants to consider many alternatives, both employers and applicants are interested in gathering and transmitting information in interviews, to determine their preferences, and to impress their counterparties with how desirable they are. Consequently, res-idency and fellowship programs conduct many interviews. Unlike the market for new economists, there is no central national meeting at which a large proportion


---Economics-2018-0-20.txt---
of interviews are conducted. Programs host interviews at their own location, inter -
viewing many candidates on the same day, and applicants travel to many interviews,
paying their own way (and taking time off from medical school or from their current
residency position). The increase in the number of interviews for residencies has grown in tandem with the increased number of applications, due in part to the advent of internet-based applications through the Electronic Residency Application Service (ERAS). There is increasing concern about the number of applications submitted and interviews conducted, both because of the time they take and their costs (see, e.g., Gruppuso and Adashi 2017). This concern is prompting discussions about ways to relieve congestion in the market by reducing the number of interviews.
Possible ways to address this congestion might include limiting the number
of applications each individual can make through ERAS, or limiting the number of interviews each residency or fellowship program can conduct. Either of these approaches might be combined with a signaling mechanism to help guide the matching for interviews. Each of these approaches raises open questions.
61 Past
experience suggests that addressing these practical problems will also lead to new theoretical understanding of the underlying market processes.
62
B. School Choice
Another domain in which clearinghouses organized by the deferred acceptance
algorithm now play a large role is in school choice. Atila Abdulkadiro˘ glu, Parag Pathak, and I helped New York City organize the clearinghouse they have used for high school admissions each year since it was introduced for students entering high school in the Fall of 2004 (Abdulkadiro˘ glu, Pathak, and Roth 2005, 2009). It replaced a congested, decentralized process in which schools made independent admissions decisions, and many students were assigned administratively to schools over which they had expressed no preference when the process ran out of time. The NYC school choice problem resembled those facing medical labor markets, with active strategic players on both sides of the market, because school principals in


---Economics-2018-0-21.txt---
NYC play an active role in admissions. So the stability of the resulting matching
was an important feature of the design: there was evidence that in the prior system, which did not produce stable matchings, school principals sometimes concealed places from the NYC Department of Education, to be able to match later with stu-dents they preferred to those to whom they might otherwise have been assigned.
Data from the preferences over schools that students submit to the current clear -
inghouse allow Abdulkadiro˘ glu, Agarwal, and Pathak (2017) to compare the welfare effects of the matchings produced by the centralized school choice system to those under the old, “uncoordinated” admissions. Using the (ordinal) rank order lists submitted by students, they assess welfare by estimating a cardinal random utility model, with trade-offs among school attributes measured in terms of the additional distance a student is willing to travel to be at a more preferred school.
63 They find
that the students placed administratively when the old congested system ran out of time were generally placed in less desirable schools than those over which they had expressed preferences. Furthermore, in the old system there was more “recon-tracting,” in which students ultimately entered different schools than those to which they had been assigned, than in the new system. They find that the new system improves welfare over the old by 80 percent of the gains that could be achieved by a utility-maximizing allocation made independent of stability constraints, and that changes in the algorithm (e.g., choosing a different stable matching, among the multiple that arise from random tie-breaking) would have very little additional effect on welfare.
64
The biggest difference is that under the old system, only about one-half of the
students were placed in the “main round” (now occupied by the deferred acceptance algorithm), whereas in the new system this number climbed to over 80 percent in the first year (with additional subsequent gains). So students who used to be assigned administratively are now largely assigned by the deferred acceptance algorithm to a school over which they have expressed a preference. That turns out to be much better for them than assignment without regard to their preferences.


---Economics-2018-0-22.txt---
School districts in other cities have since adopted similar clearinghouses. Boston
Public Schools, after lengthy study, also adopted a clearinghouse based on the
deferred acceptance algorithm, for students entering in Fall of 2006 (Abdulkadiro˘ glu and Sönmez 2003; Abdulkadiro˘ glu et al. 2005, 2006). An important issue in Boston was how strategy-proofness “leveled the playing field” between families that were sophisticated about the way the school choice mechanism worked, and those who might simply submit their true preferences even when this was not their best course of action (Abdulkadiro˘ glu et al. 2005; Pathak and Sönmez 2008).
65 Subsequently,
my colleagues and I have collaborated on the design of school choice clearing-houses in Denver, Washington, DC, Newark, New Orleans, Camden, Indianapolis, and Chicago.
66 In general, our goal is to enroll both district and charter schools in
a unified enrollment system that will give a single best offer to each student. This is because a big cause of congestion in school assignment occurs when some students are admitted to multiple schools and others must wait for their decision before the unused places become available, and this happens when district schools and charter schools run independent admissions.
67
In many of these cities, the individual schools are not strategic players, and
instead of using preference orderings of students compiled by school principals, the clearinghouse takes as input priority lists established by the school district to determine each student’s priority at each school.
68 These priorities, and even the
menu of schools to which students living in different parts of the city may apply, are subject to changes from time to time. This reflects the fact that which children attend which schools is an intense focus of city politics, and so the priorities that different groups of children have for different schools is part of the school choice process that is perhaps inevitably subject to change over time. I think effective school choice design should seek to put in place a mechanism that allows city politics to do its work of adjusting how school places are distributed among different constituencies, but allows the information provided each year by parents to be effectively elicited and used to place children in schools that will be good for them, with as efficient allocation of school places as possible.
69


---Economics-2018-0-23.txt---
Market and marketplaces of all kinds also require a certain amount of trust to
gain and keep public support, and clearinghouses that operate under clear rules can
make school assignment transparent even in cities in which school assignments were once widely viewed as corrupt. This role of the school choice clearinghouse in Washington, DC was emphasized by the forced resignation of the school system Chancellor in February 2018, who had earlier in the year asked a Deputy Mayor to transfer the Chancellor’s daughter into a popular high school to which she could not have been assigned according to the rules. ( Note again that market participants can
have large strategy sets that sometimes allow marketplace rules to be circumvented.) However, the existence of formal rules made their violation apparent in this case, and the Mayor quickly demanded and received the resignation of both officials involved, in an attempt to maintain trust in the system ( Stein, Jamison, and Nirappil 2018) .
Because the schools are not strategic players in many cities, the case for requiring
stable matchings is not as compelling as in markets in which stability helps achieve long-term orderly participation in a clearinghouse. Other strategy-proof clearing-house mechanisms have been considered, although seldom adopted.
70
The two main sources of congestion in school choice are in applications, and in
offers and acceptances. When applications are decentralized, it may be hard to learn about schools and how to apply to them, and time consuming to submit many differ -
ent applications. (So this is an example in which bringing applications together in a single marketplace can reduce congestion.) And when admissions and acceptances are decentralized, some students are offered multiple positions, and others must wait while they decide. In school choice, congestion can therefore be reduced by making the market thicker so that it includes all schools in a single unified admissions clear -
inghouse that gives each student a single offer.
Computerized clearinghouses thus help deal with school choice congestion in
three ways: by bringing together the application process for many schools in a single platform, by using computer speed for processing offers, acceptances, and rejections according to the submitted preference lists, and by asking for those preference lists (and hence acceptance and rejection decisions) in advance, so the process is not
delayed while participants decide on their preferences.
71
To summarize, centrally organized computerized clearinghouses that produce
stable matchings based on preference orderings submitted by market participants have succeeded as a form of market organization in a wide range of medical and


---Economics-2018-0-24.txt---
related labor markets, and in school choice. But they have remained a relatively rare
form of marketplace in the economy at large.
One feature of the markets that have adopted these clearinghouses is that almost
all the positions to be filled become vacant at the same time, and most of the appli-cants for these positions come from an easily identified set of candidates, who apply for very few other kinds of positions and who make up the lion’s share of those hired for these positions. (Thus, most medical graduates go on to residency positions, most medical subspecialty fellowships are filled by candidates who have already become certified in the underlying specialty, most eighth graders need to be assigned to high schools, etc.) Prior to adopting a centralized clearinghouse, each of these markets also struggled with establishing and maintaining thickness and/or dealing with congestion.
Another reason we don’t see so many centralized clearinghouses may be that
when preference formation is very costly, eliciting preferences in advance may not be feasible. For example, in senior labor markets, preferences over possible posi-tions may involve additional searches, for jobs for spouses and schools for children. These additional searches may often be too costly to launch over a wide range of possible positions, and so may be feasible only late in the recruiting process, or following receipt of a job offer, when the search can be focused on a small number of possibilities. Similarly, when employers consider very diverse pools of candi-dates, or candidates consider very diverse kinds of employment, the difficulties in organizing participation in a clearinghouse by the relevant candidates and firms may be insurmountable. As a practical matter, an existing marketplace often has to be performing very badly before a major reorganization becomes attractive to many participants, since otherwise it is difficult for a redesign to be a Pareto improvement that lifts all boats or most of them (and because some market participants may have invested in dealing with the current market environment).
72 So there will be markets
for which this kind of centralized clearinghouse may not be feasible, as well as those for which this form of organization may not be appropriate.
73


---Economics-2018-0-25.txt---
There’s still a lot to learn about the range of markets that could benefit from
centralized clearinghouses, and, more generally, which forms of marketplaces best
facilitate which kinds of markets.
But that question supposes that we want to facilitate markets. There are often
markets that a substantial number of people wish to impede or prevent entirely, but that others wish to participate in. These present different kinds of market design problems.
IV . Repugnant T ransactions, Forbidden Markets, and Black Markets
Tastes differ, so it is not surprising to find transactions that some people find
appealing and others find appalling.74 More puzzling are transactions that some
people would like to engage in but that others would like to forbid, even when negative externalities are hard to identify or measure. I’ll reserve use of the word “repugnant” for these latter kinds of transactions, as in Roth (2007).
Same-sex marriage is an example of such a transaction: two people would like
to marry each other, while others don’t think they should be allowed to. Laws and customs preventing same-sex marriage have been struck down around the world in recent decades, amid considerable political turmoil. In the US, these laws fell state by state through a combination of court decisions, legislation, and referenda, culminating in a Supreme Court decision in 2015 making same-sex marriage legal in every state. (This decision came a half-century after the Supreme Court decision that legalized interracial marriage in every state.)
75
Laws that criminalize or strictly limit transactions sometimes contribute to the
design of black markets. For example, in the United States it was illegal to sell most alcohol during Prohibition (1920–1933), and today prostitution and narcotic drugs like heroin are broadly illegal throughout the United States. But black markets for alcohol were widespread during Prohibition, and both sex and narcotics are readily found for sale in many American venues today.
Markets for drugs provide a clear example of how illegal markets can thrive.
Under American law, opioids and other drugs with a high potential for abuse and addiction are legally classified as either Schedule I drugs, like heroin, which have no currently accepted medical use in the United States, or Schedule II drugs, like oxycodone and fentanyl, which are available as prescription painkillers, but cannot be sold or administered legally except when prescribed by a physician.
76 The law
pro
vides severe penalties for sale, resale, or inappropriate prescription of such drugs,
and is vigorously enforced—often with mandatory minimum sentencing laws—so


---Economics-2018-0-26.txt---
that American prisons are filled with people convicted of drug offenses. The Federal
Bureau of Prisons reports as of January 2018 that 46 percent of all inmates were convicted of drug offenses.
77 Yet there are over 60,000 deaths a year from opioid
overdoses in the United States. And users have large strategy sets: these deaths are split between Schedule I drugs like heroin and Schedule II prescription opioids. That is, opioids can be supplied both by criminal organizations that produce the drugs, and by the diversion or misuse of legal prescription drugs.
78
There is of course a simple economic model of rational crime and addiction that
predicts that if the penalties are high enough, very few people will sell drugs ille-gally or use them, so that the prisons will be virtually empty and deaths from over -
doses will be minimal. But that is not what we are seeing: there is some flaw in that market design.
Yet not every badly functioning market is ripe for redesign.
79 Sometimes the
political and social obstacles to a good design, that historically caused a market to work poorly, persist. Thus, although there is a much bigger problem with opioids in the United States than in Europe (where there are fewer overdose deaths, and substantially less prescription of pain relief medicines), there is no clear politically feasible path forward for American policymakers who might wish, e.g., to decrimi-nalize narcotic addiction as in Portugal (Greenwald 2009), where the legal options successfully divert customers from the illegal ones. The most likely regulatory ave-nues to limit legal opioids will involve changing prescription practices to treat pain much less, or very differently (as is the case in many other countries).
80
Modest marketplace design changes in how we deal with the opioid crisis may
be more quickly attainable (but by no means politically easy). Steps to reduce the harms of the drug markets are being explored in various venues. The most widely supported of these in the United States are needle exchanges to reduce the spread of infectious diseases among intravenous drug users (US Department of Health and Human Services 2016). More controversial are “drug checking” to allow purchasers of illegal drugs to have them tested for purity and potency (Hungerbuehler, Bhecheli, and Schaub 2011), and supervised injection facilities to promote safer drug injection practices and monitor potential overdoses (Potier et al. 2014). All of these are “harm reduction” measures intended to acknowledge the presence of active black markets, and to respond with modifications of these illegal marketplaces to give their partici-pants access to some ancillary infrastructure and services from legal sources.
81


---Economics-2018-0-27.txt---
Just as legal markets require social support to thrive, laws banning markets require
social support to have a good chance to be effective. In the United States, marijuana
is still a banned Schedule I drug under Federal law.82 But after a long history as a
popular illegal drug, 30 states and Washington, DC have legalized marijuana in some forms and for at least some uses, and legalization has proceeded in other countries as well.
83 And just as small marketplaces have to accommodate themselves to large
markets, small banned markets are influenced by adjacent legal
mark
etplaces.84 For
example, as of this writing, marijuana remains banned in Idaho, which shares bor -
ders with three states (Washington, Oregon, and Nevada) in which marijuana is
legal for recreational use, and three more (Utah, Wyoming, and Montana) in which some uses of marijuana are legal. I anticipate that enforcement of Idaho’s ban will become more difficult.
85 There are indications that legalization has resulted in harm
reduction: it appears that competition from legal suppliers of marijuana has reduced violent crime in the United States connected with imports of marijuana by criminal organizations in Mexico (Gavrilova, Kamada, and Zoutman forthcoming).
Among the oldest repugnant markets are markets for sex.
86 Prostitution is called
the oldest profession,87 and there is a long history of largely failed attempts to eliminate
it. In many countries prostitution (the selling of sex) is a criminal offense. In Iceland, Norway, and Sweden the purchase of sex has lately been criminalized while the sale has been decriminalized, and in many countries the participation of third
parties (“pimps” or brothels) is outlawed even when the purchase and sale of sex is
legal in at least some venues.
88 In the United States, prostitution is widely illegal,
from sites that are not themselves fully legal; see, e.g., Kral and Davidson (2017) on an “Unsanctioned Supervised
Injection Site in the US.” But see Massachusetts Medical Society (2017) for a proposal to set up a pilot program in the United States. Opponents argue that harm reduction may make drug use more socially acceptable and wide-spread, or even that reducing the risk of death and disease may lessen the disincentives and lead to a growth of addiction.


---Economics-2018-0-28.txt---
with the exception of some counties in Nevada, and a period of legal indoor prostitution
in Rhode Island from about 2003 to 2009, following a 2003 judicial decision involving an apparent oversight in an amended anti-prostitution law that defined prostitution as involving outdoor solicitation, which was rectified in 2009 to make both the buying and selling of sex illegal in any venue.
There is evidence that decriminalization confers some harm reduction.
Cunningham and Shah (forthcoming) report that the period in which indoor pros-titution thrived in Rhode Island resulted in fewer sexual crimes and lower rates of sexually transmitted disease. Similar results regarding crime in the Netherlands are reported by Bisschop, Kastoryano, and van der Klaauw (2017) following the establishment of regions (“tippelzones”) in which licensed prostitutes could legally solicit customers on the street.
The market for sex has many marketplaces.
89 It appears that outdoor solicitation
(“streetwalking”) is the most dangerous of these for sex workers, in terms of
encounters both with violent customers and with law enforcement.90 Some of the
more discreet markets are hard for law enforcement to successfully prosecute.
For example, Elliott Spitzer resigned as Governor of New York State after just one
year, after being reported as a customer of an “escort agency” called the Emperor’s Club.
91 Only the organizer of that agency was convicted of a crime. But although
Spitzer was never charged, the repugnance felt toward prostitution is reflected in the speed with which his political career ended. The New York Times broke the story on March 10, 2008 with the headline “Spitzer Is Linked to Prostitution Ring.”
92 Just
two days later, the headline was “Spitzer Resigns, Citing Personal Failings.”93
Part of the negative reaction to Spitzer’s patronization of prostitutes was that he
had prosecuted prostitution when he was Attorney General. One of the organizations Spitzer had prosecuted adopted the market design that gives rise to the expression “call girls.” It operated by phone, and customers called to arrange that sex workers would come to them.
The internet has provided the newest marketplaces for prostitution, and between
2002 and 2010 one of the most active internet marketplaces was Craigslist’s “erotic services” ads. Craigslist eventually closed this section in response to pressure from legal authorities who threatened prosecution for violating the anti-prostitution laws. However, Cunningham, DeAngelo, and Tripp (2017) use the different times at which this service was introduced in different cities to estimate that it reduced the female homicide rate by 17 percent. They attribute this to a reduction in street solicitation, as prostitutes who would formerly have tried to find clients outdoors were able to

---Economics-2018-0-29.txt---
screen them more reliably if the initial contacts were through email.94 So there also
seem to be some harm reduction benefits of moving marketplaces for prostitution
off the street.
Another market that is repugnant and legally limited or banned in many venues,
but is fully legal in others, is surrogacy, i.e., the market for someone to bear a child for someone else. The most usual form is gestational surrogacy, in which a woman agrees to be impregnated with a fertilized egg, and to bring the child to term. In this kind of surrogacy, the pregnant surrogate mother is not genetically related to the child. The surrogate parents, i.e., the parents who intend to assume parental rights and raise the child, may have contributed both sperm and egg, or just the egg, or just sperm, or have obtained the fertilized egg with the help of both a sperm and egg donor. Male couples are often clients in surrogate pregnancies, since they have no womb between them. However single parents and heterosexual couples can also be clients for a variety of medical and other reasons.
95
In some jurisdictions (including much of Western Europe), surrogacy is illegal,
and there are legal obstacles that may make it difficult for parents to repatriate a surrogate child born elsewhere. An extreme example comes from Italy, where the courts removed a surrogate child from the parents’ custody and placed it with a social service agency for subsequent adoption. (In this case the surrogate parents were not genetically related to the child.) The European Court of Human Rights (ECHR) ruled that Italy was within its rights “to reaffirm the State’s exclusive com-petence to recognize a legal parent-child relationship” (ECHR 2017). But the urge to have children is strong, and adoption is not a perfect substitute, so families are also willing to take extreme measures. Sweden, where surrogacy is illegal, has been a pioneer in womb transplantation, for example.
In other places, for example England and Canada and some American states,
surrogacy is legal but “commercial surrogacy” is not: i.e., it is illegal to pay the sur -
rogate mother (although some expenses may be reimbursed). This severely limits the availability of surrogates in these places, mostly to family members, so that for example a woman’s mother may become her surrogate, and give birth to her own (the surrogate’s) grandchild. Other limitations on surrogacy limit its reliable use in family planning: in England and Canada the birth mother (i.e., the surrogate) is the legally presumed mother of the child, and is not allowed to give up her maternal


---Economics-2018-0-30.txt---
rights until some days after the child is born. It of course happens in some cases that
the surrogate chooses to retain parental rights, including custody of the child.
For these reasons and others, there is a lively market in “fertility tourism” to
places where surrogacy is legal and reliable, such as California. In California there are reliable commercial contracts that permit surrogate parents to pay a surrogate, and be named as parents on the California birth certificate.
96
There have also been thriving markets for surrogacy in Asia, but some of the
countries that hosted many fertility tourists are now outlawing commercial surro-gacy for foreign parents.
Thus, the repugnance associated with surrogacy is complex.
97 Some repugnance
focuses on issues related to the rights of children and the regulation of family for -
mation, while other concerns focus on the rights of surrogates, and the potential that they can be exploited. These latter concerns motivate laws against compensating surrogates. Note that in jurisdictions that permit surrogacy but forbid compensa-tion, a transaction that is apparently not repugnant when supplied for free becomes repugnant when money is added. This isn’t such an uncommon distinction, (e.g.,
prostitution is banned in many places where promiscuity is allowed, and charging interest on loans was repugnant in the middle ages, although loans were allowed), and is related to concerns about exploitation and coercion.
98 Similar concerns arise
in connection with bans against compensating kidney donors.
V . Kidney Exchange
Kidney failure (End Stage Renal Disease, or ESRD) is a major cause of death
in both the developed and developing world. Care for ESRD is a $50 billion a year industry in the United States, and takes up about 7 percent of the Medicare budget. Transplantation is the treatment of choice: dialysis can extend a patient’s life but is hardly a cure for ESRD, while most patients can resume an essentially healthy life after receiving a transplant (Liyange et al. 2015; USRDS 2017).
Around 100,000 American patients are on the waiting list for a deceased donor
transplant, but in 2017 only about 14,000 deceased donor kidneys were transplanted, while around 8,000 patients either died while waiting or were removed from the waiting list after becoming too sick to transplant. Healthy people have two kidneys and can remain healthy with one, so live donation is a very practical possibility, and in 2017 almost 6,000 living transplants were performed in the United States.
99


---Economics-2018-0-31.txt---
Two facts strike an economist looking into kidney transplantation for the first
time. There is a dramatic shortage of organs compared to the need. And it is against
the law almost everywhere in the world to pay a living kidney donor, or the family of a deceased donor, for a donation. By law, kidneys must be gifts, offered at a price of zero.
Becker and Elías (2007) argue that repealing the laws against paying donors
could solve the organ shortage, and estimate that the market clearing price would be low. Held et al. (2016) estimate that the amount that the American healthcare system would save from each additional transplant and could therefore afford to pay for a kidney donation would be much higher than needed to end the kidney shortage, because transplantation is both much cheaper and more beneficial than the alterna-tive treatment of dialysis. These papers are part of a large literature arguing for or against compensation for donors, not only of organs for transplant, but of other parts related to the body, like blood and blood plasma, sperm and eggs, surrogacy, etc.
100
However, only one country in the world, Iran, has a market in which living kidney donors can legally be paid.
101 Everywhere else, it is illegal to compensate donors,
although there are active black markets. So buying and selling kidneys is a repug-nant transaction in the sense I described earlier.
Kidney exchange is a way to increase the number of organs available for transplant
without paying donors. Sometimes a person is healthy enough to donate a kid-ney, and would like to donate to a loved one but cannot, because not every kidney is compatible with every patient. In the past, a patient with a willing but incompatible living donor would have to continue to wait for a deceased donor, but the idea of kidney exchange is that two incompatible patient-donor pairs could exchange kid-neys, so that each patient would receive a compatible kidney from the other patient’s donor. This was first suggested by a surgeon who studied transplant incompatibil-ity (Rapoport 1986), who proposed that an international registry of incompatible
patient-donor pairs be established to facilitate kidney exchange. The first kidney exchanges were not performed until the 1990s, and interestingly the first kidney exchange in Europe was between two pairs of different nationalities: Swiss and German. When the surgeries were reported in Theil et al. (2001), some German critics compared it to organ trafficking, and indeed repugnance almost completely prevents kidney exchange in Germany up to the present time.
102 For the early history
of kidney exchange, see Wallis et al. (2011).


---Economics-2018-0-32.txt---
In the United States, the first kidney exchange occurred in 2000, and, soon after,
Utku Ünver, Tayfun Sönmez, and I started to formulate ways in which exchanges
could be organized on a larger scale (Roth, Sönmez, and Ünver 2004, 2005b, 2007, and Roth et al. 2006). We considered how to organize exchanges between different numbers of patient-donor pairs, and in potentially long non-simultaneous chains started by a non-directed donor (i.e., a donor who does not have a particular patient
in mind). We helped our surgical colleagues, led by Frank Delmonico, to found the New England Program for Kidney Exchange (NEPKE) to build a database of patient donor pairs and facilitate exchange at scale among the 14 transplant centers in New England (Delmonico et al. 2004; Roth, Sönmez, and Ünver 2005b).
Today, kidney exchange has become established as a standard mode of transplantation
in the United States, and is growing around the world.
103 But initial progress was
slow. By the end of 2007, NEPKE had facilitated only 22 transplants.104 It became
clear that effective market designs were going to have to deal with the detailed oper -
ations of kidney exchange marketplaces, and not just with their overall architecture.
The logistics of trades among cycles of patient-donor pairs are subject to congestion
(since all parts of the exchange need to be conducted simultaneously),
and so there has been increased attention to the chains of transplants started by a non-directed donation, which don’t have to be performed simultaneously. (Since a
non-directed donor does not ha
ve a particular patient in mind, he or she can initiate a
chain of transplants by donating to a patient in a pair waiting for a kidney exchange, whose donor gives to someone else and so on. Each incompatible patient-donor pair receives a kidney before they donate their own, so that no pair bears the risk that a broken link in the chain would leave them still needing a transplant but no longer having a kidney to exchange.) The first such chain was organized by the Alliance for Paired Donation (APD), and reported in Rees et al. (2009). One of the big oper -
ational issues was how to manage the sometimes long non-simultaneous chains that became possible. These chains were initially controversial, but early experience together with computational simulations from clinical data, paying careful attention to how such chains could be organized, eventually allowed them to become standard practice (Ashlagi et al. 2011a, b).
105
As chains became national in scope, hospitals with different costs started shipping
kidneys to each other, and there were some financial frictions (e.g., one hos-pital might charge much more than another for a nephrectomy, which could make


---Economics-2018-0-33.txt---
it difficult for them to bill each other for their costs). Proposals to overcome such
frictions have to date been implemented only partially (see Rees et al. 2012).
Fumo et al. (2015) discuss many incremental design changes made over the
years by the APD. Many of these were related to the fact that an optimized kidney exchange involves many particular proposed transplants, each of which has to be individually approved by the surgeons and patients and donors involved. It can take time to get these approvals, and there were initially many rejections of proposed transplants, which made the kidney exchange process slow, and the marketplace congested.
Some sources of congestion and declined transplants were relatively simple to
fix, e.g., by promptly updating the database so that no time is wasted proposing transplants for patients who may have already received a deceased donor transplant, or died.
106 Other sources of delay through rejected offers were more complex.
Surgeons’ preferences over kidneys are not so easy to elicit, because a kidney and its donor have many properties that can interact. It is difficult to entice surgeons to review in advance potential kidneys that they may never be offered: consequently many kidneys that are medically compatible may nevertheless be rejected. We have made partial progress on this by introducing a threshold language that allows sur -
geons to constrain the set of compatible kidneys that they can be offered, so that offers are more likely to be accepted.
107
A different set of operational problems arose as transplant centers became accustomed
to kidney exchange. Directors of transplant centers have bigger strategy sets than individual surgeons, since they see multiple patient-donor pairs and can choose which ones to enroll in an interhospital exchange. Ashlagi and Roth (2014) note that transplant centers may have incentives to withhold their easy to match patient-donor pairs from the interhospital exchanges, and transplant them internally, and Agarwal et al. (2017, 2018) observe that this is in fact happening to a surprisingly large degree (they report that more than one-half of kidney exchanges are done inter -
nally).
108 This reduces the thickness of the inter-hospital kidney exchange marketplaces,
not only because it reduces the number of pairs that they enroll, but because it means that those that do enroll may have difficulty finding donors and recipients



---Economics-2018-0-34.txt---
with whom to match.109 This is one of the reasons that non-directed donors and
potentially long chains are so useful: a high concentration of hard to match pairs
means that few simple exchanges between two pairs will be feasible (and these are the exchanges that are further removed from the interhospital marketplace because they can be transplanted internally by their own transplant center).
110
Throughout these developments (and unlike the situation in Germany), kidney
exchange faced little repugnance in the US, although this wasn’t initially obvious. The National Organ Transplant Act (NOTA ) of 1984 specifies that “It shall be unlawful
for any person to knowingly acquire, receive, or otherwise transfer any human organ for valuable consideration for use in human transplantation.” This raised a potential barrier to kidney exchange, if a kidney in return for a kidney is viewed as “valuable consideration” of the kind precluded by the NOTA. The Department of Justice initially declined to issue an opinion on the legality of kidney exchange. But as kidney exchange began to be performed, Congress was prevailed upon to amend the NOTA via the Norwood Act (Public Law 110-144, 2007), which said that the sentence about valuable consideration “does not apply” to kidney exchange.
111
Kidney exchange is one of few examples that achieve the triple aim of healthcare
reform: improved care, reduced cost, and increased access. There is still work to do to make kidney exchange marketplaces work better.
112 But if I stopped here, it could
appear as if the story were a simple one, of victory after victory for market design and kidney exchange.
But these victories come in a war that we are losing: there are more people waiting
for kidney transplants today than there were when I first began to think about transplants.
113 So it will not be enough to keep improving the current operations of


---Economics-2018-0-35.txt---
existing kidney exchange organizations around the world. The large-scale further
progress that is needed (while we wait until transplants are no longer needed) will depend on increasing the scale and scope of kidney exchange.
For example, given the substantial number of transplants produced by chains
begun by living non-directed donors, it would be desirable to allow some chains to be initiated by deceased donors. Presently, virtually all deceased donors are non-di-rected, but each deceased donor kidney results in only a single transplant. Melcher et al. (2016) propose allowing chains to be initiated by deceased donors, which would
require changes in regulation that are currently under consideration in the United States.
114 Increasing the average number of transplants from a deceased donor kidney
even from one to two would more than double the reach of kidney exchange.
The approach that I think has the greatest possibility to reduce suffering from
kidney failure—both in the developed world and in the developing world, where it is also major cause of death—is to expand kidney exchange globally, including to places where transplantation is not widely available because of financial con-straints.
115 This is feasible because in the developed world transplantation is so
much cheaper than dialysis that each transplant generates savings sufficient to pay for the inclusion of a foreign pair in kidney exchange, for free, including funds to pay for their long-term post-surgical care after returning home.
116 Rees et al.
(2017a) set forth a proposal for Global Kidney Exchange (GKE), and report on the first foreign pair included in an American chain in this way.
117 That patient-donor
pair was a married couple from the Philippines, where national insurance does not cover treatment for ESRD, but where high quality hospitals exist that can conduct transplants and care for patients and donors post transplant.
The reaction to the GKE proposal has included both support in the US and
Europe, and vigorous opposition that finds GKE repugnant and equates it with ille-gal organ trafficking.
118 A related objection is that GKE would inevitably become
entangled with black markets for kidneys in poor countries. Delmonico and Ascher (2017) write that ethical GKE with patient-donor pairs from the developing world “is not feasible when the culture is so experienced with organ sales.” They argue that efforts to ban kidney sales have failed to such an extent that transplanting patients
114 The proposal for deceased-donor initiated chains met with an initially enthusiastic political reception (see
https://marketdesigner.blogspot.com/2016/06/white-house-organ-summit-video-and.html), and is now wending
its way more slowly through bureaucratic channels. In 2018 a deceased-donor initiated chain was reported in Italy (http://marketdesigner.blogspot.com/2018/04/deceased-donor-kidney-exchange-chain-in.html). It would also be useful to increase the supply of deceased donors. Other avenues that might possibly increase the scope
of exchange involve including other organs in exchanges (see Dickerson and Sandholm 2017; Sönmez and Ünver 2017; Samstein et al. 2018).



---Economics-2018-0-36.txt---
from poor countries would result in paid donors infiltrating American health care in
contrived guises as spouses and relatives.119
So, GKE, which is still in its earliest stages (six such chains have been accomplished
as of this writing), is likely to face the full set of complications that kidney exchange has faced around the world. These include not only the market design issues involved in attracting participants, eliciting their information, and resolv-ing analytic, computational, and operational obstacles, but also in garnering social support.
120
In these respects, kidney exchange and its continued development represent
issues that are typical of market design.
VI. Open Questions , Engineering Challenges, and Opportunities
Markets and marketplaces, broadly understood, are woven into the fabric of the
human environment, so opportunities to build new ones are ubiquitous, and there are still many unresolved scientific and engineering questions about how they work, and what makes them work well.
I’ve already mentioned open questions in the context of the particular markets
I’ve discussed. More generally, what are the roles of marketplaces in markets? What properties of transactions, in addition to prices, do marketplaces help deter -
mine? Which markets are adequately organized without a coordinated marketplace (coordinated in time, or space, or both)? For which markets is a centralized mar -
ketplace desirable? For which markets does a centralized clearinghouse (possibly
computer
-assisted) offer additional benefits? Understanding what well-designed
centralized marketplaces can accomplish will also illuminate how decentralized matching markets succeed and fail, how markets mediate inequality, and other ques-tions underlying the organization of market economies.
Computers have begun to play multiple roles in marketplaces, no longer just
as intermediaries or record keepers. “Smart marketplaces” help markets clear in ways that parallel-processing by participants could not. For example, there are com-putationally difficult problems involved in determining optimal kidney matchings, or in selecting optimal combinations of bids to maximize revenue in a complex

---Economics-2018-0-37.txt---
auction for radio spectrum. Computational speed also helps deal with congestion.
Ads can be matched with internet users by auctions that run quickly enough to keep up with internet attention spans, and stable matchings can be determined without delay. But computerized markets can also increase congestion by easing access, e.g., by increasing the number of job applications it is easy for a job seeker to send. How will computerized speed and intelligence influence the scope and scale of mar -
ketplaces? How will artificial intelligence and machine learning techniques help automate parts of market design?
121 Somewhat separately, as computers become
increasingly important as infrastructure for markets and capital equipment for firms, markets for computation, such as cloud computing, may fundamentally change some
of the make or buy decisions of firms, and how computers are owned and accessed.
Privacy issues arise with special force when markets are computerized, because
computerized markets can create “big” data of participants’ behavior, both of con-ventional sorts (prices, transactions, buyers, sellers) and new sorts (e.g., clickstream and cookie data on search behavior, and location and social network data). There are already markets for these data, connected not only to advertising, but to other kinds of marketing (such as marketing candidates to voters). These data can be combined with other data in ways that allow individuals to be tracked in detail. So a big question for all marketplaces, but especially those that are computerized, is: how can and should privacy be preserved? A technical literature on differential privacy addresses the extent to which privacy can be preserved by databases that employ some randomness (see, e.g., Dwork and Roth 2014). Related questions have to do with markets for individualized data (or, conversely, privacy). These present important
unresolved conceptual difficulties. For example, individuals’ privacy may be correlated with their value for privacy, i.e., a marketplace that allows individuals to implicitly or explicitly express a reservation value for their data may cause individ-uals to reveal their data even when they decline to sell it. As Ghosh and Aaron Roth (2015) put it: “An individual’s cost for privacy may itself be private information. Suppose that Alice visits an oncologist, and subsequently is observed to signifi-cantly increase her value for privacy…”
Rules are data for the study of how marketplaces are designed, and the computerization
of marketplaces also gives us access to data on market rules, precisely encoded in algorithms and apps. Contracts are data too, and contracts are what is for sale in many marketplaces, including computerized marketplaces. The increased availability of precisely coded rules and contracts may lead to an explosion of empir -
ical game theory, parallel to the growth of applied microeconomic research fostered by big datasets of individual behavior.
Repugnance and its causes and consequences are worth much further study.
Economists often shrink from having third parties prevent voluntary transactions



---Economics-2018-0-38.txt---
between informed adults if no negative externalities can be identified. But notice
I say “adults.”122 Children, particularly when they are small, may need to be prevented
from harming themselves, and paternalism (“parentalism?”) is an obligation of being a responsible parent. When might society have such responsibilities toward ordinarily competent adults? To put it another way, when does repugnance point to important issues left out of many economic models?
123
Black markets are often the result of banned markets. If we do decide that some
markets are unacceptable, we need to better understand which markets can be banned effectively (like those for ozone-depleting chlorofluorocarbons), and which must
just be accommodated or contained (like those for narcotics and prostitution). Two questions of a practical engineering sort are how should we best go about banning markets we can effectively prevent, and how should we limit the damage caused by repugnant markets we can’t prevent? For markets we can’t prevent, we need to understand when we should be regulating legal markets, and when we should be engaging in harm reduction in illegal markets, and to do this we need to understand in each case what we want, and what we can get.
Aside from questions about market design generally, many particular design issues
present themselves. Here are two that seem to me particularly worth mentioning.
Refugee resettlement and large-scale human migration present some of the
most pressing engineering problems facing market design. Although recent polit-ical upheavals around the world suggest that immigration may arouse considerable repugnance among some members of host populations, it is clear that resettlement of refugees and migrants is a matching problem. Refugees can’t simply choose where they wish to go, but neither can national and international authorities simply tell them where to settle (Roth 2015b). Once refugees have been granted asylum in a particular country, we need to figure out how to settle them in cities, and housing, and jobs in ways that will allow them to effectively integrate into the host country economy and society. This has a resemblance to school choice, but the differences are as important as the similarities (cf. Delacretaz, Kominers, and Teytelboym 2016; Andersson and Ehlers 2017; Andersson, Ehlers, and Martinello 2018; Bansak et al. 2018; and Jones and Teytelboym forthcoming).
124 A more difficult problem may be
matching refugees and migrants to host countries (cf. Moraga and Rapoport 2015a, b; and van Basshuysen 2017). What is clear is that our present methods for handling human migration leave a lot to be desired: we need to learn to do this better (espe-cially if sea level rises in the coming century).


---Economics-2018-0-39.txt---
Open science presents a different, more open-ended kind of engineering problem.
Do the incentives for scientific investigation and publication need to be modified
to keep up with the changing ways science is produced and disseminated? Gall, Ioannidis, and Maniadis (2017) point out that fostering a reliable scientific literature is a market design problem. Scientific journals in particular are market-places for ideas that can experiment with ways of increasing the long-term reliabil-ity of reported results, for example by allowing study designs to be pre-registered, requiring data to be made widely available, encouraging replications, and generally making it easier to identify (and reward) robustly replicable findings.
125
Any list of open questions is necessarily arbitrary and incomplete, but this is par -
ticularly true of engineering challenges. Some kinds of questions can ( eventually)
be answered definitively, but engineering isn’t like that. Pythagoras’s Theorem is as true today as it was when he proved it, and applies as well to modern right triangles as to ancient ones, but the bridges that were built in his time have been replaced by stronger and longer bridges, and today’s bridges will be replaced by better ones in the future. The job of engineering is to use available knowledge to make things work bet-ter, and the market designs of today will be replaced by different designs as our needs change and as we understand better how to achieve our goals. Of course engineer -
ing and science progress together. Market design is going to require reliable domain knowledge about how particular markets work. And, to quote Bob Wilson: “for the theorist, the problems encountered by practitioners provide a wealth of topics.”
126
VII. Concluding Remarks
Markets and marketplaces are like languages; both are ancient human artifacts.
Whole languages are hard to redesign, but smaller parts, e.g., technical vocabularies, are easier. And so it is with marketplaces: a marketplace is a piece of the market, not the whole. Marketplace designers don’t have control over the whole strategy space: market participants have lots of options.
127 And marketplace behavior evolves as
participants gain experience in developing and deploying their strategies. Practical market design must often proceed in advance of reliable theory, and so market designers also need big strategy sets, that include theory, field studies, computation, and experimentation.
128


---Economics-2018-0-40.txt---
Note that market designers are not mostly academics: the design of marketplaces
offers rich rewards to entrepreneurs, who must also experiment to develop, maintain,
and defend marketplaces that attract wide and steady participation. So market design as an emerging academic discipline not only helps in the design of new mar -
kets and marketplaces, it helps us better understand a large part of the general econ-omy. And markets may be becoming even more important as computerized markets together with mobile computing and communication make marketplaces ever more omnipresent. This is changing the boundaries of firms as more things that firms have traditionally done internally (like record keeping, computing, and logistics) become available through markets.
In conclusion, in honor of its centennial in 1990, the Economic Journal solicited
a set of papers looking into the next 100 years of various parts of economics. I was tasked with looking into the future of game theory. Noting that game theory itself was not yet 50 years old (counting from von Neumann and Morgenstern 1944), I confined myself to looking ahead only 50 years. It seemed to me then that, looking ahead, the prospects were mixed—game theory had delivered important conceptual insights, but if we wanted it to remain a foundation of economics we would have to do more. Here is part of the final paragraph (Roth 1991b):
in the long term, the real test of our success will be not merely how well we
understand the general principles which govern economic interactions, but how well we can bring this knowledge to bear on practical questions of microeconomic engineering… Just as chemical engineers are called upon not merely to understand the principles which govern chemical plants, but to design them, and just as physicians aim not merely to understand the biological causes of disease, but their treatment and prevention, a measure of the success of microeconomics will be the extent to which it becomes the source of practical advice, solidly grounded in well tested theory, on designing the institutions through which we interact with one another.
Rereading today what I wrote almost 30 years ago, I’m glad that we still have 20
years of running room. We’ve already made a little progress, and there’s every reason
to think we can make more. We have begun to understand better and even to play a role in how marketplaces transform markets and sometimes create whole new mar -
kets. Economics is still an early-stage science, and an even earlier-stage engineering discipline. So this is an exciting time to be an economist. There is lots of progress we still need to make, it’s important for the world that we do so, and there are some indications of fruitful paths to follow. I’m looking forward with substantial optimism.


---Economics-2019-0-03.txt---



---Economics-2019-0-04.txt---
Since 1980, interest rates on US government bonds have steadily decreased. They
are now lower than the nominal growth rate, and according to current forecasts, this
is expected to remain the case for the foreseeable future. Ten-year US nominal rates hover around 3 percent, while forecasts of nominal growth are around 4 percent ( 2
percent real growth, 2 percent inflation) . The inequality holds even more strongly in
the other major advanced economies. The 10-year UK nominal rate is 1.3 percent, compared to forecasts of 10-year nominal growth around 3.6 percent (1.6 percent
real, 2 percent inflation) . The 10-year Euro nominal rate is 1.2 percent, compared to
forecasts of 10-year nominal growth around 3.2 percent ( 1.5 percent real, 1.7 percent
inflation) .
1 The 10-year Japanese nominal rate is 0.1 percent, compared to forecasts
of 10-year nominal growth around 1.4 percent ( 1.0 percent real, 0.4
percent infl
ation) .
The question I ask in this lecture is what the implications of such low rates should
be for government debt policy. It is an important question for at least two reasons. From a policy viewpoint, whether or not countries should reduce their debt, and by how much, is a central policy issue. From a theory viewpoint, one of the pillars of macroeconomics is the assumption that people, firms, and governments are subject to intertemporal budget constraints. If the interest rate paid by the government is less than the growth rate, then the intertemporal budget constraint facing the government no longer binds. What the government can and should do in this case is definitely worth exploring.
The paper reaches strong, and, I expect, surprising, conclusions. Put (too) simply,
the signal sent by low rates is not only that debt may not have a substantial fiscal cost, but also that it may have limited welfare costs.
Given that these conclusions are at odds with the widespread notion that government
debt levels are much too high and must urgently be decreased, I consider several counterarguments, ranging from distortions, to the possibility that the future may be very different from the past, to multiple equilibria. All these arguments have merit, but they imply a different discussion from that dominating current discus-sions of fiscal policy.
The lecture is organized as follows.Section I looks at the past behavior of US interest rates and growth rates. It concludes
that the current situation is actually not unusual. While interest rates on pub-lic debt vary a lot, they have on average, and in most decades, been lower than growth rates. If the future is like the past, the probability that the US government can do a debt rollover, that it can issue debt and achieve a decreasing debt to GDP ratio without ever having to raise taxes later, is high.
That debt rollovers may be feasible does not imply however that they are desir -
able. Even if higher debt does not give rise later to a higher tax burden, it still has effects on capital accumulation, and thus on welfare. Whether and when higher debt increases or decreases welfare is taken up in Sections II and III.


---Economics-2019-0-05.txt---
Section II looks at the effects of an intergenerational transfer ( a conceptually simpler
policy than a debt rollover, but a policy that shows most clearly the  rele
vant effects
at work)  in an overlapping generation model with uncertainty. In the certainty context
analyzed by Diamond (1965) , whether such an intergenerational transfer from young
to old is welfare improving depends on “the” interest rate, which in that model is simply
the net marginal product of capital. If the interest rate is less than the growth rate, then the transfer is welfare improving. Put simply, in that case, a larger intergenera-tional transfer, or equivalently an increase in public debt, and thus less capital, is good.
When uncertainty is introduced however, the question becomes what interest rate
we should look at to assess the welfare effects of such a transfer. Should it be the average safe rate, i.e., the rate on sovereign bonds ( assuming no default risk) , or
should it be the average marginal product of capital? The answer turns out to be: both.
As in the Diamond model, a transfer has two effects on welfare: an effect through
reduced capital accumulation, and an indirect effect, through the induced change in the returns to labor and capital.
The welfare effect through lower capital accumulation depends on the safe rate.
It is positive if, on average, the safe rate is less than the growth rate. The intuitive reason is that, in effect, the safe rate is the relevant risk-adjusted rate of return on capital, thus it is the rate that must be compared to the growth rate.
The welfare effect through the induced change in returns to labor and capital
depends instead on the average (risky) marginal product of capital. It is negative if, on average, the marginal product of capital exceeds the growth rate.
Thus, in the current situation where it indeed appears that the safe rate is less
than the growth rate, but the average marginal product of capital exceeds the growth rate, the two effects have opposite signs, and the effect of the transfer on welfare is ambiguous. The section ends with an approximation that shows most clearly the relative role of the two rates. The net effect may be positive, if the safe rate is suffi-ciently low and the average marginal product is not too high.
With these results in mind, Section III turns to numerical simulations. People live
for two periods, working in the first, and retiring in the second. They have separate preferences vis-à-vis intertemporal substitution and risk. This allows to look at differ -
ent combinations of risky and safe rates, depending on the degree of uncertainty and the degree of risk aversion. Production is constant elasticity of substitution (CES)  in
labor and capital, and subject to technological shocks; being able to vary the elastic-ity of substitution between capital and labor turns out to be important as this elasticity determines the strength of the second effect on welfare. There is no technological progress, nor population growth, so the average growth rate is equal to zero.
I show how the welfare effects of a transfer can be positive or negative, and how
they depend in particular on the elasticity of substitution between capital and labor. In the case of a linear technology (equivalently, an infinite elasticity of substitution between labor and capital), the rates of return, while random, are independent of capital accumulation, so that only the first effect is at work, and the safe rate is the only relevant rate in determining the effect of the transfer on welfare. I then show how a lower elasticity of substitution implies a negative second effect, leading to an ambiguous welfare outcome.
I then turn to debt and show that a debt rollover differs in two ways from a transfer
scheme. First, with respect to feasibility: so long as the safe rate remains less than the


---Economics-2019-0-06.txt---
growth rate, the ratio of debt to GDP decreases over time; a sequence of adverse shocks
may however increase the safe rate sufficiently so as to lead to
e
xplosive dynamics,
with higher debt increasing the safe rate, and the higher safe rate in turn increasing debt over time. Second, with respect to desirability: a successful debt rollover can yield positive welfare effects, but less so than the transfer scheme. The reason is that a debt rollover pays people a lower rate of return than the implicit rate in the transfer scheme.
The conclusion of Section III is that the welfare effects of debt depend not only on
how low the average safe rate is, but also on how high the average marginal product is. With this in mind, Section IV returns to the empirical evidence on the marginal product of capital, focusing on two facts. The first fact is that the ratio of the earnings of US corporations to their capital at replacement cost has remained high and rela-tively stable over time. This suggests a high marginal product, and thus, other things equal, a higher welfare cost of debt. The second fact, however, is that the ratio of the earnings of US corporations to their market value has substantially decreased since the early 1980s. Put another way, Tobin’s q , which is the ratio of the market value of
capital to the value of capital at replacement cost, has substantially increased. There are two potential interpretations of this fact. First, that capital at replacement cost is poorly measured and does not fully capture intangible capital. Second, that an increasing proportion of earnings comes from rents. Both explanations ( which are
the subject of much current research)  imply a lower marginal product for a given
measured earnings rate, and thus a smaller welfare cost of debt.
Section V goes beyond the formal model and places the results in a broader but
informal discussion of the costs and benefits of public debt.
On the benefit side, the model above has looked at debt issuance used to finance
transfers in a full employment economy; this does not do justice to current policy discussions, which have focused on the role of debt finance to increase demand and output if the economy is in recession, and on the use of debt to finance public invest-ment. This research has concluded that, if the neutral rate of interest is low and the effective lower bound on interest rates is binding, then there is a strong argument for using fiscal policy to sustain demand. The analysis above suggests that, in that very situation, the fiscal and welfare costs of higher debt may be lower than has been assumed, reinforcing the case for a fiscal expansion.
On the cost side, ( at least)  three arguments can be raised against the model above
and its implications. The first is that the risk premium, and by implication the low safe rate relative to the marginal product of capital, may not reflect risk preferences but rather distortions, such as financial repression. Traditional financial repression, i.e., forcing banks to hold government bonds, is gone in the United States, but one may argue that agency issues within financial institutions or some forms of financial regulation such as liquidity ratios have similar effects. The second argument is that the future may be very different from the present, and the safe rate may turn out much higher than in the past. The third argument is the possibility of multiple equilibria, that if investors expect the government to be unable to fully repay the debt, they may require a risk premium which makes debt harder to pay back and makes their expec-tations self-fulfilling. I discuss all three arguments but focus mostly on the third. It is relevant and correct as far as it goes, but it is not clear what it implies for the level of public debt: multiple equilibria typically hold for a large range of debt, and a realistic reduction in debt while debt remains in the range does not rule out the bad equilibrium.


---Economics-2019-0-07.txt---
Section VI concludes. To be clear, the purpose of the lecture is not to advocate
for higher public debt, but to assess its costs. The hope is that this lecture leads to a
richer discussion of fiscal policy than is currently the case.
I. Inter est Rates, Growth Rates, and Debt Rollovers
Interest rates on US bonds have been and are still unusually low, reflecting in
part the after-effects of the 2008 financial crisis and quantitative easing. The current (December 2018) 1-year T-bill nominal rate is 2.6 percent, substantially below the most recent nominal growth rate, 4.8 percent (from 2018:II to 2018:III, at annual rates).
The gap between the two is expected to narrow, but most forecasts and market
signals have interest rates remaining below growth rates for a long time to come. Despite a strong fiscal expansion putting pressure on rates in an economy close to potential, the current 10-year nominal rate remains around 3 percent, while forecasts of nominal growth over the same period are around 4 percent. Looking at real rates instead, the current 10-year inflation-indexed rate is around 1 percent, while most forecasts of real growth over the same period range from 1.5 percent to 2.5 percent.
2
These forecasts come with substantial uncertainty.Some argue that these low rates reflect “secular stagnation” forces that are likely
to remain relevant for the foreseeable future. They point to structurally high saving and low investment, leading to a low equilibrium marginal product of capital for a long time to come (for example, Summers 2015, Rachel and Summers 2018). Others point to an increased demand for safe assets, leading to a lower safe rate for a given marginal product (for example, Caballero, Farhi, and Gourinchas 2017a). An interesting attempt to identify the respective roles of marginal products, rents, and risk premia is given by Caballero, Farhi, and Gourinchas (2017b).
Others point instead to factors such as aging in advanced economies, better social
insurance or lower reserve accumulation in emerging markets, which may lead instead to higher rates in the future (for a discussion of the role of different factors, see for example Rachel and Smith 2015, Lunsford and West 2018)
3.
Interestingly and importantly however, historically, interest rates lower than
growth rates have been more the rule than the exception, making the issue of what debt policy should be under this configuration of more than temporary interest.
I shall limit myself here to looking at the United States since 1950, but the conclu-sion holds for a large number of countries, over long periods of time.
4


---Economics-2019-0-08.txt---
Figure 1 shows the evolution of the nominal GDP growth rate and the 1-year
Treasury bill rate. Figure 2 shows the evolution of the nominal GDP growth rate and
the 10-year Treasury bond rate. Together, they have two basic features:
• On a
verage, over the period, nominal interest rates have been lower than the
nominal growth rate.5 The 1-year rate has averaged 4.7 percent, the 10-year rate
has averaged 5.6 percent, while nominal GDP growth has averaged 6.3 percent.6
• Both the 1-year rate and the 10-year rate were consistently below the growth
rate until the disinflation of the early 1980s. Since then, both nominal inter -
est rates and nominal growth rates have declined, with rates declining faster than growth, even before the financial crisis. Overall, while nominal rates vary

---Economics-2019-0-09.txt---
substantially from year to year, the 1-year rate has been lower than the growth
rate for all decades except for the 1980s. The 10-year rate has been lower than the growth rate for 4 out of 7 decades.
Given that my focus is on the implications of the joint evolution of interest rates
and growth rates for debt dynamics, the next step is to construct a series for the rele-vant interest rate paid on public debt held by domestic private and foreign investors. I proceed in three steps, (i) taking into account the maturity composition of the debt, (ii) taking into account the tax payments on the interest received by the holders of public debt, and (iii) taking into account Jensen’s inequality. (Details of construction
are given in online Appendix A.)
7
To take into account maturity, I use information on the average maturity of the
debt held by private investors (that is excluding public institutions and the Fed). This average maturity went down from eight years and four months in 1950 to three years and four months in 1974, with a mild increase since then to five years today.
8
Given this series, I construct a maturity-weighted interest rate as a weighted aver -
age of the 1-year and the 10-year rates using

Man
y, but not all, holders of government bonds pay taxes on the interest paid,
so the interest cost of debt is actually lower than the interest rate itself. There is no direct measure of those taxes, and thus I proceed as follows.
9,10
I measure the tax rate of the marginal holder by looking at the difference between
the yield on AAA municipal bonds (which are exempt from federal taxes) and the yield on a corresponding maturity Treasury bond, for both 1-year and 10-year bonds, denoted
i
mt1    and   i mt10    respectively. Assuming that the marginal investor is
indifferent between holding the 2, the implicit tax rate on 1-year Treasuries is given by
τ
1t   = 1 −   i  mt1  / i 1t   , and the implicit tax rate on 10-year Treasuries is given by
τ
10t   = 1 −   i  mt10  / i 10t   .11 The tax rate on 1-year bonds peaks at about 50 percent in
the late 1970s (as inflation and nominal rates are high, leading to high effective tax
rates), then goes down close to 0 until the financial crisis, and has increased slightly since 2017. The tax rate on 10-year bonds follows a similar pattern, down from about 40 percent in the early 1980s to close to 0 until the financial crisis, with a small increase since 2016.
12 Taking into account the maturity structure of the debt,
I then construct an average tax rate in the same way as I constructed the interest rate above, by constructing


---Economics-2019-0-10.txt---
Not all holders of Treasuries pay taxes however. Foreign holders, private and
public (such as central banks), Federal retirement programs, and Fed holdings are
not subject to tax. The proportion of such holders has steadily increased over time,
reflecting the increase in emerging markets’ reserves (in particular China’s), the
growth of the Social Security Trust Fund, and more recently, the increased holdings of the Fed, among other factors. From 15 percent in 1950, it now accounts for 64 percent today.
Using the maturity adjusted interest rate from above,
t   , the implicit tax rate,
t   , and the proportion of holders likely subject to tax,   β  t   , I construct an “adjusted
interest rate” series according to

Its characteristics are shown in Figures 3 and 4. Figure 3 plots the adjusted rate
against the 1-year and the 10-year rates. Figure 4 plots the adjusted tax rate against
the nominal growth rate. They yield two conclusions:
First, over the period, the average adjusted rate has been lower than either the



---Economics-2019-0-11.txt---
largely reflects the nonneutrality of taxation to inflation in the 1970s and 1980s,
which is much less of a factor today. In 2018, the rate was around 2.4 percent.
•
Second, o
ver the period, the average adjusted rate has been substantially lower
than the average nominal growth rate, 3.8 percent versus 6.3 percent.
The last potential issue is Jensen’s inequality. The dynamics of the ratio of debt
to GDP are given by

t    is the ratio of debt to GDP (with both variables either in nominal or in real
terms if both are deflated by the same deflator), and
t    is the ratio of the primary
deficit to GDP (again, with both variables either in nominal or in real terms). The
evolution of the ratio depends on the relevant product of interest rates and growth
rates (nominal or real) over time.
Given the focus on debt rollovers, that is, the issuance of debt without a
later increase in taxes or reduction in spending, suppose we want to trace debt dynamics under the assumption that
t    remains equal to zero.13 Suppose that
 is distributed normally with mean  μ   and v ariance   σ    2  . Then,
the evolution of the ratio will depend not on
We
have seen that, historically,

as between −1 percent and −2 percent. The standard
deviation of the log ratio over the same sample is equal to 2.8 percent, implying
a variance of 0.08 percent, thus too small to affect the conclusions substantially. Jensen’s inequality is thus not an issue here.
14
In short, if we assume that the future will be like the past (admittedly a big if),
debt rollovers appear feasible. While the debt ratio may increase for some time due to adverse shocks to growth or positive shocks to the interest rate, it will eventually decrease over time. In other words, higher debt may not imply a higher fiscal cost.
In this light, it is interesting to do the following counterfactual exercise.
15 Assume
that the debt ratio in year
t
w
as what it actually was, but that the primary balance was
equal to zero from then on, so that debt in year

Figures
5 and 6 show what the evolution of the debt ratio would have been, starting
at different dates in the past. For convenience, the ratio is normalized to 100 at
each starting date. Figure 5 uses the non-tax adjusted rate, and Figure 6 uses the tax-adjusted interest rate.
Figure 5 shows that, for each starting date, the debt ratio would eventually have
decreased, even in the absence of a primary surplus. The decrease, if starting in the


---Economics-2019-0-12.txt---
1950s, 1960s, or 1970s, is quite dramatic. But the figure also shows that a series
of bad shocks, such as happened in the 1980s, can increase the debt ratio to higher levels for a while.
Figure 6, which I believe is the more appropriate one, gives an even more optimistic
picture, where the debt ratio rarely would have increased, even in the
1980s: the

reason being the higher tax revenues associated with inflation during that period.
What these figures show is that, historically, debt rollovers would have been feasible.
Put another way, it shows that the fiscal cost of higher debt would have been small, if not zero. This is at striking variance with the current discussions of fiscal space, which all start from the premise that the interest rate is higher than the growth rate, implying a tax burden of the debt.
The fact that debt rollovers may be feasible (i.e., that they may not have a fiscal
cost) does not imply however that they are desirable (that they have no welfare cost). This is the topic taken up in the next two sections.


---Economics-2019-0-13.txt---
II. Inter generational Transfers and Welfare
Debt rollovers are, by their nature, non-steady-state phenomena, and have potentially
complex dynamics and welfare effects. It is useful to start by looking at a simpler
policy, namely a transfer from the young to the old (equivalent to pay-as-you-go social security), and then to return to debt and debt rollovers in the next section.
The natural setup to explore the issues is an overlapping generation model under
uncertainty. The overlapping generation structure implies a real effect of intergen-erational transfers or debt, and the presence of uncertainty allows to distinguish between the safe rate and the risky marginal product of capital.
16
I proceed in two steps, first briefly reviewing the effects of a transfer under
certainty, following Diamond (1965), then extending it to allow for uncertainty. (Derivations are given in online Appendix B.)
17,18
Assume that the economy is populated by people who live for two periods, working
in the first period, and consuming in both periods. Their utility is given by
 are consumption in the first and the second period of life, respectively.
(As I limit myself for the moment to looking at the effects of the transfer
on utility in steady state, there is no need for now for a time index.) Their first and
second period budget constraints are given by
where
W
is the w
age,
K
is sa
ving (equivalently, next period capital),
D
is the transfer
from young to old, and
R
is the rate of return on capital.
I ignore population gro
wth and technological progress, so the growth rate is equal
to zero. Production is given by a constant returns production function,

It is convenient to normalize labor to 1, so  Y

(K, 1)  . Both factors are paid
their marginal product.
16 In this framework, the main general equilibrium effect of intergenerational transfers or debt is to decrease
capital accumulation. A number of recent papers have explored the effects of public debt when public debt also
provides liquidity services. Aiyagari and McGrattan (1998) for example explore the effects of public debt in an economy in which agents cannot borrow and thus engage in precautionary saving; in that framework, debt relaxes the borrowing constraint and decreases capital accumulation. Angeletos, Collard, and Dellas (2016) develop a model where public debt provides liquidity. In that model, debt can either crowd out capital, for the same reasons as in Aiyagari and McGrattan, or crowd in capital by increasing the available collateral required for investment. These models are obviously very different from the model presented here, but they share a focus on the low safe rate as a signal about the desirability of public debt. Finally, to the extent that it focuses on economies where the safe rate may be less than the growth rate, it is related to the literature on rational bubbles in dynamically efficient economies with financial frictions, for example Martin and Ventura (2016), or Farhi and Tirole (2012).

---Economics-2019-0-14.txt---
The first-order condition for utility maximization is given by
The effect of a small increase in the transfer
D
on utility is gi
ven by
dU

=
The first term in brackets, call it
d
U
a   , represents the partial equilibrium, direct,
effect of the transfer. The second term, call it
d
U
b   , represents the general equilibrium
effect of the transfer through the induced change in wages and rates of return.
Consider the first term, the effect of debt on utility given labor and capital prices.
Using the first-order condition gives

(the case known as “dynamic inefficiency”), then, ignoring the
other term, a small increase in the transfer increases welfare. The explanation is
straightforward. If
R

<

1
, the transfer gi
ves a higher rate of return to savers than
does capital.
Take the second term, the effect of debt on utility through the changes in
W

and

R
.
An increase in debt decreases capital and thus decreases the wage and increases
the rate of return on capital. What is the effect on welfare?
Using the factor price frontier relation

, rewrite this second term as

Using the first-order condition for utility maximization gi
ves
d
U
So, if
R

<

1
then, just lik
e the first term, a small increase in the transfer increases
welfare (as the lower capital stock leads to an increase in the interest rate). The
explanation is again straightforward: given the factor price frontier relation, the decrease in the capital leads to an equal decrease in income in the first period and increase in income in the second period. If
R

<

1
, this is more attracti
ve than what
capital provides, and thus increases welfare.
Using the definition of the elasticity of substitution
η

≡
the defi -
nition of the share of labor,
α

=
F
N  /F , and the relation between second deri vatives
of the production function,
F
NK   = −  K  F  KK   , this second term can be rewritten as
(2)
d
U


---Economics-2019-0-15.txt---
Note the following two implications of equations (1) and (2):
•
The sign of the tw
o effects depends on
R

−

1
. If
R

<

1
, then a decrease in
capital accumulation increases utility
. In other words, if the marginal product is
less than the growth rate (which here is equal to 0), an intergenerational transfer
has a positive effect on welfare in steady state.
•
The strength of the second ef
fect depends on the elasticity of substitution

η

. If
for example  η

=

∞

so the production function is linear and capital accumulation
has no effect on either wages or rates of return to capital, this second effect is equal to 0.
So far, I just replicated the analysis in Diamond.
19 Now I introduce uncertainty in
production, so the marginal product of capital is uncertain. If people are risk averse, the average safe rate will be less than the average marginal product of capital. The basic question becomes: what is the relevant rate we should look at for welfare pur -
poses? Put loosely, is it the average marginal product of capital
ER
, or is it the a
verage
safe rate
ER
f  , or is it some other rate altogether?
The model is the same as before, except for the introduction of uncertainty.
People born at time
t
ha
ve expected utility given by (I now need time subscripts
as the steady state is stochastic)
U
t   ≡  (1 − β) U ( C 1,t  )  + βEU  ( C 2,t+1  )  .
Their budget constraints are given by
C
1t   =  W  t   −  K t   − D ;  C  2t+1   =  R t+1   K t   + D  .
Production is given by a constant returns production function
Y
t   =  A t   F ( K t−1  , N)  ,
where
N

=

1
and
A
t    is stochastic. (The capital at time  t   reflects the sa ving of the
young at time
t

−

1
, thus the timing con
vention.)
At time
t
, the first-order condition for utility maximization is gi
ven by
(1 − β) U′ ( C 1,t  )  = β E  [ R t+1  U′ ( C 2,t+1  ) ]  .
We can now define a shadow safe rate
R
t+1  f   , which must satisfy

R
t+1  f   E [U′ ( C 2,t+1  ) ]  = E  [ R t+1  U′ ( C 2,t+1  ) ]  .


---Economics-2019-0-16.txt---
Now consider a small increase in  D   on utility at time  t  :

As before, the first term in brackets, call it
d
U
at   , reflects the partial equilibrium,
direct, effect of the transfer, the second term, call it
d
U
bt   , reflects the general equilibrium
effect of the transfer through the change in wages and rates of return to capital.
Take the first term, the effect of debt on utility given prices. Using the first-order
condition gives

So, using the definition of the safe rate:

So, to determine the sign ef
fect of the transfer on welfare through this first channel,
the relevant rate is indeed the safe rate. In any period in which
R
t+1  f    is less than
1, the transfer is welf
are improving.
The reason why the safe rate is what matters is straightforward and important:
the safe rate is, in effect, the risk-adjusted rate of return on capital.20 The intergenerational
transfer gives people a higher rate of return than the risk-adjusted rate of
return on capital.
Take the second term, the effect of the transfer on utility through prices:

Or using the factor price frontier relation:

In general, this term will depend both on
d
K
t−1    (which affects  d W t   ) and on  d  K  t
(which affects
=
dK
, it can be re
written, using the same steps as in the certainty case, as

Thus, the relevant rate in assessing the sign of the welfare effect of the transfer
through this second term is the risky rate, the marginal product of capital. If
R
t    is



---Economics-2019-0-17.txt---
less than 1, the implicit transfer due to the change in input prices increases utility. If
R
t    is greater than 1, the implicit transfer decreases utility.
The reason why it is the risky rate that matters is simple. Capital yields a rate of
return
R
t+1   . The change in prices due to the decrease in capital represents an implicit
transfer with rate of return   R
t+1  / R t   . Thus, whether the implicit transfer increases or
decreases utility depends on whether
R
t    is less or greater than 1.
Putting the two sets of results together: if the safe rate is less than 1, and
the risky rate is greater than 1 (the configuration that appears to be relevant
today) the two terms now work in opposite directions. The first term implies that an
increase in debt increases welfare. The second term implies that an increase in debt instead decreases welfare. Both rates are thus relevant.
To get a sense of relative magnitudes of the two effects, and therefore which one
is likely to dominate, the following approximation is useful. Evaluate the two terms at the average values of the safe and the risky rates, to get


sign
where, from the accumulation equation, we ha
ve the following approximation:21

Note that, if the production is linear
, and so
η

=

∞
, the second term in equation
(
6) is equal to 0, and the only rate that matters is
E
R
f  . Thus, if  E  R    f   is less than 1, a
higher transfer increases welfare. As the elasticity of substitution becomes smaller,
the price effect becomes stronger, and, eventually, the welfare effect changes sign and becomes negative.
In the Cobb-Douglas case, using the fact that
ER

≈
(1 − α) / (αβ)  , (the approximation
comes from ignoring Jensen’s inequality) the equation reduces to the simpler
formula:

Suppose that the average annual safe rate is 2 percent lower than the growth rate,
so that
E
R
f  , the gross rate of return over a unit period, say 25 years, is   0.98    25  = 0.6  ,
then the welfare effect of a small increase in the transfer is positive if
ER

is less than
1.66, or equivalently, if the average annual marginal product is less than 2 percent
above the growth rate.22



---Economics-2019-0-18.txt---
Short of a much richer model, it is difficult to know how reliable these rough
computations are as a guide to reality. The model surely overstates the degree of
non-Ricardian equivalence: debt in this economy is (nearly fully) net wealth, even if
R
f   is greater than 1 and the government must levy taxes to pay the interest to keep
the debt constant. The assumption that capital and labor are equally risky may not
be right: holding claims to capital (i.e., shares) involves price risk, which is absent from the model as capital fully depreciates within a period; on the other hand, labor income, in the absence of insurance against unemployment, can also be very risky. Another restrictive assumption of the model is that the economy is closed: in an open economy, the effect on capital is likely to be smaller, with changes in public debt being partly reflected in increases in external debt. I return to the issue when discussing debt (rather than intertemporal transfers) later. Be this as it may, the anal-ysis suggests that the welfare effects of a transfer may not necessarily be adverse, or, if adverse, may not be very large.
III. Simulations: T ransfers, Debt, and Debt Rollovers
To get a more concrete picture, and turn to the effects of debt and debt rollovers
requires going to simulations.23 Within the structure of the model above, I make the
following specific assumptions (derivations and details of simulations are given in online Appendix C).
I think of each of the two periods of life as equal to 25 years. Given the role of
risk aversion in determining the gap between the average safe and risky rates, I want to separate the elasticity of substitution across the two periods of life and the degree of risk aversion. Thus, I assume that utility has an Epstein-Zin-Weil representation of the form (Epstein and Zin 2013, Weil 1990):
The log-log specification implies that the intertemporal elasticity of substitution
is equal to 1. The coefficient of relative risk aversion is given by
γ

.
As the strength of the second effect above depends on the elasticity of substitution
between capital and labor, I assume that production is characterized by a
constant
elasticity of substitution production function, with multiplicati
ve uncertainty:
Y
where
A
t    is white noise and is distributed log normally, with  ln   A  t   ∼     (μ;  σ   2 )   and
ρ

=
(η − 1) /η , where  η   is the elasticity of substitution. When  η  =  ∞, ρ  =  1   and
the production function is linear.



---Economics-2019-0-19.txt---
Finally, I assume that, in addition to the wage, the young receive a nonstochastic
endowment,
X
. Gi
ven that the wage follows a log normal distribution and thus can
be arbitrarily small, such an endowment is needed to make sure that the deterministic
transfer from the young to the old is always feasible, no matter what the real-ization of
W

.24 I assume that the endowment is equal to 100 percent of the average
wage absent the transfer.
Given the results in the previous section, I calibrate the model so as to fit a set
of values for the average safe rate and the average risky rate. I consider average net annual risky rates (marginal products of capital) minus the growth rate (here equal to 0) between 0 percent and 4 percent. These imply values of the average 25-year gross risky rate,
ER
, between 1.00 and 2.66. I consider a
verage net annual safe rates
minus the growth rate between −2 percent and 1 percent; these imply values of the average 25-year gross safe rate,
E
R
f  , between 0.60 and 1.28.
I choose some of the coefficients a priori. I choose
b

(which is equal to the capital
share in the Cobb-Douglas case) to be 1/3. For reasons explained below, I choose
the annual value of
σ
a    to be a high 4 percent a year, which implies a value of  σ   of
√ _ 25   × 4%  =  0.20  .
Because the strength of the second effect above depends on the elasticity of substitution,
I consider two different values of

which corresponds to the
linear production function case, and in which the price ef
fects of lower capital accumulation
are equal to 0, and
η

=

1
, the Cobb-Douglas case, which is generally seen
as a good description of the production function in the medium run.
The central parameters are, on the one hand,
β
and
μ
, and on the other
,

γ

.
The parameters
β

and μ determine (together with
σ
, which plays a minor role
)
the average level of capital accumulation and thus the average marginal product of capital: i.e., the average risky rate. In general, both parameters matter. In the linear production case however, the marginal product of capital is independent of the level of capital, and depends only on
μ
; thus, I choose

μ

to fit the average value of the
marginal product
ER
. In the Cobb-Douglas case, the mar
ginal product of capital is
instead independent of

μ

and depends only on
β
; thus, I choose
β
to fit the a
verage
value of the marginal product
ER

.
The parameter
γ
determines, together with
σ
, the spread between the risk
y rate
and the safe rate. In the absence of transfers, the following relation holds between the two rates:
ln
R
This relation implies however that the model suffers from a strong case of the
equity premium puzzle (see, for example, Kocherlakota 1996). If we think of

σ

as
the standard deviation of TFP growth, and assume that, in the data, TFP growth is
a random walk (with drift), this implies an annual value of
σ
a    of about 2 percent,
equivalently a value of
σ
o
ver the 25-year period of 10 percent, and thus a value of
σ
2   of 1 percent. Thus, if we think of the annual risk premium as equal to, say, 5  percent,
which implies a v
alue of the right-hand side of 1.22, this implies a value of

γ

,



---Economics-2019-0-20.txt---
the coefficient of relative risk aversion of 122, which is clearly implausible. One of
the reasons why the model fails so badly is the symmetry in the degree of uncer -
tainty facing labor and capital, and the absence of price risk associated with holding shares (as capital fully depreciates within the 25-year period). If we take instead
σ

to
reflect the standard deviation of annual rates of stock returns, say 15 percent a year (its historical mean), and assume stock returns to be uncorrelated over time, then

σ
o
ver the 25-year period is equal to 75 percent, implying values of

γ

around 2.5.
There is no satisfactory way to deal with the issue within the model, so as an uneasy compromise, I choose
σ

=

20%

. Given
σ

,
γ

is determined for each pair of average
risky and safe rates.25
I then consider the effects on steady-state welfare of an intergenerational transfer.
The basic results are summarized in Figures 7 to 10.
Figure 7 shows the effects of a small transfer (5 percent of (pre-transfer) average
saving) on welfare for the different combinations of the safe and the risky rates (reported, for convenience, as net rates at annual values, rather than as gross rates at 25-year values), in the case where
η

=

∞
and, thus, production is linear
. In this
case, the derivation above showed that, to a first order, only the safe rate mattered. This is confirmed visually in the figure. Welfare increases if the safe rate is negative (more precisely, if it is below the growth rate, here equal to 0), no matter what the average risky rate.
Figure 8 looks at a larger transfer (20 percent of saving), again in the linear
production case. For a given
E
R
f  , a larger  ER   leads to a smaller welf are increase if
welfare increases, and to a larger welfare decrease if welfare decreases. The reason
is as follows. As the size of the transfer increases, second period income becomes less risky, so the risk premium decreases, increasing
E
R
f   for given average  ER  . In


---Economics-2019-0-21.txt---
the limit, a transfer that led people to save nothing in the form of capital would eliminate
uncertainty about second period income, and thus would lead to
E
R
f  = ER  .
The larger  ER
,
the faster  E
R
f   increases with a large transfer; for  ER   high enough  ,
and
for  D

large enough,  E
R
f   becomes larger than 1, and the transfer becomes welfare
decreasing.
In other words, even if the transfer has no effect on the average rate of return to
capital, it reduces the risk premium, and thus increases the safe rate. At some point,
the safe rate becomes positive, and the transfer has a negative effect on welfare.
Figures 9 and 10 do the same, but now for the Cobb-Douglas case. They yield
the following conclusions. Both effects are now at work, and both rates matter. A lower safe rate makes it more likely that the transfer will increase welfare; a higher risky rate makes it less likely. For a small transfer (5 percent of saving), a safe rate
2 percent lower than the growth rate leads to an increase in welfare so long as the risky rate is less than 2 percent above the growth rate. A safe rate 1 percent lower than the growth rate leads to an increase in welfare so long as the risky rate is less than 1 percent above the growth rate. For a larger transfer (20 percent of saving), which increases the average
R
f   closer to 1, the trade-off becomes less attractive. For
welfare to increase, a safe rate 2 percent lower than the growth rate requires that
the risky rate be less than 1.5 percent above the growth rate; a safe rate of 1 percent below the growth rate requires that the risky rate be less than 0.7 percent above the growth rate.
I have so far focused on intergenerational transfers, such as we might observe in
a pay-as-you-go system. Building on this analysis, I now turn to debt, and proceed in two steps, looking first at the effects of a permanent increase in debt, then looking at debt rollovers.
Suppose the government increases the level of debt and maintains it at this higher
level forever. Depending on the value of the safe rate every period, this may require either issuing new debt when
R
t  f  < 1   and distrib uting the proceeds as benefits, or
retiring debt, when
R
t  f  > 1   and financing it through tax es. 


---Economics-2019-0-22.txt---
assume that benefits and taxes are paid to, or levied on, the young. In this case, the
budget constraints faced by somebody born at time
t
are gi
ven by

So, a constant le
vel of debt can be thought of as an intergenerational transfer, with
a small difference relative to the case developed earlier. The difference is that a gener -
ation born at time
t  mak
es a net transfer of
D R
t  f   when young, and receives, when old,
a net transfer of  D R
t+1  f   , as opposed to the one-for-one transfer studied earlier. Under
certainty, in steady state,   R
f   is constant and the two are equal. Under uncertainty, the



---Economics-2019-0-23.txt---
variation about the terms of the intertemporal transfer implies a smaller increase in
welfare than in the transfer case. Otherwise, the conclusions are very similar.
This is a good place to discuss informally a possible extension of the closed
economy model, and allow the economy to be open. Start by thinking of a small open economy that takes
R
f   as given and unaffected by its actions. In this case, if
R
f   is less than 1, an increase in debt unambiguously increases welfare. The reason is
that capital accumulation is unaffected, with the increase in debt fully reflected in an
increase in external debt, so the second effect characterized above is absent. In the case of a large economy such as the United States, an increase in debt will lead to both an increase in external debt and a decrease in capital accumulation. While the decrease in capital accumulation is the same as above for the world as a whole, the decrease in US capital accumulation is smaller than in the closed economy. Thus, the second effect is smaller; if it was adverse, it is less adverse. This may not be the end of the story however. Other countries suffer from the decrease in capital accumulation, leading possibly to a change in their own debt policy. I leave this extension to another paper, but, in the current context in which the difference between the interest rate and the growth rate varies across countries, it is clearly of relevance today.
Let me finally turn to the effects of a debt rollover, where the government, after
having issued debt and distributed the proceeds as transfers, does not raise taxes thereafter, and lets debt dynamics play out.
The government issues debt
D
0   . Unless the debt rollover fails, there are neither
taxes nor subsidies after the initial issuance and associated transfer. The budget con-straints faced by somebody born at time
t
are thus gi
ven by

and debt follo
ws
D
t   =  R  t  f   D t−1   .
First, consider sustainability
. Even if debt decreases in expected value over time,
a debt rollover may fail with positive probability. A sequence of realizations of
R
t  f  > 1   may increase debt to the le vel where   R    f   becomes larger than 1 and then
remains so, leading to a debt explosion. At some point, an adjustment will have to
take place, either through default, or through an increase in taxes. The probability of such a sequence over a long but finite period of time is however likely to be small if

R
f   starts far below 1.26
This is shown in Figure 11, which plots 1,000 stochastic paths of debt evolutions,
under the assumption that the production function is linear, and Figure 12, under the
assumption that the production function is Cobb-Douglas. In both cases, the initial



---Economics-2019-0-24.txt---
increase in debt is equal to 15 percent of (pre-debt) average steady state saving.27
The underlying parameters in both cases are calibrated so as to fit values of
ER

and

E
R
f   absent debt corresponding to −1 percent for the annual safe rate, and 2 percent
for the annual risky rate.
Failure is defined as the point where the safe rate becomes sufficiently large
and positive (so that the probability that debt does not explode becomes very
small, depending on the unlikely realization of successive large positive shocks
which would take the safe rate back below the growth rate). Rather arbitrarily, I choose the threshold to be 1 percent at an annual rate. If the debt rollover fails, I assume, again arbitrarily and too strongly, that all debt is paid back through a tax on the young. This exaggerates the effect of failure on the young in that period, but is simplest to capture.
28



---Economics-2019-0-25.txt---
In the linear case, the higher debt and lower capital accumulation have no effect
on the risky rate, and a limited effect on the safe rate, and all paths show declining
debt. Four periods out (100 years), all of them have lower debt than at the start.
In the Cobb-Douglas case, with the same values of
ER
and
E R
f   absent debt, bad
shocks, which lead to higher debt and lower capital accumulation, lead to increases in
the risky rate, and by implication, larger increases in the safe rate. The result is that, for the same sequence of shocks, now 5 percent of paths, fail over the first 4 periods: 100 years, if we take a period to be 25 years. The failing paths are represented in red.
Second, consider welfare effects. Relative to a pay-as-you-go scheme, debt rollovers
are much less attractive. Remember the two effects of an intergenerational transfer. The first comes from the fact that people receive a rate of return of 1 on the transfer, a rate which is typically higher than
R
f  . In a debt rollover, they receive a rate
of return of only
R
f  , which is typically less than 1. At the margin, they are indifferent
to holding debt or capital. There is still an inframarginal effect, a consumer surplus
(taking the form of a less risky portfolio, and thus less risky second period con-sumption), but the positive effect on welfare is smaller than in the straight transfer scheme. The second effect, due to the change in wages and rate of return on capital, is still present, so the net effect on welfare, while less persistent as debt decreases over time, is more likely to be negative.
These effects are shown in Figures 13 and 14, which show the average welfare
effects of successful and unsuccessful debt rollovers, for the linear and Cobb-Douglas cases.


---Economics-2019-0-26.txt---
In the linear case, debt rollovers typically do not fail and welfare is increased
throughout. For the generation receiving the initial transfer associated with debt issuance,
the effect is clearly positive and large. For later generations, while they are, at the margin, indifferent between holding safe debt or risky capital, the inframarginal gains (from a less risky portfolio) imply slightly larger utility. But the welfare gain
is small (equal initially to about 0.18 percent and decreasing over time), compared to the initial welfare effect on the old from the initial transfer (8.75 percent).
In the Cobb-Douglas case however, this positive effect is more than offset by the
price effect, and while welfare still goes up for the first generation (by 2 percent), it is typically negative thereafter. In the case of successful debt rollovers, the average adverse welfare cost decreases as debt decreases over time. In the case of unsuccess-ful rollovers, the adjustment implies a larger welfare loss when it happens.
29
If we take the Cobb-Douglas example to be more representative, are these Ponzi
gambles, as Ball, Elmendorf, and Mankiw (1998) have called them, worth it from a welfare viewpoint? This clearly depends on the relative weight the policymaker puts on the utility of different generations. If the social discount factor it uses is close to 1, then debt rollovers under the conditions underlying the Cobb-Douglas simulation are likely to be unappealing, and lead to a social welfare loss. If it is less than 1, the large initial increase in utility may well dominate the average utility loss later.
30
IV . Ear nings versus Marginal Products
The argument developed in the previous two sections showed that the welfare
effects of an intergenerational transfer, or an increase in debt, or a debt
rollover, depend both on how low the average safe rate and how high the average marginal product of capital are relative to the growth rate. The higher the average marginal product of capital, for a given safe rate, the more adverse the effects of the transfer. In the simulations above (reiterating the caveats about how seriously one should take the quantitative implications of that model), the welfare effects of an average marginal product far above the growth rate typically dominated the effects of an average safe rate slightly below the growth rate, implying a negative effect of the transfer (or of debt) on welfare.
Such a configuration would seem to be the empirically relevant one. Look at
Figure 15. The red line gives the evolution of the ratio of pre-tax earnings of US nonfinancial corporations, defined as their net operating surplus, to their capital stock measured at replacement cost, since 1950. Note that, while this earnings rate declined from 1950 to the late 1970s, it has been rather stable since then, around a high 10 percent, so 6 to 8 percent above the growth rate. (See online Appendix D for details of construction and sources.)


---Economics-2019-0-27.txt---

---Economics-2019-0-28.txt---
Look at the blue line however. It shows the evolution of the ratio of the same
earnings series, now to the market value of the same firms, constructed as the sum
of the market value of equity plus other liabilities minus financial assets. Note how it has declined since the early 1980s, going down from roughly 10 percent then to about 5
percent today
. Put another way, the ratio of the market value of firms to
their measured capital at replacement cost, known as Tobin’s q, has roughly doubled since the early 1980s, going roughly from one to two.
There are two ways of explaining this diverging evolution; both have implications
for the average marginal product of capital, and, as result, for the welfare effects of debt.
31 Both have been and are the subject of much research, triggered by an appar -
ent increase in markups and concentration in many sectors of the US
economy
(e.g.,
De Loecker and Eeckhout 2017, Gutiérrez and Philippon 2017, Philippon 2017, Barkai 2018, Farhi and Gourio 2018).
The first explanation is unmeasured capital, reflecting in particular intangible
capital. To the extent that the true capital stock is larger than the measured capi-tal stock, this implies that the measured earnings rate overstates the true rate, and by implication overstates the marginal product of capital. A number of researchers have explored this hypothesis, and their conclusion is that, even if the adjustment already made by the Bureau of Economic Analysis is insufficient, intangible capi-tal would have to be implausibly large to reconcile the evolution of the two series. Measured intangible capital as a share of capital has increased from 6 percent in 1980 to 15
percent
today. Suppose it had in fact increased to 25 percent. This would
only lead only to a 10 percent increase in measured capital, far from enough to explain the divergent evolutions of the two series.
32

---Economics-2019-0-29.txt---
The second explanation is increasing rents, reflecting in particular the increasing
relevance of increasing returns to scale and increased concentration.33 If so, the
earnings rate reflects not only the marginal product of capital, but also rents. By
implication, the market value of firms reflects not only the value of capital but also the present value of rents. If we take all of the increase in the ratio of the market value of firms to capital at replacement cost to reflect an increase in rents, the dou-bling of the ratio implies that rents account for roughly one-half of earnings, and the marginal product of capital for the other half.
34,35
As with many of the issues raised in this lecture, many caveats are in order, and
they are being taken on by current research. Movements in Tobin’s q, the ratio of market value to capital, are often difficult to explain.
36 Yet, the evidence is fairly
consistent with a decrease in the average marginal product of capital, and by impli-cation, a smaller welfare cost of debt.
V . A Br oader View: Arguments and Counterarguments
So far, I have considered the effects of debt when debt was used to finance inter -
generational transfers in a full employment economy. This was in order to focus on the basic mechanisms at work. But it clearly did not do justice to the potential ben-efits of debt finance, nor does it address other potential costs of debt left out of the model. The purpose of this last section is to discuss potential benefits and potential costs. As this touches on many aspects of the economy and many lines of research, it is informal, more in the way of remarks and research leads than definitive answers about optimal debt policy.
Start with Potential Benefits.—Even within the strict framework above, the focus
on steady-state utility (in the case of intergenerational transfers, or of a permanent
increase in debt) ignored the transition to the steady state, and in particular, the effect on the initial (old) generation of the initial transfer (in the case of intergen-erational transfers), or the initial spending financed by debt (in the case of constant debt). Steady-state utility is indeed the correct variable to focus if the policymaker values the current and all future generations equally. To the extent however that the social welfare discount rate is less than 1, a negative effect on steady-state welfare may be more than offset by the increase in utility of the initial generation. As argued


---Economics-2019-0-30.txt---
above, the same argument applies to debt rollovers. The initial increase in utility
may more than offset negative utility effects later on.37
Going beyond the framework above, a standard argument for deficit finance in a
country like the United States is its potential role in increasing demand and reducing the output gap when the economy is in recession. The financial crisis, and the role of both the initial fiscal expansion and the later turn to fiscal austerity, have led to a resurgence of research on the topic. Research has been active on four fronts.
The first has revisited the size of fiscal multipliers. Larger multipliers imply a
smaller increase in debt for a given increase in output. Looking at the Great Recession, two arguments have been made that multipliers were higher during that time. First, the lower ability to borrow by both households and firms implied a
stronger ef
fect of
current income on spending, and thus a stronger multiplier. Second, at the effective lower bound, monetary authorities did not feel they should increase interest rates in response to the fiscal expansion.
38
The second front, explored by DeLong and Summers (2012) has revisited the
effect of fiscal expansions on output and debt in the presence of hysteresis. They have shown that even a small hysteretic effect of a recession on later output might lead a fiscal expansion to actually reduce rather than increase debt in the long run, with the effect being stronger, the stronger the multipliers and the lower the safe interest rate.
39 Note that this is a different argument from the argument developed
in this paper: the proposition is that a fiscal expansion may not increase debt, while I argue that an increase in debt may have small fiscal and welfare costs. The two arguments are clearly complementary however.
The third front has been that public investment has been too low, often being the
main victim of fiscal consolidation, and that the marginal product of public capital is high. The relevant point here is that what should be compared is the risk-adjusted social rate of return on public investment to the risk-adjusted rate of return on pri-vate capital, i.e., the safe rate.
The fourth front has explored the role of deficits and debt if we have indeed
entered a long-lasting period of secular stagnation, in which large negative safe interest rates would be needed for demand to equal potential output but monetary policy is constrained by the effective lower bound. In that case, budget deficits may be needed on a sustained basis to achieve sufficient demand and output growth. Some argue that this is already the case for Japan, and may become the case for other advanced economies. Here, the results of this paper directly reinforce this argument. In this case, not only are budget deficits needed to eliminate output gaps,


---Economics-2019-0-31.txt---
but, because safe rates are likely to be far below potential growth rates, the welfare
costs of debt may be small or even altogether absent.
Let me however concentrate on the potential costs of debt, and on some counterarguments
to the earlier conclusions that debt may have low fiscal or welfare costs. I can think of three main counterarguments.
The first is that the safe rate may be artificially low, so the welfare implications
above do not hold. It is generally agreed that US government bonds benefit not only from low risk, but also from a liquidity discount, leading to a lower safe rate than would otherwise be the case. The issue however is whether this discount reflects technology and preferences or, instead, distortions in the financial system. If it reflects liquidity services valued by households and firms, then the logic of the earlier model applies. The safe rate is now the liquidity-adjusted and risk-adjusted equivalent of the marginal product of capital and is thus what must be compared to the growth rate. If however, the liquidity discount reflects distortions, for example financial repression forcing financial institutions to hold a certain proportion of their portfolios in government bonds, then indeed the safe rate is no longer the appropri-ate rate to compare to the growth rate. It may be welfare improving in this case to reduce financial repression even if this leads to a higher safe rate, and a higher cost of public debt.
40 Straight financial repression is no longer relevant for the United
States, but various agency issues internal to financial institutions as well as financial regulations such as minimum liquidity ratios, may have some of the same effects.
The second counterargument is that the future may be different from the past,
and that, despite the long historical record, the safe interest rate may become consis-tently higher than the growth rate. History may indeed not be a reliable guide to the future. As the debate on secular stagnation and the level of the long-run Wicksellian rate (the safe rate consistent with unemployment remaining at the natural rate) indicate,
the future is indeed uncertain. It may be that some of the factors underlying low rates will fade over time. Or it may be because public debt increases to the point where the equilibrium safe rate actually exceeds the growth rate. In the formal model above, a high enough level of debt, and the associated decline in capital accumula-tion, eventually leads to an increase in the safe rate above the growth rate, leading to positive fiscal costs and higher welfare costs. Indeed, the trajectory of deficits under current fiscal plans is indeed worrisome. Estimates by Sheiner (2018), for example, suggest that even under the assumption that the safe rate remains below the growth rate, we may see an increase in the ratio of debt to GDP of close to 60
percent of
GDP between no
w and 2043. If so, using a standard (but admittedly rather uncertain
as well) back-of-the-envelope number that an increase in debt of 1
percent
of GDP
increases the safe rate by 2–3 basis points, this would lead to an increase in the safe rate of 1.2 percent to 1.8 percent, enough to reverse the inequality between the safe rate and the growth rate.
The evidence on indexed bonds suggests however two reasons to be relatively
optimistic about the sign of the inequality. The first is that, to the extent that the US government can finance itself through inflation-indexed bonds, it can actually lock in a real rate of 1.1 percent over the next 30 years, a rate below even pessimistic


---Economics-2019-0-32.txt---
forecasts of growth over the same period. The second is that investors seem to give
a small probability to a major increase in rates. Looking at 10-year inflation-in-dexed bonds, and using realized volatility as a proxy for implied volatility (option markets are not deep enough to derive implied volatility directly), suggests that the market puts the probability that the rate will be higher than 200 bp in 5 years around 5–15

percent.41
In short, one can surely not exclude the possibility that debt will indeed be more
costly in the future, and the safe rate may exceed the growth rate. The welfare implications however are continuous, and for reasonably small positive differences between the interest rate and the growth rate, the welfare costs will remain small. The basic intuition remains the same. The safe rate is the risk-adjusted rate of return on capital. If it is higher but not much higher than the growth rate, lower capital accumulation may not have major adverse welfare effects.
The third counterargument relies on the existence of multiple equilibria and may
be the most difficult to counter.
42 Suppose that the model above is right, and that
investors believe debt to be safe and are willing to hold it at the safe rate. In this case, the fiscal cost of debt may indeed be zero, and the welfare cost may be small. If however, investors believe that debt is risky and ask for a risk premium to com-pensate for that risk, debt payments will be larger, and debt will indeed be risky, and investors’ expectations may be self-fulfilling.
The mechanics of such fiscal multiple equilibria were first characterized by Calvo
(1988), later on by Giavazzi and Pagano (1990), and more recently by Lorenzoni and Werning (2018). In this case, over a wide range of debt, there may be two
equilibria, with the good one being the one where the rate is low, and the bad one characterized by a high risk premium on public debt, and a higher rate.
43
The question is what practical implications this has for debt levels.The first question is whether there is a debt level sufficiently low as to eliminate
the multiplicity. If we ignore strategic default, there must be some debt level low enough that the debt is effectively safe and there is only one equilibrium. The proof is by contradiction. Suppose investors worry about risk and increase the required rate. As the required rate increases, the state may indeed default. But suppose that, even if it defaults, debt is low enough that, while it cannot pay the stated rate, it can pay the safe rate. This in turn implies that investors, if they are rational, should not and will not worry about risk.
This argument however raises two issues. First, it may be difficult to assess what
such a safe level of debt is: it is likely to depend on the nature of the government, its ability to increase and maintain a primary surplus. Second, the safe level of debt may be very low, much lower than current levels of debt in the United States or in Europe. If multiple equilibria are present at, say 100
percent of GDP
, they are likely
to still be present at 90
percent as well; going ho
wever from 100
percent of GDP to

---Economics-2019-0-33.txt---
90 percent requires a major fiscal consolidation and, if the fiscal consolidation can -
not be fully offset by expansionary monetary policy, an economic contraction. As
Giavazzi and Pagano, and Lorenzoni and Werning, have shown, other dimensions of debt and fiscal policy, such as the maturity of debt or the aggressiveness of the fiscal rule in response to higher interest rates, are likely to be more important than the level of debt itself, and help eliminate the bad equilibrium. To be more concrete, it may be that, rather than embarking on fiscal austerity if it cannot be fully offset by looser monetary policy, it is better to rely on an aggressive contingent fiscal rule to eliminate the bad equilibrium.
VI. Conclusions
In this lecture, I have looked at the fiscal and welfare costs of higher debt in an
economy where the safe interest rate is less than the growth rate. I have argued that this is a relevant empirical configuration, and indeed has been the norm rather than the exception in the United States in the past. I have argued that both the fiscal and welfare costs of debt may then be small, smaller than is generally taken as given in current policy discussions. I have considered a number of counterarguments, which are indeed valid, and may imply larger fiscal and welfare costs. The purpose of this lecture is most definitely not to argue for higher debt per se, but to allow for a richer discussion of debt policy and appropriate debt rules than is currently the case.


---Economics-2020-0-00.txt---
In the last decades of the twentieth century, US monetary policy wrestled
with the problem of high and erratic inflation. That fight, led by Federal Reserve
chairs Paul V olcker and Alan Greenspan, succeeded. The result—low inflation and
well-anchored inflation expectations—provided critical support for economic stability
and growth in the 1980s and 1990s, in part by giving monetary policymakers
more scope to respond to  short-term fluctuations in employment and output without
having to worry about stoking high inflation.
However, with the advent of the new century, it became clear that low inflation
was not an unalloyed good. In combination with historically low real interest rates—
the result of demographic, technological, and other forces that raised desired global
saving relative to desired investment—low inflation (actual and expected) has translated
into persistently low nominal interest rates, at both the long and short ends of


---Economics-2020-0-01.txt---
the yield curve. Chronically low interest rates pose a challenge for the traditional
approach to monetary policymaking, based on the management of a  short-term policy
interest rate. In the presence of an effective lower bound on nominal interest
rates—due to, among other reasons, the existence of cash, which provides investors
the option of earning a zero nominal return—persistently low nominal rates constrain
the amount of “space” available for traditional monetary policies. Moreover,
as the experience of Japan in recent decades has demonstrated, low inflation can
become a  self-perpetuating trap, in which low inflation and low nominal interest
rates make monetary policy less effective, which in turn allows low inflation or
deflation to persist.
In the United States and other advanced economies, the critical turning point was
the global financial crisis of  2007–2009. The shock of the panic, and the subsequent
sovereign debt crisis in Europe, drove the US and global economies into deep recession,
well beyond what could be managed by traditional monetary policies. After
cutting  short-term rates to zero (or nearly so), the Federal Reserve and other central
banks turned to alternative policy tools to provide stimulus, including  large-scale
purchases of financial assets (quantitative easing), increasingly explicit communication
about the central bank’s outlook and policy plans (forward guidance), and,
outside the United States, some other tools as well.
We are now more than a decade from the crisis, and the US and global economies
are in much better shape. But, looking forward, the Fed and other central banks are
grappling with how best to manage monetary policy in a  twenty-first century context
of low inflation and low nominal interest rates. On one point we can be certain: the
old methods won’t do. For example, simulations of the Fed’s main macroeconometric
model suggest that the use of policy rules developed before the crisis would
result in  short-term rates being constrained by zero as much as  one-third of the time,
with severe consequences for economic performance (Kiley and Roberts 2017). If
monetary policy is to remain relevant, policymakers will have to adopt new tools,
tactics, and frameworks.
The subject of this lecture is the new tools of monetary policy, particularly
those used in recent years by the Federal Reserve and other  advanced-economy
central banks.1 I focus on quantitative easing and forward guidance, the principal
new tools used by the Fed, although I briefly discuss some other tools, such as
funding-for-lending programs, yield curve control, and negative interest rates. Based
on a review of a large and growing literature, I argue that the new tools have proven
quite effective, providing substantial additional scope for monetary policy despite
the lower bound on  short-term interest rates.2 In particular, although there are dissenting
views, most research finds that central bank asset purchases meaningfully
ease financial conditions, even when financial markets are not unusually stressed.
Forward guidance has become increasingly valuable over time in helping the public
understand how policy will respond to economic conditions and in facilitating


---Economics-2020-0-02.txt---
commitments by monetary policymakers to  so-called  lower-for-longer rate policies,
which can add stimulus even when short rates are at the lower bound. And, for the
most part, in retrospect it has become evident that the costs and risks attributed to the
new tools, when first deployed, were overstated. The case for adding the new tools
to the standard central bank toolkit thus seems clear.
But how much can the new tools help? To estimate the policy space provided
by the new tools, I turn to simulations of the Fed’s FRB/US model (Brayton et al.
2014). Assuming, importantly, that the (nominal) neutral rate of interest, defined
more fully below, is in the range of 2 to 3 percent—consistent with most current
estimates for the US economy—then the simulations suggest that a combination of
asset purchases and forward guidance can add roughly 3 percentage points of policy
space. That is, when the new tools are used, monetary policy can achieve outcomes
similar to what traditional policies alone could attain if the neutral interest rate were
3 percentage points higher, in the range of  5–6 percent—which, it turns out, is close
to what is needed to negate the adverse effects of the effective lower bound in most
circumstances. In particular, as I will argue, in this scenario the use of the new tools
to increase policy space seems preferable to the alternative strategy of raising the
central bank’s inflation target.
An important caveat to these conclusions, as already indicated, is that they apply
fully only when the neutral interest rate is in the range of  2–3 percent or above. If the
neutral rate is below 2 percent or so, the new tools still add valuable policy space but
are unlikely to compensate entirely for the constraint imposed by the lower bound.
The costs associated with a very low neutral rate, measured in terms of deeper and
longer recessions and inflation persistently below target, underscore the importance
for central banks of keeping inflation and inflation expectations close to target. A
neutral rate below 2 percent or so also increases the relative attractiveness of alter -
native strategies for increasing policy space, such as raising the inflation target or
relying more heavily on fiscal policy to fight recessions and to keep inflation and
interest rates from falling too low.
I. Assessing the New Tools of Monetary Policy
When the  short-term policy interest rate reaches the effective lower bound, monetary
policymakers can no longer provide stimulus through traditional means.3
However, it is still possible in those circumstances to add stimulus by operating
on  longer-term interest rates and other asset prices and yields. Two tools for doing
that, both actively used in recent years, are (i) central bank purchases of  longer-term
financial assets (popularly known as quantitative easing, or QE), and (ii) communication
from monetary policymakers about their economic outlooks and policy plans
(forward guidance).4 I’ll discuss QE first, returning later to forward guidance, as
well as to some other new tools used primarily outside the United States.



---Economics-2020-0-03.txt---
I focus throughout this lecture on monetary tools aimed at achieving employment
and inflation objectives, excluding policies aimed primarily at stabilizing dysfunctional
financial markets, such as the Federal Reserve’s emergency credit facilities
and currency swaps and the European Central Bank’s (ECB) Securities Markets
Program, under which the ECB made targeted purchases to help restore confidence
in sovereign debt markets.5 The stabilization of financial markets improves economic
outcomes, of course, but  lender-of-last-resort programs are not useful outside
of a crisis and thus should not be viewed as part of normal monetary policy.
A. Central Bank Asset Purchases (QE)
The Fed announced its first program of  large-scale asset purchases in November
2008, when it made public its plans to buy  mortgage-backed securities (MBS) and
debt issued by the  government-sponsored enterprises (GSEs), Fannie Mae and
Freddie Mac. In March 2009, in an action that would become known as QE1, the
Federal Open Market Committee (FOMC) authorized both increased purchases of
MBS and, for the first time,  large-scale purchases of US Treasury securities. Asset
purchases under QE1 totaled about $1.725 trillion (Bhattarai and Neely 2016).
Three other major programs would follow: (i) QE2, announced in November 2010,
in which the Fed committed to $600 billion in additional Treasuries purchases; (ii)
the Maturity Extension Program, announced in September 2011 and extended in
June 2012, under which the Fed lengthened the average maturity of its portfolio
by selling off  short-term Treasuries and buying  longer-term government debt; and
(iii) QE3, announced in September 2012, an  open-ended program that committed
the Fed to buying both Treasury securities and MBS until the outlook for the labor
market had improved “substantially.” In 2013, hints that asset purchases might begin
to slow led to a “taper tantrum” in bond markets, with the  10-year yield rising by
nearly one percentage point over several months. The Fed’s purchases did not end
however until October 2014. Total net asset purchases by that point were about $3.8
trillion, approximately 22 percent of 2014 GDP. Most purchases were of  longer-term
securities; between 2007 and late 2014 the average duration of the Fed’s portfolio
increased from 1.6 years to 6.9 years (Engen, Laubach, and Reifschneider 2015).
The Fed was by no means the only central bank to employ asset purchases as
a monetary policy tool. The first to confront the lower bound, the Bank of Japan,
adopted an asset purchase program in March 2001, but its focus was increasing
the monetary base rather than reducing  longer-term rates by buying  longer-term
assets. The BOJ began aggressive purchases of  longer-term securities in 2013 with
the advent of “Abenomics,” the set of policies advocated by Prime Minister Shinzo
Abe. The Bank of England adopted QE more or less in parallel with the Federal
Reserve, announcing its first major program in March 2009, a few days ahead of
the Fed’s QE1 announcement. The BOE then periodically increased targets for


---Economics-2020-0-04.txt---
its total purchases in response to economic developments. The European Central
Bank faced political and legal opposition to asset purchases and undertook its first
large QE program in pursuit of monetary policy objectives only in January 2015.
Variants of QE have been employed by smaller economies, including Sweden and
Switzerland.
The types of assets purchased varied considerably by central bank. Facing tighter
legal constraints than most of its peers, the Fed was able to purchase only Treasury
securities and securities issued by the GSEs, which by late 2008 were fully backed
by the federal government. Other central banks had wider authorities, and to varying
degrees bought not only government debt but also corporate bonds, covered bonds
issued by banks, and even equities.
In the immediate aftermath of the financial crisis, the relative lack of experience
with QE created substantial uncertainty about how effective asset purchases would
be in easing financial conditions, if they would help at all. Indeed, some benchmark
models predict that asset purchases will have no or at best transient effects on asset
prices (Eggertsson and Woodford 2003). The positive case for QE rested on two
arguments. First, if investors have “preferred habitats” because of specialized exper -
tise, transaction costs, regulations, liquidity preference, or other factors, then changing
the net supplies of different securities or classes of securities should affect their
relative prices. This portfolio balance effect was modeled formally by Vayanos and
Vila (2009), who showed that, generally, the effect will not be undone by the efforts
of arbitrageurs. US policymakers saw QE as working in part by removing duration
risk from the Treasury market, pushing investors to bid up the values of both
remaining  longer-term Treasuries and close substitutes, such as  mortgage-backed
securities and corporate bonds. In addition, MBS purchases were expected to reduce
the spread between Treasury yields and mortgage rates.
Second, QE may have a signaling effect if it serves as a commitment mechanism,
or perhaps as a signal of seriousness, leading investors to believe that policymakers
intend to keep  short-term policy rates low for an extended period. Although
several channels have been proposed for how this might work, in practice much of
the signaling effect appears tied to investors’ beliefs about the likely sequencing of
policies. With encouragement from policymakers, market participants are typically
confident that central banks will not raise  short-term interest rates so long as asset
purchases are continuing. Since QE announcements typically include information
about the likely duration of purchases, which may be measured in quarters or years,
and since QE programs are rarely terminated prematurely (because of the likely
costs to policymakers’ credibility), the initiation or extension of a QE program often
pushes out the expected date of the first  short-term rate increase. Observing this
signal that short rates will be kept low, investors bid down  longer-term rates as well.
Longer-term yields can be conceptually divided into (i) the average expected
short rate over the life of the security, and (ii) the difference between the total yield
and the average expected short rate, known as the term premium. To a first approximation,
portfolio balance effects work by affecting the term premium, while the
signaling effect works by influencing expectations of future short rates. Using that
approximation to distinguish the portfolio balance and signaling channels is not
straightforward, however, because term premiums and expected future short rates are
not directly observable. There are also indirect effects to account for: for  example,


---Economics-2020-0-05.txt---
changes in term premiums arising from the portfolio balance channel, if they influence
the economic outlook, will also affect expectations of future short rates.
If QE successfully reduces  longer-term interest rates, through either portfolio balance
or signaling channels, then the presumption is that the economy will respond
much in the same way that it does to conventional monetary easing, as a lower cost
of capital, higher wealth, a weaker currency, and stronger balance sheets increase
spending on domestic goods and services.
QE Event Studies: Some Initial Evidence.—Did  post-crisis QE work, in the sense
of meaningfully affecting broad financial conditions? Early QE announcements, at
least, appeared to have substantial market impacts across a wide range of financial
assets. This fact is  well documented by event studies, which look at asset price
changes in narrow time windows around QE announcements.
An illustrative event study for the Fed’s QE1 program is shown in Table 1, which
reports the changes in key asset prices and yields summed over five days, identified
by Gagnon et al. (2011), on which important information bearing on QE1 became
public.6 Evidently, QE1 had powerful announcement effects, including a full per -
centage point decline in the yield on  10-year Treasuries and more than a percentage
point decline in the yields on  mortgage-backed securities. Qualitatively, these
results hold up well for different choices of event days or for shorter or longer event
windows. Similar  event-study results are obtained for the introduction of QE, at
about the same time, by the Bank of England (Joyce et al. 2011).
The strong market reactions to the initial rounds of QE encouraged policymakers
at the time, and they should refute strong claims that central bank asset purchases
are neutral. However, critics have made two rejoinders to the  event-study evidence.7
First, in contrast to the results shown in Table  1 for QE1, event studies of later
rounds of quantitative easing have tended to find much less dramatic effects. For
example, Krishnamurthy and  Vissing-Jorgenson (2011) looked at the market reactions
associated with the introduction of QE2, the second round of US quantitative
easing. Using two identified announcement days and  one-day event windows, they



---Economics-2020-0-06.txt---
found the total decline in the  10-year Treasury yield associated with QE2 was a
relatively moderate 18 basis points, well less than the QE1 effect even with some
adjustment for the different sizes of the two programs. Analogous results have been
found in event studies of other  later-round QE programs, in both the United States
and in other countries. A possible interpretation is that the initial rounds of QE were
particularly effective because they were introduced, and provided critical liquidity,
in a period of exceptional dysfunction in financial markets. However, if QE only
works in such extraordinary circumstances, it is of limited use for monetary policymakers
during calmer times.
The second point raised by critics is that event studies, by their nature, capture
asset market reactions over only a short period. It may be that these studies reveal
only  short-term liquidity effects, analogous (although much larger) to the  within-day
price effects of an unexpectedly large purchase or sale of a stock. Such effects would
be expected to dissipate quickly and would not provide much monetary accommodation,
since private spending decisions presumably respond only to persistent
changes in financial conditions. A variant of this objection, which takes a slightly
longer-term perspective, begins by pointing out that  longer-term Treasury yields did
not consistently decline during periods in which asset purchases were being carried
out. For example, the  10-year yield at the termination of QE1 purchases was actually
higher than it was before QE1 was announced. Perhaps investors came to appreciate
over time that  asset-purchase programs would not be effective? Using time series
methods, Wright (2011) argues that the effects of  post-crisis policy announcements
died off fairly quickly.
Additional Evidence on the Effects of QE.—These two critiques of the  event-study
evidence raise important issues. However, other evidence on the effects of QE provides
counterpoints. I take each of the critiques in turn.
First, although the weaker effects on asset prices found in event studies of later
rounds of QE could be the result of the calmer market conditions, those findings
could also reflect that later rounds of QE were better anticipated by investors, who
by then had been educated about the tool and the willingness of central banks to use
it. If later QE rounds were largely anticipated, then their effects would have been
incorporated into asset prices in advance of formal announcements, accounting for
the  event-study results (Gagnon 2018).
Surveys of market participants and media reports suggest that later rounds of QE
in the United States and elsewhere were in fact widely anticipated. For example,
according to the New York Fed’s survey of primary dealers, prior to the announcement
of QE2 in November 2010, dealers placed an 88 percent probability that the
Fed would undertake another round of asset purchases. The primary dealers also
expected the program to be significantly larger and more extended than what was
subsequently announced (Cahill et al. 2013, Appendix A). It’s not surprising then
that the market reaction on the date of the QE2 announcement was small; in fact,
10-year Treasury yields rose slightly on the day, presumably reflecting investor disappointment
about the program’s scale.
For  event-study researchers, a possible way to address this problem is to include
more event days, to capture more announcements, data releases, and other events
bearing on the probability of new asset purchases (Greenlaw et al. 2018). However,


---Economics-2020-0-07.txt---
adding event days also adds noise from  nonmonetary news affecting asset prices. A
more direct solution to this identification problem is to try to control for the policy
expectations of market participants, then to observe the effects on asset prices of the
unexpected component of QE announcements.
For traditional monetary policy, based on management of the  short-term interest
rate, fed funds and Eurodollar futures markets provide useful estimates of policy
expectations (Kuttner 2001), but no analogous markets exist for asset purchases
and other nontraditional policies. As an alternative, several researchers have used
surveys and media reports to try to quantify those expectations. For example, in
the European context, De Santis (2019) attempted to estimate the financial mar -
ket effects of the ECB’s Asset Purchase Program, its first foray into  large-scale
QE, announced in January 2015. ECB policymakers and media commentary had
strongly foreshadowed the program, so its actual announcement—like the announcement
of later rounds of QE in the United States—had only modest market effects,
with the average ( GDP-weighted)  10-year sovereign debt yield in the euro area
declining by about 10 basis points. To try to control for market expectations of
ECB actions, De Santis counted the number of news stories on Bloomberg that
contained various keywords. From these, he created an index of media and market
attention to QE in Europe. Controlling both for this measure and for macroeconomic
and  country-specific factors, De Santis found that the ECB’s initial QE program
reduced average  10-year sovereign debt yields by 63 basis points over the period
from September 2014 to October 2015. This reduction is economically significant
and, when adjusted for the size of the program, comparable to estimates from event
studies of early QE programs in the United States and the United Kingdom, even
though in early 2015 European financial markets were functioning normally.
A related empirical strategy for measuring the effects of QE relies on the fact that,
even when the size of a QE program was well anticipated, market participants may
have been unsure about the specific assets to be purchased. If the portfolio balance
effect is operating, then news that an unexpectedly large share of the central bank’s
planned purchases will be devoted to a particular asset should raise the price of that
asset relative to others. An impressive literature has been built on this insight.8 For
example, in a careful study, Cahill et al. (2013) used data on  within-day prices on
all outstanding US Treasury securities (excluding  inflation-indexed bonds) for the
period 2008 to 2012. Their goal was to study, over time frames as short as 30 minutes,
not just how QE announcements affected overall yields but how they affected
the relative yields of individual securities. That led them to focus on announcements
about which securities would be targeted for purchase. To measure unexpected shifts
in purchase plans, the authors used the Primary Dealer Survey and other sources.
To illustrate their approach: On November 3, 2010, at 2:15 pm, the FOMC
announced QE2, a plan to purchase $600 billion of Treasury securities. As already
discussed, the program was largely anticipated, and the announcement accordingly
had little effect on Treasury yields overall. However, at the same time as the FOMC
announcement, the New York Fed released information about how it planned to
allocate purchases across Treasury securities of different maturities. This document



---Economics-2020-0-08.txt---
revealed that, in a change unexpected by the primary dealers, bonds between 10 and
30 years maturity would make up only about 6 percent of planned purchases, compared
to 15 percent in earlier rounds. If the portfolio balance channel is operating,
that news should have led to a decline in the prices and a rise in the yields of the
longer-maturity bonds, relative to those with shorter maturity. That was indeed what
the authors found.
Cahill et al. (2013) performed similar analyses of QE1, the Fed’s decision in
August 2010 to keep its asset holdings constant by reinvesting the proceeds of
maturing securities, the Maturity Extension Program, and the extension of the MEP
that preceded QE3. (Their study was completed before the announcement of QE3.)
They found in each case that unanticipated changes in implementation plans had
significant  cross-sectional effects on bond prices and yields. Their estimated effects
are both economically large and, importantly, show no tendency to decline over time
or as the size of the central bank’s balance sheet increases. These results, which
have been replicated in a number of studies, including for the United Kingdom,
once again do not support the view that QE is only effective when markets are
dysfunctional.9
Cahill et al. (2013), like most studies in this literature, looks at the differential
impact of asset purchase programs on Treasury debt of varying maturity. But
the Fed’s purchase programs also differed in how they treated Treasuries versus
mortgage-backed securities, with QE1 including substantial MBS purchases for
example, but QE2 involving only purchases of Treasuries. If portfolio balance
effects are at work, then unanticipated changes in the  Treasury-MBS mix should
affect the relative yields of those asset classes. That too seems to have been the
case, as illustrated for example by Krishnamurthy and  Vissing-Jorgenson (2011) in
their comparison of the effects of QE1 and QE2. Relatedly, Di Maggio, Kermani,
and Palmer (2015) considered the effects of the Fed’s QE programs on the relative
returns to MBS issued by the GSEs, which were eligible for purchase by the Fed,
and MBS backed by larger (jumbo) mortgages, which by law cannot be guaranteed
by the GSEs and thus were not eligible for Fed purchase. These authors found that
QE1, which included large quantities of MBS purchases, depressed mortgage rates
in general by more than 100 basis points but reduced the rates on jumbo mortgages
by only about half as much, consistent with the portfolio balance effect. In contrast,
they found that QE2 and the Maturity Extension Program, neither of which included
MBS purchases, did not differentially affect rates on  GSE-eligible mortgages and
jumbo mortgages.
Note that studies of the  cross-sectional  asset-price impacts of QE announcements
should reflect only portfolio balance effects. Studies have also found evidence of signaling
effects, that is, QE announcements tend to be associated with changes in the
expected path of  short-term interest rates (Bauer and Rudebusch 2014). In the “taper
tantrum” episode of 2013, market participants were surprised by my comments in a
congressional testimony and a press conference that asset purchases might soon be

---Economics-2020-0-09.txt---
slowed; the significant increases in  longer-term yields and expected  short-term rates
that followed show that signaling effects can be powerful and were not restricted to
the earliest QE announcements. I will return to the role of policy communications.
The evidence described so far suggests that, once we control for the fact that mar -
ket participants substantially anticipated later rounds of QE, the impact of asset pur -
chases did not significantly diminish over time, as financial conditions calmed, or as
the stock of assets held by the central bank grew. That still leaves the second broad
objection to the  event-study evidence, that those studies prove only that announcements
of asset purchases have  short-run effects on asset prices and yields. If the
effects of announcements are quickly reversed, then QE programs would likely be
ineffective in stimulating the economy.
The claim that the effects of QE announcements were mostly transitory, due for
example to pure liquidity effects, is not particularly persuasive on its face. The nor -
mal presumption is that the effects on asset prices identified by event studies should
be largely persistent, even if the event window is relatively short. If that were not
the case—if the effects of asset purchase announcements were predictably temporary—
then smart investors could profit by betting on reversals. Indeed, in a response
to Wright (2011), Neely (2016) showed that time series models that imply reversals
of the effects of QE announcements do not predict asset prices as well out of sample
as the simple assumption that asset prices tomorrow will be the same as today. In
other words, predicting reversals of the effects of asset purchase programs is not
a  money-making strategy, as we should expect. Moreover, Neely (2010), Gagnon
et al. (2011), and many others found that the prices of assets not subject to Fed
purchases—including corporate bonds, equities, the dollar, and a variety of for -
eign assets—moved substantially following announcements of asset purchase programs,
and in much the same way as following conventional policy announcements.
QE also appeared to stimulate the global issuance of corporate bonds (Lo Duca,
Nicoletti, and Martinez 2016) and to reduce the cost of insuring against corporate
credit risk via credit default swaps (Gilchrist and Zakrajšek 2013). The  cross-asset
impacts seem inconsistent with the view that the  event-study findings reflect only
asset-specific liquidity effects.10
As noted earlier, proponents of the view that QE had only transient effects sometimes
point out that  longer-term yields did not reliably decline during periods in
which the Fed was executing its announced asset purchases. In part, this pattern can
be explained by the confounding influences on yields of other factors, including fiscal
policy, global conditions, and changes in sentiment. For example, the rise in yields
in the latter part of 2009, during the implementation of QE1, was seen at the Fed not
as a policy failure but rather as an indication that its aggressive monetary policies,
together with other factors such as the Obama administration’s fiscal program and
the successful stress tests of major banks, were increasing public confidence in the
economic outlook. Indeed, judging from the returns to  inflation-protected securities,
much of the increases in  10-year yields during the implementation phases of QE1
and QE2 reflected higher inflation expectations, a desired outcome of the programs.

---Economics-2020-0-10.txt---
A deeper response to this argument turns on the distinction between two views
of how QE works, the  so-called stock and flow views. The standard portfolio balance
channel of QE, recall, holds that policymakers can affect  longer-term yields by
changing the relative supplies—that is, the stocks outstanding—of various financial
assets. In this stock view of QE, the effect of asset purchases on yields at each point
in time depends on the accumulated stock of central bank purchases and (because
asset markets are  forward-looking) on expected stocks of central bank holdings at
all future dates. The alternative flow view holds that the current pace of purchases is
the critical determinant of asset prices and yields. This view implicitly underlies the
argument that the effectiveness of QE can be evaluated by looking at the behavior of
longer-term yields during periods of active  central-bank purchases. The flow view
would be correct if QE affected asset prices and yields primarily through  short-run
liquidity effects.
However, the stock view conforms better to the underlying theory and has better
empirical support (D’Amico and King 2013). Substantial research has tried to
quantify the dynamic relationship between yields and the relative supplies of securities
under the stock view. In an important article, Ihrig et al. (2018) estimated
an  arbitrage-free model of the term structure of Treasury yields, in which current
and expected holdings of securities by the Fed are allowed to influence yields.11
They carefully modeled the evolution of the Fed’s balance sheet, given its purchases
and the maturing of existing securities, and they developed reasonable measures of
market expectations of future purchases. They also incorporated estimates of new
Treasury debt issuance, which partially offset the effects of the Fed’s purchases on
the net supply of government debt (Greenwood et al. 2014).
Putting these elements together, Ihrig et al. (2018) found significant effects of
the Fed’s asset purchase programs on Treasury yields. For example, their estimates
suggest that, at inception, QE1 reduced the  10-year term premium by 34 basis
points, the Maturity Extension Program reduced term premiums by an additional
28 basis points, and QE3 reduced term premiums yet more, by 31 basis points on
announcement and more over time. This finding is consistent with other papers that
show no reduction in the effectiveness of later programs relative to the earliest ones.
Their results also imply substantial persistence: although the effect of any given
QE program decayed over time, as securities matured and ran off the Fed’s balance
sheet, Ihrig et al. (2018) estimated that the cumulative effect of the purchases on
the  10-year yield exceeded 120 basis points when net purchases ended in October
2014 and was still about 100 basis points as of the end of 2015. In related work, Wu
(2014) found quite similar results, crediting Fed asset purchases with more than half
of the 217 basis point decline in  10-year Treasury yields between the Lehman failure
and the taper tantrum. Altavilla, Carboni, and Motto (2015) and Eser et al. (2019)
estimated related models for the euro area, likewise finding that ECB purchases had
sizable and persistent impacts on asset prices—notwithstanding, once again, that the
ECB’s program was announced at a time of low financial distress.


---Economics-2020-0-11.txt---
In sum, while there is room for disagreement about the effects of QE on
longer-term yields, most evidence supports the view that they were both economically
significant and persistent. In particular, the research rejects the notion that QE
is only effective during periods of financial disruption. Instead, once market participants’
expectations are accounted for, the impact of new purchase programs seems
to have been more or less constant over time, independent of market functioning, the
level of rates, or the size of the central bank balance sheet.
B. Forward Guidance
The second new tool used by almost all major central banks in recent years,
other than asset purchases, is forward guidance. Forward guidance, or “open mouth
operations” (Guthrie and Wright 2000), is communication about how monetary policymakers
expect the economy and policy to evolve. Forward guidance takes many
forms (such as the specification of policy targets, economic and policy projections)
and occurs in many venues (speeches and testimonies, monetary policy reports).
The Fed took several steps to enhance its communications during the  post-crisis
period, including introducing press conferences by the chair, setting a formal inflation
target, and releasing more detailed economic projections by FOMC participants,
including policy rate projections. I focus here though on formal guidance by
the policy committee about the future paths of key policy instruments, especially
policy rates and asset purchases.
Forward guidance, at least in a broad sense, was not new to the  post-crisis period.
The Fed used variants of forward guidance in the Greenspan era, for example, in the
promises of the FOMC in  2003–2004 to keep rates low “for a considerable period” or
to remove accommodation “at a pace that is likely to be measured.” Ample evidence
suggests that these and other  pre-crisis communications by the FOMC affected mar -
ket expectations of policy rates and thus asset prices and yields generally. For example,
Gürkaynak, Sack, and Swanson (2005), using a  high-frequency event study,
showed that the effects of monetary policy announcements on asset prices can be
decomposed into two factors: one associated with unexpected changes in the current
setting of the federal funds rate and the other with news about the expected future
path of the funds rate, which the authors associated with the (implicit or explicit)
forward guidance in the policy statement. Both factors are important, with the
forward guidance factor being particularly influential in determining  longer-term
yields. Other central banks had also used communication as a policy tool before the
crisis, an early example being the Bank of Japan, whose  zero-interest-rate policy
included a promise not to raise the policy rate from zero until certain conditions had
been met.
Campbell et al. (2012) introduced the useful distinction between Delphic and
Odyssean forward guidance. Delphic guidance (after the oracles at the Temple of
Apollo at Delphi) is intended only to be informative, to help the public and market
participants understand policymakers’ economic outlook and policy plans. In contrast,
Odyssean guidance goes beyond simple economic or policy forecasts by incor -
porating a promise or commitment by policymakers to conduct policy in a specified,
possibly  state-contingent way in the future (as when Odysseus bound himself to the
mast to avoid the temptations of the Sirens).


---Economics-2020-0-12.txt---
Both Delphic and Odyssean guidance have potentially important roles when
policymakers confront the lower bound on rates. Delphic guidance that helps the
public better understand the central bank’s reaction function may be valuable at the
lower bound since—given the “history dependence” of optimal monetary policy
(Woodford 2013)—the responses of monetary policymakers to a given configuration
of inflation and employment after a period at the lower bound may be quite different
than during  more normal times. Odyssean guidance is useful at the lower bound
because optimal monetary policy in those circumstances may be at least somewhat
time-inconsistent, in the sense of Kydland and Prescott (1977)—that is, at the lower
bound, monetary policymakers may want to commit to  interest-rate paths or to other
actions from which they will have incentives to deviate in the future.
For example, when  short-term rates cannot be reduced further, policymakers
may want to put downward pressure on  longer-term rates by persuading mar -
ket participants that they intend to keep the policy rate at the lower bound for an
extended period—a  so-called  lower-for-longer policy—even if that involves a
possible ( time-inconsistent) overshoot of their inflation target. As I will discuss,
lower-for-longer policies are in turn closely related to  so-called makeup strategies,
in which policymakers promise to compensate for protracted undershoots of their
inflation or employment goals by a period of overshoot (Yellen 2018). Odyssean
guidance can make such commitments clear and create a reputational stake for the
central bank to follow through.
In the immediate aftermath of the financial crisis, most examples of forward guidance
were qualitative, using language similar to Greenspan’s “considerable period”
rather than precisely specifying the future path of rates or the conditions under which
rates would be raised. Some research has criticized the Fed’s guidance during this
time. Woodford (2012) argued that the guidance in the FOMC’s policy statements
lacked sufficient commitment to be effective—that is, the language was Delphic
when it should have been Odyssean. Engen, Laubach, and Reifschneider (2015)
noted that, in  2009–2010, private (Blue Chip) forecasters continued to believe that
the Fed intended to begin raising rates relatively soon, notwithstanding (qualitative)
guidance to the contrary; according to these authors, the forecasters’ beliefs evidently
reflected both a misunderstanding of the Fed’s reaction function and excessive
optimism about the likely speed of the recovery. Campbell et al. (2017) concluded
that Fed forward guidance only became Odyssean (that is, effectively committing to
lower for longer) in 2011, at which point it began to lead to better macroeconomic
outcomes. Gust et al. (2017) similarly found, in the context of an estimated dynamic
stochastic general equilibrium (DSGE) model, that market participants only gradually
understood the FOMC’s  lower-for-longer message. Supporting the critics’ view
is that, despite the Fed’s efforts to talk down rates, the  two-year Treasury yield—
an indicator of  near-term monetary policy expectations—remained near 1 percent
through the spring of 2010, declining only gradually after that.
Over time, the FOMC pushed back against the excessively hawkish expectations
of market participants with more precise and aggressive forward guidance. In
August 2011, the FOMC for the first time explicitly tied its guidance to a date, indicating
that it would keep the fed funds rate near zero “at least through  mid-2013.”
In January 2012 it extended that commitment “at least through late 2014,” and
in September 2012 it extended the commitment yet again to “at least through


---Economics-2020-0-13.txt---
mid-2015.” In December 2012, the FOMC switched from guidance specifying a
date for policy action (calendar guidance) to a description of the conditions that
would have to be met for rates to be raised ( state-contingent guidance). Specifically,
policymakers promised not even to consider raising the policy rate until unemployment
had fallen at least to 6.5 percent, as long as inflation and inflation expectations
remained moderate. A year later, this statement was strengthened further, with the
FOMC indicating that no rate increase would occur until “well past the time” that
unemployment declined below 6.5 percent. In principle,  state-contingent guidance,
which ties future policy rates to economic conditions, is preferable to calendar guidance
because it permits the market’s rate expectations to adjust endogenously to
incoming information bearing on the outlook (Feroli et al. 2017). However, calendar
guidance has the  not-inconsiderable advantages of simplicity and directness, and it
can be adjusted if needed (Williams 2016).
The increasingly explicit guidance by the FOMC ultimately had the desired effect
of shifting market rate expectations in a dovish direction:  two-year Treasury yields
declined to about 0.25 percent in the second half of 2011, where they remained for
several years. Table 2, using the event study methodology described earlier, shows
the sum of  one-day responses of several key asset prices to the first two calendar
guidance announcements, in August 2011 and January 2012. The table shows that
the Fed’s announcements appear to have moved interest rates down significantly,
increasing stimulus. The two announcements were also associated with a decline in
the dollar (not shown) and a rise in equity prices.
Other evidence suggests that these announcements worked as intended: Femia,
Friedman, and Sack (2015) showed that, during this period, professional forecasters
reacted to FOMC guidance by repeatedly marking down the unemployment rate
they expected to prevail when the Committee lifted the funds rate from zero, implying
a perceived change in the Fed’s reaction function in the  lower-for-longer direction.
Using information drawn from interest rate options, Raskin (2013) came to a
similar conclusion. Carvalho, Hsu, and Nechio (2016), counting particular words in
magazine and newspaper articles to measure policy expectations, found that unanticipated
communications by the Fed influenced  longer-term interest rates, while
Del Negro, Giannoni, and Patterson (2012) concluded that forward guidance positively
affected inflation and growth expectations.
I have been discussing QE and forward guidance separately, but in practice the
two tools are closely intertwined. As noted earlier, QE works in part by  implicitly

---Economics-2020-0-14.txt---
signaling the likely path of policy rates; increasingly, central banks (notably the
ECB) have made this connection explicit, for example, by promising no rate
increases until well after the conclusion of asset purchase programs. Policymakers
can also offer guidance about future asset purchases (Greenwood, Hanson, and
Vayanos 2015) or even tie the trajectory of asset holdings to the level of rates, as
when the FOMC indicated that it would begin to pare down its balance sheet only
after the policy rate had moved sufficiently above zero. And both asset purchases
and forward guidance affect asset prices in complicated ways, making it difficult to
separate the effects of the two tools (Eberly, Stock, and Wright 2019). An interesting
attempt at making that decomposition is the work of Swanson (2017), who extended
the  event-study methods of Gürkaynak, Sack, and Swanson (2005) to the  post-crisis
period. He showed that, during  2009–2015, movements in asset yields and prices
during  30-minute windows around FOMC announcements were dominated by two
factors: (i) changes in the expected path of the federal funds rate, which Swanson
identified with forward guidance, and (ii) changes in the level of  long-term interest
rates, which he identified with QE. With these identifying assumptions, he found
that both forward guidance and QE significantly and persistently affected a range of
asset prices, in a manner comparable to  pre-crisis policies.
The Fed’s experience during the  post-crisis era illustrates the more general point
that central banks, collectively, have been learning how to make better use of for -
ward guidance. Like the Fed, the Bank of England moved from qualitative guidance
to explicit,  state-contingent guidance. The Bank of Japan has used increasingly
aggressive guidance, both  state-contingent and calendar. The ECB has employed
statements and press conferences effectively to guide expectations about how it will
deploy its complex mix of policy tools. Charbonneau and Rennison (2015) provides
a chronology and a review of the evidence on  post-crisis forward guidance.
Altavilla et al. (2019) used a statistical analysis similar to that of Gürkaynak, Sack,
and Swanson (2005) and Swanson (2017) to identify the key dimensions of ECB
communication. Hubert and Labondance (2018) found that the ECB’s forward guidance
persistently lowered rates over the entire term structure.
Overall, the evolving evidence suggests that forward guidance can be a powerful
policy tool, with the potential to shift the public’s expectations in a way that increases
the degree of accommodation at the lower bound. Communication can also reduce
perceived uncertainty and, through this channel, lower risk premiums on bonds and
other assets ( Bundick, Herriford, and Smith 2017) . And, like Draghi’s famous “whatever
it takes” statement in July 2012, timely communication can reduce perceived
tail risks, promoting confidence ( Hattori, Schrimpf, and Sushko 2016) . The limits to
forward guidance depend on what the public understands, and what it believes. In
normal times, the general public does not pay much attention to central bank statements,
so robust policies should be designed to be effective even if they are followed
closely only by financial market participants. Even sophisticated players can misunderstand,
as in the taper tantrum, which means that policymakers must communicate
consistently and intelligibly.
Ensuring the credibility of forward guidance is also essential. The personal reputations
and skills of policymakers matter for credibility, but since policymakers can
bind neither themselves nor their successors, institutional reputation is important as
well. Policymakers have an incentive to follow through on earlier promises because


---Economics-2020-0-15.txt---
they want to be able to make credible promises in the future (Nakata 2015). The success
of frameworks like inflation targeting—which grant policymakers only “constrained
discretion” (Bernanke and Mishkin 1997)—shows that these reputational
forces can be quite effective. On the other hand, failure to achieve stated targets over
a long period can damage institutional credibility, as shown by the difficulty that the
Bank of Japan has had in raising inflation expectations (Gertler 2017).
Forward guidance in the next downturn will be more effective—better understood,
better anticipated, and more credible—if it is part of a policy framework clearly
articulated in advance. As of this writing, the Federal Reserve is formally reviewing
its policy framework and considering alternatives. Many of these frameworks,
including variants of  price-level targeting and average inflation targeting, involve
the  lower-for-longer or “makeup” policies described earlier. Bernanke, Kiley, and
Roberts (2019), using simulation methods, found that several of the frameworks
under consideration offer substantial promise to improve economic outcomes when
encounters with the lower bound are frequent. Importantly, many of the alternatives
they considered are only mildly  time-inconsistent, involving only modest overshoots
of the inflation target. Moreover, as Bernanke, Kiley, and Roberts (2019) discussed,
several of these frameworks involve only tweaks to the current  inflation-targeting
framework, which would ease any transition; and their effectiveness is not substantially
reduced if they affect the beliefs and behavior of only financial market par -
ticipants, as opposed to the general public. Improving policy and communications
frameworks to incorporate more-systematic forward guidance at the lower bound
should be a high priority for central banks.
C. Other New Monetary Policy Tools
The Federal Reserve supplemented traditional policy with QE and forward guidance
during the  post-crisis period, as did other central banks. But major central
banks outside the United States also used some other tools, which I will discuss
briefly here. As already noted, I will not discuss policy tools aimed primarily at
financial (as opposed to macroeconomic) stabilization.
The alternative tools fell into several major categories. First, unlike the
Fed, which by law was largely limited to purchasing only government bonds or
government-guaranteed MBS, other central banks also purchased a range of private
assets, including corporate debt, commercial paper, covered bonds, and (in the case
of the Bank of Japan) even equities and shares in real estate investment trusts. These
programs likely gave those central banks greater ability to affect private yields, especially
credit spreads, although plenty of evidence suggests spillovers from sovereign
debt purchases to private yields as well (D’Amico and Kaminska 2019). Purchasing
private assets has disadvantages as well: they involve taking credit risk, as well as
the  interest-rate risk associated with all QE programs, and they may generate political
controversies if they create the perception that the central bank is favoring some
firms or industries.
Second, several foreign central banks subsidized bank lending through cheap
long-term funding, usually on the condition that banks increase their lending to
approved categories of borrowers. Leading examples include the Bank of England’s
Funding for Lending Scheme and the ECB’s Targeted  Long-Term Refinancing


---Economics-2020-0-16.txt---
Operations. Unlike  crisis-era programs aimed at stemming the financial panic, these
lending programs were aimed at broader economic stabilization, by overcoming
lending bottlenecks in  bank-dominated economies and, more generally, by offering
bank-dependent borrowers the same access to credit as borrowers with access to
securities markets. Most of the available evidence on these programs suggests that
they lowered bank funding costs, promoted lending, and improved monetary policy
pass-through to the real economy (Andrade et al. 2019; Churm et al. 2018; Cahn,
Matheron, and Sahuc 2017). However, the efficacy of these programs seems likely
to depend in a complicated way on the health of the banking system: if banks are
well-capitalized, then their need for cheap liquidity from the central bank may be
limited. Conversely, if banks are short of capital, their lending may be constrained
or their incentives to make good loans distorted, notwithstanding the availability of
low-cost funding.
Third, the Bank of Japan, the ECB, and the central banks of several smaller
European countries adopted negative  short-term interest rates, enforced by a charge
on bank reserves. The ability of the public to substitute into cash, which pays a
zero nominal rate, or to  prepay nominal liabilities such as taxes, limits how far into
negative territory the short rate can fall. Negative rates also raise financial stability
concerns. One risk is that bank capital and lending capacity will be impaired by
negative rates, because in practice banks cannot easily pass negative rates on to
retail depositors. Indeed, a “reversal rate” of interest may exist, below which the
adverse effects of the negative rate on bank capital and lending make it economically
contractionary on net (Brunnermeier and Koby 2017). However, central banks
can address the reversal rate problem through various devices, such as paying a
higher rate to banks on a portion of their reserves (tiering), as has been done by the
BOJ and the ECB. In addition, when rates decline, banks benefit from the upward
revaluation of their assets and from improvements in overall economic conditions,
which reduce credit losses.
Within the limited range so far experienced, negative policy rates appear to have
been passed through to bank lending rates, money market rates, and  longer-term
interest rates (Arteta et al. 2018, Hartmann and Smets 2018). An International
Monetary Fund (2017) review summed up by saying, “Experience is limited, but so
far [negative interest rate policies] appear to have had positive, albeit likely small,
effects on domestic monetary conditions, with no major internal side effects on bank
profits, payment systems, or market functioning.” The evidently moderate costs and
benefits of negative rates seem disproportionate to the rhetorical heat they stimulate
in some quarters. Money illusion can be powerful.
Finally, the Bank of Japan in September 2016 initiated a program of “yield curve
control,” a framework which includes a peg for the  short-term interest rate (as in
traditional policymaking) but also a target range for the yield on  10-year Japanese
government bonds (JGBs), enforced by purchases of those bonds. Yield curve control
allows the BOJ to provide stimulus by lowering interest rates throughout the
term structure. Yield curve control may be thought of as a form of QE that targets
the price of bonds and leaves the quantity of bonds purchased by the central bank
to be determined endogenously, rather than the reverse as in standard QE programs.
Because the program is credible and because many private holders of JGBs appear
not to be very  price-sensitive, it seems that the BOJ can maintain low  long-term


---Economics-2020-0-17.txt---
rates with a slower rate of asset purchases than in the prior ( quantity-based QE)
regime, reducing concerns about whether there are enough JGBs available for the
BOJ to buy. Yield curve control, besides providing the ability to target financial condition
more precisely, thereby also appears—in the Japanese context, at least—to be
more sustainable.
Clearly, novel monetary policy options extend beyond QE and forward guidance.
Will the Federal Reserve ever adopt any of these supplementary tools? Other than
GSE-backed mortgages, the Fed does not have the authority to buy private assets,
except under limited emergency conditions, and—in light of the political risks and
philosophical objections by some FOMC participants—seems unlikely to request
the authority. A program targeting bank lending, such as the Bank of England’s
Funding-for-Lending Scheme or the ECB’s targeted refinancing operations, is
conceivable and was indeed discussed at the Fed during the  post-crisis period. At
the time, though, FOMC participants did not believe that bank liquidity was constraining
lending, and there were some reservations about the  quasi-fiscal and credit
allocation aspects of subsidizing bank loans. Freeing up bank lending is also macroeconomically
more consequential in jurisdictions like the euro area and Japan
where the bulk of credit flows through banks, as opposed to securities markets. Still,
one can imagine circumstances—for example, if constraints on bank lending are
interfering with the transmission of monetary policy—in which this option might
resurface in the United States.
Federal Reserve officials believe that they have the authority to impose negative
short-term rates (by charging a fee on bank reserves)  but have shown little appetite for
negative rates thus far because of the practical limits on how negative rates can go and
because of possible financial side effects. That said, categorically ruling out negative
rates is probably unwise, as future situations in which the extra policy space provided
by negative rates could be useful are certainly possible. Moreover, theory and empirical
evidence suggest that ruling out negative  short-term rates reduces the ability of the
central bank to influence  longer-term rates near the lower bound, through QE or other
means (Grisse, Krogstrup, and Schumacher 2017) . Maintaining at least some constructive
ambiguity about the possibility of negative policy rates thus seems desirable.
Yield curve control in the Japanese style—that is, pegging or capping very
long-term yields—is probably not feasible, or at least not advisable, in the United
States, given the depth and liquidity of US government securities markets. If
long-term yields were pegged, and market participants came to believe that the future
path of policy rates was likely higher than the targeted yield, the Fed might need to
buy a large share of the outstanding bonds to try to enforce the peg. Those purchases
in turn would flood the banking system with reserves and expose the central bank
to large capital losses. However, pegging Treasury yields at a shorter horizon, say
two years, would likely be feasible and might prove a powerful method for reinforcing
forward rate guidance. Board staff analyzed this possibility in 2010 (Bowman,
Erceg, and Leahy 2010) and it has been recently raised by Brainard (2019).
D. Costs and Risks of the New Policy Tools
The appropriate use of new policy tools depends not only on their benefits but
on their potential costs and risks. I briefly discuss here the potential costs and risks


---Economics-2020-0-18.txt---
of these policies, especially QE, that most concerned US policymakers in real time.
My principal sources are FOMC minutes and transcripts, and a survey of FOMC
participants about the costs and risks of asset purchases that they discussed at their
December 2013 meeting. In retrospect, most of the costs and risks that concerned
policymakers and outside observers have not proved significant.12 The possible
exception is the risk of financial instability, which I leave to last.
Impairment of Market Functioning.—Central banks using QE have tried to
ensure that securities markets continue to function well, for example, by putting
limits on the fraction of individual issues eligible for  asset-purchase programs. The
JGB market in Japan has at times had very low activity outside of central bank pur -
chases. In the other major economies however there has been only limited evidence
of poor market functioning, absence of  two-way trade, or loss of price discovery.
Asset purchases likely improved market functioning during the global financial crisis
and during the European sovereign debt crisis, by adding liquidity, promoting
confidence, and strengthening the balance sheets of financial institutions.
High Inflation.—FOMC participants were appropriately skeptical of the crude
monetarism sometimes espoused in the early days of QE, which held that the large
increases in the monetary base associated with asset purchases—which are financed
by crediting banks’ reserve accounts at the central bank—would lead to runaway
inflation. Fed policymakers and staff understood that, with  short-term interest rates
near zero, the demand for bank reserves would be highly elastic and the velocity of
base money could be expected to fall sharply.13 However, some FOMC participants
did express concern about the possibility that the combination of extraordinary monetary
measures and large fiscal deficits could  unanchor inflationary expectations, the
determinants of which are poorly understood. This was a minority view and, of
course, inflation and inflation expectations remained low—often frustratingly so—
in all major jurisdictions despite the use of QE and other new tools.
Managing Exit.—FOMC participants worried about whether the expansion of the
Fed’s balance sheet could ultimately be reversed without disrupting markets, and
about how (in a mechanical sense)  short-term interest rates could be raised when the
time came to do so if banks remained inundated with reserves. The taper tantrum of
2013 would show that the communication around ending or reversing growth in the
central bank balance sheet can indeed be delicate. To bolster confidence both inside
and outside the Fed, Board staff and FOMC participants worked to develop methods
for raising the policy rate at the appropriate time—including the payment of interest
on bank reserves, which would ultimately become the key tool. These efforts largely
succeeded, as the Fed’s balance sheet has neared its new  steady-state level and rates
were raised from zero with only occasional and relatively minor disruptions thus far.


---Economics-2020-0-19.txt---
Distributional Considerations.—FOMC participants did not often discuss distributional
considerations—mainly, the effects of low interest rates on savers and
the purported tendency of the new monetary tools to increase inequality—nor were
these concerns included in the internal 2013 FOMC survey about potential costs.
However, these issues were (and remain) prominent in the political debate in the
United States and several other countries. Most policymakers believe that monetary
policies that promote economic recovery have broadly felt benefits, including higher
employment, wages, profits, capital investment, and tax revenues; lower borrowing
costs; and reduced risk of unwanted disinflation or even a deflationary trap. Given
these benefits, it would be unwise to avoid accommodative monetary policies even
if they did have some adverse distributional implications. In any case, the research
literature is close to unanimous in its finding that the distributional effects of expansionary
monetary policies are small, once all channels are considered, and may even
work in a progressive direction, for example by promoting a “hot” labor market.14
Inequality is primarily structural and slowly evolving rather than cyclical, and as
such should be addressed by the fiscal authorities and other policymakers, not central
banks.
Capital Losses.—The large, unhedged holdings of  longer-term securities associated
with asset purchase programs risked substantial capital losses if interest
rates had risen unexpectedly, losses which in turn could have ultimately reduced
the Federal Reserve’s remittances of profits to the Treasury.15 The social costs of
any such losses would probably have been small: they would not have affected the
ability of the Fed to operate normally, and—even ignoring offsetting gains to investors—
government revenue gains from a stronger economy would have more than
compensated for reduced remittances. Nevertheless, FOMC participants worried
about the political fallout and threats to Fed independence that large losses could
have produced.16
Several factors mitigated but did not eliminate this risk. First, a significant por -
tion of the Fed’s liabilities—namely currency—pays no interest, providing some
cushion to the Fed’s ability to make payments to the Treasury. Additional cushion
is provided by securities owned by the Fed prior to the introduction of QE, which
enjoyed capital gains when yields fell. Second, since the Fed earns the  long-term
interest rate on its holdings of bonds but pays the  short-term interest rate on bank
reserves, it earns a net positive return when the yield curve has its normal upward


---Economics-2020-0-20.txt---
slope. Third, the taxpayer is hedged against Fed capital losses in the sense that a rise
in  longer-term yields is more likely to occur when the economy is strengthening,
which increases tax revenues. (However, some FOMC participants worried about a
sudden, spontaneous rise in long rates—the bursting of the “bond bubble.”) In fact,
the Fed’s asset purchase programs proved hugely profitable, with net profits resulting
in remittances to the Treasury totaling about $800 billion between 2009 and
2018, about triple  pre-crisis rates. Such an outcome, though likely, was not guar -
anteed, and the risk of capital losses is likely to remain a concern for policymakers
involved in large  asset-purchase programs.17
Financial Instability.—FOMC participants worried that  post-crisis monetary policies
raised risks of financial instability—a natural concern, given the recentness and
largely unpredicted nature of the global financial crisis. Many mechanisms linking
monetary policy to stability risks were suggested, including but not limited to the
creation of asset bubbles; incentivizing “reach for yield” and excessive  risk-taking
by investors; the promotion of excessive leverage or maturity transformation; and
the destabilization of the business models of insurance companies and pension
funds, which rely on receiving adequate  long-run returns, and of banks, whose profits
depend in part on their ability to earn positive net interest margins. US central
bankers also heard frequently from their foreign counterparts, especially in emerging
markets, about the “spillover effects” of Fed policies on financial conditions
abroad (Rey 2013).
We are far from a full understanding of the links between monetary policy and
financial stability. A good bit of evidence suggests that monetary easing works in
part by encouraging private actors to take risks—the  so-called  risk-taking channel
(Borio and Zhu 2012). Easy money increases  risk-taking through several mechanisms:
it improves the overall economic outlook and reduces downside risks; it
strengthens bank and borrower balance sheets, which increases the willingness of
lenders to extend credit; and it reduces the cost of liquidity, which complements
risk-taking by banks (Drechsler, Savov, and Schnabl 2018). Increased  risk-taking
is by no means always a bad thing, of course: encouraging banks, borrowers, and
investors to take reasonable risks, rather than hoarding cash and hunkering down, is
a desirable goal for policies aimed at ending a recession or crisis and restoring nor -
mal growth. However,  risk-taking may become excessive if investors and lenders are
less than perfectly rational about trading off risk and return, if institutional arrangements
distort incentives for  risk-taking, or if there are externalities associated with
increased leverage or illiquidity (Stein 2013). So these concerns can hardly be dismissed;
indeed, given the economic damage that a financial crisis can cause, we
must be humble about our understanding and remain vigilant for building risks.
There is a lively debate, which I will not try to resolve here, about the extent
to which monetary policymakers should take financial stability considerations into
account when setting interest rates (Svensson 2016; Gourio, Kashyap, and Sim
2017; Adrian and Liang 2018). Most participants in that debate agree that the first


---Economics-2020-0-21.txt---
line of defense against financial instability risks should be targeted regulatory and
macroprudential policies, and that monetary policy should be brought to bear only if
(i) those targeted policies are insufficient and (ii) the benefits of using monetary policy
to reduce crisis risks exceed the costs of undershooting  near-term inflation and
employment objectives. The main source of disagreement is the great uncertainty
associated with trying to measure those costs and benefits. In this context it is worth
noting that the United States seriously lags many other countries in the development
of macroprudential tools; for example, the United Kingdom, Canada, and several
Asian countries are well ahead in their ability to address dangerous credit booms,
particularly real estate credit booms.
A narrower but still important question is whether the new monetary tools pose
greater stability risks than traditional policies or, for that matter, than the generally
low rate environment expected to persist even when monetary policies are at a neutral
setting. There is not much evidence that they do. For example, QE is arguably
less risky than other expansionary policies, because it flattens the yield curve and
thus reduces the incentive for maturity transformation (Woodford 2016; Greenwood,
Hanson, and Stein 2016). As often noted, the portfolio balance effect of QE involves
pushing some investors out of  longer-term Treasuries into other, possibly riskier
assets; but in general equilibrium, by removing duration risk from the system, QE
reduces the riskiness of  private-sector portfolios in aggregate, increases the supply of
safe and liquid assets, and helps compensate for reduced private  risk-bearing capacity
during periods of high uncertainty (Caballero and Kamber 2019). The spillover
effects of the new monetary tools on foreign economies do not appear markedly different
from spillovers from traditional policies (Curcuru et al. 2018). And as already
mentioned, the new tools are most likely to be used when the economy is depressed
and increased  risk-taking is desirable. For example,  Chodorow-Reich (2014) found
that, in the  post-crisis period, monetary easing had beneficial effects on banks and
life insurance companies without inducing excessive  risk-taking.
II. Macroeconomic Effects of the New Monetary Tools
So far, I have argued that the new monetary tools, including QE and forward
guidance, can materially affect financial conditions, in much the same way traditional
monetary policies do when rates are away from the lower bound. The more
important question, of course, is whether using these new tools near the lower bound
can lead to significantly better economic outcomes, similar to what traditional policies
can deliver away from the lower bound. That question occupies the remainder
of this lecture.
A. The New Monetary Tools and the Great Recession
Our only historical experience involving extensive application of the new monetary
tools is the recovery from the Great Recession. As I’ve discussed, the Federal
Reserve and other major central banks, including the Bank of England, the ECB,
and the Bank of Japan, all made use of the new tools at various times. In each of
these jurisdictions, the recessions were nevertheless severe and the recoveries slow,
showing the limits of monetary policy. But, on the other hand, even away from the


---Economics-2020-0-22.txt---
lower bound, monetary policy has never proved able to reverse large shocks, only
to mitigate them and speed recovery. Moreover, the  post-crisis period featured some
unusual headwinds, including—in the United States—the  after-effects of the financial
crisis and the housing bust and a transition from fiscal expansion to austerity.
And the recovery, though not rapid, was unusually sustained, in 2019 becoming the
longest documented expansion in US history. Did the limits imposed by the lower
bound make the Great Recession significantly worse, or the recovery slower? How
much did the new tools help?
The literature does not provide a consensus on these questions. Some work suggests
that the new tools substantially overcame the limitations imposed by the lower bound.
For example, Fernald et al. ( 2017) , using growth accounting methods, attributed the
slow pace of the US recovery from the Great Recession primarily to subdued productivity
growth and  demographically-induced declines in labor force participation,
both trends in place before the financial crisis. They noted that indicators of resource
utilization like the unemployment rate—which are more subject to the influence of
monetary policy than is potential growth—behaved relatively normally during the
recovery.18 If the recovery from the Great Recession was relatively normal, given the
size of the shock and the economy’s underlying growth potential, then presumably the
lower bound could not have been a major constraint on policy.
An interesting alternative approach to measuring the effectiveness of  post-crisis
monetary policy is through the construction of “shadow”  short-term interest rates.
Following an insight of Black (1995), Wu and Xia (2016) used an affine term structure
model to make inferences from the full yield curve about what the  short-term
interest rate would have been had it not been constrained by the lower bound. During
normal times, the  so-called shadow rate they estimated generally equals the actual
policy rate, while during  lower-bound periods the shadow rate is usually negative.
Wu and Xia interpreted the shadow rate as a summary measure of the stance of
monetary policy, including nontraditional measures, finding that its relationships
with asset prices and macro variables look much like those of the actual federal
funds rate before the crisis. Based on this measure, these authors found that the
new tools provided only limited stimulus early in the recovery, but that, ultimately,
these tools provided about 3 percentage points of additional accommodation (that
is, the shadow rate fell, by 2015, to about −3 percent). They estimated that this
easing resulted in a slightly greater reduction in the unemployment rate than in a
counterfactual with traditional policies and no lower bound. Krippner (2015) provided
an alternative shadow rate series, which he argued is more robust to alternative
estimation assumptions and sample periods. His measure also implies that the new
policy tools delivered significant stimulus—the equivalent of setting the  short-run
policy rate more than 5 percentage points below zero by 2013—but, like Wu and
Xia (2016), Krippner found that monetary stimulus was more limited in the earliest
stages of the recovery.



---Economics-2020-0-23.txt---
A small literature has cited indirect evidence to argue that the lower bound did
not much constrain  post-crisis monetary policy in the United States (see Swanson
2018 for further discussion). For example, Swanson and Williams (2014) showed
that market interest rates reacted to economic news in about the same way after the
crisis as before, suggesting that the proximity of the lower bound had limited effect
on market expectations of the monetary policy response to changes in the outlook.
Debortoli, Galí, and Gambetti (2019) found that the cyclical behaviors of key macroeconomic
and financial variables during the Great Recession were not atypical, as
they should have been if monetary policy had been constrained by the lower bound.
However, in contrast to the research cited above, other work finds that the constraint
imposed by the lower bound led to materially worse outcomes after the crisis,
despite the new policy tools. In a 2015 paper, Engen, Laubach, and Reifschneider
used simulations of FRB/US, a model of the US economy used extensively by
Federal Reserve Board staff in forecasting and policy analysis, to perform a retroactive
assessment of the macroeconomic effects of the Fed’s policies. They found
that, taken together, QE and forward guidance helped to ease financial conditions
after the lower bound was reached, but that the economy nevertheless enjoyed a
meaningful boost beginning only in 2011. The limited benefit of the new tools in
2009–2010, according to their simulations, was the result of the ineffectiveness of
the Fed’s early forward guidance, as noted earlier; the gradual accumulation of the
effects of asset purchases, which peaked only after the introduction of QE3 in 2012;
and, importantly, the lagged effects inherent in all monetary policies. In 2011, these
authors found, use of the new tools began to speed up the recovery appreciably. That
led to an unemployment rate in early 2015 about 1.25 percentage points lower and,
slightly later, to inflation about 0.5 percentage points higher, relative to the counter -
factual with no use of new tools.
In a similar vein, Eberly, Stock, and Wright (2019) found that  post-crisis monetary
policies reduced the slope of the yield curve and assisted the recovery. However,
they estimated that an unconstrained monetary policy would have set the federal
funds rate as much as 5 percentage points below zero, and that the new policies made
up only about 1 percentage point. Gust et al. (2017), previously cited, also found
that the lower bound significantly constrained monetary policy during and after the
crisis. Based on simulations, they calculated that the lower bound accounted for
about 30 percent of the sharp contraction in 2009 and also contributed importantly
to the slowness of the recovery.
My reading of the  post-crisis experience is that, in both the United States and
elsewhere, the new policy tools helped ease financial conditions and led ultimately
to significantly better economic outcomes than would have otherwise occurred. In
particular, model simulations do not fully account for the beneficial effects of the
policy interventions on confidence,  risk-taking, and credit flows, each of which was
badly damaged by the crisis. However, it also seems unlikely that the new tools
deployed during the Great Recession entirely compensated for the limits imposed
by the lower bound. Particularly in the early  post-crisis period, monetary policymakers
were uncertain about the economic outlook, concerned about the costs and risks
of the new tools, and learning through experience how to implement and communicate
about them. Thus, policymakers were more cautious and less effective than
they would be later. Notably, in the United States, neither QE nor forward guidance


---Economics-2020-0-24.txt---
was explicitly tied to economic conditions until the introduction of  state-contingent
guidance for QE3 in late 2012. And, in retrospect, the economic thresholds set by
the FOMC for those policies were insufficiently aggressive. It also took time for
financial market participants and others to understand the new tools and the evolving
reaction functions of central banks. While these observations imply that the early
stage of the recovery from the Great Recession were less strong than it conceivably
could have been, they also suggest that, with improved understanding and the benefit
of experience, these policies could be significantly more effective the next time
they are needed.19
B. How Much Policy Space Can the New Monetary Tools Provide?
Looking forward, in a world in which  short-term interest rates could hit the lower
bound much more frequently, and traditional monetary policies accordingly could
be less effective, how much additional policy space can the new monetary tools provide?
As noted by Kiley and Roberts (2017) and others, and as I will demonstrate,
the answer depends importantly on the prevailing level of the neutral interest rate,
defined here as the interest rate associated with full employment and inflation at
target in the  long-run steady state—and, therefore, with monetary policies that, on
average, are neither expansionary nor contractionary. I focus here on the potential
only of QE and forward guidance, leaving aside other tools, like  funding-for-lending
and negative interest rates, that might provide some additional space if necessary.
A small literature has used DSGE models to study this issue. For example, Gertler
and Karadi (2013) modeled QE as a means by which the central bank can substitute
for the private sector in intermediating credit during periods of financial stress.
Incorporating this interpretation into an otherwise standard new Keynesian model,
they found in simulations that QE can have powerful effects on output but—not
unexpectedly, given their modeling approach—that these effects are largely confined
to periods of disruption in private credit flows. Sims and Wu (2019) expanded the
Gertler-Karadi framework to include forward guidance (and negative interest rates
as well). They found that QE is particularly effective in achieving monetary policy
objectives, largely offsetting the lower bound in both a simulation meant to mirror
the Great Recession and in simulations aimed at capturing the  long-run behavior
of the US economy. These studies, and others in the DSGE tradition, improve our
understanding of specific mechanisms by which QE and other new tools affect the
economy. The trade-off is that, in their focus on particular mechanisms, these models
may omit other key channels. For example, the  Gertler-Karadi framework does
not include a portfolio balance channel for QE, despite the evidence in its favor.
With the microfoundations of the new tools still under construction, many studies
of the potential role for such policies have used FRB/ US or other macroeconometric
models favored by central banks. These models, though motivated by economic
theory, typically contain ad hoc elements. Their advantages are that they are  flexible,
include substantial sectoral detail, and—having been workhorses of policy analysis
for many years—tend to produce quantitatively plausible results in diverse scenarios.


---Economics-2020-0-25.txt---
Importantly for this purpose, FRB/ US and similar models include rich descriptions of
the financial sector and the linkages between financial conditions and the real economy.
Given estimates of the financial effects of new policy tools, these models can
therefore be used to trace out the expected macroeconomic consequences of their use.
For example, Reifschneider ( 2016)  used FRB/ US simulations to evaluate the
ability of the Fed to respond to a hypothetical severe recession. He found that,
assuming a nominal neutral interest rate of 3 percent, a combination of forward
guidance and QE could provide enough stimulus to substantially offset the effects
of the lower bound. Chung et al. ( 2019) , also using FRB/ US simulations and
studying a similar scenario, were somewhat more pessimistic, finding that for -
ward guidance and QE would be “modestly effective” in promoting recovery and
returning inflation to target but would do little to limit the initial rise in unemployment
in a severe recession. The differences in their results appear to be due to
more conservative assumptions by Chung et al. ( 2019)  about the effects of asset
purchases on rates, as well as assumptions about initial conditions, the baseline
policy rule, and expectations formation that differ from some other papers in this
literature. Their finding that the new monetary tools would do little to limit the initial
rise in unemployment following a severe recessionary shock seems largely to
reflect estimated lags of monetary policy, traditional or nontraditional. The existence
of these lags provides an argument for  more proactive policies, and perhaps
for policy frameworks that lead financial market participants and others to under -
stand in advance that policymakers will respond aggressively when the short rate
hits the lower bound.
An alternative approach to using FRB/US to study a single historical episode or
hypothetical scenario is to use the model to simulate the  long-run behavior of the US
economy under differing policy rules or frameworks, with shocks drawn from the
historical distribution of model residuals. Because they allow researchers to study
the performance of alternative policies under a realistic mix of shocks, including the
possibility that adverse shocks may hit an economy already in recession,  so-called
stochastic simulations arguably provide more robust assessments of policy alternatives.
Studying the  long-run behavior of the model, as opposed to simulating the
“next recession,” is also more useful from the perspective of choosing among policy
frameworks or toolkits, which once decided upon are likely to remain in place for
an extended period.
Kiley and Roberts ( 2017)  and Bernanke, Kiley, and Roberts (2019)  used stochastic
simulations in FRB/ US to study the performance of alternative monetary
policy rules and frameworks near the lower bound. However, neither of these
papers considered central bank asset purchases. Kiley ( 2018)  remedied this omission,
using FRB/ US simulations to study the effects of the systematic use of QE
(without forward guidance) . He considered Fed balance sheet rules of varying
aggressiveness, in which the quantity of assets purchased after the policy rate
hits zero is tied to the estimated output gap. For a nominal neutral interest rate of
3 percent, he found that QE programs can overcome a significant portion of the
effects of the lower bound. Chung et al. ( 2019) , mentioned above, also conducted
stochastic simulations that included asset purchases, incorporating anticipatory
effects of future purchases. As noted, their work finds somewhat more modest
effects of QE on the economy.


---Economics-2020-0-26.txt---
Some New Simulation Results:  In the rest of this lecture, I present new simulation
results aimed at assessing how much policy space the new monetary tools could
provide prospectively. Like the papers described immediately above, I employ stochastic
simulations of FRB/US to compare expected economic performance under
alternative monetary policy frameworks, given historically realistic patterns of
shocks. So as to consider cases in which the lower bound is an important constraint,
I assume that the real and nominal neutral interest rates remain historically low.
Specifically, letting   r   ∗   be the real neutral rate, I consider the cases   r   ∗   = 1 and   r   ∗   = 0,
which bracket some standard estimates (Williams 2018). Unless otherwise stated, I
will also assume that the central bank’s inflation target is 2 percent and that  long-run
inflation expectations are anchored at that level. If   i   ∗   is the nominal neutral interest
rate, then the two cases considered correspond to   i   ∗   = 3 and   i   ∗   = 2. We know from
Kiley and Roberts (2017) that   i   ∗   = 3 is sufficiently low as to lead, under traditional
monetary policies, to frequent encounters with the lower bound. However, it is possible
of course that the nominal neutral rate is, or will become, lower even than my
assumed case of   i   ∗   = 2. I return to this possibility below.
The alternative policies to be evaluated are listed in the leftmost panels of Tables
3 and 4. Briefly, the policies considered are as follows.
Baseline Rules.—The traditional policy approach, taken here as the baseline, is
represented by variants of the standard Taylor (1993) rule. I assume the “balanced
approach” version of the rule, which gives greater relative weight to unemployment
than Taylor’s original formulation (Yellen 2017). I assume also that the rule
is inertial, that is, the policy rate depends in part on the lagged policy rate, with a
coefficient of 0.85 in quarterly data. I report results for the central case in which the
central bank’s inflation target equals 2 percent (  π   ∗   = 2) but also for alternative inflation
targets (  π   ∗   = 4 and   π   ∗   = 5). Higher inflation targets are assumed here to result
in  one-for-one increases in the nominal neutral rate, thereby affording more policy
space. Also reported are results for the case, labeled “unconstrained” in the tables, in
which   π   ∗   = 2 but the constraint imposed by the zero lower bound on the  short-term
rate is ignored. Comparing the unconstrained case to other policy rules measures the
costs imposed by the lower bound.
Threshold Forward Guidance.—As in Chung et al. (2019) and Bernanke, Kiley,
and Roberts (2019), I consider variants of forward guidance in which, once the
short-term rate is constrained by the lower bound, the central bank promises to
hold the policy rate at zero until inflation has reached a specified level.20 Once that
threshold is reached, policy reverts to the baseline Taylor rule. I report results for
three alternative inflation thresholds: 1.75, 2.0, and 2.25 percent. I also considered
thresholds based on the unemployment rate (not reported), with results that were
qualitatively similar to the policies using inflation thresholds.
In some standard models, the implied effects of forward guidance on the economy
are implausibly large, the  so-called forward guidance puzzle (Del Negro, Giannoni,
and Patterson 2012; McKay, Nakamura, and Steinsson 2016). The  puzzle appears


---Economics-2020-0-27.txt---
to be smaller in FRB/US than in other models (Chung 2015, Kiley and Roberts
2017). Nevertheless, to be conservative, I imposed the restriction that agents do not
expect forward guidance to be effective for more than 28 quarters. After that, they
expect policy to revert to the baseline Taylor rule. Assuming that guidance is credible
for as long as 28 quarters seems reasonable, because the guidance is at worst
mildly  time-inconsistent—the implied overshoots of the inflation target are small
and might well be welcome if the FOMC were concerned about reinforcing the
credibility of its symmetric inflation target after a period of inflation undershoot. In
any case, as will be seen, I do not find forward guidance to be particularly powerful
in my simulations, and my overall results do not rely on unrealistic foresight on the
part of the public.
Quantitative Easing.—Following Kiley (2018), I consider four alternative QE
policies, ranging from least aggressive (A) to most aggressive (D). In all cases, QE
is assumed to be deployed when the output gap has become sufficiently large. For
example, under QE(A), the least aggressive policy, QE is assumed to be initiated
when the output gap is 5 percent, with purchases continuing at a rate of $25 billion
per quarter for every percentage point that the output gap exceeds 5 percent. The
QE(D) policy, the most activist, is initiated when the output gap is 2.5 percent, at a
pace of $50 billion per quarter for each percentage point that the output gap exceeds
2.5 percent. For each of the QE programs, when the output gap shrinks sufficiently,
asset purchases are assumed to end, and the central bank’s balance sheet begins a
gradual  wind-down. (See Kiley 2018 or the online Appendix for precise specifications.
) The simulations keep track of the size of the central bank’s balance sheet, 


---Economics-2020-0-28.txt---
with new QE programs assumed to add to assets already on the balance sheet from
any prior programs. Thus, it is possible to observe the  long-run distribution of balance
sheet sizes associated with any particular strategy.
Following Engen, Laubach, and Reifschneider (2015); Reifschneider (2016); and
Kiley (2018), and consistent with the empirical evidence (e.g., Ihrig et al. 2018), I
assume that $500 billion in asset purchases by the Fed lowers the  10-year Treasury
yield by 20 basis points, and the  5-year and  30-year yields by 17 and 7 basis points,
respectively.21 Assuming smaller effects of QE on rates would increase the scale of
asset purchases required to achieve any particular economic outcome in my simulations
but otherwise not affect the results. I assume that the effects of QE on
longer-term rates are linear, exhibiting neither increasing nor decreasing returns.
Since the ability of QE to depress term premiums may be limited in practice, I also
ran simulations (not reported) in which term premiums are constrained from falling
more than 120 basis points below their  steady-state,  no-QE level, consistent with the
maximum effect found by Ihrig et al. (2018) for the Great Recession period. This
additional assumption led to no material change in the results.
Combination Policies.—Finally, I consider policies that combine forward
guidance, with an inflation threshold of 2 percent, with QE programs of varying
aggressiveness.
For each policy rule to be evaluated, I ran 500 simulations of FRB/US, drawing
model residuals from the  1970–2015 period. Each simulation is 200 periods (quar -
ters). Results are reported for the second 100 quarters of each simulation, with the
first 100 quarters used to set initial conditions. Except for the  28-quarter limit on
the credibility of forward guidance, I assume  model-consistent expectations, that is,
the agents in the model are assumed to understand the dynamics of the economy,
including the form of the policy rule. Since the comparisons here are between steady
states,  model-consistent expectations seems the right assumption. An alternative
assumption is that only participants in financial markets know the model and the
policy rule; see Bernanke, Kiley, and Roberts (2019) for a discussion. Simulations
using this alternative expectational assumption do not change the broad conclusions
of the study; under the alternative assumption, forward guidance becomes
somewhat less effective (not surprisingly), but QE becomes slightly more effective.
The solution method imposes a lower bound of zero on the current  short-term inter -
est rate and on expected future  short-term rates.  Longer-term interest rates are not
prevented from going negative, reflecting negative term premiums; see below for
further discussion.
Results are shown for the case   r   ∗   = 1 in Table 3, and   r   ∗   = 0 in Table 4. The cor -
responding nominal neutral interest rates,   i   ∗  , are in each case assumed to be the sum
of the assumed real neutral rate,   r   ∗  , and the inflation target,   π   ∗  . In particular, the
nominal neutral rate is 3 percent in Table 3 and 2 percent in Table 4, except for the
two policy rules in each table that assume an inflation target higher than 2 percent.



---Economics-2020-0-29.txt---
For each policy rule, the mean loss across simulations is shown in the first column
of the table. The loss for each simulation is defined as the sum of the squared
deviations of inflation from target and of the unemployment rate from the natural
unemployment rate (the “unemployment gap”), except that no penalty is assigned
when unemployment is below the natural rate. Higher losses imply worse average
performance. The results are not much affected if positive and negative unemployment
gaps are treated symmetrically, as is more conventional, or if the loss is calculated
in terms of the output gap instead of the unemployment gap. Mean losses are
a useful summary measure that provides a natural way to rank alternative policies,
evaluate the marginal benefits of new policy tools, or—by comparing losses of alter -
native policies to that of the unconstrained baseline Taylor rule—assess how close
those policies can come to fully offsetting the effects of the lower bound.
Also shown in Tables 3 and 4, for each policy rule, are the mean unemployment
gap across simulations, the mean inflation rate, the percentage of quarters in which
the simulated economy is at the effective lower bound (ELB frequency), the mean
duration of  lower-bound episodes (mean ELB duration), the mean stock of assets
held by the central bank as the result of QE programs (in billions of dollars), and the
mean peak stock of assets attained, all averaged over the 500 simulations of the rule.
There are several takeaways from Tables 3 and 4. First, as previous work has
demonstrated, traditional policy rules with a 2 percent inflation target (and with no
use of the new policy tools) perform quite poorly when neutral interest rates are
low. For example, when   r   ∗   = 1 and   i   ∗   = 3 (Table 3), the baseline rule leaves the
economy at the lower bound some 31 percent of the time, with mean inflation less
than 1 percent, well short of the 2 percent target, and a mean unemployment gap of
0.7 percent. When   r   ∗   = 0 and   i   ∗   = 2 (Table 4), the baseline policy rule leaves the 
---Economics-2020-0-30.txt---
economy at the lower bound 56 percent of the time. These results, which are similar
to those of Kiley and Roberts (2017), are if anything too favorable to the baseline
rules, since they assume that inflation expectations remain at target. If expectations
fall in response to persistently  below-target inflation, as no doubt they ultimately
would, then the nominal neutral interest rate would decline, reducing policy space
yet further.
Second, higher inflation targets, which for these simulations I assume to result
one-for-one in higher nominal neutral rates, improve the performance of the baseline
rules, by increasing policy space and making encounters with the lower bound
less frequent. For example, with   r   ∗   = 1 (Table 3), an inflation target of 5 percent
would result (by assumption) in a neutral nominal rate   i   ∗   = 6. In this case, according
to these simulations, the lower bound would bind less than 2 percent of the
time, and the mean loss associated with the baseline rule would be close to the fully
unconstrained case, illustrating the increased policy space. Importantly, though, the
losses calculated here ignore the costs of having a permanently higher inflation rate,
as well as the costs of the transition to a higher inflation target, both of which likely
would be significant.22 Moreover, it may be that raising the inflation target would
provide less additional space than assumed in these simulations, because at higher
inflation rates economic behavior could change in ways that reduce the potency of
monetary policy (L’Huillier and Schoenle 2019).
Third, taken separately, both forward guidance and QE lead to improved perfor -
mance, relative to the baseline rule with a 2 percent inflation target. However, the
economic benefits of forward guidance alone in these simulations are moderate, and
less than in much of the literature (e.g., Bernanke, Kiley, and Roberts 2019). The
reduced impact of forward guidance in these simulations appears to be the result
of my limitation of credible guidance to 28 quarters in the future, which implies
that this policy is relatively ineffective during long episodes at the lower bound. In
contrast, QE taken alone appears powerful in these simulations: with   r   ∗   = 1 and
i   ∗   = 3 (Table 3), the most aggressive QE program, QE(D), achieves economic outcomes
that are about the same as the baseline rule with a 5 percent inflation target
(for which   i   ∗   = 6), and that are also relatively close to the unconstrained baseline
rule, consistent with Kiley (2018). With   r   ∗   = 1, the mean peak stock of central bank
assets for this policy (over a typical  25-year period) is about $2.6 trillion, a high
value but not out of the range of recent experience. However, the QE(D) policy
results in the economy spending about 11 percent of the time at the lower bound,
compared to 2 percent of the time for the balanced Taylor rule with a 5 percent inflation
target. With   r   ∗   = 0 and   i   ∗   = 2 (Table 4), the performance of the QE(D) program
deteriorates somewhat but remains close to that of the baseline rule with a 5 percent
inflation target.
Fourth, importantly, for both   r   ∗   = 1 (Table 3) and   r   ∗   = 0 (Table 4), corresponding
to nominal neutral rates of 3 percent and 2 percent respectively, combinations of
QE and forward guidance can almost fully compensate for the effects of the lower



---Economics-2020-0-31.txt---
bound. For example, for   r   ∗   = 1, the combination of a 2 percent inflation threshold
for forward guidance and the QE(C) program produces a lower average loss than
the baseline policy with a 5 percent inflation target; and the same forward guidance
combined with QE(D) implies a lower loss than even the unconstrained baseline
policy. For   r   ∗   = 0, the combination of a 2 percent inflation threshold and the QE(D)
policy yields a lower loss than the baseline policy with a 5 percent inflation target,
and an only slightly greater loss than that of the (hypothetical) unconstrained baseline
policy. None of these combination policies implies unreasonably high aver -
age durations of  lower-bound episodes or peak asset  central-bank accumulations
outside historical experience. In contrast to the baseline rules, these policies also
produce inflation rates near target, consistent with inflation expectations remaining
well anchored.
The finding that, with an inflation target of 2 percent, a plausible combination of
new policy tools performs as well as traditional policies with an effective inflation
target of 5 percent, yields two implications. First, it suggests that, when the nominal
neutral rate is in the range of  2–3 percent, the new monetary tools are capable
of adding about 3 percentage points worth of policy space, relative to traditional
policies. That is, when the nominal neutral rate is in the range of  2–3 percent, the
use of the new tools allows policymakers to achieve economic outcomes similar to
those that could be attained by traditional policies if the nominal neutral rate were
3 percentage points higher, which in turn are close to the outcomes traditional policies
could achieve in the absence of any constraint on  short-term rates. Second, the
finding significantly weakens the argument for raising the inflation target to create
more policy space. Unless the direct costs of low nominal interest rates, in terms
of increased financial stability risks for example, exceed the costs of reaching and
maintaining 5 percent inflation, the use of new policy tools is preferable to a higher
inflation target.
Obviously, there are qualifications. Tables 3 and 4 include no standard errors, but
in reality, the uncertainty surrounding the reported estimates is high. Reasonable
people can disagree about the precise effects of asset purchases on financial conditions,
the credibility of forward guidance, or the effects of changes in financial
conditions on growth and inflation. And whatever its strengths, FRB/US is just one,
imperfect model of the US economy. On the other hand, the reported simulations
may in some ways understate the potential of new monetary tools. For example, I
have ruled out the use of negative  short-term rates,  funding-for-lending programs,
and other tools that might provide additional policy space. I have also excluded
MBS purchases, which could reduce the spread between mortgage rates and
Treasury yields. A policy framework that made inflation thresholds a condition for
raising rates from the lower bound, if it led market participants to expect that policy
response even in advance of rates hitting the bound, would be more powerful than
the  threshold-based guidance in the simulations. And FRB/US does not include all
of the new tools’ potential channels of effect, such as the possible impacts of asset
purchases on bank lending (Rodnyansky and Darmouni 2017).
The Role of the Neutral Interest Rate:  An especially important qualification to
my results is the uncertainty about the level of the neutral interest rate. As I have discussed,
if the nominal neutral rate is in the range of  2–3 percent or higher, the use of


---Economics-2020-0-32.txt---
the new monetary tools appears able (based on the simulations) largely to overcome
the effects of the lower bound on  short-term rates, adding about 3 percentage points
of policy space. If the assumed nominal neutral rate is much lower than 2 percent
or so, simulations of FRB/US (see online Appendix) continue to show substantial
benefits from using the new monetary tools, relative to baseline policies that make
no use of these tools. However, in this case the losses for all policies are greater than
those of the (hypothetical) unconstrained policy, implying that even active use of
the new tools cannot compensate for the constraint of the lower bound. Moreover,
for very low nominal neutral rates, simulations of all policies—both traditional and
those employing the new monetary tools—show the economy spending a large fraction
of the time at the lower bound, and with  longer-term interest rates frequently
in negative territory, reflecting negative term premiums. For any given policy, these
adverse outcomes are much less likely in the simulations when the neutral rate is in
the  2–3 percent range.23
A finding that  long-term rates are frequently negative in the simulations is a concern,
for several reasons. First, as noted earlier, if bond market participants believe
that the lower bound on  short-term rates is zero, then they may be unwilling to
hold  longer-term bonds yielding less than zero (Grisse, Krogstrup, and Schumacher
2017). Thus, at least for the United States and other jurisdictions that have indicated
that they are unlikely to use negative  short-term rates, simulations that assume
that  longer-term rates can be negative may overstate the amount of stimulus that is
attainable in practice. Second, interest rates that are zero or negative much of the
time raise concerns about financial instability and other risks, as discussed earlier.
Thus, although it remains true that the new monetary tools add policy space when
the nominal neutral rate is below 2 percent or so, in that range none of the policy
approaches considered here can be thought fully satisfactory.
What is the best estimate of the nominal neutral rate? Sufficient conditions for the
nominal neutral rate to be in the  2–3 percent range assumed here are that inflation
expectations be close to the 2 percent target and that the real neutral rate be at least
zero. Both conditions are consistent with most estimates for the United States. For
example, as of this writing, all 17 FOMC participants reporting in the Fed’s Summary
of Economic Projections have provided estimates of the  long-run nominal policy
rate between 2.0 and 3.3 percent (with a median of 2.5 percent), implying a real
neutral rate between 0 and 1.3 percent (median 0.5 percent). Surveys of professional
forecasters and primary dealers provide similar values (Joergensen and Meldrum
2019). The Federal Reserve Bank of New York, using the methods of Laubach and
Williams (2003), reports a current estimate of the real neutral rate above 0.9 percent,
implying a nominal neutral rate close to 3.0 percent. Alternatively, using estimates
from the term structure model of Adrian, Crump, and Moench (2013), the New
York Fed reports a  10-year average expected short rate, a proxy for the nominal
neutral rate, of 2.8 percent. Combining macroeconomic and term structure data,
Davis, Fuenzalida, and Taylor (2019) obtains estimates close to those of Laubach



---Economics-2020-0-33.txt---
and Williams (2003); see also Bauer and Rudebusch (2017). All of these estimates
are in the  2–3 percent range in nominal terms. 24
On the other hand, Kiley (2019) has recently argued that, when global factors
are taken into account, the best estimate of the real neutral rate in the United States
might be much lower than the current consensus, perhaps as low as −1 percent,
implying a nominal neutral rate in the range of 1 percent. Although Kiley’s estimate
is an outlier, it does underline the uncertainty around all estimates of the neutral rate.
One current source of uncertainty is the difficulty in determining whether the sharp
reductions in global interest rates in the wake of the financial crisis and the Great
Recession imply corresponding permanent—indeed, ongoing—declines in neutral
rates. Since the usual arguments hold that   r   ∗   is determined primarily by  slow-moving
forces like demography and technology, it is possible that the estimates by Kiley
and others are taking too much signal from recent developments, which may reflect
persistent but ultimately fading  after-effects of the crisis as well as of shortfalls in
aggregate demand in some major foreign economies. On the other hand, perhaps the
overheated financial conditions before the crisis kept rates artificially high, and the
recent downward trend in the neutral rate will continue.
If the nominal neutral rate in the United States does ultimately prove to be as
low as 1 percent, as Kiley finds, then moving that rate higher—to provide more
monetary policy space, and perhaps for other reasons including reducing financial
stability risks—will be an important goal of public policy. In that situation, the case
for a moderate rise in the inflation target would certainly be stronger, notwithstanding
the associated costs. That step seems premature as of this writing. However, a
cautious approach could include making plans to increase the countercyclicality of
fiscal policy, for example, by increasing the use of automatic stabilizers. And cer -
tainly, whatever the levels of the neutral rate, it is essential that the Fed keep inflation
expectations near target, defending against inflation shortfalls as vigorously as
it defended against  too-high inflation in the past.
My simulation results apply only to the United States and cannot be directly
extended to other countries. Two conclusions of this lecture can safely be extended,
however: that the new monetary tools are effective and should remain in the policy
toolbox; and that the ability of those tools to overcome the (possibly negative) lower
bound on rates is greater, the higher the nominal neutral rate in the jurisdiction in
question. In that respect, it is interesting that (other than Kiley 2019), many cur -
rent estimates place the real neutral rate in major foreign economies above zero.
For example, the New York Fed reports estimates (based on Holston, Laubach, and
Williams 2017) of the real neutral rates of Canada, the euro area, and the United
Kingdom of 1.6, 0.2, and 1.4 percent, respectively. Using the methods of Laubach
and Williams (2003) as well as a DSGE model, Okazaki and Sudo (2018) estimate
the real neutral rate in Japan to be close to 1.0 percent. Estimates by Davis,
Fuenzalida, and Taylor (2019) of the real neutral rate in six advanced economies
range between roughly zero and modestly positive. If the real neutral rate is zero or
above, and with most countries targeting inflation around 2 percent, then  shortfalls



---Economics-2020-0-34.txt---
of the nominal neutral rate from 2 percent or so primarily reflect shortfalls of inflation
expectations. In other words, the current constraints on monetary policy in several
regions, notably the euro area and Japan, arguably arise in substantial part from
the fact that, for various reasons, inflation expectations in those regions have fallen
too low. The challenge for those central banks, perhaps in collaboration with the
fiscal authorities, is to move inflation expectations closer to target. If that can be
achieved, then monetary policy, augmented by the new policy tools, could regain
much of its potency.
III. Conclusions
This lecture has reviewed the experience with, and the future potential of, the
new monetary tools, especially quantitative easing and forward guidance. Although
there are dissenting views, most research finds that QE has significant and persistent
effects on financial conditions, even when financial markets are not dysfunctional.
Forward guidance can help inform financial markets about policymakers’ likely
responses to economic developments and allow them to commit to future policy
actions, including  lower-for-longer rate policies, that create greater stimulus today.
Major central banks actively used both QE and forward guidance following the
financial crisis. Although these tools helped cushion the economic effects of the crisis,
their application was hindered at times, with the benefit of hindsight, by excessive
concern about the costs and risks of the new tools and by policymakers’ need
to learn how better to structure and to implement these policies, and how to communicate
about them. Better execution of the new policy tools, together with increased
public understanding and acceptance, should make these tools more effective in
the future. For example, policy frameworks that lay out in advance the nature of
the forward guidance policymakers expect to use at the lower bound will make that
guidance clearer, more credible, and more effective when it is needed. New tools
other than QE and forward guidance, such as  funding-for-lending programs, negative
policy rates, and yield curve control, could in some circumstances be useful
as well. In particular, the Federal Reserve should maintain constructive ambiguity
about negative policy rates and consider yield curve control at shorter horizons as
a means of reinforcing forward guidance. However, although the new tools can be
used with the knowledge that their costs and risks have generally proved moderate,
vigilance against risks to financial stability remains essential.
How much policy space can the new monetary tools provide? The answer depends
on the level of the nominal neutral interest rate, the interest rate consistent with full
employment and inflation at target in the long run. If that rate is in the range of 2 to
3 percent or higher, consistent with most estimates for the United States, then simulations
of the Fed’s FRB/ US model suggest that a combination of QE and forward
guidance can largely compensate for the effects of the lower bound, providing about
3 percentage points of additional policy space. In these circumstances, the use of new
policy tools seems clearly preferable to raising the inflation target, a measure that may
increase policy space by a comparable amount but that carries with it the additional
costs of the transition to the higher target and of the higher  long-run inflation rate.
However, if the nominal neutral interest rate is much lower than 2 percent, then
the model simulations imply that the new monetary tools—while still providing


---Economics-2020-0-35.txt---
valuable policy space—can no longer fully compensate for the effects of the lower
bound. Moreover, in that case, any monetary policy approach, with or without the
new tools, is likely to involve extended periods of short rates at the lower bound, as
well as  longer-term yields that are often very low or negative, which may pose risks
to financial stability or impose other costs. Should the neutral rate ultimately prove
to be that low, then additional measures to increase policy space, including a moderate
increase in the inflation target or significantly greater reliance on active fiscal
policy for economic stabilization, might become necessary. For now, though, major
changes seem premature. A reasonable interim approach could involve working to
increase the countercyclicality of fiscal policy, for example, through increased use
of automatic stabilizers. And, of course, in any case, the Fed must work to keep
inflation expectations from falling below target, which would bring down the neutral
interest rate and limit policy space.
I began this lecture by referring to the victory over inflation under Fed chairs
V olcker and Greenspan, an experience that promoted the view among central bankers
that lower inflation is always better. We have come almost full circle: in a world
in which low nominal neutral rates threaten the capacity of central banks to respond
to recessions,  too-low inflation can be dangerous. Consistent with their declared
“symmetric” inflation targets, the Federal Reserve and other central banks should
defend against inflation that is too low as least as vigorously as they resist inflation
that is too high.


---Economics-2022-0-00.txt---
In the textbook model of labor markets—synthesized by Hicks (1932)—product
and factor markets are perfectly competitive and wages are equated to marginal products.
1 Just one year after Hicks, Robinson (1933) developed an alternative framework
for understanding  firm-specific wage setting and coined the term “monopsony.” The
book attracted a lot of attention, and at least some labor economists were enthusiastic.
Reynolds (1946, p. 390) wrote that the concept of an  upward-sloping supply
curve of labor to the firm “… has made its way rapidly into the textbooks and seems
well on its way to being generally accepted as a substitute for the horizontal supply
curve of earlier days.” But Reynolds’s prediction was premature. By the 1960s the
concept of monopsony had been relegated to discussions of company towns. Indeed,
in the preface to the second edition to her book, Robinson (1969) observed, “All this
had no effect. Perfect competition, supply and demand … and marginal products
still reign supreme in orthodox teaching.”
At the risk of following too closely in Reynolds’s footsteps, in this paper I will
try to make the case that the time has come to recognize that many—or even most—
firms have some  wage-setting power. Such a shift was made with respect to firm’s
price-setting power many decades ago. Economists now routinely accept that the
prices of products like gasoline, breakfast cereal, and ketchup are set with some
degree of market power, even in  online markets. In the past few years we may have
reached a tipping point for a similar transition in labor economics, driven by the
combination of new (or at least  post-1930) theoretical perspectives, newly available
data sources, and accumulating evidence on several different fronts.


---Economics-2022-0-01.txt---
In the final chapter of her book, Robinson (1933) laid out a model of a firm with a
combination of  price-setting and wage-setting power, and showed that the result was
a “double wedge” between marginal productivity and wages, reflecting the markup
of prices over marginal costs and the markdown of wages relative to value marginal
products.2 Why didn’t this idea catch on?
I think there are several explanations. The first is that her framework describes
“perfect” monopoly and “perfect” monopsony. She offers very little guidance on
intermediate levels of market imperfection in either market, and says nothing about
the interactions between competing firms in such intermediate cases—a criticism
raised in the early review by Kaldor (1934) and freely acknowledged by Robinson
herself (Robinson 1953).3
A second and related reason is that the simple geometric apparatus developed by
Robinson (and also used by Chamberlain in his book published in the same year)
was not very useful for further analytical exercises. Stigler (1949) made this point
forcefully with respect to Chamberlain’s theory of monopolistic competition, arguing
“… it has not been useful in the concrete analysis of economic problems, in the
sense that it does not contain more accurate or more comprehensive implications
than neoclassical theory.” The importance of a tractable framework is underscored
by the current status of Chamberlain’s idea. Once Spence (1976) and Dixit and
Stiglitz (1977) wrote down constant-elasticity-of-substitution (CES) style models
models of consumer demand, and showed how to embed those preferences in a general
equilibrium setting, monopolistic competition took off, and is now a workhorse
model for problems in macroeconomics, international trade, and economic geography
(see Brakman and Heijdra 2004).
A third explanation is that in simple monopsony models, firms are ready and
willing to hire any qualified worker who is willing to accept their offered wage.
Indeed, a monopsonistic firm is always starved for labor. Proposing such a model in
the depths of the Great Depression was not ideal timing for Robinson. In contrast, in
today’s economy the idea of  labor-starved firms is more attractive.
Fourth, the question of how wages and prices are set got caught up in the grand
ideological debate over alternative economic systems that occupied many minds
during the twentieth century. Robinson rather dogmatically insisted that any diver -
gence between marginal products and wages represented a failure of market capitalism.
Chamberlain, for his part, spent many years defending the welfare properties
of monopolistic competition (e.g., Chamberlain, 1950). Throughout the 1930s and
into the Cold War era, economists were more interested in arguing about (often
ill-posed) normative questions than in understanding the positive implications of
alternative models of wage and price setting. In this context, Arthur Pigou’s labeling
of the gap between marginal productivity and wages as an index of “exploitation”



---Economics-2022-0-02.txt---
was clearly unfortunate. And Robinson’s public persona as a  hard-left polemicist
(Aslanbeigui and Oakes 2009) did not help.
For these and perhaps other reasons,4 by the 1970s the standard  graduate-level
textbooks in microeconomics theory (e.g., Malinvaud 1972) chose to give only a
brief discussion of market power in output markets, and to complete ignore monopsony.
Students of that generation had heard of imperfect competition and market
power in their undergraduate courses, but had almost no formal training in the analytics
of such models.
II. New Theoretical Frameworks
Early analysts (including Robinson and Reynolds) recognized two alternative
explanations for a  less-than-perfectly elastic supply of labor to a given firm: infor -
mation frictions and idiosyncratic preferences for different jobs. New models of
optimal search and of the demand for differentiated products that were developed in
the 1970s provided the foundations to formalize these explanations.
A. Search Models
Research in the late 1960s (including McCall 1970 and Mortensen 1970) led to
an elegant theory of optimal search by unemployed workers faced with an exogenous
distribution of potential wage offers. Almost immediately, Diamond (1971)
and Rothschild (1973) noted difficulties with endogenizing the wage offer distribution
in this setting. To sidestep this problem, much of the subsequent literature has
followed the lead of Diamond (1982); Mortensen (1982); and Pissarides (1985) and
switched to a model of search over job match quality (see Pissarides 2010). Since
wages have no allocative role in such models, they are not particularly helpful for
analyzing wage-setting power. The canonical status of these models may have also
led to an  overemphasis on the importance of match effects in wage determination
and labor market dynamics.
An alternative approach, developed by Burdett (1978) and Burdett and Mortensen
(1998) (hereafter BM) is to assume that employed workers also search for better
opportunities.  On-the-job search is empirically important; it also counters an
employer’s temptation to reduce wages for unemployed job seekers to the bare minimum.
BM consider a world where each firm posts a single wage, and can recruit
workers either from unemployment or  lower-paying firms. As Manning (1994,
2003) showed, such a “job ladder” model offers many insights into the links between
worker turnover and wages. It also provides a simple framework for thinking about
the degree of market power of any single employer. Postel Vinay and Robin (2002)
generalized BM by allowing firms to (perfectly) price discriminate against different
workers, depending on their preceding job and any job offers so far. This sequential
auction framework creates a more complex relationship between firm mobility and



---Economics-2022-0-03.txt---
wages (see Di Addario et al. forthcoming for a simple exposition focusing on starting
wages for each job).
B. Differentiated Demand Models
Chamberlain (1933) considered a model in which firms produce a differentiated
set of products and set prices ignoring strategic interactions with other producers.
This model translates directly to the supply side,5 though to the best of my knowledge
Bhaskar and To (1999) were the first to try to formalize the idea of monopsonistic
competition. Chamberlain’s simple graphical analysis was reproduced
in many undergraduate textbooks, but (as noted above) had a limited impact on
subsequent research until Spence (1976) and Dixit and Stiglitz (1977) wrote down
CES-style models of representative agent preferences that rationalized his framework.
Models based on these preferences (and generalizations with a nested CES
structure) have proven amenable to a multitude of applications in different fields.
Recently, Berger, Herkenhoff, and Mongey (2021) have adapted the approach to the
study of wage setting.
An alternative approach to modeling demand for differentiated products is the
multinomial logit (MNL) model proposed by McFadden (1974, 1978). The MNL
and its generalizations specify  individual-level preferences that lead to convenient
expressions for the share of consumers that purchase each product (Berry 1994),
and are widely used in industrial organization (IO) and labor economics. Card
et al. (2018) proposed the use of MNL style preferences to model the dispersion in
tastes for different workplaces. If employers ignore strategic interactions in wage
setting, their setup leads to very simple expressions for the supply of labor to individual
firms which can be used to rationalize the firm effects in a model like that of
Abowd, Kramarz, and Margolis (1999). Azar, Berry, and Marinescu (2019) adapt
this approach (with nested MNL preferences) to model the supply of applicants
to different job openings. Likewise, Lamadon, Mogstad, and Setlzer (2022) use a
nested MNL specification to model the supply of workers to individual firms.
While the “representative agent CES” approach and the “individual level MNL”
approach might appear to be very different ways of modeling consumer demand
(or labor supply), Anderson, de Palma, and Thisse (1978) and Verboven (1996)
showed that at the market level they are isomorphic (subject to functional form
choices about the terms in the CES function and the indirect utility function in the
MNL).6 This isomorphism is extremely convenient and in principal allows analysts
to proceed with either approach, and build on advances that have been made in the
two literatures.


---Economics-2022-0-04.txt---
III. Empirical Evidence in the First Three Decades of Modern Labor Economics:
1965–1995
“Modern” labor economics began in the  mid-1960s with the release of individual
microdata from the 1960 census (e.g., Cain 1966; Hanoch 1967; Bowen and
Finegan 1969), the Survey of Consumer Finances (e.g., Stafford 1968) and the
Survey of Economic Opportunity (e.g., Ashenfelter 1972). As noted by Stafford
(1986), these new datasets, along with  cross-sectional microdata from the Current
Population Surveys and longitudinal data from the Panel Study of Income Dynamics
and the National Longitudinal Surveys, propelled research in the field for the next
few decades and shaped our current understanding of the labor market.
Considerations of employer  wage setting played little role in this stream of
research. One reason for this was the influence of economists at the University of
Chicago, who were at the forefront of the new “analytical” labor economics (Rees
1976), and strongly advocated for neoclassical modeling. Even more importantly,
the newly available micro datasets had almost no information on employers. Thus,
it was extremely convenient to frame the analysis in the setting described by Hicks
(1932), where individual employers are irrelevant.
There were a couple of exceptions to this general rule. One was the analysis
of wage setting under collective bargaining. Here, most analysts followed Lewis
(1963) in modeling a unionized sector where wages were pushed above the competitive
level, and a nonunion sector where wages were determined under perfect
competition. There was little attention to the role of  firm-specific factors, apart from
a small literature based on wage contracts (e.g., Hamermesh 1970; Riddell 1979;
Christofides, Swidinsky, and Wilton 1980) that eventually turned to the question
of how employment and wages are jointly determined under collective bargaining
(e.g., Brown and Ashenfelter 1986; Card 1986, 1990).
A second exception was the literature on quits, turnover, and the returns to senior -
ity. Pencavel (1972) and Parsons (1972) presented  multi-period models of employer
wage setting with a trade-off between wages and quit rates—foreshadowing the
dynamic monopsony literature discussed below. While the  wage-setting equations
in these papers are clearly interpretable in a monopsony framework, neither author
acknowledged any connection with Robinson, or noted that in a perfectly competitive
labor market the quit rate should rise to 100 percent if the wage is set below the
“market” rate.
A problem faced by both papers was the confusion surrounding Becker’s (1962)
analysis of  firm-specific human capital, which addressed what we now call the
problem of “ relationship-specific investments.”7 Many labor economists interpreted
Becker as saying that firms choose wages to reduce quits (e.g., Parsons 1972 and
Hashimoto 1981) assuming that quits are a smooth function of wages. This is equivalent
to monopsonistic wage setting.8


---Economics-2022-0-05.txt---
In addressing the closely related problem of optimal turnover in a model with
a fixed (but unknown) match component, Jovanovich (1979) showed that an equilibrium
contract pays the worker the expected value of her  match-specific productivity,
and allows her to quit when the option value of the current job falls below
the option value of a fresh job.9 Jovanovich’s model has features of the canonical
Diamond-Mortensen-Pissarides search model, but incorporates  job-to-job mobility,
leading to something like a “ worker-specific job ladder” as jobs that are revealed
to be worse matches (and therefore have lower pay) are terminated.10 Topel and
Ward (1992) interpreted the patterns of wages and turnover for young male workers
as evidence of this process, but they did not have rich enough data to tell whether
wages include a  match-specific component (as in Jovanovich 1979) or whether
later-career jobs pay higher wages to all workers (as in the  BM model). In my view,
the simple event studies developed by Card, Heining, and Kline (2013) and the
surprisingly small job match component uncovered in that paper (and many later
studies) suggest that workers tend to move up the same job ladder (as in Abowd,
Kramarz, and Margolis 1999).
Finally, there were a few studies of specific institutional settings where firm wage
setting power seemed possible. Sullivan (1989), for example, showed that increases
in the number of nurses were correlated with  hospital-specific wage increases,
suggesting that employers were facing  upward-sloping supply curves for nursing
labor. Ransom (1993) used university payroll data to show that wages of professors
decline with tenure—a pattern he attributed to monopsonistic wage discrimination.
IV . What Happened in the 1990s?
Four new types of evidence have accumulated in the past 25 years that suggest
employer  wage-setting power is  nonnegligible: evidence on quit and recruiting
responses to wages, evidence on the relationship between wages and firm productivity,
evidence on the concentration of employment in small numbers of employers,
and evidence of conspiracies and other forms of firm behavior targeted at suppressing
firm-to-firm mobility and wage growth.
A. Quit, Recruiting, and Application Elasticities
Though many economists acknowledge that quit and recruitment rates vary with
wages, the connection between these responses and the elasticity of supply that is
relevant for a monopsonistic wage setter does not seem to have been fully appreciated
until the seminal paper by BM (which circulated for many years prior to its
publication). Card and Krueger (1995) noted that in any steady state, the elasticity
of labor supply is just the sum of the absolute values of the elasticities of recruiting
and quitting. Manning (2003) showed that in a simple job ladder model the
two are equal: thus, an analyst can estimate one or the other and double it to yield



---Economics-2022-0-06.txt---
an  estimate of the overall supply elasticity. Manning’s insight provides a tractable
method of estimating labor supply elasticities that has been implemented in many
different settings.
Perhaps the most compelling evidence based on this approach comes from the
experiment on public sector hiring conducted by Dal Bo, Finan, and Rossi (2013).
These authors randomly varied the salaries announced at different job sites to potential
job applicants for a position in the office of the Regional Development Program
in Mexico. Taking account of the combined impact of higher wages on application
rates and on the probability of accepting a job, they calculate that the elasticity of
recruiting with respect to wages is around 2.1 (though rather imprecisely estimated).
Using Manning’s shortcut, the implied (steady state) elasticity of labor supply is
around 4.2.11 In a simple monopsonistic model such an elasticity implies that wages
are marked down relative to marginal revenue products by about 20 percent.
Observational studies of the partial correlation between wages and quit or recruiting
rates tend to yield elasticities in the same range (see Sokolova and Sorensen,
2021 for a  meta-analysis of this literature and Ashenfelter et al. forthcoming for
an overview of a studies published in a recent issue of the Journal of Human
Resources). For example, Bassier, Dube, and Naidu (2021) study job ending rates
of workers using administrative data from the State of Oregon, and estimate elasticities
in the range of −1 to −2.5, with a preferred point estimate of −2.1, implying
a steady state labor supply elasticity of 4.2. Azar, Berry, and Marinescu (2019) use
data from a large online job posting service to study the application choices of job
searchers. Adopting the estimation approach of Berry, Levinsohn, and Pakes (2004)
(and using instrumental variables for posted wages) they infer that the  firm-specific
elasticity of applications with respect to wages is around 2.9. Assuming the recruiting
elasticity is the same as the application elasticity this implies a steady state labor
supply elasticity of just under six.
B. The Relationship between Wages and Firm Productivity
In a competitive labor market, more and less productive firms pay the same
wages for workers, even if the more productive firms are larger. In imperfectly competitive
labor markets, however, more productive firms will generally have to pay
more to maintain a larger workforce. Card et al. (2018) developed a simple partial
equilibrium model where workers have MNL preferences over different firms and
firms set wages without accounting for strategic interaction effects (i.e., a model of
monopsonistic competition). They then calibrated the model to (roughly) match the
observed degree of  pass-through from  value added per worker to wages. In the existing
literature researchers typically find that wages are about 0.5 to 1.5 percent higher
at firms with 10 percent higher productivity. In the parameterization of preferences
adopted by Card et al., this degree of  pass-through is consistent with  firm-specific
supply elasticities of about four.
Lamadon, Mogstad, and Setlzer (2022) present a more extensive analysis
of the  pass-through of  firm-specific and  market-wide value added per worker to


---Economics-2022-0-07.txt---
firm-specific wages and interpret the effect in a model of monopsonistic competition
with nested MNL preferences over firms and markets. Their estimate of the
parameter determining the elasticity of supply to each firm is five, broadly consistent
with the calibration by Card et al. and with the evidence from quit and recruiting
elasticities.
A related method of estimating the degree of  wage-setting power is to look at
establishment-level responses of employment and wages to an exogenous shock
(similar to the pioneering study by Sullivan 1989). Berger, Herkenhoff, and Mongey
(2021) uses evidence on  firm-specific reactions to state tax changes to infer the
degree of oligopsony power in a setting with strategic interactions among wage setters
(based on Atkeson and Burstein 2008). They estimate that the average markdown
of wages relative to marginal revenue products is around 25 percent (equivalent to
the markdown in a simple monopsonistically competitive model with  firm-specific
elasticities of around 3.5). Kroft et al. (2020) extend the setup in Lamadon, Mogstad,
and Setlzer (2022) using information on successful bids in government procurement
auctions as  firm-specific demand shocks that affect employment and wages at larger
construction firms. They estimate labor supply elasticities in the range of four to
five.
C. The Number of Competitors for Labor Services
In thinking about  price-setting or  wage-setting power many economists turn
instinctively to the question of how many potential sellers or buyers are present
in a market, or to the degree of market concentration measured by the
Herfindahl-Hirschman index (HHI). As noted by Berry, Gaynor, and Scott Morton
(2019); Syverson (2019); and Eeckhout (2021), simple measures of the number of
competitors or their concentration do not necessarily provide a clear index of market
power. Nevertheless, a common perception (among judges for example) is that the
number of potential employers for any given worker is large, and that the market
power of employers is therefore negligible.12
One of the most surprising findings in the recent literature is that for many workers
in many local markets the number of potential employers is relatively small,
particularly when the “market” is defined by actively searching firms.13 Azar et al.
(2020), for example, use data on the  near universe of US vacancy listings to calculate
HHIs for labor markets at the  narrowly defined  occupation-by-commuting
zone (CZ) level. They estimate that an average labor market has an HHI of around
4300—equivalent to 2.3 equal sized recruiting firms. This is low enough to possibly
raise concerns about the effect of mergers and acquisitions on labor outcomes (see
Naidu and Posner 2021).
A growing number of papers study the relationship between average wages
for a specific subgroup of workers in a given local market and the HHI of potential
employers in that market. These studies differ in how they define the set of

---Economics-2022-0-08.txt---
potential employers (based on industry or occupation), how they count employment
(based on the stock of employment, the number of job openings, or some
transition-probability-adjusted stock of employment), and whether they use a purely
observational approach, or implement a research design that isolates some exogenous
component of the local HHI. Despite these differences, most recent studies
seem to show a negative effect of higher concentration on wages, with elasticities
between the HHI and wages on the order of −0.05 to −0.15.
For example, Azar, Marinescu, and Steinbaum (2022) use data from a large
national employment website to study the relationship between posted wages for
jobs in a given occupation and CZ and the HHI of employers listing vacancies in
that occupation and location. They find smaller elasticities of posted wages with
respect to the HHI in simple ordinary least squares (OLS) models, but larger elasticities
when they instrument the HHI with the  leave-out mean number of competitors
searching for workers in that same occupation in other markets. Rinz (2020)
estimates HHIs from counts of  establishment-level employment by CZ and industry,
then relates these to administrative earnings from tax data. In OLS models he finds
that wages are slightly higher in more concentrated markets, but in models that use
the  leave-out mean of the HHI for the same industry in other locations as an instrumental
variable, he obtains negative elasticities in the range of Azar, Marinescu, and
Steinbaum (2022).
Recent studies by Arnold (2020) and Prager and Schmidt (2021) use event study
designs to look at the effects of merger and acquisition activity on local HHIs
and wages. In my opinion, these designs provide the best available evidence that
employer consolidations that raise the HHI have significant negative effects on
wages, at least for workers who are highly attached to the affected industry.14
D. Conspiracies and Other Arrangements to Suppress Competition
Adam Smith (2003, p.94-95) wrote that employers “are always and everywhere
in a sort of tacit, but constant and uniform combination, not to raise the wages of
labor above their actual rate.”15 He also noted, however, that “(w)e seldom, indeed,
hear of this combination, because it is the usual, and one may say, the natural state
of things, which nobody ever hears of.” While discoveries of employer collusion are
still relatively rare, in the past two decades there have been a number of lawsuits
and public disclosures that provide the details of some agreements to suppress competition.
These provide a useful perspective on the mechanisms generating market
power for employers.
The  best-known lawsuit concerned “no poaching” and “no solicitation” agreements
affecting software and animation engineers in Silicon Valley (see Ashenfelter
et al. forthcoming for more details).16 The agreement originated in the  mid-1980s
when Lucasfilm sold its computer animation division to Steve Jobs, who then


---Economics-2022-0-09.txt---
renamed the company “Pixar.” To avoid bidding wars over employees, Lucasfilm
and Pixar agreed (i) not to “cold call” each other’s employees; (ii) to notify the
other company should they receive an application for employment; (iii) and that
all offers to employees at the other company would be “final,” with no further bidding.
Ultimately this agreement was extended to other  high-tech firms (e.g., Google,
Microsoft, and Oracle) and lasted over 20 years, until 2008.
The size of the settlement to affected engineers ($585 million in two suits), and
other wage adjustments made after the agreement was made public (e.g., a 10 per -
cent  across-the-board increase offered by Google to all its employees in November
2010) suggest that the suppression of  between-firm competition was successful—a
validation of the idea that at least some labor markets are vulnerable to wage fixing.
Another interesting lawsuit concerned a “no hire” agreement between the medical
schools at Duke University and University of North Carolina (Seaman v. Duke).
This case, which resulted in a settlement of around $10,000 for each member of the
medical faculties at the two schools, reveals how localized competition appears to
matter, even for workers who arguably face a national market.
While one might be tempted to think that “no hire” and “no poaching” agreements
affect only highly skilled workers, Ashenfelter and Krueger (2021) found that
no poaching clauses were widespread in US franchise agreements.17 These agreements
typically prohibit a franchisee from hiring another franchisee’s employees
for some  prespecified period of time after an employee’s departure. For example,
a standard franchise agreement for McDonald’s as of 2016 had a clause stating:
“Franchisee shall not employ or seek to employ any person who is at the time
employed by McDonald’s, any of its subsidiaries … or otherwise induce, directly
or indirectly, such person to leave such employment” (quoted in Ashenfelter and
Krueger 2021). The prohibition extended to employees for six months after leaving
another McDonald’s job.
Another strand of recent research has focused on the prevalence of  noncompete
agreements, which prohibit employees from moving to jobs at “competitor” firms
for a specified period (e.g., Starr 2019; Balasubramanian et al. 2020). Again, a sur -
prising fact is the prevalence of these agreements even for relatively  low-wage workers.
Recently, however, a number of states have enacted legislation that prohibits
noncompete agreements for “ low-wage” workers (e.g., earning less than $100,000
per year in Washington State—see Goldstein and Oberlander 2021).
The popularity of  no-poaching and  noncompete agreements seems to confirm the
basic insights of a  BM-style job ladder model. Since the quit rate in such models
depends in part on the rate at which workers obtain offers at other employers, limits
on poaching or  firm-to-firm mobility will reduce quits and increase monopsonistic
power.
V . An Agenda for the Future
It is presumptuous for anyone to try to influence the direction of research in a
large and fractious field like labor economics. Nevertheless I have two suggestions



---Economics-2022-0-10.txt---
for where I see the most exciting possibilities for progress: more and better models;
and a sustained effort to move the entire topic of wage setting into the hands of
(labor) economists.
A. Models
There are two main approaches to modeling the factors that generate
upward-sloping supply curves: search frictions (which Manning 2021 calls the
“new monopsony”) and idiosyncratic preferences for jobs (which Manning calls the
“new classical monopsony”). Both approaches have some strengths and some weaknesses.
The search approach directly addresses turnover, which is a key feature of
labor markets and appears to be the main mechanism for  between-firm competition.
Models with on-the-job search also create a job ladder, which is a very useful construct
for understanding the costs of job displacement and the effects of recessions
(e.g., Altonji, Smith, and Vidangos 2013; Moscarini and  Postel-Vinay 2018).
But the lack of information presumed in a typical posted wage search model is
troubling. There is plenty of evidence that most workers know about at least some
higher-paying jobs. (Everyone at Berkeley knows that salaries are higher at Stanford,
for instance). Firms’ positions on the wage ladder are relatively stable, so it seems
possible to learn about opportunities through referrals (Caldwell and Harmon 2019)
or other channels. And if the number of potential employers for a typical worker is
a low as recent research suggests, it’s hard to imagine that workers aren’t aware of
many of the relevant opportunities.
Models based on idiosyncratic preferences, on the other hand, ignore imperfect
information but assume that most people simply don’t want another job, even if it
pays more. On the positive side, these models build directly on established frameworks
from IO and trade: the accumulated experience in those fields will be very
helpful, particularly in addressing strategic interactions between wage setters (as
in Berger, Herkenhoff, and Mongey 2021). On the negative side, there is no job
ladder or any particular cost of losing the current job: everyone is employed at their
best option, given the wage and  nonwage amenities offered by different employers.
Employers are starving for workers, but are nonetheless setting wages below
marginal revenue products to capture some of the surplus from inframarginal workers.
Such a framework seems unlikely to yield helpful insights about recessions or
depressed local labor markets.
Manning (2021) suggests that one way to combine some of the strengths from
both approaches is to assume that workers have idiosyncratic preferences over current
job openings, and that—as in directed search models—one of the attributes of
an opening is the size of the application pool. This seems like a promising direction.
Another idea is to assume more complicated  task-based production functions
for firms that lead to minimum skill standards—so many jobs are “off limits” for
most workers, even within a given observed skill group (e.g., Haanwinckel 2020;
Huckfeldt 2022). This might be a way to incorporate the cyclical upgrading process
discussed by Reder (1955) and Okun (1973).
A related modeling issue is how to incorporate strategic interactions in wage setting.
We know that firms spend a lot of resources monitoring wages of other employers
through specialized  sector-specific surveys. We also know that even at the low


---Economics-2022-0-11.txt---
end of the labor market, firms respond to  wage-setting choices by their competitors
(Derenoncourt et al. 2021). It therefore seems necessary to move beyond the “no
strategic interactions” case considered in several recent studies. Berger, Herkenhoff,
and Mongey (2021) have made some initial progress in this direction.
B. Who Should Study Wage Setting?
Once we accept that firms set wages, the analysis of wage setting becomes a part
of labor economics, just like the analysis of price setting is a part of IO. Right now,
much of the practical discussion of wage setting is done by  noneconomists. Human
resources departments at large corporations are often staffed by people with primary
training in social psychology or sociology. Most business schools have almost no
courses on wage setting, and few if any that feature standard economic ideas.
By insisting that “markets set wages,” labor economists ceded the field, and had
very little to say about questions like the design of online labor markets, or the
effects of  no-solicitation or  no-poaching agreements—other than that they should
not matter. We also distanced ourselves from other economists—particularly those
in IO—who were busy developing useful models of market power and strategic
decision making.
One of the most exciting developments in the field today is the evidence of labor
economists taking questions about wage setting seriously. This effort began with
Manning’s (2003) landmark book: I hope that the growing body of work since
then finds its way into the classroom and into the textbooks soon. I also expect this
work to lead to some  rethinking on policies such as minimum wages, the regulation
of trade unions, and  anti-trust (see Langella and Manning 2021, and Naidu and
Posner 2021). Perhaps we may even see a  reevaluation of the widespread belief that
excessive wages are the root cause of many economic problems. After all, if your
employer set your wage, it’s hard to believe that it’s too high.