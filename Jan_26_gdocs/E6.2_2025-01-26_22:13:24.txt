
# E6.2
 ## Economics-1991-0


### ---Economics-1991-0-03.txt---
I

As the Second World War was drawing
near its resolution, economic theory entered
a phase of intensive mathematization that
profoundly transformed our profession. In
several of its main features that phase had
no precedent, and it will have no successor.
Assessing it requires a multidimensional
analysis acknowledging the contributions to
economics that were made, as well as the
tensions among economists that were
heightened.

The development of mathematical economics
during the past half-century can be
read in the total number of pages published
each year by the leading periodicals in the
field, an index that I will follow at first.
From 1933, the date when they both started
publication, to 1959, those periodicals were
Econometrica and the Review of Economic
Studies, and the index tells of the decline
from a high point, above 700 pages in 1935
to the lowest point, below 400 pages in
1943-1944. But 1944 marked the beginning
of a period of explosive growth in which
Econometrica and the Review of Economic
Studies were joined in 1960 by the International
Economic Review, in 1969 by the

Journal of Economic Theory, and in 1974 by
the Journal of Mathematical Economics. In
1977, these five periodicals together published
over 5,000 pages. During the period
1944-1977, the index more than doubled
every nine years. By that measure, 1944 was
a sharp turning point in the history of mathematical
economics. It was also the year in

which John von Neumann and Oskar

Morgenstern published the Theory of Games
and Economic Behavior.

While the professional journals in the field
of mathematical economics grew at an unsustainably
rapid rate, the American Economic

Review underwent a radical change

in identity. In 1940, less than 3 percent of
the refereed pages of its 30th volume ventured
to include rudimentary mathematical
expressions. Fifty years later, nearly 40 percent
of the refereed pages of the 80th volume
display mathematics of a more elaborate
type.

At the same time, the mathematization of
economists proceeded at an even faster pace
in the 13 American departments of economics
labeled by a recent assessment of

research-doctorate programs in the United
States (Lyle V. Jones et al., 1982) as "distinguished"
or "strong" according to the

scholarly quality of their faculties. Every
year the Fellows of the Econometric Society
(ES) certify new members by election into
their international guild, which increased in
size from 46 in 1940 to 422 in 1990. For
those 13 departments together, the proportion
of ES Fellows among professors was

less than 1 percent in 1940; it is now close
to 50 percent. It equals or exceeds 50 percent
for six of them, which were among

those assessed as the eight strongest. So
mathematized a faculty expects its students
to have what it considers to be minimal
mathematical proficiency, and knowledge of
calculus and linear algebra is required, or
forcefully recommended, for admission to
all 13 graduate programs.

Several scholarly recognitions lay additional
emphasis on the role that mathematical
culture is now playing in our profession.
Of the 152 members of the economics section
of the American Academy of Arts and
Sciences, 87 are Fellows of the Econometric
Society; and of the 40 members of the economics
section of the National Academy of


### ---Economics-1991-0-04.txt---
Sciences of the United States, 34 are ES
Fellows. From 1969 to 1990, 30 economics
Nobel awards were made, and 25 of the
laureates are, or were, ES Fellows. Since it
was first presented to Paul Samuelson in
1947, the John Bates Clark medal of the
American Economic Association has been
given to 21 economists, of whom 20 are ES
Fellows; and of the 26 living past presidents
of our Association, 13 are ES Fellows.
One may wish that those counts had not
been made. One may argue about points of
their interpretation. But they belong in our
common knowledge, and their thrust is unequivocal.
They indicate how extensive the

mathematization of economics and how
deep the accompanying change of our field
were over the past five decades.

The perception of the depth of that
change is reinforced by a comparison of the
levels of mathematics required in 1940 and
in 1990 to follow the development of economic
theory in every direction it was taking.
Fifty years ago, basic undergraduate
preparation in mathematics was almost always
sufficient. Today, graduate training in
mathematics is necessary. If, instead of being
a follower, one wishes to be an active
participant in that development along its
most technical avenues, a high degree of
mathematical professionalism is called for.
Several faculty members of the 13 departments
of economics mentioned previously

were actually identified as mathematicians
by their doctorates; four of them served as
chairmen of those departments during the
past 25 years. If still sharper focus brings
out the intellectual leaders of that development,
prominent among them is John von

Neumann, one of the foremost mathematicians
of his generation.

In that development process, mathematical
economics was continuously redefined as
new territories were included within its outward-
moving frontier and as topics that were
once at that frontier became standard parts
of the graduate, if not of the undergraduate,
economic-theory curriculum.

II

Before the contemporary period of the
past five decades, theoretical physics had
been an inaccessible ideal toward which
economic theory sometimes strove. During
that period, this striving became a powerful
stimulus in the mathematization of economic
theory.

The great theories of physics cover an
immense range of phenomena with a

supreme economy of expression. Of this,
James Clerk Maxwell (1865) had given a
notable example, as he described the electromagnetic
field by means of eight equations

at the time when mathematical economics
was born and came of age in the

middle of the 19th century. This extreme
conciseness is made possible by the privileged
relationship that developed over several
centuries between physics and mathematics.
In turn, the former presented the

latter with open problems, or found to questions
raised by physical theory ready-made
answers discovered by mathematicians in
their abstract universe. Sometimes the
causal linkage of research done in each one
of the two fields could not easily be unraveled;
and, on occasion, the same scientist
made inextricably intertwined contributions
to both disciplines.

The benefits of that special relationship
were large for both fields; but physics did
not completely surrender to the embrace of
mathematics and to its inherent compulsion
toward logical rigor. The experimental results
and the factual observations that are at
the basis of physics, and which provide a
constant check on its theoretical constructions,
occasionally led its bold reasonings to
violate knowingly the canons of mathematical
deduction.

In these directions, economic theory could
not follow the role model offered by physical
theory. Next to the most sumptuous

scientific tool of physics, the Superconducting
Super Collider whose construction cost
is estimated to be on the order of $1010
(David P. Hamilton, 1990; see also Science,
5 October 1990), the experiments of economics
look excessively frugal. Being denied
a sufficiently secure experimental base, economic
theory has to adhere to the rules of
logical discourse and must renounce the
facility of internal inconsistency. A deductive
structure that tolerates a contradiction
does so under the penalty of being useless,


### ---Economics-1991-0-05.txt---
since any statement can be derived flawlessly
and immediately from that contradiction.
In its mathematical form, economic theory
is open to an efficient scrutiny for logical
errors. The rigor that has been reached
as a consequence is in sharp contrast to the
standards of reasoning that were accepted
in the late 1930's. Few of the articles published
then by Econometrica or by the Review
of Economic Studies would pass the

acid test of removing all their economic
interpretations and letting their mathematical
infrastructure stand on its own. The
greater logical solidity of more recent analyses
has contributed to the rapid contemporary
construction of economic theory. It has
enabled researchers to build on the work of
their predecessors and to accelerate the cumulative
process in which they are participating.


But a Grand Unified Theory will remain
out of the reach of economics, which will
keep appealing to a large collection of individual
theories. Each one of them deals

with a certain range of phenomena that it
attempts to understand and to explain.
When it acquires an axiomatic form, its
explicit assumptions delimit its domain of
applicability and make illegitimate overstepping
of its boundary flagrant. Some of those
theories take a comprehensive view of an
economic system and bring insights into the
solutions of several global problems. For
instance, prices contribute to achieving an
efficient use of resources, to equalizing supply
and demand for commodities, and to

preventing the formation of destabilizing
coalitions. In every case, a theoretical explanation
must be provided. The assumptions,

which cannot be satisfied by all economic
observations, are the present outcome of a
continuing weakening process.

A global view of an economy that wants
to take into account the large number of its
commodities, the equally large number of
its prices, the multitude of its agents, and
their interactions requires a mathematical
model. Economists have successfully constructed
such a model because the central

concept of the quantity of a commodity has
a natural linear structure. The action of an
agent can then be described by listing the
quantity of its input or output for each
commodity (opposite signs differentiating
inputs from outputs). That list can be treated
as the list of the coordinates of a point in
the linear commodity space. Similarly, the
price system of an economy can be treated
as a point in the linear price space, dual of
the commodity space, whose dimension is
also the number of commodities.

In those two linear spaces, the stage was
set for sometimes dazzling mathematical
developments that began with the elements
of differential calculus and linear algebra
and that gradually called on an ever broader
array of powerful techniques and fundamental
results offered by mathematics. Thus,
the three roles of prices given earlier as
instances were illuminated by basic mathematical
theorems: the first, the achievement
of an efficient use of resources, by results of
convex analysis; the second, the equalization
of supply and demand for commodities,
by results of fixed point theory; the third,
the prevention of the formation of destabilizing
coalitions, by results of the theory of
integration and of nonstandard analysis. In
those three cases, the lag between the date
of a mathematical discovery and the date of
its application to economic theory decreased
over time. It was notably short for
nonstandard analysis, founded at the beginning
of the 1960's by Abraham Robinson1

and applied to economics by Donald Brown
and Abraham Robinson (1972).

The last, and most recently developed, of
those three instances can be chosen, as can
either of the other two, for a more detailed
illustration. Competition is perfect when every
agent's influence on the outcome of
economic activity is insignificant. The influence
of their totality on that outcome is,
however, significant. It is to solve the problem
of aggregating negligible quantities so
as to obtain a nonnegligible sum that integration
was invented. In this perspective,

the application of integration theory to the
study of economic competition is entirely
natural. That application requires the set of
agents to be large-larger than the set of
integers. Treating the set of the agents of an
economy as the rich collection of the points


### ---Economics-1991-0-06.txt---
of an interval of real numbers has long been
familiar in descriptions of economic data. It
became familiar in economic theory as well
after Robert J. Aumann (1964) showed that,
in a pure exchange economy composed of
insignificant agents, the formation of destabilizing
coalitions is prevented if and only if
all those agents base their decisions on a
price system.

The concept of a convex set (i.e., a set
containing the segment connecting any two
of its points) had repeatedly been placed at
the center of economic theory before 1964.
It appeared in a new light with the introduction
of integration theory in the study of
economic competition: if one associates with
every agent of an economy an arbitrary set
in the commodity space and if one averages
those individual sets over a collection of
insignificant agents, then the resulting set is
necessarily convex.2 But explanations of the
three functions of prices taken as examples
can be made to rest on the convexity of sets
derived by that averaging process. Convexity
in the commodity space obtained by aggregation
over a collection of insignificant

agents is an insight that economic theory
owes in its revealing clarity to integration
theory.

An economist who experiences such an
insight belongs to the group of applied
mathematicians, whose values he espouses.
Mathematics provides him with a language
and a method that permit an effective study
of economic systems of forbidding complexity;
but it is a demanding master. It ceaselessly
asks for weaker assumptions, for

stronger conclusions, for greater generality.
In taking a mathematical form, economic
theory is driven to submit to those demands.
The gains in generality that it has achieved
as a result, in little more than a century,
stand out when the first formulations of
the theories of general equilibrium (Leon
Walras, 1874-1877) and of the core of an
economy (Francis Y. Edgeworth, 1881 pp.
34-8) are placed side by side with the recent
treatments of those subjects to which
The New Palgrave is an introduction and a
bibliographical key (John Eatwell et al.,
1987-1989). Walras's consumers and producers
have been freed from many of their

constraining characteristics; Edgeworth's
universe of two consumers and two commodities
has been vastly expanded.

Mathematics also dictates the imperative
of simplicity. It relentlessly searches for
short transparent proofs and for the theoretical
frameworks in which they will be

inserted. Participating in that pursuit, economic
theory was sometimes drawn by drives
toward greater generality and toward greater
simplicity in the same direction, rather than
in opposite directions. Cohort after cohort,
students of consumer theory have learned
about the concept of decreasing marginal
rate of substitution for two commodities on
an indifference curve and about its extension
to the multicommodity case. Notably
more general, and notably simpler, is the
concept of convexity of the set of points
preferred to a given point in the commodity
space. Welfare economics presents another
instance. One of its main theorems formulates
precisely the principle enunciated by
Adam Smith (1776). If all the agents of an
economy are in equilibrium relative to a
price system, then they utilize their collective
resources optimally. The proof of that
theorem (Kenneth J. Arrow, 1951) has become
so simple that it can be given without
mathematical symbols. It is, at the same
time, of utmost generality; in relating two
basic concepts of economic theory to each
other, it uses no assumption.

In its attempts to attain its many objectives,
economic theory was helped by greater
abstraction. Preference theory supplies an
example again. Significant research efforts
were expended on solutions of the integrability
problem. That problem can be bypassed
altogether, and greater simplicity can
be achieved by moving from the commodity
space to the more abstract space of the
pairs of its points. In this space, whose
dimension is twice the number of commodities,
the pairs of commodity points indifferent
to each other are now assumed to

form a smooth (hyper)surface. As another
instance of the generality permitted by abstraction,
consider the notion of a commod-

20n this direct consequence of a theorem of A. A.
Lyapunov, see Karl Vind (1964).


### ---Economics-1991-0-07.txt---
ity, which can be treated as a primitive
concept, with an unspecified interpretation,
in an axiomatic economic theory. A newly
discovered interpretation can then increase
considerably the range of applicability of
the theory without requiring any change in
its structure. Thus, by making the transfer
of a good or service between two agents
contingent on the state of the world that
will obtain, Arrow (1953) made possible the
immediate extension of the economic theory
of certainty to an economic theory of uncertainty
by a simple reinterpretation of the
concept of a commodity. The theory of financial
markets has been influenced by that
view of uncertainty, and their practice has
not been unaffected. Finally, take the problem
of existence of a general equilibrium,
once considered to be one of the most abstract
questions of economic theory. The

solutions that were proposed in the early
1950's paved the way for the algorithms for
the computation of equilibria of Herbert E.
Scarf (1973) and for several of the developments
of applied general equilibrium analysis
(Scarf and John B. Shoven, 1984). In this
case, abstraction in economic theory led to
the study of fundamental problems of great
generality, but also to a broad range of
applications.

III

The list of advances that the mathematization
of economic theory helped or permitted
is already long; and in one aspect it may
appear lengthy. Ceteris paribus, one cannot
prefer less to more rigor, lesser to greater
generality, or complexity to simplicity; but
other things are not equal, and in the estimate
of many members of our Association

the cost of that mathematization sometimes
outweighs its benefit. Two of its presidential
addresses notably confronted that difficult
analysis and stressed the price that economics
paid for its increased use of mathematics.
Wassily Leontief's (1971) observations
were factual, and Robert A. Gordon's
(1976) comments relevant when they were
made in 1970 and in 1975. They still are
today, for, in spite of their authorities, enhanced
by the platform from which they

were speaking, and in spite of the wide
diffusion of their critiques, neither Leontief
nor Gordon altered the course of the development
they were assessing. In the past two
decades, economic theory has been carried
away further by a seemingly irresistible current
that can be explained only partly by the
intellectual successes of its mathematization.


Essential to an attempt at a fuller explanation
are the values imprinted on an

economist by his study of mathematics.
When a theorist who has been so typed
judges his scholarly work, those values do
not play a silent role; they may play a decisive
role. The very choice of the questions
to which he tries to find answers is influenced
by his mathematical background.

Thus, the danger is ever present that the
part of economics will become secondary, if
not marginal, in that judgment.

The reward system of our profession reinforces
the effects of that autocriticism. Decisions
that shape the career of an economic
theorist are made by his peers. Whether
they are referees of a journal or of a research
organization, members of an appointment
or of a promotion committee,

when they sit as judges in any capacity, their
verdicts will not be independent of their
own values. An economist who appears in
their court rarely ignores his perception of
those values. If he believes that they rate
mathematical sophistication highly, and if
he can prove that he is one of the sophisticates,
the applause that he expects to receive
will condition his performance.

The same effects are also amplified by the
relentless pressure to publish exerted by his
environment. There are indeed instances of
extreme restraint in scientific publication,
and some of them have become legend. The
mathematical papers of Bernhard Riemann
(1826-1866) take 506 pages in the volume
that collected them (Riemann, 1876). The
molecular structure of DNA was announced
by James Watson and Francis Crick (1953)
in a one-page article. But it is easier to
explain those examples away than to follow
them. The environment of a scholar demands
papers, and the temptation to supply
them without restraint may become overpowering


### ---Economics-1991-0-08.txt---
to an economic theorist who has

developed proficiency in his research style.
The precocious development of that proficiency
is a comparative advantage that a

mathematical approach bestows on him.
The spread of mathematized economic
theory was helped even by its esoteric character.
Since its messages cannot be deciphered
by economists who do not have the

proper key, their evaluation is entrusted to
those who have access to the code. But
acceptance of their technical expertise also
implies acceptance of their values. Our profession
may take pride in its exceptional

intellectual diversity, one of whose clearest
symbols is an Ely lecture given by an economic
historian at a session chaired by a
mathematical economist. Yet that diversity
is strained by the increasing impenetrability
to the overwhelming majority of our Association
of the work done by its most mathematical
members.

IV

The bond that ties economists together in
their study of a common subject has not
been tested only by differences in methodologies.
It has also been tried by differences
in ideologies. In their endeavors to make
their field into a science, economists must
renounce a favorite mode of thinkingwishful
thinking; they must be impartial

spectators of a play in which they are the
actors. While they attempt to keep that
inhuman stance, they are pressed to give
immediate answers to societal questions of
immense complexity and thereby to abandon
the exacting slowness of the step-by-step
scientific approach. Divisions according to
methodologies and ideologies, criticism from
outside and from inside, and intellectual
fashions that sweep our discipline make each
one of its steady developments remarkable.
The mathematization of economic theory
was one of them for a century and a half.
During the past five decades it became one
of the prime movers in the transformation
of our field. The extent of that mathematization
has given rise to discordant assessments
of its effects and to attempts to

change its heading. The quality of assessments
of the phase that economic theory

underwent and the effectiveness of attempts
to alter the course of its evolution will gain
from a detailed analysis of the processes
that led to its present state.
 ## Economics-1992-0


### ---Economics-1992-0-03.txt---
Global warming from carbon dioxide was
an esoteric topic 15 years ago, unknown to
most of us. But in a few years, helped along
by some hot summers, it has climbed to the
top of the international agenda. Cabinets,
Parliaments, and heads of government have
issued pronouncements on reducing carbon
emissions, and in June of this year more
than a hundred governments will be represented
by ministers or heads of government
at a great United Nations Conference on
Environment and Development to be held
in Rio de Janeiro. Together with nongovernmental
organizations representing labor,

business, students, environmentalists,
scientists, and groups concerned with health
and child development and family planning,
these representatives are expected to need
25,000 hotel rooms. A "framework agreement"
is widely expected, together with

some institutional arrangements that will
keep global environmental issues permanently
on every government's agenda. And

at the center of these issues will be the
phenomenon that has come to be known as
the "greenhouse effect."

The greenhouse effect itself is simple
enough to understand and is not in any real
dispute. What is in dispute is its magnitude
over the coming century, its translation into
changes in climates around the globe, and
the impacts of those climate changes on
human welfare and the natural environment.
These are beyond the professional

understanding of any single person. The
sciences involved are too numerous and diverse.
Demography, economics, biology, and
the technology sciences are needed to project
emissions; atmospheric chemistry,

oceanography, biology, and meteorology are
needed to translate emissions into climates;
biology, agronomy, health sciences, economics,
sociology, and glaciology are needed
to identify and assess impacts on human
societies and natural ecosystems. And those
are not all.

There are expert judgments on large
pieces of the subject, but no single person
clothed in this panoply of disciplines has
shown up or is likely to. So, I venture to
offer my judgment.

First on the principle. The metaphor of
the greenhouse is not quite appropriate, but
the basic idea is not in dispute. The earth is
bathed in sunlight, some reflected and some
absorbed. If the absorption is not matched
by radiation back into space, the earth gets
warmer until the intensity of that thermal
radiation matches the absorbed incoming
sunlight. Some gases in the atmosphere that
are transparent to sunlight absorb radiation
in the infrared spectrum, blocking that outward
radiation and warming the atmosphere.
When the atmosphere has warmed

enough to intensify the thermal radiation so
that it matches the absorbed incoming sunlight,
equilibrium is achieved at the higher
temperature. These so-called "greenhouse"
gases can be identified in the laboratory.
Carbon dioxide is one of them; methane is
another, as is nitrous oxide, as are the chlorofluorocarbons
(CFC's).

The principle has been in practice for
decades. On a clear day in January, the
earth and its adjacent air in Orange County
California warm nicely, but the warmth radiates
rapidly away during the clear nights,
and frost may threaten the trees. Smudge
pots, burning cheap oil on a windless night,
produce substances, mainly carbon dioxide,
that absorb the radiation and protect the
trees with a blanket of warm air. Greenhouses,
in contrast, mainly trap the air

warmed by the earth's surface and keep it


### ---Economics-1992-0-04.txt---
from rising to be replaced by cooler air. The
phenomenon should have been called the
"smudgepot effect," but it is too late to do
anything about it.

A first step in pursuing this phenomenon
is to assess how much warming might go
with an enhanced concentration of these
gases. That cannot be done in the laboratory;
there are too many feedbacks. A

warmer atmosphere will contain more water
vapor; water vapor itself is a greenhouse
gas. Changes in temperature and humidity
will change cloud cover; clouds can reflect
or absorb incoming or outgoing light according
to their composition and altitude. The
average temperature is only one dimension;
temperatures at different altitudes and different
latitudes matter. But a starting point
has been the change in average surface atmospheric
temperature expected to accompany

a specified increase in the concentration
of greenhouse gasses; and arbitrarily,
but reasonably, the base case is taken as a
doubling of the concentration.

A moment on why a doubling is the

benchmark. To compare estimates of warming,
people must use the same hypothesized
concentration of greenhouse gases in the
atmosphere. (Alternatively, they could use
the same hypothesized temperature increase
and estimate the corresponding concentration.
) Doubling, like a half-life in reverse,
is a natural unit if it is within the
range of practical interest, and it is. A doubling
is expected sometime in the next century,
so it is temporally relevant; and a
doubling is estimated to make a substantial
but not cataclysmic difference. If fixation on
a doubling seems to imply an upper limit on
any expected increase, the implication is
unfortunate: enough fossil fuel exists to support
several doublings.

In 1979, a committee of the National
Academy of Sciences (NAS) (1979 p. 2)
estimated the change in average temperature
to accompany a doubling of carbon

dioxide in the atmosphere: three degrees
Celsius, with a range of 1.5 degrees to either
side. (In the last 15 years other greenhouse
gases have received attention; these
other gases can be converted to their carbon
dioxide equivalents and the original
estimate applied to the mixture.) The NAS
appointed another committee a few years
later to reexamine that estimate, and the
new committee saw no reason to change it
(NAS, 1982 p. 51). An intergovernmental
panel on climate change (IPCC), consisting
of scientists from many nations, revisited
the estimate in 1990 and concluded, from
the several climate models they had examined,
that "the models results do not justify
altering the previously accepted range of 1.5
to 4.5 degrees C" (IPCC, 1990 p. xxv). Thus,
the estimate appears to be robust over time,
but the spread of uncertainty remains large:
the upper limit is three times the lower
limit. (No quantitative interpretation of
these upper and lower "limits" has been
made public. Both National Academy reports
referred to them as "probable error.")
II

The uncertainties are even greater in
translating a temperature change into climates.
The media support a popular view

that things will just get hotter; a news magazine
cover was a sweating global face. But
the laboratories that do the meteorology do
not simply predict warming; they do not
even predict that the most noticeable effects
will necessarily be temperature changes.
Among the great driving forces of weather
and climate is the temperature differential
between equatorial and polar regions; convection
currents coupled with the rotation

of the earth are engines of atmospheric
circulation and, ultimately, ocean circulation.
The models predict greater temperature
change in the polar regions than near
the equator. This change in gradient can
drive changes in circulation. The results may
be warmer in some places and colder in
others, wetter in some places and drier in
others, cloudier in some places and sunnier
in others, stormier in some places and less
stormy in others-generally a complex of
changes that would bear no easy relation to
an average change in global temperature.
The change in average temperature is
useful as an index of climate change. It is


### ---Economics-1992-0-05.txt---
thought, and the models demonstrate, that
the greater the change in average temperature
the greater the departure of current
climates from what they are now. Thus,
while it is wrong to think that what is going
to happen can be readily characterized as
"warming" it is not erroneous to take that
average warming as a rough measure of the
extent or severity of change to be expected.
Unfortunately the widespread reference to
"global warming" promotes the notion that
things will simply get hotter. (Interestingly,
virtually all public discussion is on hotter
summers, not warmer winters; a hundred
years ago popular discussion of a warming
trend would likely have concentrated on the
milder winters to be expected.)

If three degrees Celsius is taken as an
index of climate change to come within the
next century or so, how big is that compared
with what has happened within the last century,
or the last 10,000 years? From what I
have just said, this cannot be answered in
terms of whether anyone would notice the
difference if every night and every morning,
every winter and every summer, temperatures
were exactly three degrees higher than
they otherwise would have been. The question
is: how would a three-degree change in
a global average compare with what has
been experienced in the past?

The answer is that for 10,000 years, since
the disappearance of the last ice age, average
temperature appears never to have varied
over anything like three degrees. A band
of one degree Celsius would cover the current
estimates of what average temperatures
have been since the dawn of recorded
history. We will be moving into a climatic
regime that has never been experienced in
the current interglacial period.

"Mankind will undergo greater climate
change in the next 100 years than has been
experienced in the last 10,000." Properly
qualified, the statement is true; what it neglects
is that peoples have been migrating
over great distances for at least several
thousand years. Goths and Vandals, Huns,
West Europeans who populated North and
South America, Southerners who went
North during the Great Depression, and
Northeasterners who moved southwest after
World War II all experienced changes in
climate greater than any being forecast by
the models. Almost everybody who attends
this lecture in New Orleans will have undergone
a greater change in the past few days
than is expected to occur in any fixed locality
during the coming century.

The changes that the models produce are
gradual both in time and in space. The
models do not produce discontinuities. Climates
will "migrate" slowly. The climate

of Kansas may become like Oklahoma's,
Nebraska's like that of Kansas, South
Dakota's like Nebraska's, but none of these
is expected to become like the climates of
Oregon, Louisiana, or Massachusetts.
A caution: the models probably cannot
project discontinuities-just gradual change
-because nothing goes into the models that
will produce catastrophes. There may be
phenomena that could produce drastic
change, but they are not known with enough
confidence to introduce them into the models.
So the reassuring gradualness may be
an artifact of the methodology. I will return
to this point later.

This greenhouse problem, if problem it
proves to be, is truly one of the "global
common." A ton of carbon emitted anywhere
on earth has the same effect as a ton
emitted anywhere else. And carbon dioxide
has a long residence time in the atmosphere:
a century or more. There may be

ways to remove it, but it doesn't disappear.
The greenhouse influence on any national
territory depends solely on the global concentration,
not in any way on what part of

the total is due to a nation's own emissions.
As I shall detail later, the costs of reducing
carbon emissions will be large compared
with any other emissions that have caused
concern. The costs of phasing out CFC's
will be in the billions of dollars per year for
some years, and complete elimination is expected
to be feasible. The cost of reducing
sulfuric acid may be in the tens of billions of
dollars. Proposals to hold emissions of carbon
dioxide constant (with a linear increase
of concentration in perpetuity) or to reduce
emissions by 50 percent below what they


### ---Economics-1992-0-06.txt---
would otherwise be, beginning perhaps in
2010, are expected to cost in the hundreds
of billions in perpetuity.

There are a few numbers worth carrying
in mind. There are 700 billion tons of carbon
in the atmosphere. (Quotations are

sometimes in tons of carbon dioxide, rather
than carbon; the figure is then 32 times as
large, about 2,600 billion.) Annual emissions
are 6 billion tons. Close to half disappears
somewhere, and a little over half

remains in the atmosphere; so the concentration
is increasing by one-half percent per
year. It has increased 25 percent in the last
hundred years. (Concentration is reported
more often than tonnage; it is currently
about 350 parts per million.) And there are
upwards of ten trillion tons of carbon fuels
out there to be burned; if it were all burned
and half stayed in the atmosphere, the concentration
could double at least three times.

If the carbon in the atmosphere has already
increased by a quarter, has the average
temperature gone up as predicted? And
were the recent hot American summers that
stirred popular interest harbingers of greenhouse
summers to come?

To the first question, the answer is that
average global temperature-summer and
winter, both hemispheres, night and
day-has apparently risen by half a degree
in the last hundred years, but whether "as
predicted" depends on what qualifications
one reads into the predictions. The pattern
differed between the Northern and Southern
Hemispheres. The global average rose
during the first 40 years of this century, was
level for the next 40 years, and rose during
the past decade. This pattern demonstrates
that, whether or not we are witnessing the
greenhouse effect, there are other decadeslong
influences that can obscure any smooth
greenhouse trend. (The carbon concentration
is not at issue; it is well measured and
shows steady rise on a decade scale.) There
are known phenomena that could account
for the irregular temperature increase of
the past century, and whether we are witnessing
the "signal" probably depends on

whether one wants high confidence to reject
a null hypothesis or is about to bet money
on whether, another 25 years from now,
looking back, all doubt will have been removed.
I don't know what bets are being

placed by "greenhouse scientists," but they
are cautious in public on the question.
To the second question-do the hot

American summers of the past few years
announce the arrival of a greenhouse, confirming
predictions?-the answer is in two

parts: maybe it's the greenhouse; but it's not
what the greenhouse models predict. The
global average in the four hot years of the
past seven was only 0.2 degrees above the
level of the preceding 40 years; and sudden
hot American summers are not what the
models predict.

III

In anticipating the impact on human welfare
or natural systems, two kinds of uncertainty
are unlikely to be dispelled soon. One
is simply the question of what the changes
will be in each region or locality. Current
models are severely limited in their agreement
with each other, in their handling of
such topographical variables as mountain
ranges, and in the fineness of the grids they
superimpose on the globe. There is no great
confidence that the models will be greatly
improved within the next decade or two. A
chaos-like process may defeat efforts to improve
local predictions; and uncertainties in
gross phenomena, such as the behavior of
ocean currents under changed climatic conditions,
may not be much better understood

soon.

Even if we had confident estimates of
climate change for different regions of the
world, there would still be uncertainties
about the kind of world it is going to be 50,
75, or 100 years from now. Imagine it were
1900 and the climate changes associated
with a three-degree average temperature increase
were projected to 1992. On what

kind of world would we superimpose either
a vaguely described potential change in climate
or even a specific description of

changes in the weather in all the seasons of
the year, even for our own country. There
would have been no way to assess the impact
of changing climates on air travel, electronic
communication, the construction of


### ---Economics-1992-0-07.txt---
skyscrapers, or the value of California real
estate. Most of us worked outdoors; life
expectancy was 47 years (it is now 75); barely
a fifth of us lived in cities of 50,000 or more.
Anticipating the automobile, we might have
been concerned with whether wetter and
drier seasons would bring more or less mud,
not anticipating that the nation's roads would
become thoroughly paved. The assessment
of effects on health would be without antibiotics
or inoculation. And in contrast to

most contemporary concern with the popular
image of hotter summers to come, I

think we would have been more concerned
about warmer winters, later frost in autumn,
and earlier thaw in the spring.

If the world, both North America and the
other continents, is going to change as much
in the next 90 years as it has changed in the
90 just past, we are going to be hard put to
imagine the effects of climate changes.
Another thought experiment: suppose the
kind of climate change expected between
now and, say, 2080 had already taken place
since 1900. Ask somebody 50, 60, or 80
years old what is different compared with
when he or she was a child. Would the
climate change be noticed? Even ask a 70-
year-old farm couple living on the same
farm where they were born: would the
change in climate be among the most dramatic
changes in either their farming or

their lifestyle? I expect changing from horses
to tractors and from kerosene to electricity,
the arrival of the telephone and the automobile
and the paving of roads, the development
of pesticides and artificial fertilizer,
the discovery of soy beans and the development
of hybrid corn, and even improvements
in outdoor clothing, veterinary

medicine, and agricultural practices generally
would swamp the climate change. And
if instead of living and working conditions
we inquire about changes in wildlife and
natural ecosystems, changes in regional climates
would have been competing, in their
impact on nature, with population growth
and economic development.

A conclusion we might reach is that a
climate change would have appeared to
make a vastly greater difference to the way
people lived and earned their living in 1900
than to the way people live and earn their
living today. Today very little of our gross
domestic product is produced outdoors, susceptible
to climate. Agriculture and forestry
account for less than 3 percent of GDP, and
little else is much affected. Some activities
-tourism and holidays, professional sports,
and school teaching-are seasonal, but
many of the seasonalities are conventions
that reflect the influence of climate in earlier
times. (Children were needed in the
fields in summer and could start school when
the harvest was in; hockey and basketball
used to be winter sports because one depended
on ice and the other could fit in a
building.)'

Manufacturing rarely depends on climate,
and where temperature and humidity used
to make a difference, air conditioning has
intervened. When Toyota chooses among
Ohio, Alabama, and Southern California for
locating an automobile assembly, geographical
considerations are important, but not
because of climate. Minerals are extracted
where they happen to occur, and oil fields
and coal mines inhabit all kinds of climates
and are little affected. The U.S. Postal Service'
s vow that neither snow nor rain nor
heat nor gloom of night will "stay these
couriers from the swift completion of their
appointed rounds" sounds quaint in the era
of e-mail and fax.

Finance is little affected by climate; similarly
for health care, or education, or broadcasting.
Transportation can be affected, but
improvements in all-weather landing and
take-off in the last 30 years are greater than
any differences that climate makes. If the
average effect is a warming, iced waterways
and snow removal may decline in importance.
Construction is affected, mainly by
cold, and if the average effect is in the
direction of warming, construction may benefit
slightly.

It is really agriculture that is affected. But
even if agricultural productivity declined by
a third over the next half century, the per


### ---Economics-1992-0-08.txt---
capita GNP we might have achieved by 2050
we would achieve only in 2051. Considering
that in most of the developed countries-the
United States, Japan, France, the United
Kingdom, the Netherlands, and Israel-the
agricultural problem has been protecting
farmers, that agricultural productivity in
most parts of the world continues to improve,
and that many crops and cultivated

plants will benefit directly from enhanced
photosynthesis due to increased carbon
dioxide, one cannot be certain that the net
impact on agricultural productivity will be
negative or, if negative, will be noticed in
the developed world.

I conclude that in the United States, and
probably Japan, Western Europe, and other
developed countries, the impact on economic
output will be negligible and unlikely
to be noticed.2 And there is no reason to
believe that in these countries there could
be a noticeable impact on health. Any influence
of climate on health in this country
would be more in the regional distribution
of the population than in changes in local
and regional climates.

Comfort is worth considering. Fortunately,
the climate models predict a greater
warming in winter than in summer. Most
people in the United States, Japan, and
Western Europe go south for vacation, both
summer and winter; and when people move
upon retiring in the United States they typically
move toward warmer climates. In future
years, elderly people may suffer more
heat stroke in summer in St. Louis, but we
can hope for fewer broken bones from ice
in Boston. (Inhaling air richer in carbon
dioxide has no effect on health.)

IV

This complacent assessment cannot be
extended to the much larger population of
the underdeveloped world. The livelihoods
earned in agriculture and other climatesensitive
outdoor activities, 3 percent in the
United States, comprise 30 percent and
more of all livelihoods in most of the developing
world. Reliable forecasts of likely
climate changes in the different areas so
dependent on agriculture are simply not
available, so no assessment, region by region,
of the effect on productivity can be
provided. There is no strong presumption
that the climates prevailing in different regions
50 or 100 years from now will be less
conducive to food production. But there is
also no assurance that climate changes will
not be harmful, and even if on balance the
impact is neutral, there may be large areas
with large populations that suffer severely.
Those people are vulnerable in a way that
Americans, Western Europeans, and

Japanese are not.

Nor can the impact on health be dismissed
or readily subsumed among generally
improving health conditions, as for the
developed world. Numerous parasitic and
other vector-borne diseases affecting hundreds
of millions of people are sensitive to
climate. Again, there is no strong presumption
that malaria mosquitos, to take an example,
will on balance benefit from climate
changes, but the risk is there.

It is with the less-developed countries that
we have to be most careful about superimposing
the climates of the future on the

economies and societies of today. As it was
in our own country during this century, the
trend in developing countries is to be less
dependent on agriculture and less vulnerable
to climate in transportation and other
activities and health. If per capita income
growth in the next 40 years compares with
the 40 years just past, vulnerability to climate
change should diminish, and the resources
available for adaptation should be

greater. I say this not to minimize concern
about climate change, but to anticipate the
question of whether developing countries
should make sacrifices in their development
to minimize the emission of gases that may
change climate to their disadvantage. Their
best defense against climate change may be
their own continued development.

This is a point worth emphasizing. Some
environmentalists argue that developing
2

A comprehensive discussion of both impacts and
costs of abatement is provided by William D. Nordhaus
(1991a). A carefully argued opposing view is that of
William R. Cline (1992).


### ---Economics-1992-0-09.txt---
countries should sacrifice some of their
hopes for economic development in the interest
of slowing the climate change that

may prove disastrous. But the advice contains
a contradiction Any disaster to developing
countries from climate change will be
a disaster to their economic development.
What is desired is to optimize development
by investing in greenhouse-gas abatement
only when that appears, subject to all the
uncertainties, to contribute more to their
development in the future than the alternative
direct investment in development. It is
not economic growth versus environment; it
is growth with the environment taken into
account.

A related point: population growth is important
for the climate change, in two respects.
One is that carbon emissions in developing
countries are positively driven by

population; population growth does not
merely dilute carbon emissions per capita,
but for a number of reasons more people
means more carbon. If China succeeds in
holding population growth to near zero for
the next couple of generations, it may do as
much for the earth's atmosphere as would a
heroic Chinese anticarbon program coupled
with 2-percent annual population growth.
The other population effect is simply that
the most likely adverse impact of climate
change on human productivity and welfare
would be on food production. In the poorest
parts of the world, the adequacy of food
depends on the number of mouths and
stomachs. In a hundred years, adverse
changes in climate for food production
would be far more tragic if the countries we
now associate with the developing world
had populations totaling 12 billion than if
they totaled 9 billion. For the developing
world, the increasing concentration of people
is probably more serious than the increasing
concentration of carbon dioxide.

At this point, I appear to have reached
the conclusion that the developed world has
no self-interest in expensively curtailing carbon
consumption and that the developing
world cannot afford to incur economic
penalties to slow the greenhouse effect.
There is a mismatch between those who
may be vulnerable to climate change and
those who can afford to do anything about
it.

V

Why should the rich developed countries
care enough about climate change to do
anything about it? The answer must depend
partly on how expensive it is going to be to
do anything about it. Abatement programs
have been examined in a number of econometric
models that suggest we might want

to treat as pertinent the sacrifice of perhaps
2 percent of world GNP in perpetuity.
A strong argument for trying seriously to
slow climate change is that the developing
countries are vulnerable and we care. Developed
countries are currently providing

$50 billion per year of assistance to the
developing world; we would be talking about
expending or forgoing perhaps 4-8 times
that much to slow emissions and slow climate
change. Whether people in the developed
democracies could be mobilized to

contribute so much to benefit, half a century
from now, the people in the countries
we now call developing I do not know, but I
believe that if the developed countries were
prepared to invest, say, $200 billion per year
in greenhouse-gas abatement, explicitly for
the benefit of developing countries 50 years
or more from now, the developing countries
would clamor to receive the resources immediately
in support of their continued development.
There would undoubtedly be

abatement opportunities so cheap that they
could compete with direct aid to developing
countries, but it would be hard to make the
case that the countries we now perceive as
vulnerable would be better off 50 or 75
years from now if 10 or 20 trillions of dollars
had been invested in carbon abatement
rather than in their economic development.
A second argument for an expensive program
of carbon abatement is that, while our
production of material goods and services
may not suffer from climate change, our
natural environment may be severely damaged.
Natural ecosystems will be destroyed;
plant and animal species will become extinct.
Places of natural beauty will be degraded.
Valuable chemistries of plant and


### ---Economics-1992-0-10.txt---
animal life will be lost before we learn their
genetic secrets. And the earth itself deserves
our respect. For many people, something
close to religious values are at stake.
This issue is doubly difficult to assess. It is
difficult to know how to value what is at
risk, and it is difficult to know just what is at
risk. Even if climate changes at each point
in time could be predicted accurately, the
impacts on natural ecosystems could not yet
be determined. And the benefits of slowing
climate change by some particular amount
would be even more uncertain. We know
that carbon fuels are not going to be discontinued;
the issue is the marginal gains, from
carbon abatement and a slowing of climate
change, in the survival of species and
ecosystems and the preservation of enjoyable
environments. This is an issue that
simply has not been addressed.

The third argument for spending heavily
to slow climate change is that the conclusions
I reported earlier may be quite wrong.
I said that the climate models predict that
climates will change slowly and not much;
the models do not produce discontinuities,
surprises, catastrophes. What is known about
weather and climate constitutes an equilibrium
system.

The possibility has to be considered that
if global temperature increases, not by the
median estimate of three degrees Celsius
for a doubling of carbon in the atmosphere,
but by four or five degrees and continues to
rise beyond the doubling because carbon
fuels are still in use worldwide, some atmospheric
or oceanic circulatory systems may

switch to alternative equilibria, producing
regional changes that are both sudden and
extreme.

Have any such possibilities been thought
of? One that was thought of but diminished
upon further investigation was the possibility
that the west Antarctic ice sheet might
glaciate into the ocean and raise the sea
level by 20 feet. As recently as 15 years ago,
the best scientific judgment was that this
could happen within 75 years as a result of
global warming. This prospect naturally attracted
attention, and further investigation
with the help of newly available satellite
sensing of glacial movement led to reassuring
estimates that if that catastrophic rise in
sea level were to happen it would take at
least a few hundred years and be gradual,
not sudden. But there isn't any scientific
principle according to which all alarming
possibilities prove to be benign upon further
investigation.

A currently discussed likely source of discontinuous
change is in the way oceans behave.
Amsterdam is north of Newfoundland,
yet is warmer, courtesy of the Gulf
Stream. There is some indication that in
earlier interglacial periods ocean currents
may have pursued different courses. If a
current like the Gulf Stream, or the
Japanese Current for the United States,
switched into an alternative pattern, the climatic
consequences might be both sudden

and severe. (Paradoxically, global warming
might freeze Western Europe.)

Insurance against catastrophes is thus an
argument for doing something expensive
about greenhouse emissions. But to pay a
couple percent of GNP as insurance premium,
one would hope to know more about

the risk to be averted. I believe research to
improve climate predictions should be concentrated
on the extreme possibilities, not

on modest improvements to median projections.


I said that current estimates suggest that
it might cost a couple percent of GNP to
postpone the doubling of carbon in the atmosphere
by several decades. Is 2 percent a

big number or a small one?

That depends on your perspective and on
what the comparison is. In recent years 100
billion dollars per year in budgets or taxes
has been a politically unmanageable magnitude
in the United States. On the other

hand, subtracting 2 percent from GNP in
perpetuity lowers the GNP curve by not
much more than the thickness of a line
drawn with a number-two pencil, or to formulate
it as I did earlier, it postpones the
GNP of 2050 until 2051. I say this not to
belittle the loss of 10 trillion dollars from
the American GNP over the next 60 years,
but only to point out that the insurance
premium, if we choose to pay it, will not
send us to the poorhouse. The proper question
is whether, if we were prepared to

spend 2 percent of our GNP in the interest
of protecting against damage due to climate


### ---Economics-1992-0-11.txt---
change, we might find better use for the
money.

I have mentioned one use: directly investing
to improve the economies of the poorer
countries. Another would be direct investment
in preserving species, ecosystems, or
wilderness areas. There is concern that many
ecosystems could not migrate as rapidly as
climate may change in the coming century;
there has been little investigation of what
might be done to facilitate the migration of
ecosystems if the alternative is to invest 5 or
10 trillions of dollars in the reduction of
carbon emissions.

VI

What can be done to reduce or offset
carbon emissions? Reducing energy use and
the carbon content of energy have received,
I believe properly, most of the attention,
especially the attention of economists. There
are other possibilities to mention.
Trees store carbon. In growing, they take
it out of the atmosphere. When they rot or
burn it goes back into the atmosphere. A
new forest will absorb carbon until it reaches
maturity (i.e., maximum carbon density) in
75 or 100 years. If it then merely replenishes
itself, with new growth replacing the
oxidized dead trees, it holds its carbon but
does not absorb more. If trees are harvested,
the lumber that becomes house

frames or furniture may last a hundred years
or more; removing mature trees and storing
them anaerobically is possible but expensive.
The most recent report of the National
Academy of Sciences considered that reforestation
in the United States might sequester
2-3 percent of current global

carbon dioxide emissions.3 The prospects
for that kind of reforestation in the rest of
the world are not nearly so promising, and
we should conclude that reforestation can
contribute, but not greatly.

Stopping or slowing deforestation is important
for reasons other than carbon emissions
but is quantitatively more important
than reforestation. Reforestation is unlikely
to take up as much as 100 billion tons of
carbon; deforestation, in areas where deforestation
is likely, could contribute several
hundred billion tons of carbon, partly because
forest subsoils contain carbon typically
greater than the amount in the trees
themselves, and this carbon is subject to
oxidation when the trees are removed.
Carbon can be "scrubbed" from stack
gases, probably not with any known technology
that would make such removal economically
competitive with reducing emissions.
(Part of the expense is disposing of sludge;
where gaseous carbon might be pumped
into the ocean or into underground cavities,
economical disposal may prove feasible.)
Parallel to reforestation is the idea of enhancing
oceanic photosynthesis, by "fertilizing"
the oceans, possibly with iron, if enough
of the carbon residues from the enhanced
growth will sink rather than remain near the
surface. Experiments would probably be reversible
and modest in scale; their political
acceptability may be tested in the near future.


Finally-although nothing is final in a
subject as new as the one we are talking
about-there are numerous possibilities for
putting substances or objects in orbit or in
the stratosphere to reflect something like 1
percent of incoming sunlight to offset a large
part of the radiation imbalance caused by
greenhouse gases. Some of these are as
apparently innocuous as stimulating cloud
formation, and some are as dramatic as
huge mylar balloons in low earth orbit. Until
very recently these possibilities were
nearly unmentionable, but they have recently
been dignified by inclusion, along with
caveats about "large unknowns concerning
possible environmental side effects," in the
1991 report of the National Academy of
Sciences. I shall not pursue them here, except
for two observations. First, if in decades
to come the greenhouse impact begins to
confirm the more alarmist expectations, and
if the economic sacrifices required to reduce


### ---Economics-1992-0-12.txt---
emissions prove unmanageable for economic
or political reasons, some of these
"geoengineering" options will invite attention.
Second, if they do, and especially if
they prove to be within the budgetary capabilities
of individual nations, international
greenhouse diplomacy will be transformed.
VII

What remains nearly certain is that the
main responses to the greenhouse threat
will be adapting to climate as climate
changes and reducing carbon emissions.
(CFC's are potent greenhouse gases and, if
unchecked, might rival carbon dioxide in
decades to come; but international actions
are making good progress and are among
the cheapest ways of reducing greenhouse
emissions.)

Like estimates of warming, estimates of
the costs of reducing emissions require some
common but arbitrary objective to be comparable.
A doubling of carbon became the

conventional benchmark for warming estimates;
no such benchmark for reduced carbon
emissions has been adopted for estimating
costs. (In principle, the estimates could
adopt that doubling: the issue could be formulated
as the cost of retarding the doubling
time by a decade, two decades, or half
a century.) Most estimates take as their
target a reduction of emissions either to a
specified fraction of what they would be in
the absence of controls, or to some fixed
ratio to the emissions of 1990 or the projected
emissions of 2000 or 2010. The estimates
examine minimum-cost trajectories,

implicitly or explicitly assuming something
like a uniform tax on the carbon content of
fuel as the policy instrument. They typically
make some assumption about a "fallback"
energy technology, at least for electricity,
available at some price in some decade of
the next century. They have to project estimates
of non-price-induced improvements

in the use or avoidance of energy by industries
and households. And if they deal with
global emissions, they have to make some
assumption about the distribution of abatement
efforts among nations, especially

among the developing countries, which, including
China, account for about a quarter

of emissions now and would be expected to
account for half by the middle of the next
century.

Any estimate of the cost of abatement
needs therefore to specify at least half a
dozen target assumptions. Furthermore, the
estimates are produced by people and institutions
that do not simultaneously estimate
the costs associated with climate change,
either damages or costs of adapting; the
estimates do not optimize the combined
costs of abatement and climate change. A
"not unreasonable" target for reduction
might be delaying a doubling by, say, four
decades. One decade might be too trivial, a
century too ambitious, and four decades an
objective in which most audiences would be
interested. But nobody who makes such an
estimate wishes to be interpreted as proposing
that when all the uncertainties about
climate changes and their impacts have been
resolved, if they ever are resolved, the optimum
reduction in emissions will be found
to retard doubling by 40 years, or any other
specified period of time.

All I can do to summarize a multitude of
estimates is to specify an order of magnitude
that many economists and the Congressional
Budget Office would not consider

outrageous. That is the figure I mentioned
earlier, possibly 2 percent of GNP for the
developed countries and a similar, but even
much more uncertain, percentage of GNP
for the developing world. The uncertainty
for the developing world is partly due to the
estimates being mainly derived from the
American economy.4

Two characteristics of these estimates
need to be emphasized. One is that they
tend to assume optimal technological adjustment,
as in response to a carbon tax. To

the extent that carbon emissions are controlled
by direct regulatory measures, there
may be the usual expected inefficiencies,
and I leave the reader to make his own
adjustment.

4Several critiques and surveys of different abatement-
cost estimates are available (see Congressional
Budget Office, 1990; Joel Darmstadter, 1991; William
D. Nordhaus, 1991b; Energy Modeling Forum, 1992).


### ---Economics-1992-0-13.txt---
The second is that, since the early years
of the energy crisis in the 1970's, there have
been enthusiastic portrayals of currently
available technologies, ranging from light
bulbs to electric motors, double-glazed windows
and improved internal-combustion engines,
that for some reason have not been

successfully marketed. The interest continues,
and the recent National Academy of

Sciences study gave sympathetic attention,
but no analysis, to a number of proposals
for residential, commercial, industrial, and
transportation energy management and for
improved electricity production and fuel
supply and concluded that, including reductions
in CFC's, "The United States could

reduce or offset its greenhouse gas emissions
by between 10 and 40 percent of 1990
levels at low cost or at some net saving, if
the proper policies are implemented" (1991
p. 73).

All of these ideas are completely orthogonal
to the econometric estimates. The

Academy panel that produced the report
was unable to offer an explanation for why
these low-cost or negative-cost technologies
have not caught on. Its quantitative assessment,
including an allowance for elimination
of CFC's, ranged from as little as 10
percent to as much as 40 percent of current
U.S. emissions; CFC's aside, their range of
possibility is from zero to about 30 percent.
Whatever the correct figure, this is probably
a once-and-for-all backlog of accumulated
technologies, which once exploited may be
permanent but not progressive. But the
strong suggestion is that there is a lot to be
accomplished in the next two or three
decades.

VIII

With these qualifications, let us look at
that 2 percent of GNP as a permanent
reduction over the coming century. I consider
it altogether improbable that the developing
world, at least for the next several
decades, will incur any significant sacrifice
in the interest of reduced carbon (nor would
I advise developing countries to do so).
Anything done to reduce emissions in China,
India, or Nigeria will be at the expense of
the richer countries.

Financing energy conservation, energy
efficiency, and switching from high-carbon
to lower-carbon or noncarbon fuels in Asia
and Africa would not only be a major economic
enterprise but a complex effort in

international diplomacy and politics. If successful,
it would increase the costs to the

developed world by at least another percent
or two on top of the 2 percent I mentioned.
It is furthermore not easy to hide the transfer
of resources on the order of a couple of
hundred billion dollars, dollars "budgeted"
somehow or other, compared with hiding
some of the costs due to regulation, such as
automobile fuel-efficiency standards in the
United States. The kind of thing we are
talking about is inducing the Chinese,
through our somehow offsetting their cost,
to forgo a massive electrification based on
coal and the cheapest coal-combustion technology.
Without engaging in blackmail, the

Chinese can assert that it is not in their
interest to do that at their own expense,
even if they are the keystone of a "social
contract" and no other nation will do anything
unless the Chinese fully participate.
I shall sketch what I can imagine as a
major attack on the greenhouse problem.
And I should be explicit about what I cannot
imagine. For reasons that I would be
delighted to elaborate but for which I cannot
take space here, a universal uniform
carbon tax is not a solution that I can imagine.
My reason is simple. A carbon tax sufficient
to make a big dent in the greenhouse
problem would have to be roughly equivalent
at least to a dollar per gallon on motor
fuel, and for the United States alone such a
tax on coal, petroleum, and natural gas
would currently yield close to half a trillion
dollars per year in revenue. No greenhouse
taxing agency is going to collect a trillion
dollars per year in revenue; and no treaty
requiring the United States to levy internal
carbon taxation at that level, keeping the
proceeds, would be ratified by the Senate.
Reduce the tax by an order of magnitude
and it becomes imaginable, but then it becomes
trivial as greenhouse policy.5


### ---Economics-1992-0-14.txt---
Tradable permits have been proposed as
an alternative to the tax. There are two
main possibilities: (i) estimating "reasonable"
emissions country by country and establishing
commensurate quotas or (ii) distributing
tradable rights in accordance with

some "equitable" criterion, such as equal
emissions per capita (a possibility that has
actually been discussed). Depending on how
restrictive the aggregate of such tradable
emission rights might be, the latter is tantamount
to distributing trillions of dollars in
discounted value and making, for a country
like Nigeria, the outcome of its population
census the country's major economic policy.
If, instead, quotas are negotiated to correspond
to every country's currently "reasonable"
emissions level, they will surely be
renegotiated every 5 or 10 years, and selling
an emissions right will be perceived as evidence
that a quota was initially too generous.
It is unlikely that governments will engage
in trades that acknowledge excessive
initial quotas.

I do not foresee negotiated national quotas
subject to serious enforcement, especially
enforcement through financial penalties.
I think any international regime for
carbon abatement can seriously include only
the developed countries, and I exclude from
this category the countries that we used to
call the Eastern Bloc. I can easily imagine
institutional arrangements that are universalist,
some kind of "framework agreement"

to which every country subscribes, with specific
commitments to be negotiated later.
But I expect serious commitments to be
undertaken only by the countries that can
afford to, and I am undecided whether an
institutional pretense of a universalist system
has advantages or, instead, the developed
world should proceed independently

and unencumbered with the need for a universalist
facade.

The model that I find most helpful in
conceptualizing a greenhouse regime among
the richer countries is the negotiations
among the countries of Western Europe for
distributing Marshall Plan dollars among
themselves and the negotiations, beginning
in 1951, on "burden sharing" in NATO.
There was never a formula for distributing
Marshall Plan dollars; there was never an
explicit criterion, such as equalizing living
standards, equalizing growth rates, maximizing
aggregate output or growth, or establishing
a floor under levels of living. Baseline
dollar-balance-of-payments deficits were a
point of departure, but the negotiations took
into account investment needs, traditional
consumption levels, war-induced capital
needs, opportunities for import substitution
and export promotion, and opportunities to
substitute intra-European trade for trade
with hard-currency countries.

The United States insisted that the recipients
argue out and agree on shares. In the
end, they did not quite make it, the United
States having to make the final allocation.
But all the submission of data and open
argument led, if not to consensus, to a reasonable
appreciation of each nation's needs.
The negotiations were professional; they
were assisted by a proficient secretariat. The
resources involved for most recipient countries
were immensely important. Good relations
were observed throughout; and proficiency
in debate, acceptance of criteria, and
negotiating etiquette steadily improved.
That is the only model I find plausible,
and I believe distribution of Marshall Plan
and defense-support funds to Europe is the
only model of multilateral negotiation involving
resources commensurate with the

cost of greenhouse abatement. (In the first
year, Marshall Plan funds were about 1.5
percent of U.S. GNP and-adjusting for
overvalued currencies-probably 5 percent
of OEEC GNP).

What that model suggests is that the main
participating countries in a greenhouseabatement
regime would submit for each

other's scrutiny and cross-examination plans
for reducing carbon emissions. The plans
would be accompanied by estimates of
emissions or emissions reduction from some
projected level, but any commitments undertaken
would be to the policies, not the

emissions. And not all of the plans would
necessarily be commitments.

The United States, for instance, could
present a plan for the introduction of a new
generation of nuclear power reactors beginning
sometime in the next century, but it is


### ---Economics-1992-0-15.txt---
difficult to see how the federal government
can commit itself to what reactors public
utilities will be purchasing 20 years from
now. The United States can have a plan to
mandate fuel-efficiency standards for automobiles,
but it takes 10 years for the standards
to work their way into the automobile
fleet, and there is no accounting procedure
that will estimate the effect on motor-fuel
consumption of any level of average fuel
efficiency a decade from now.

The current popular expectation is that
participation in any greenhouse regime will
take the form of commitments to specified
percentage reductions of emissions below
those of some specified year, like 1990 or
2000. I cannot help believing that adoption
of such a commitment is an indication of
insincerity. A serious proposal would specify
policies, like taxes, regulations, and subsidies
and would specify programs (like research
and development), accompanied by

very uncertain estimates of their likely effect
on emissions. In an international public
forum, governments could be held somewhat
accountable for the policies they had
or had not put into effect, but probably not
for the emissions levels achieved.

Such a modest beginning will require
finding a way to sublimate the current international
enthusiasm for a new universalist

greenhouse regime into institutional arrangements
that are helpful but noncommittal

when the U.N. Conference on Environment
and Development convenes next

June. This will require an understanding
among the developed countries that it is
initially up to them to find a way to mobilize
their populations in support of national
greenhouse policies.

Ix

A major commitment to financing emissions
abatement in the developing world is
surely too far away to need specific plans
now. A developing-world carbon-abatement
effort would, in principle, be altogether different
from foreign aid as we have known it
since World War II. In principle it would all
be directed, from whatever sources and
through whatever channels, to protecting
that same global common. There would be,
for the first time, a single criterion: economizing
carbon. In the abstract, aid recipients
in the war on greenhouse gases would
not compete; they would not make IndiaPakistan
comparisons, or Arab-Israel, or

Poland-Czechoslovakia. All would in principle
benefit equally from maximum carbon
conservation, wherever it could be achieved.
Trees may grow more rapidly, in carbon
content, in Madras or Szechuan or Borneo
or Alaska or South Carolina, but if someone
were willing to finance the growth of a tree
to absorb carbon dioxide, the citizens of
those states should not have the slightest
care where the tree were to be planted; they
all benefit solely from the carbon fixed in
the tree and benefit more, the faster the
tree grows, no matter where it grows.
It wouldn't work that way, of course.
Somebody gets the shade, or leases land for
the tree; and if it's not a tree but a nuclear
power plant to supplant coal, there are local
impacts that make huge differences, and
negotiations over sharing the cost differential
between the coal and the nuclear plants.
But it is worth noticing that if there were a
"pure" carbon-abatement or carbon-absorbing
technology, one that accomplished nothing
else, there should be no dispute about
locating it wherever it would be most effective.
That is new in foreign aid and foreign
investment.

If the developed countries ever manage
to act together toward the developing countries,
their bargaining position is probably
enhanced by the fact that cleaner fuels and
more efficient fuel technologies bring a
number of benefits other than reduced carbon,
and recipients of greenhouse aid will
be actively interested parties, not merely
neutral agents attending to the global atmosphere.
At the same time, large nations like
India and China will be aware of the extortionate
power that resides in ambitious

coal-development projects.

On a greatly reduced scale, there may be
something constructive to do more immediately.
There is a huge difference between

transferring "technology" and transferring
capital goods that embody technology or,
going further, financing entire investments


### ---Economics-1992-0-16.txt---
(local construction, etc.) in which the technology
is embedded. The difference in cost
is at least an order of magnitude. While the
developed countries are feeling their way
into some common attack on their own carbon
emissions, a tangible expression of their
interest and an effective first step would be
to establish a permanent means of funding
technical aid and technology transfer for
developing countries, as well as research,
development, and demonstration in

carbon-saving technologies suitable to those
countries. Eventually the rural Chinese
household may cook more efficiently with
nuclear-powered electricity, but for another
generation or two what is important is less
carbon-wasteful ways of cooking and heating.


Maybe there is a role here for the carbon
tax. Western Europe, North America, and
Japan will be burning 3 or 4 billion tons of
carbon per year for the next decade. Taxing
themselves, that is, contributing in proportion
to the carbon they consume, at one,
two, or three dollars per ton, they could
contribute to a fund that might begin at $3
billion per year and grow to $10 billion. The
carbon tax is a little arbitrary here, and a
U.S. administration may be wary about a
precedent that carries over when the tax
rises an order of magnitude, but compared
with alternative criteria for sharing costs it
might not even be a bad precedent.
 ## Economics-1993-0


### ---Economics-1993-0-03.txt---
I. The Allocation Task of Yesteryear
Nearly two score years ago, on the occasion
of Columbia's bicentennial celebration,
Sir Dennis Robertson gave an address entitled
"What Do Economists Economize," the
burden of which was that, since presumably
economists are the most expert economizers,
they should economize the most precious
thing in the world, namely, love, or
altruism. This would be done in part by so
arranging things that in the ordinary conduct
of life individual choices made on the
basis of self-interest in terms of market
prices would at least be consistent with maximizing
social welfare, so that the exercise
of scarce resources of altruism could be
concentrated on situations where Adam
Smith's unseen hand could not be made to
serve. To me, one implication of this was
that economists should see to it that market
prices correctly reflect the relevant marginal
social cost of various alternatives. I have
devoted a major part of my career to the
promotion of such marginal-cost pricing, but
thus far with a notable lack of practical
success outside academia.

At the time of Robertson's address, indeed,
there was a certain euphoria prevailing
among at least part of the economics
profession over the prospect of curbing the
business cycle and maintaining a high level
of economic activity through Keynesian fiscal
policy. Under these circumstances it was
reasonable to think that the chief remaining
job of the economist was to assure a
Pareto-efficient allocation of a given aggregate
of resources. The event, however,

proved otherwise. The conventional wisdom
of regarding budget deficits as improvident
prodigality, and government debt as the
legacy of a craven deferral of burden to the
future, resumed command.

One eminent economist is said to have
remarked, in effect, that it was the function
of the science of public finance to see to it
that nothing of importance is ever done or
left undone merely for financial reasons.
Alas, the financial reasons have thus far
carried the day, and we have not had anything
approaching real full employment

since the Korean War, or indeed in peacetime
at any time since 1925, if then, at least
in terms of the Beveridge definition of full
employment as a situation wherein there
are at least as many unfilled job openings as
there are unemployed individuals seeking
work.

In the Eisenhower years, the conventional
wisdom held sway in spite of the

absence of serious contraindications to the
Keynesian prescription. In the 1960's, the
simple Keynesian analysis began to be called
into question by the emergence of stagflation,
a phenomenon not contemplated by

the earlier Keynesian models. A new relationship,
the Phillips curve, relating the

evolution of inflation to the level of unemployment
was added to the economists'

armamentarium, with its "non-inflationaccelerating
rate of unemployment" or

NIARU.

This NIARU is of course not a fixed
datum, but varies over time and place according
to the sociopolitical ambience, the
mechanics of the labor market, and the
vigor of competition. It may have been rising
over time as a result of the increased
sophistication and differentiation of products,
real and factitious, giving sellers, as
the ones most knowledgeable about the
characteristics of their products and their
markets, considerable leeway to raise their
prices without unacceptable loss of sales.
This process is ultimately held in check only


### ---Economics-1993-0-04.txt---
by the presence of underutilized labor and
other resources. Currently in the United
States the NIARU appears to be around
4-6 percent.

In some quarters this NIARU has even
been termed the "natural" rate of unemployment,
in one of the most vicious euphemisms
ever coined. Some have even gone

so far as to define "full employment" as
being the NIARU. But while 5-percent unemployment
might be barely tolerable if it

meant that everyone would be taking an
additional two weeks of vacation every year
without pay, it is totally unacceptable as a
social goal when it means unemployment
rates of 10, 20, or even 40 percent among
disadvantaged groups, with resulting increases
in poverty, homelessness, poor

health, drug addiction, and crime. Yet the
hard political fact is that at such a NIARU
the great majority of the voting population,
including most of the politically active upper
and middle classes, will have relatively
little personal experience of severe unemployment,
while nearly everyone will have

some direct experience of inflation. Many
seem to feel that if only prices would stop
rising they would benefit correspondingly by
having their income go further, giving relatively
little thought to the effect on their
incomes. Even those with large mortgages
or other debts, who would actually gain
from inflation, tend to concur in the notion
that they suffer from it. It is thus extremely
difficult to get political support for antiunemployment
measures that are perceived

as involving a threat of inflation, at least
until unemployment reaches 7 percent or
more, at which point unemployment becomes
a more widespread threat.

Actually it is the uncertainty as to the
rate of inflation, and not its level, that does
the damage. An assured, moderate rate of
inflation can be adapted to by adjusting
nominal rates of interest and the terms of
long-term contracts involving money payments.
The "menu cost" of changing price

tags and catalog quotations is probably less
important than the mental effort required
of consumers in forming an idea of what an
appropriate current price is for infrequently
purchased items, such as furniture or clothing.
An inflation rate assured to stay between
5 percent and 6 percent, say, might
even have advantages. Monetary policy
would be more powerful in stemming a
downturn in that very low and even negative
real rates of interest would become feasible
as a stimulus to investment. It might in
principle be easier to keep inflation within a
1-percent range between 5 percent and
6 percent, than to keep it within a 2-percent
range between - 1 percent and + 1 percent,
given the smaller real value of noninterest-
bearing moneys in circulation, even
allowing for the superior political focusing
power of a target of 0 percent as compared
to one of 5.5 percent.

The base of the income tax would be
broadened also, making it possible to have a
tax that is more progressive and more productive
of revenue with lower marginal rates
and less of a distortionary effect. A tax
based on nominal accrued income would in
effect be a tax on a base consisting of real
income plus a percentage of net worth.
While this is not what is meant by an ideologically
pure income tax, in terms of its

practical effects it can be deemed a superior
tax.

It is the possibility of substantial changes
in the rate of inflation, either up or down,
that does the damage. Such changes involve
a disappointment of expectations and a redistribution
of wealth and income derived

from a given national product that is capricious
and often inequitable, but it does not
of itself substantially reduce the amount to
be distributed. Unemployment, on the other
hand, directly and definitely reduces the
total product to be distributed. Unanticipated
changes in the rate of inflation, up or
down, may be considered to be a form of
legitimized embezzlement, whereas unemployment
is vandalism.

Nevertheless, the stance of the politicofinancial
establishment is still to look at the
bottom line as the ultimate reality, whether
of the corporation or the national budget,
and since money is the measure of all good
and evil in this kind of calculus, anything
that impugns the value of money is viewed
as a kind of sacrilege reinforced by a lurking
fear of starting down a slippery slope to


### ---Economics-1993-0-05.txt---
hyperinflation. We find the Federal Reserve
System poised to slam on the brakes at the
first sign of a resurgence of inflation, a
posture not calculated to inspire investment
in durable capital.

On the political side, we see the House
voting by a substantial majority in favor of a
constitutional amendment to require a balanced
budget, fortunately falling short of
the required two-thirds. This was done in
spite of the fact that the nominal budget as
currently computed is not a valid measure
of any significant economic quantity. The
nominal deficit would be reduced by selling
the Pentagon to a life insurance company
subject to a long-term lease-back and repurchase
option; this at least would do no

harm, unlike the sale of natural resources to
private exploiters which would actually decrease
the real heritage handed down to the
future, on the pretext of reducing the transfer
requirements embodied in the national
debt.

II. Recycling Savings Through Public
Capital Formation

From a classical standpoint, of course,
the difficulty is that no account is taken
of the distinction between transactions on
current account and on capital account. If
AT&T, General Motors, and households
had been constrained to operate under the
restrictions of the proposed balanced-budget
amendment, we would now have far fewer
telephones, automobiles, and houses. A
capital budget, with a vast expansion of
government capital outlays on roads,
bridges, research, education, and the like,
financed by borrowing, might go a considerable
way toward improving the unemployment
situation. But there is no assurance
that it could do the whole job.

III. Eliminating the Corporation Income Tax
Other classical approaches to improving
the unemployment situation exist but have
their own political opposition and in any
case are too weak to make much of a dent
in a very large need. One such measure
would be the abolition of the corporation
income tax, which is by far the most serious
hurdle in the way of private capital formation
of a kind requiring equity funding. Unlike
the capital-gains tax, the corporate income
tax is a tax largely above or before the
market, requiring a rate of return on investment
sufficient to cover the corporation tax
and leave a rate of return after tax comparable
to other investments, whereas the

capital-gains tax operates largely as a reduction
in the return to the investor after or
below the market, comparable to the reduction
of net income to the taxpayer resulting
from the personal income tax on other income.
In addition, the corporation tax

causes inefficient allocation of investment
between equity-type and loan-type investments;
it encourages thin equity and resulting
bankruptcies and reorganizations, and it
lubricates takeovers and mergers of dubious
intrinsic merit.

Reduction of the tax on capital gains, on
the other hand, might actually depress economic
activity if the additional savings out
of the tax reduction were to exceed the
additional capital formation induced. This is
the more likely in that most of the tax
reduction is likely to be saved immediately,
whereas the inducement to capital formation
is in terms of a tax reduction in a
relatively remote future, subject to legislative
vicissitudes. At best, special treatment
of capital gains greatly increases the complexity
of the tax law and diverts investment
flows from their most efficient use. There is
nothing to indicate that investments likely
to yield returns in forms defined by the tax
code as capital gains will have any superior
social value: gains from land speculation, in
particular, add nothing to the real availability
of resources.

As for the corporate income tax, in spite
of its many defects from the standpoint of
economic efficiency, it has enormous political
popularity due to the fact that nearly
everyone thinks that it is paid by someone
else. Indeed economists have differed widely
in their assignment of the "burden" of the
tax, owing to a failure to specify, or even to
consider, the macroeconomic policy changes
necessarily involved in a change in the tax.
Unlike most other taxes, the corporation tax


### ---Economics-1993-0-06.txt---
inflicts a double whammy on the economy
in that it both extracts income from the
stream of purchasing power and reduces the
recycling of savings through investment. If
imposed on a revenue-neutral basis it causes
unemployment, while if a budgetary adjustment
is made to maintain employment constant,
its burden can be thought of as falling
on future wage earners, who will have less
capital with which to work.

Problems of the deferral of income

through undistributed profits, as well as the
deferral of taxation to the time of realization
of capital gains, would ideally be met
by putting the personal income tax on a
cumulative basis, along lines I developed
while working with Carl Shoup in 1938,
whereby the deferral of the reporting of
income, by whatever means, merely involves
the borrowing of the deferred tax at a suitable
rate of interest. About two-thirds of
the internal revenue code would become
redundant, with the possible exception of
the need to deal with the international jet
set and revolving-door marriages; large
numbers of tax techies would be able to
apply themselves to more productive employment.


Failing this, an approximation to a level
playing field might be had by imposing a
small annual tax on the accumulated undistributed
surplus of corporations, roughly

equal to the interest on the stockholders'
postponed individual income tax. Similarly,
there should be a surcharge on realized
capital gains, proportionate to the length of
time held, to offset the gain from the deferral
of the tax.

If there is nevertheless a need to cater to
a political demand for something that can
be labeled a corporation tax, this might be
satisfied by levying a corporation tax on
dividends, interest, and retained earnings at
a rate corresponding to the first-bracket rate
of the individual income tax and exempting
such interest and dividends from this "normal"
rate, going back to the pre-1934 practice
of dividing the income tax into a normal
tax and a progressive surtax. To even things
up neatly, normal tax paid on other forms of
income should be deductible in computing
the base for the progressive surtax paid by a
minority of taxpayers. It would still be appropriate
to have an undistributed surplus

tax to correspond to this surtax.

IV. Tax-Exempt Bonds

Another measure that might slightly improve
investment allocation would be to replace
the exemption of interest on state and
local bonds by a taxable tax credit at a rate
that would maintain the market value of the
bonds. Low-bracket taxpayers would be little
affected, while the entire loss of revenue
to the Treasury would accrue as a subsidy to
the issuers. Upper-bracket taxpayers would
no longer have an incentive to invest in such
bonds rather than in riskier investments
more suitable to their status.

V. Taxing Imputed Income

A more important but politically more
difficult measure would be to require the
inclusion in taxable income of the rental
value of owner-occupied residences. This
would not only improve the equity and progressivity
of the income tax but go a substantial
way toward making more units

available for rental and, to a modest extent,
promoting the construction of additional
affordable rental housing and abating the
problem of homelessness. A similar case
can be made for including in the income tax
base a net rental value of nonbusiness automobiles
(equal to interest on the market

value of the car), in this case reducing the
discrimination against the use of public
transit.

VI. Shifting Property Taxes from

Improvements to Land

A measure that could provide a powerful
stimulus to investment in property improvements
would be to replace part or all of the
property tax by a tax on land value only, a
proposal that can be traced all the way back
to Franqois Quesnay and the French physiocrats
but which is more recently associated
with the name of Henry George. This


### ---Economics-1993-0-07.txt---
would remove the very serious deterrent
effect of the property tax on improvements.
Unfortunately from the standpoint of a national
employment policy, this tax is largely
levied by local governments, which are often
constrained by constitutional provisions or
state laws. Nevertheless some means of
bringing pressure to bear on these governments
to make this change might be found.
Some Pennsylvania governments are already
doing this. When levied for municipal purposes,
it might be appropriate to exempt

from the tax a flat amount per square foot
as representing the value of circumambient
agricultural land for which the urban government
can claim no credit; this would also
mitigate discriminations at jurisdictional
boundaries.

It is perhaps worth noting that the significance
of a government debt would be drastically
different in a community relying exclusively
on a land-value tax. Such a debt

would in effect be a collective mortgage on
the land, especially if it can be assumed that
land values in the community will vary proportionately
over time. Since the interest on

the community debt will generally be lower
than interest charged on individual mortgages,
it can be in the general interest of all
the taxpayers of the community for the government
to borrow as much as the market

will take, even to finance current outlays,
provided a suitable margin is left to deal
with emergencies. On the other hand such
debt financing performs no recycling of savings,
there is no room for Keynesian fiscal
policy, and Ricardian equivalence is in full
sway. This does not detract, however, from
the powerful stimulating effect of a reduction
in the tax on improvements.

VII. Limitations of "Supply-Side" Measures
Under current conditions, however, such
"supply-side" measures designed to operate
by reducing the cost of capital are likely to
be severely limited in their effect as long as
nearly all types of capital facilities are idle
or underutilized. Very little "widening" investment
is likely to take place as long as

there is excess capacity in place. At most,
some "deepening" investment in new products
or technologies may take place, or there
may be corners of the economy where relatively
rapid growth has kept capacity fully
utilized. Even in such cases, investment in
capital facilities may depend more on appraisals
of an uncertain market for the

product than on the cost of capital.
This is likely to be true not only of tax
policy but even more of monetary policy. In
any attempt to emerge from present rates of
unemployment even only down to the

NIARU within any reasonable time period,
monetary policy is likely to prove a weak
reed, sometimes aptly described as pushing
on a string. The main difficulty is that monetary
policy bears primarily on short-term
interest rates and credit availability and in
its usual practice does not directly control
long-term rates, which are the important
rates for most decisions involving real
durable capital formation. The posture of
the Federal Reserve System in holding itself
ready to slam on the brakes at the first sign
of resurgent inflation is poorly adapted to
bringing long-term rates down. It does not
appear that the Fed has either the will or
the resources to do enough about long-term
rates to do very much to increase capital
formation, especially when idle and underused
capacity pervades much of the economy.


VIII. Savings Recycling by Government
This brings us inevitably around to fiscal
policy. Here it is necessary to stop thinking
of the conventional nominal budget deficit,
or even of a current-account deficit in a
budget drawn up in terms of distinguishing
capital and current-account items, and to
start thinking of fiscal policy in terms of its
role in recycling savings, in excess of what is
recycled by private investment, into the
stream of purchasing power. The conventional
wisdom seems to argue that increased
employment requires that the economy
grow, growth requires investment, and investment
requires savings; therefore let's

encourage saving through IRA's, tax expenditure
rather than income, and tighten our


### ---Economics-1993-0-08.txt---
belts to restore the economy to its normal
state of health.

It doesn't work that way. Savings are not
like a sack of potatoes which if not sold at
the current price will stay on hand and put
a downward pressure on the price until sold.
Savings not immediately taken up to create
capital simply vanish in reduced income,
without even exerting a downward pressure
on interest rates. If I yield to the allurements
of tax concessions to IRA's to the

point of not having my hair cut, this puts $8
more in my bank account, but $8 less in the
barber's account; there is nothing that makes
it any easier for anyone to obtain funds with
which to create capital, nor anything that
makes the prospect more attractive. As
Gertrude Stein remarked, "the money is
always there, it's the pockets that keep
changing." If the barber reacts by curtaling
his consumption, this further reduces national
income and saving. I may succeed in
my attempt to save, but only by reducing the
saving of others by even more. Savings are
an extremely perishable entity. Say's law
fails as soon as part of the income generated
in the process of producing the supply
is shunted off into savings that fail to get
converted into new capital goods.

On the other hand, if some genius invents
a new product or process and obtains a
credit or borrows the funds needed to finance
the capital involved in its production,
this added real wealth is, ipso facto, someone'
s saving. Instead of Say's law, we have
"capital formation creates its own saving."
Similarly, if the government borrows funds
created by credit expansion and recycles
them into purchasing power through outlays,
whether on current or capital account,
this creates both income out of which additional
savings will be attempted and demand
that may induce the private investment
to meet it.

Not all deficit financing, however, results
in recycling of savings, whether measured by
the current capriciously defined nominal
deficit or by a more rational definition involving
accounting for government assets.

We have seen that in a community relying
exclusively on a land tax, recycling does not
take place. Nor would the sale of the Pentagon,
or the purchase of an office building
currently being rented by the government,
offset by bond transactions, involve any
change in the level of recycling. Government
recycling is in principle the excess of those
government outlays that are regarded by
their recipients as income over those government
receipts that are regarded by their
payors as reductions in their disposable income.
Even this is subject to some caveats: if
government investment in a power plant, for
example, substitutes for investment that
would otherwise have been made by private
enterprise, there is no net recycling.
On the whole, however, recycling tends to
vary in rough correlation with the nominal
deficit, and the strength of the notion in the
minds of the public and their representatives
that deficits are bad and that the

"budget" should be balanced may make it
difficult to achieve an adequate level of
recycling. Some help in this respect may be
obtained by going to a capital budget system,
in which balance would be sought only
for the current-account part of the budget,
borrowing for the capital account being justified
by comparisons with corresponding

private practices and by the thought that
future generations being burdened with the
debt would also reap benefits from the capital
passed on to them. While this may constrain
choice away from what rational voters
would have chosen as the optimal level of
government capital formation, there would
seem to be sufficient scope for government
capital investment to provide sufficient recycling
to bring about full employment, particularly
if investments in education, research,
space exploration, and the like are considered
eligible for treatment as capital investment.
Some of these projects, even if they
would not stand scrutiny aside from their
function in justifying income recycling, may
nevertheless have the same kind of justification
as the building of the Egyptian pyramids
had for Keynes. On general welfare

grounds, one might well prefer recycling in
terms of borrowing to finance health care to
borrowing to finance space stations, but if
borrowing for health care is deemed to create
an ideologically sinful current-account
deficit, space stations it will have to be.


### ---Economics-1993-0-09.txt---
IX. The Need for Direct Inflation Control
Long before the economy reaches a really
satisfactory level of full employment, however,
as employment gets to the NIARU

level, and inflation threatens to accelerate,
the Fed is likely to try to slam on the
brakes, and demands for a more stringent
budget balancing and cutback of "government
waste" are likely to be heard in the
halls of Congress. To get anywhere near a
satisfactory level of unemployment, some
method of dealing with inflation will have to
be devised. We are short of tools.

In effect, the economy can be thought of
as having three major parameters that we
would like to control: the level of employment
of human and other resources, the

price level, and the division of the resulting
total product between provision for current
wants and investment in growth and the
future. At the same time, we have only two
major policy tools: monetary and fiscal policy.
In an era when inflation was not a

threat, one could think of these two tools as
controlling the level of employment and the
rate of growth, with low interest rates combined
with a deficit or surplus sufficient to
maintain full employment leading to high
investment and growth, and conversely.
However, with a need to control inflation as
well, relying on only two dimensions of control
is like trying to fly an airplane without
ailerons, which were the third dimension of
control that was the key to the success of
the Wright brothers. A new tool is needed.
Over the past three decades a number of
proposals for direct control of inflation have
been made, but none has achieved general
acceptance. Wartime control of specific
prices, accompanied by rationing, was accepted
as an emergency measure and

worked in part because of patriotic willingness
to conform and in part because, being
temporary, past prices could be continued
without becoming absurd. As a permanent
scheme this is probably unworkable

and certainly unacceptable. More recent
schemes have involved tax incentives of
various kinds to provide a countervailing
downward pressure against the inherent
inflationary tendency of an imperfectly
competitive system. Such schemes have
generally suffered from difficulties in measuring
price changes at an individual-firm
level, capriciousness of results when tied to
such taxes as the corporation income tax,
and possible time lags in adjusting the
strength of the incentives to changing circumstances.


X. Market-Based Inflation-Control Plans
A few years ago David Colander came to
visit me and reported on a proposal by
Abba Lerner for a market in rights to raise
prices. Those wishing to raise their prices
would be required to purchase the right
from those prepared to lower their prices,
thus assuring a constant overall price level.
While this neatly circumvents the problem
of adjusting the strength of incentive to
changing inflationary pressures, the problem
remains of how to measure price

changes in the face of quality changes, new
products, and variations in the terms of sale
such as delivery, reliability, service, credit
terms, tie-in sales, and the like.

More pregnant was the question of how
to deal with cases in which prices paid to
suppliers have risen. A somewhat similar
problem arises with gross receipts taxes,
which discriminate in favor of vertically integrated
operations and against situations in
which the product passes through several
hands on the way to the market. In Europe
this problem has been solved by shifting
from gross receipts taxes and retail sales
taxes to value-added taxes, which immediately
suggests that instead of a market in
rights to raise prices we have a market in
rights to value added.

XI. Control with Marketable Gross

Markup Warrants

For semantic reasons I have chosen to
speak in terms of "gross markups" rather
than value added, as being more suggestive
of something to be restrained rather than
promoted. In principle, gross markups simply
refers to the excess of sales revenue over
amounts paid for nonprime inputs. In operation,
warrants for gross markups for a


### ---Economics-1993-0-10.txt---
prospective accounting period would be issued
to each firm on the basis of the gross
markups for a corresponding preceding period,
plus or minus adjustments for changes
in prime inputs such as labor and invested
capital. These warrants would be issued in
sufficient total face value to correspond to
the value at a desired price level of the
output expected to be produced by the inputs
against which the warrants were issued.
They would be freely tradable for cash in a
competitive market, and if at the end of the
accounting period a firm is found to have
retained or acquired fewer warrants than
the actual amount of its gross markups for
the period, a penalty tax would be assessed.
This tax would not be a substantial source
of revenue, but would serve merely as an
enforcement device. It could be set at a
level fairly certain to be higher than the
market price of the warrants.

Adjustment of the warrant issue for
changes in investment could be made simply
on the basis of a uniform percentage of
such change. Adjustment for changes in employment
is somewhat more difficult: a flat

amount per employee or man-hour takes
too little account of variations in qualifications,
while to allow adjustments equal to
payrolls would run a danger of allowing
inflationary wage increases. Some formula
such as a percentage of payrolls plus a flat
amount per employee might be satisfactory;
such a formula would involve a certain bias
in favor of the employment of low-skill labor,
which may be considered desirable in
view of the fact that this is where the unemployment
problem is most serious.

Administration would seem to pose no
insurmountable problems. Determination of
gross markups is essentially no different than
the assessment of a value-added tax such as
is widespread in Europe. Adjustment for
investment can be made on the basis of
accounts already needed for income-tax
purposes, while adjustments for employment
can be related to the social-security
records. Some special methods may have to
be developed for dealing with the selfemployed
and very small firms, and possibly

some classes of firms could be excluded
from the scheme, as is sometimes done with
the value-added tax.

XII. Prospects for Rapidly Reaching
Genuine Full Employment

With such a scheme in place, what can we
plan for in terms of getting from where we
are to full employment? Currently unemployment
is reported as about 7.5 percent,

and full employment can be reckoned at
about 1.5 percent, giving a slack to be made
up of 6 percent. Using Okun's ratio of percentage
change in GNP to percentage

change in reported unemployment of 2.5,
we have a slack of 15 percent to be made
up. If this slack can be taken up within two
years, this will be 7.5 percent per year; if to
this we add 2.5 percent for growth in the
labor force and in productivity, we get 10-
percent annual growth in GNP over two
years. After two years, we hit the fullemployment
ceiling, and growth thereafter

will be limited to the labor force and productivity
factor, possibly between 2 percent

and 4 percent.

Is public finance up to the job of reaching
the goals thus defined in terms of the limits
of our real resources? Possibly, but it requires
breaking new ground. One would

have to begin with increasing government
recycling as rapidly as possible by 8-10 percent
of GNP in order to inaugurate the

1.0-percent growth rate. How rapidly this
could be done would of course depend on
the political and legislative ambience. From
some points of view the fastest and easiest
way to do this is by tax cuts. Unfortunately,
if tax cuts are temporary they tend to be
viewed as windfalls to be saved rather than
spent, so that only part of the tax cuts are
effectively recycled. Alternatively, if not announced
as temporary, tax cuts tend to create
a resistance to later tax increases called
for by full-employment conditions and large
debt-service requirements. This is especially
threatening in the present context of political
campaigning on the basis of promises of
no new taxes. Perhaps the best tax cut would
be a cut in the payroll taxes, as promising
the maximum proportion of recycling, if this


### ---Economics-1993-0-11.txt---
can be done in the face of outcries that this
would be jeopardizing the financial soundness
of the social-security system.

Outlays on actual programs, on the other
hand, are somewhat harder to start and
stop rapidly. There is also the need not to
get too far ahead of the effective operation
of whatever anti-inflation program is put in
place, whether the program of gross markup
warrants proposed above or some other, lest
anticipatory speculation and inflation get
out of hand. The exact program for the
start-up period will require careful study.
What happens after the first few months
will depend to a large extent on what Keynes
called the "animal spirits" of the financial
community. At one extreme there could be
such horror and alarm at the violation of
the conventional wisdom concerning the
sinfulness of deficits as to produce a
widespread hibernation and flight to foreign
shores. More likely, once the financial community
has become convinced of the seriousness
of the administration's purpose to

bring about full employment, and once it is
anticipated that demand will shortly use up
the spare capacity of existing productive
facilities, private capital formation may pick
up to the point of absorbing and recycling
individual savings sufficiently so that government
recycling may for the time being

become unnecessary. At the same time, government
revenues from increased GNP will

increase and outlays for unemployment insurance
and welfare will decrease. Also,

there may be a need to shut down those
governmental programs that compete for
real resources with private capital formation,
in order to avoid a real "crowding out"
(as contrasted with the financial crowding
out alleged to occur as a result of government
borrowing associated with a tax cut).
As a result, a brief period of budget balance
or even of surplus may become appropriate.
As the economy hits the ceiling of full
employment, however, still another transition
becomes necessary. For a while capital
formation may continue on its momentum,
recycling savings but producing excess capacity
that either cannot find labor with

which to operate or cannot find markets in
which to sell its product. Within a short
time after hitting the full-employment ceiling,
capital formation will have to drop from
that appropriate to a 10-percent growth rate
to that suited to a far slower growth rate. At
this point attempted savings may again exceed
what can be absorbed by private capital
formation, even at very low rates of
interest. Other ways to recycle the excess
will again become necessary, one of which
will be renewed government recycling.
XIII. Long-Term Excess of Demand Saving
over Private Investment

There is, indeed, no principle of economics
that says that there will always be a
feasible rate of interest that will equate
desired savings and private capital-formation
under conditions of steady full employment.
Current trends seem to be such as to
make such a possibility unlikely. One factor
has been a spate of capital-saving innovations
and practices. Fiber optics, when fully
utilized, costs less per unit of service than
previous technologies by orders of magnitude,
leaving ductways planned for copper
conductors forever surplus; electronic exchanges
occupy a fraction of the space

formerly required by equivalent electromechanical
exchanges; just-in-time practices

reduce investment in inventory; improved
communications enable more freight to be
carried on a single track line with sidings
than was formerly carried by a full two-track
line; a man assembling electronic gear with
a soldering iron uses far less capital than
the man in the pulpit of a rolling mill, and
service industries generally use less capital
per employee than manufacturing, mining,
or transportation.

Moreover, before gross investment can
begin to recycle private savings, it must first
recycle funds set aside in depreciation,
amortization, depletion, and obsolescence
charges, while rapid obsolescence due to
accelerating technological progress makes
capital formation relatively insensitive to
changes in interest rates. Very low or negative
interest rates may stimulate investment
in nondepreciating assets such as land, but


### ---Economics-1993-0-12.txt---
even this is limited by the possibility that
speculative bubbles may burst, and in any
case relatively little recycling is produced
thereby, except to the extent that the enhanced
asset values cause owners to feel

wealthier and spend more.

On the savings side, increased longevity
and the high cost of old-age illness lead to
increased savings through funded pensions
and other provisions for retirement. For this
purpose, the lower the rate of interest, the
greater is the amount of current savings that
must be put aside to provide a given level of
retirement security. More recently the increased
concentration of income among the

very wealthy, who have a high propensity to
save, not so much for eventual consumption
but largely to accumulate chips with which
to play financial games and exercise economic
power, has further added to the savings-
recycling problem. Some recycling may
take place through investment abroad, reflected
in a positive trade balance and the
production of goods for export, though it is
uncertain how far this can be carried in the
face of political instability, the danger of
creating repayment problems, and the resistance
of foreign governments that do not

have an effective full-employment policy of
their own to our exporting our unemployment
to them in this way.

On balance, it may prove impossible, for
the foreseeable future, to maintain a steady
state of genuinely full employment without
a substantial amount of government recycling
of savings, a chronic budget deficit,
and a long-term increasing trend in the national
debt, however distasteful this may be
to those ideologically addicted to a balanced
budget. It may even prove necessary
for the debt to grow at a rate faster than the
growth of GNP. The burden of servicing
this debt might be kept within bounds by
reducing real interest rates, close to zero if
need be, though this might imply a higher
level of private investment than would be
chosen on its own merits. Even contemplating
such prospects calls for a significant
expansion in our range of habitual thought.
XIV. The Task Before Us

This, then, is the challenge I lay before
the economics profession. There is no reason
inherent in the real resources available
to us why we cannot move rapidly within
the next two or three years to a state of
genuinely full employment and then continue
indefinitely at that level. We would
then enjoy a major reduction in the ills of
poverty, homelessness, sickness, and crime
that this would entail. We might also see
less resistance to reductions in military expenditure,
to liberalization of trade and immigration
policy, and to conservation and

environmental protection programs.

I lay before you a plan I believe can
accomplish this. It involves government recycling
of excess savings plus a method of

keeping inflation under control. I believe it
can do the job while preserving the essentials
of a free-market system. There may be
some details to be worked out, but I am
confident that the basic concept is sound
and workable.

We simply cannot carry on as we have
been doing without falling apart as a community
and losing what is left of our status
of world leadership. If you don't think that
something like this can be made to work,
then it is up to us to get together to find
something that will. Otherwise, if we continue
to tie our hands with financial shibboleths
and models that tacitly assume a fixed
total of resource utilization, we are no better
than the feckless castaway whose contribution
to the solution of the problem of

dealing with cases of canned goods was "let's
just assume we have a can-opener."
 ## Economics-1994-0


### ---Economics-1994-0-03.txt---
Forty years ago economists discovered the
"residual." The main message of this literature,
that growth in conventional inputs

explains little of the observed growth in
output, was first articulated by Solomon
Fabricant in 1954 and emphasized further
by Moses Abramovitz (1956), John Kendrick
(1956), and Robert Solow (1957).1 The pioneers
of this subject were quite clear that
this finding of large residuals was an embarrassment,
at best "a measure of our ignorance"
(Abramovitz, 1956 p. 11). But by

attributing it to technical change and other
sources of improved efficiency they turned
it, perhaps inadvertently, from a gap in our
understanding into an intellectual asset, a
method for measuring "technical change."
Still, it was not a comfortable situation, and
a subsequent literature developed trying to
"explain" this residual, or more precisely, to
attribute it to particular sources (Griliches
1960, 1963a,b, 1964; Edward Denison, 1962;
Dale Jorgenson and Griliches, 1967). The
consensus of that literature was that, while
measurement errors may play a significant
role in such numbers, they could not really
explain them away. The major sources of
productivity growth were seen as coming
from improvements in the quality of labor
and capital and from other, not otherwise
measured, sources of efficiency and technical
change, the latter being in turn the
product of formal and informal R&D investments
by individuals, firms, and governments,
and the largely unmeasured contributions
of science and other spillovers. The
prescription of additional investments in education,
in science, and in industrial R&D

followed from this reading of history as did
also the hope and expectation that the recently
observed rates of "technical change"
would continue into the future.

This general view of the sources of growth
was put into doubt by the events of the
1970's and 1980's. Beginning in 1974 (or
perhaps already in 1968) productivity growth
slowed down significantly in the United
States and abroad, and it has not fully recovered
yet, at least as far as national aggregates
are concerned. The many explanations
that were offered for these events were not
very convincing (see e.g., Denison, 1979;
Martin Baily and Robert Gordon, 1988;
Griliches, 1988). As time went on and the
direct effects of the energy-price shocks
wore off but the expected recovery did not
come or came only weakly, more voices
were heard arguing that the slowdown might
not be temporary; that the energy-price
shocks just revealed what was already there
-a decline in the underlying trend of technical
change in the world economy; that the
growth opportunities that had opened up in
the late 1930's and had been interrupted by
World War II have been exhausted, reflecting
perhaps the completion of an even

longer cycle, going back to the beginnings of
this century (see e.g., Alfred Kleinknecht,
1987; Gordon, 1993a). Even more ominously,
the slowdown was blamed on diminishing
returns to science and technology in
general and the onset of widespread socioeconomic
sclerosis (see e.g., William Nordhaus,
1972, 1989; Mancur Olsen, 1982;


### ---Economics-1994-0-04.txt---
F. M. Scherer, 1983, 1986; Robert Evenson,
1984; Baily and A. K. Chakrabarti, 1988).
This is a rather pessimistic view of our
current situation, and I would like to argue
that the observed facts do not really support
it. But that will not be easy, both because
some of the "facts" are contradictory and
because our measurement and observational
tools are becoming increasingly inadequate
in the context of our changing economy.
Nevertheless, I will review some of the
evidence for such views and argue with their
interpretation. There are several possibilities
here: (i) this view is true and that is sad;
(ii) it is not true and recovery is around the
corner if not already underway; (iii) it may
be true, but whatever is or is not happening
has little to do with diminishing returns to
science or industrial R&D. Or, (iv) it may
be that we just do not know. As is the case
with global warming, we may not have an
adequate understanding of the mechanisms
producing growth or adequate data to adjudicate
whether there has or has not been an
underlying trend shift. If that is true, as is
most likely, the question arises as to why we
don't know more after years of research
done by so many good people. What is it
about our data and data acquisition structure,
and possibly also our intellectual

framework, that prevents us from making
more progress on this topic?

In discussing this range of topics, I will
concentrate primarily on the R&D component
of this story-not because it can explain
much of the productivity slowdown (it
cannot), and not just because this is where I
have done most of my recent work, but
because it illustrates rather well the major
point I want to make here tonight: that our
understanding of what is happening in our
economy (and in the world economy) is constrained
by the extent and quality of the

available data. I will also allude briefly to
similar issues which arise in interpreting the
productivity contribution of computers in
the economy. Parallel tales about data constraining
our understanding could also be

told about other potential productivityslowdown
villains: energy-price shocks, insufficient
investment in'physical capital, and
possible declines in human-capital investments.
Having reached the verdict of "not

proven," largely on account of insufficient
evidence, I shall make a number of more
general remarks on the state of our data
and the possible reasons for it. The major
message that I will be trying to convey is
that we often misinterpret the available data
because of inadequate attention to how they
are produced and that the same inattention
by us to the sources of our data helps explain
why progress is so slow. It is not just
the measurement of productivity that is affected.
Other fields of empirical economics
are also struggling against the limitations
imposed by the available data. Great advances
have been made in theory and in

econometric techniques, but these will be
wasted unless they are applied to the right
data.

I. The "Facts"

There are three sets of "facts" to look at:
what has happened to productivity, what
has happened to investment in R&D and
science, and what has happened to the relationship
between them. Sometime in the

late 1960's measured productivity growth in
the United States started to slow down.
After a mild recovery in the early 1970's,
the world economy was hit by two successive
oil-price shocks which dropped economic
growth rates in most of the developed
economies to levels significantly below
those experienced in the 1960's and early
1970's. While the effects of the oil-price
shocks wore off and real energy prices declined
to close to their earlier levels, productivity
growth rates did not recover much.

At this point, and also somewhat earlier,
many observers started wondering whether
something more fundamental than just an
energy-price-shock-induced business cycle
was afoot. Standing in the early 1980's and
looking back at the recent past, one would
have observed a decline in total patents
granted in the United States beginning in
the early 1970's and a decline in the share
of GNP being devoted to industrial R&D
starting in the mid-1960's, the timing looking
suspiciously appropriate for declining
productivity growth rates 5-10 years later.


### ---Economics-1994-0-05.txt---
One could also see a continuous and worrisome
decline in the number of patents received
per corporate R&D dollar (see below)
. But there were also many other events
clouding this picture, making one wonder
whether faltering R&D and scientific efforts
are really the culprits behind our current
woes.

A number of discordant facts are important
for an understanding of what happened.
First, the productivity-growth decline
in many other countries was larger,
absolutely, than in the United States, and
there it was not associated with declines in
R&D investment.2 Second, as illustrated in
Figure 1, the sectors where the productivity
slowdown has persisted in the United States
are largely outside of manufacturing, communications,
and agriculture (see Gordon,

1987). Besides mining and public utilities,
which were affected more specifically by the
energy-price shocks, it has lingered particularly
in construction, finance, and other services
where output measurement is notoriously
difficult. Third, the decline in patent
grants in the 1970's was just a bureaucratic
mirage, an example of fluctuations induced
by changes in the data-generating process (a
budgetary crisis in the Patent Office) rather
than a reflection of the underlying activity
itself.3 The number of patent applications
did not decline significantly during this period,
but also it did not grow. The latter
fact, coupled with a continuous upward
growth in the absolute level of companyfinanced
R&D, resulted in a persistent decline


### ---Economics-1994-0-06.txt---
in the patents per R&D ratio in the
United States (and also in most of the other
countries for which we have data). This
raised the specter of diminishing returns to
R&D and offered the hypothesis of "exhaustion
of inventive opportunities" as a

potential explanation for the productivity
slowdown.

This hypothesis has been examined recently
by various authors. There are basically
two styles of analysis: one focuses directly
on the link, if any, between R&D and
productivity growth (see e.g., Griliches,
1986a; Bronwyn Hall, 1993; Scherer, 1993),
while the other uses patents as indicators of
the output of the R&D effort and looks at
what has happened to the "knowledgeproduction
function" (see e.g., Griliches,

1990; Ricardo Caballero and Adam Jaffe,
1993; Robert Evenson, 1993; Samuel

Kortum, 1993). The bridge that is missing
between these two approaches would examine
the units in which patents affect productivity
growth and ask whether they have

stayed constant over time. Without such
constancy, no clear interpretation is possible.


II. Productivity Growth and the Role of R&D
In parallel to the aggregate "residual"
literature, a more micro-oriented approach
had developed. It took the study of technical
change, diffusion, and the role of formal
R&D as its main challenge, with the hope
of bringing more of it within the realm of
economic analysis, helping thereby also to
explain some of this residual away. Using
modern language, one can interpret Edwin
Mansfield's and my own early work on diffusion
and on the role of R&D in agriculture
and manufacturing as trying to endogenize
as much of technical change as was

possible (Griliches, 1957, 1958, 1964;
Mansfield, 1961, 1965). Other important
contributors to this literature were Richard
Nelson, Scherer, Jacob Schmookler, and
Nestor Terleckyj. By expanding the notion
of capital to include also R&D capital and
estimating its effects, this literature documented
the contribution of public and private
investments in R&D and their spillovers
to the growth of productivity.4 But the magnitude
of the estimated effects was modest,
not enough to account for the bulk of the
observed residual or the fluctuations in it
(Griliches, 1988). The experience here was
similar to other attempts to account for the
residual, such as using "embodiment" theories
to magnify the potential effects of capital
accumulation (Denison, 1962; Nelson,
1962) or looking for increasing returns to
scale (Griliches and Vidar Ringstad, 1972).
These various effects are real and nonnegligible,
but not large enough.

There is one other way of trying to make
something more out of the R&D story: the
possibility that the productivity impact of
R&D has declined over time-that the coefficients
have changed. This hypothesis has

been investigated repeatedly by a number of
researchers with mixed results. Studies that
used data through the 1970's and early
1980's found no decline in the relevant coefficients.
More recent studies that analyze

data through the late 1980's report more
mixed results, varying strongly with how the
computer industry and its deflator are handled
in the analysis.5 At the same time, the
stock market's valuation of R&D fell significantly,
both in terms of ex post returns to
4This literature has been surveyed in Griliches (1979,
1991), Jacques Mairesse and Mohamed Sassenou
(1991), Wallace Huffman and Evenson (1993), and M. I.
Nadiri (1993).

5As reported in Griliches (1986a), I found no signif-
icant decline in the relevant coefficients through the
mid-1970's. Frank Lichtenberg and Donald Siegel
(1991) replicated and extended this work to the early
1980's and found increases in the relevant coefficients
through 1985. B. Hall (1993) updated and extended the
Griliches and Mairesse (1984) study of publicly traded
U.S. manufacturing firms to the end of the 1980's and
found that the R&D coefficients came close to disap-
pearing in the 1970's and early 1980's but recovered in
the late 1980's to about half or more of their original
size. Her result is very sensitive, however, to the partic-
ular deflators used in constructing the output measure.
When separate industry-level deflators are used, in-
cluding the newly revised deflator for the output of the
computer industry, there is no evidence of a decline in
the "potency" of R&D at all; the estimated coefficients
rise rather than fall. See also Englander et al. (1988)
Pari Patel and Luc Soete (1988), Sveikauskas (1990),
and Scherer (1993).


### ---Economics-1994-0-07.txt---
R&D in the 1980's (Michael Jensen, 1993)
and the market's view of current R&D investments
(Bronwyn Hall and Robert Hall,

1993; B. Hall, 1993).

My own recent foray into this type of
analysis of industry data at the three-digit
SIC level is summarized in Table 1.6 It
reports estimates from regressions of growth
rates in total factor productivity (TFP)
on the rate of investment in R&D (the
R&D-sales ratio), where the estimated coefficient
can be interpreted as the excess

gross rate of return to R&D (Griliches,
1979). The earlier 1958-1973 period yields
an estimate on the order of 0.33, while the
estimate for the later 1973-1989 period even
rises a bit, to 0.36. So far, so good! But
when one excludes the outlier computer
industry (see Fig. 2) the estimated coefficient
falls from 0.36 to 0.13 for 1973-1989
and even lower for 1979-1989. Only one
observation out of 143 does this!7

These results raise a major data conundrum:
is it right to treat the computer in-


dustry as an outlier and exclude it from
such calculations just because the productivity
measure may be better there? It is quite
possible that if other technologically advanced
industries (such as instruments,

communications equipment, and pharmaceuticals)
had their price indexes adjusted

in a similar fashion, Figure 2 would look
much better, with the computer industry not
being as much of an outlier and with the
whole period showing much higher (social)
returns to R&D. That this is indeed the
case can be seen in Figure 3, where only
three such adjustments are made, but before


### ---Economics-1994-0-08.txt---
I discuss it, I need to digress briefly
and remind you about the developments in
computer price measurement.

Quality change is the bane of price and
output measurement. Until 1986, computer
prices were treated as unchanged in the
national income accounts. It took 25 years
for the recommendations of the Stigler
committee (Griliches, 1961; National Bureau
of Economic Research, 1961) to have a
noticeable effect on official practice, but
when they did, they did it with a bang! In
1986 the Bureau of Economic Analysis
(BEA) introduced a new computer price
index, based on hedonic regression methods,
into the national accounts and revised
them back to 1972 (Rosanne Cole et al.,
1986).8 This index was falling by about 15
percent per year or more (as compared to
the assumed value of zero before), and that
had several major implications, including
the fact that it made the apparent recovery
in manufacturing productivity in the 1980's
much stronger, about one-third of the total
coming from the introduction of this price
index alone (Gordon, 1993b).

There was nothing wrong with the price
index itself. It was, indeed, a major advance,
and the BEA should be congratulated for
making it, but the way it was introduced
created some problems. First, it was a
unique adjustment. No other high-tech
product had received parallel treatment, and
thus it stuck out like a sore thumb. This had
the unfortunate consequence that the productivity
growth in the computer industry

itself was seriously overestimated, because
some of its major inputs, such as semiconductors,
were not similarly deflated. Second,
it was introduced into a framework with
fixed weights, wrecking havoc on it. Using
fixed 1982 weights and a sharply falling price
index implied the absence of a "real" computer
industry in the early 1970's and a very
rapid growth in its importance, leading to a
more than doubling of the share of machinery
in total manufacturing output by the
0.15 -

0.10- 4

v ~~~~~~+

0.05-

0.00-+

-0.05 -

0.000 0.025 0.050 0.075 0.100 0.125
R&D-Sales Ratio in 1984

FIGURE 3. REVISED TOTAL-FACTOR-PRODUCTIVITY
GROWrH (PER ANNUM), U.S. THREE-DIGIT
MANUFACTURING INDUSTRIES, 1978-1989
Note: Computers adjusted downward; electronic components
and drugs adjusted upward.

late 1980's. This last problem has largely
been solved recently with the introduction
of "benchmark-weighted" estimates of gross
domestic product (GDP) and the moving
away from fixed-weights national income accounting
(Allan Young, 1992). But the first

problem, the uniqueness of this adjustment
in the face of similar, though perhaps not as
extreme, problems elsewhere remains to
haunt us.

What I have done in Figure 3 (and in row
3b of Table 1) is to adjust the estimated
TFP growth in the computer industry downward
by deflating materials purchases in
this industry, which to a significant extent
consist of purchases of other computer components
and semiconductors, by the same

output price index. I have also substituted a
similar price index in the semiconductors
(electronic components) industry and also
adjusted the growth of TFP in the pharmaceuticals
industry upward to reflect the

exclusion of price declines due to the introduction
of generics in the current measurement
procedures. (I shall come back to

discuss this last adjustment later on.) So
adjusted, Figure 3 does not look all that
8For historical background on these developments
see Jack Triplett (1989) and Ernst Berndt (1991 Ch. 4).


### ---Economics-1994-0-09.txt---
bad, and row 3b in Table 1 indicates no
decline in the R&D coefficient even without
the computer industry.

What is one to make of these conflicting
stories? It seems that the observed decline
in the R&D coefficients did not begin seriously
until the latter half of the 1970's, with
the second oil-price shock and the rise in
the dollar exchange rate. The abruptness of
the decline argues against a "supply-side"
explanation in terms of exhaustion of inventive
opportunities. It is more likely that the
peculiar aggregate shocks of that time went
against R&D-intensive industries: first, because
they hit energy-intensive industries
such as chemicals and petroleum refining
more severely; and second, because the subsequent
rise in value of the dollar and the
expansion in imports that followed hit some
of the more high-tech R&D-intensive industries
even harder, leading to declines in
"competitiveness," losses of rents, and the
appearance of excess capacity. The subsequent
rise in the R&D coefficients (if it did
in fact occur), the rise in corporate R&D
investments through most of the 1980's, and
the rise in patenting in, the late 1980's (as
we shall see), all argue against interpreting
these coefficient movements as reflecting
"real" declines in the once and future
"potency" of R&D. What did happen,

though, was a sharp widening of the differential
between social and private returns to
R&D. The internationalization of R&D, the
rise in the technical and entrepreneurial
skills of our competitors, and the sharp rise
in the dollar exchange rate in the mid-1980's,
all combined to erode, rather rapidly, the
rents accruing to the earlier accumulated
R&D capital and to the technical-expertise
positions of many of our enterprises. This
rise in the rate of private obsolescence and
the fall in the "appropriability" of R&D led
to sharp declines in both profitability and
real product prices. The latter, if they were
actually reflected in the appropriate price
indexes, would show up as an increase in
productivity, rather than a decline.
Before accepting this inconclusive verdict,
one still has to face the evidence of declining
patent-to-R&D ratios. Figure 4 plots
domestic patent applications divided by total
company-financed R&D expenditures in
U.S. industry (in 1972 dollars) and by the
total number of scientists and engineers in
industry. Looking at the right half of this


### ---Economics-1994-0-10.txt---
plot (the last couple of decades) we see a
more or less continuous decline with a small,
but possibly significant, turnaround in the
late 1980's. Similar trends can be seen also
in other countries, even in Japan (Evenson,
1991). But before one takes this as an indicator
of our recent problems, one should

glance also at the left side of this figure,
which goes back to the early 1920's. How
long has this been going on? This ratio
keeps falling, both through good times
(while productivity growth rates were rising)
and bad times. If this was not a cause for
worry earlier, why should one worry about it
now?9

III. Patents: A Shrinking Yardstick?
To decide whether we should be worried
by what is happening with the patent numbers
we need to know what they measure.

Since I have discussed this at some length
elsewhere (Griliches, 1990), I will make only
two points here. First, the interpretation of
Figure 4 need not be pessimistic. Its message
may not be what meets the eye. And,
second, the meaning of both the numerator
and the denominators of the ratios plotted
in Figure 4 may have changed significantly
over time.

If patents can be taken as indicators of
invention, and if the value of an invention is
proportional to the size of its market (or
economy), then the fact that their total
numbers remained roughly constant over
long time periods is consistent with nondeclining
growth rates of output and overall

productivity.10 If inventions are "produced"
by a combination of current R&D and the
existing state of knowledge (incorporating
the accumulated effects of science and
spillovers from the previous research activities
of others), and if R&D is invested approximately
"optimally," then under reasonable

assumptions, a rise (or fall) in the
underlying knowledge stock will affect them
both in parallel fashion and will leave their
ratio unchanged.1" There will be, therefore,
no evidence in this ratio on the underlying
state of the "stock of knowledge." Moreover,
it will be declining with growth in the
size of the market, since a rise in the value
of inventions will push R&D up until present
costs equal again the present value of
future (private) returns.

The rate of growth of domestic patents
was close to zero during the last three
decades. That by itself should not be worrisome.
If their average value had been growing
at the same rate as the economy as a
whole, there would be no reason for us to
worry about it. But there were long periods
when the actual numbers were worse than
that. During 1965-1985 the number of domestic
patent applications declined by - 0.6
percent per year while company-financed
R&D expenditures were growing by 4.8 percent
per year, in constant prices. But a
negative growth rate in the number of inventions
and a positive one in R&D are

inconsistent with an unchanging inventions
production function, unless the overall pool
of available knowledge is declining, or more
Actually quite a few people worried about it then
also: see Griliches (1990) for more detail and W.
Fellner (1970), who worried about the rising real cost
of R&D as an indicator of diminishing returns.
10This follows from the nonrival nature of inventions
(see Kenneth Arrow, 1962; Paul Romer, 1990).
"Assume an aggregate inventions "production function"
of the form N = RYZ, where R is a measure of
current R&D inputs and Z represents all other shifters
of this function: the accumulation of one's own past
R&D successes and also spillovers from the research
efforts of others. Then, y < 1 implies short-run diminishing
returns to current R&D, a "fishing-out" phenomenon
given the current "state of the art" Z. To the
extent that endogenous (and exogenous) forces "recharge"
the pool (in Evenson's [1991] terminology) and
change Z as the result of the direct and indirect
additions to the overall stock of knowledge, there need
not be diminishing returns to R in the long run. If R is
chosen so as to equate the value of its marginal prod-
uct, V(yN/R), to the marginal real cost of R, C, and if
V is the expected present value of an invention, one
can rewrite the first-order condition as N/R = C/yV,
which yields the major conclusion that the ratio of
inventions per unit of R&D is independent of the state
of general knowledge Z. Moreover, N/R will be declining
in V, the size of the market. For a more
detailed elaboration of such models see the "qualityladders"
approach of Gene Grossman and Elhanan
Helpman (1991), Caballero and Jaffe (1993), and Kortum
(1993).


### ---Economics-1994-0-11.txt---
likely, unless the relationship between inventions
and the number of patents applied

for has been changing.

The suspicion that the relationship between
the number of patents and the number
of inventions (weighted by their relative
economic importance) has been changing is
not new. Schmookler (1966) stops most of
his analysis with pre-World War II data,
believing that the meaning of the patent
statistics changed at that time. What needs
to be reconciled in the data is the sharp
contrast between the rapidly growing R&D
series during 1953-1968 (and earlier) and
the essentially flat patent series. There are a
number of not mutually exclusive possibilities
here:

(i) The fast-growing R&D expenditures,
fueled by the new global opportunities
that opened up in the post-World War
II period, were being invested in face
of rapidly diminishing returns.

(ii) Some of the observed growth in R&D
could be spurious, the result of reclassification
of informal technological activities
into formal R&D under the

pressure of tax accountants, publicrelations
experts, and R&D tax credits.

(iii) The rise of formal R&D-based invention
crowded out smaller, less valuable

individual-inventor-based patents, while
the rise in the cost of patenting (in
terms of the time costs of dealing with
the patent system) and the more recent
sharp rise in fees may have selected out
a large number of potentially low-valued
patents. Given the evidence that

the value distribution of inventions and
patents is extremely skewed, with only
a small fraction having a high present
value, such a crowding out could raise
average values significantly, though the
required rate is rather on the high
side.12

It is also likely that the threshold for what
is patentable has risen, given the large influx
of foreign patent applications into the
U.S. system all impinging on a relatively
slow-growing and budget-constrained patent
office.'3 On the other hand, the legal
status of patents in the United States has
improved significantly with the creation of a
special patents court, driving up the expected
private value of a patent. Given the
presence of so many opposing forces, there
is no compelling need to reply on the
exhaustion-of-inventive-opportunities hypothesis,
especially since patents-to-R&D

ratios were falling much more drastically
during the "good times" of the past than
recently.14 Moreover, if we do take these
numbers seriously, then good news is just
around the corner: domestic patent applications
have risen sharply in the last five years
(see Fig. 5), implying a potential resurgence
in the rate of technological change. This
leaves us, however, more or less where we
started, with the productivity slowdown
largely unexplained.


### ---Economics-1994-0-12.txt---
200,000

150,000-

100,000 -

100 000 Total t\/\ ,>/ Domestic /

50,000I

Foreign _--

1880 1900 1920 1940 1960 1980

FIGURE 5. PATENT APPLICATIONS IN THE UNITED STATES, 1880-1992
IV. Why Is the Glass Half-Empty?

Economists have not been very successful
in explaining what has happened to the
economy during the last two decades, nor
have they been able to agree on what should
be done about it. I will argue that data and
measurement difficulties may in fact be a
major source of this failure. This point will
be made not to provide us with an alibi, but
rather to temper the pretentiousness of
some of our pronouncements and to urge us
toward the more mundane task of observation
and measurement.

Why don't we know more after all these
years? Our data have always been less than
perfect. What is it about the recent situation
that has made matters worse?

The brief answer is that the economy has
changed and that our data-collection efforts
have not kept pace with it. "Real" national
income accounts were designed in an earlier
era, when the economy was simpler and had
a large agricultural sector and a growing
manufacturing sector. Even then, a number
of compromises had to be made to get measurement
off the ground. In large sectors of
the economy, such as construction and most
of the services, government, and other public
institutions, there were no real output
measures or relevant price deflators. Imagine
a "degrees of measurability" scale, with
wheat production at one end and lawyer
services at the other. One can draw a rough
dividing line on this scale between what I
shall call "reasonably measurable" sectors
and the rest, where the situation is not
much better today than it was at the beginning
of the national income accounts. Table
2 shows the distribution of nominal GDP by
major industrial sector. In the early postWorld
War II period, the situation was not
all that bad: about half of the overall economy
was "measurable" in this sense. By

1990, however, the fraction of the economy
for which the productivity numbers are half
reasonable had fallen to below one-third.
Figure 6 tells the same story with employment
numbers. Measurement problems have

indeed become worse. Our ability to interpret
changes in aggregate total factor productivity
has declined, and major portions

of actual technical change have eluded our
measurement framework entirely.15

15 1 An argument could be made that this story would
not be so bleak if we had focused on consumption
expenditures instead, since many of the offending inAn


### ---Economics-1994-0-13.txt---
example of the consequences of this
shift is what has come to be known as the
"computer paradox." We have made major
investments in computers and in other
information-processing equipment. The
share of "information" equipment in total
producer investment in durable equipment,
in current prices, has more than doubled,
from about 17 percent in 1960 to 36 percent
in 1992. Computers alone went up from less
than 1 percent to 11 percent of the total;
and that does not allow for improvements in
the quality of this equipment, which has
been happening at a very fast rate-on the
order of 15-30 percent per year (see Jack
Triplett, 1989; Berndt and Griliches, 1993).
Why has this not translated itself into visible
productivity gains? The major answer to
this puzzle is very simple: over threequarters
of this investment has gone into

our "unmeasurable" sectors (see Table 3),
and thus its productivity effects, which are
likely to be quite real, are largely invisible in
the data.

That there were gains is not really in
doubt. Just observing the changes in the
way banks and airlines operate, and in the
ways in which information is delivered to
firms and consumers, would lead one to
conclude that we are in the midst of a major
technical revolution. Effective distances are
declining rapidly in many parts of the world.
The rise of ATM networks in banking has
resulted in substantial though largely unmeasured
time savings for consumers. It is

less clear, however, whether the large expansion
of the securities industry has been
associated with a similar productivity increase
or was primarily a response to a real
decline in the cost of rent-seeking induced
by the falling price of informationprocessing
(see Timothy Bresnahan et al.,

1992).

There is also some scattered evidence for
the positive contribution of computers in
manufacturing, but given the needle-in-the
haystack aspect of this problem, it is not
particularly strong (see e.g., Alan Krueger,


### ---Economics-1994-0-14.txt---
125

100

Government '-r

75

_ | ~Tota1l ; _>,_

50 > , Trade, Finance, and Other Services
?'z' ~~~~~~~Construcition,_ __

25gp ';~~-

Agriculture, Mining, Manufacturing,
Transportation, and Public Utilities
1930 1940 1950 1960 1970 1980 1990

FIGURE 6. PERSONS ENGAGED IN PRODUCTION BY INDUSTRY, UNITED STATES,
1929-1980

TABLE 3-INVESTMENT IN COMPUTERS (OCAM) IN THE U.S. ECONOMY
(PERCENTAGE OF TOTAL)

Industry 1979 1989 1992

Agriculture 0.1 0.1 0.1

Mining 2.4 1.1 0.9

Construction 0.1 0.3 0.2

Manufacturing 29.4 20.3 20.0

Transportation 1.3 2.0 1.0

Communication 1.5 1.4 1.5

Utilities 1.2 2.8 3.7

Trade 19.9 16.3 20.0

Finance, insurance, and real estimate (F.I.R.E.) 32.5 38.7 37.8
Other services 11.6 17.0 13.9

"Unmeasurable" sectorsa 64.1 72.3 71.9
Plus consumer and government

purchases as percentage of all

computer (OCAM) purchases 67.7 77.6 77.0
Notes: OCAM = office, computing, and accounting machinery.
Source: Unpublished BEA tabulations.
aConstruction, trade, F.I.R.E., and other services.
1991; Donald Siegel and Griliches, 1992;
Erik Brynjolfson and Lorin Hitt, 1993; Igal
Hendel, 1993). Some of the gains from computers
have been reflected in higher wages
of their operators and in the more general
rise in the returns to education and "skill"
(Chinhui Juhn et al., 1993). More generally,
we may be just at the beginning of the
computer era, early in its diffusion and
learning stages, with most of the productivity
contributions still to come, as we learn
how to use computers more effectively and


### ---Economics-1994-0-15.txt---
integrate them more efficiently into the existing
production structures (Paul David,

1991).

Similar arguments, can be (and have been)
made about the difficulties in measuring the
contribution of R&D to productivity growth
(see Griliches, 1979). From one-third to over
half of all industrial R&D is "sold" to the
government, either in the form of research
contracts and prototypes or indirectly in the
form of weapons and space equipment, and
its direct productivity effects do not show up
in the data at all. Private R&D investment
is also likely to have followed the economy
and shifted its targets toward the fastergrowing
sectors, with more invention and

technical change occurring exactly where we
have more trouble in measuring them.
Not only has the economy shifted into
uncharted waters, but even in the "measurable"
sectors accelerating rates of change
have destroyed the basis for some of the
older compromises. Currently, new goods
are introduced into the various official price
indexes rather slowly. While attempts are
being made to reduce the revision cycle in
the producer price index from five to two
years for some of the more high-tech goods,
this may still not be fast enough. In the
personal-computers market, for example,
the life of a model has recently fallen to a
year or less (Berndt et al., 1993).
Dealing with the quality-change problem
by treating every version of a product sold
to a different type of customer as a separate
commodity, as is currently the predominant
official practice, creates its own problems.
By linking out the decline in prices experienced
by consumers in their shift to supermarkets,
discount stores, and mail-order

purchases, it underestimates significantly not
only the output of se;rvices, but also the
output of some of the more "standard"
manufacturing industries (Marshall Reinsdorf,
1993). A prime example of that is the
treatment of generics in the pharmaceutical
price indexes. The stylized facts are as follows:


(i) Generics are introduced at roughly half
the price of the original brand.

(ii) The brand price, however, does not,
decline (it sometimes even goes up),
with the ex-monopolist depreciating
optimally her original position and with
generics gaining between half and

three-quarters of the market for the
particular drug.

(iii) But because generic versions are
treated as separate commodities, in
spite of what the FDA says, the price
index does not fall, and since the value
of shipments declines as the market
shifts to generics (and to hospital and
HMO formularies), so does measured

"output" in this industry and the associated
productivity measures (Griliches

and lain Cockburn, 1993).

This might explain the rather strange
fact that during the last decade pharmaceuticals,
an industry with one of the highest
R&D-sales ratios, had a rather dismal
productivity-growth performance. This was
the period with an increasing penetration of
generics, which should have reduced measured
prices in this industry but did not.
The measurement environment has deteriorated
also in other ways. There is less

willingness on the part of firms and consumers
to respond to detailed questions,

and our government has done little to emphasize
the importance of good economic

data to its own functioning or the overall
understanding of our economy. The consequence
of such deterioration can be illustrated
by the uncertainty about the level of
industrial investment in basic research, an
investment which many think is crucial to
our long-run economic performance

(Griliches, 1986a). Because the question that
asks about the allocation of total R&D expenditures
by the "character of work" is not

mandatory and is also not an easy one to
answer, less than half of all the firms surveyed
in 1988 answered it. As a result of
such nonresponse, the best that can be done
is to produce a "reasonable" range of estimates,
based on alternative imputation algorithms,
from $2.5 to $8.2 billion (and a

"central" guess of $3.9 billion), which leaves
us really in the dark as to what has happened


### ---Economics-1994-0-16.txt---
to such investments recently (Eileen I.
Collins, 1990).

V. Data Woes

Why are the data not better? The facts
themselves are not in dispute. Every decade
or so a prestigious commission or committee
produces a report describing in detail
various data difficulties and lacunae: the
Stigler committee report on government
price statistics (National Bureau of Economic
Research, 1961) is still a living document,
as are the related Ruggles report

(Richard Ruggles, 1977), the Rees productivity
report (National Academy of Sciences,
1979), the Bonnen report (J. T. Bonnen,
1981), the Creamer GNP improvement report
(D. Creamer, 1977), the recent OTA

report (Office of Technology Assessment,
1989), and many others. But life goes on,
and change in this area is very slow. Why? I
don't really have good answers to this question,
and the topic itself is much larger than
can be handled in this address, but at least
three observations come to mind:

(i) The measurement problems are really
hard.

(ii) Economists have little clout in Washington,
especially as far as data-collection
activities are concerned. Moreover,
the governmental agencies in

these areas are balkanized and underfunded.


(iii) We ourselves do not put enough emphasis
on the value of data and data

collection in our training of graduate
students and in the reward structure of
our profession. It is the preparation
skill of the econometric chef that

catches the professional eye, not the
quality of the raw materials in the meal,
or the effort that went into procuring
them (Griliches, 1986b).

In many cases the desired data are unavailable
because their measurement is really
difficult. After decades of discussion we
are not even close to a professional agreement
on how to define and measure the

output of banking, insutrance, or the stock
market (see Griliches, 1992). Similar difficulties
arise in conceptualizing the output
of health services, lawyers, and other consultants,
or the capital stock of R&D. While

the tasks are difficult, progress has been
made on such topics. The work of Jorgenson
and Barbara Fraumeni (1992) on the

measurement of educational output is an
example both of what can be done and of
the difficulties that still remain. But it is not
reasonable for us to expect the government
to produce statistics in areas where the concepts
are mushy and where there is little
professional agreement on what is to be
measured and how. Much more could be
done, however, in an exploratory and research
mode.16 Unfortunately, the various

statistical agencies have been both starved
for funds and badly led, with the existing
bureaucratic structure downplaying the research
components of their enterprise when
not being outright hostile to them, research
being cut first when a budget crunch happens
(Triplett, 1991).

Our current statistical structure is badly
split, there is no central direction, and the
funding is heavily politicized. How else can
one explain that the national income accounts
and the BEA as a whole receive only
one-third, and health and education statistics
each less than one-half of the funds
allocated to agricultural statistics?17 How
does one explain the failure of the most
16I refrain from offering a detailed list of my own
favorite data improvements; but a census of real wealth
(i.e., a survey of structure, equipment, and other resources
and their utilization-not just what is on the
books, but what is actually out there in the field) would
be high on my list. I would also like to see a survey of
patent owners on the use and potential value of their
property rights.

17j am not arguing that too much is being currently
spent on agricultural statistics. That would require a
substantive analysis, which has not been done. I am
saying, however, that the other areas of federal statistics
could use both more funding and a redirection of
existing funding. We are also currently spending far
more on monthly employment and average hourly
earnings data than we spend to collect all of the other
inputs and outputs annually. With Congressional prodding,
we spend much more on local-markets data than
on national-level data.


### ---Economics-1994-0-17.txt---
recent attempt at getting more money for
economic statistics, the late "Boskin initiative"
? Central economic statistics do not
have a clear constituency that lobbies on
their behalf. Recent governments seem not
to care enough, or to have enough energy to
fight for something that has a more distant
horizon than the next election. One hopes
for some improvement in this situation from
the current administration. It has people
who know better in reasonably important
positions. Still, with the main focus on the
daily crisis and the continuing budget battles
with Congress, I am not all that optimistic.
But if we want progress in this area,
if we care, we need to make our opinions
heard. We need to convince Congress (and
ourselves) that the requests for additional
funding of the statistical infrastructure are
justified as investments in general knowledge
and more informed policy formation;
that they are not just self-serving, intended
to allow us to publish more articles or run
thousands more regressions; that it is indeed
important to know what is happening
and to understand where we might be going
or drifting.18

We need also to make observation, data
collection, and data analysis a more central
component of our graduate teaching. How
can we expect our community to fight for
the budgets of the BEA, BLS, or Census, if
the average student doesn't really know how
the data that they use are manufactured or
what the national accounts are made of.19
We also need to teach them to go out and
collect their own data on interesting aspects
of the economy and to rely less on "given"
data from distant agencies.'o There are encouraging
signs that some of this is happening,
especially in the micro area. One is
much more cheered by work such as that of
Robert Fogel (1986) on heights and nutrition,
Alan Krueger and Orley Ashenfelter

(1992) on twins, Richard Levin et al. (1987)
on the appropriability of technology,
Rebecca Henderson and Cockburn on pharmaceutical
R&D, Richard Freeman and

Harry Holtzer (1986) on inner-city youths,
Schankerman and Pakes (1986) on patent
renewal data, Manuel Trajtenberg (1990a)
on CT scanners, and Trajtenberg (199Gb)
and Adam Jaffe et al. (1993) on patent
citations, where researchers go out, collect,
and create new data sets, than by the
20,000th regression on the Robert Summers
and Alan Heston (1991) data set, illuminating
as it may be. But unless we transmit this
message to our students, we will not be able
to convince others that this is a cause worth
supporting.

VI. Expanding the Framework

Is there something possibly wrong with
the way we ask the productivity question,
with the analytical framework into which we
force the available data? I think so. I would
focus on the treatment of disequilibria and
the measurement of knowledge and other
externalities. The current measurement
framework proceeds as if all investment
and employment decisions are made at
known and common factor and product
prices, throwing all of the heterogeneity
and uncertainty-the surprises and the disappointments-
into the residual category.

An alternative view would see measured
productivity growth as a summation of
above- (and below-) average returns to various
current investment decisions and capital
gains (or losses) on existing physical- and


### ---Economics-1994-0-18.txt---
human-capital stocks.2' The appearance of
such investment opportunities is the essence
of growth and change. They are largely disequilibrium
phenomena, resulting in a

lurching from one "steady state" to another
rather than something smooth and exponential.
The presence of locally increasing

returns, network externalities, asymmetric
information, and heterogeneous expectations,
the appearance of new products and

technologies, and the changes in the political
and regulatory environments are all
sources of such "excess" returns, while the
ex post fixity of much of the investment in
both physical and human capital causes capital
gains and losses and unanticipated "obsolescence"
in the various stocks. We will

have to figure out how to take the residual
apart along such lines to make more
progress in understanding its proximate
sources.

Our theories tend to assume that we are,
indeed, at the frontier and that we can only
either move along it or try to shift it, the
latter being a difficult and chancy business.
In fact we may be far from our existing
"frontiers." Harvey Leibenstein's (1966)
ideas about X-efficiency, or more correctly
X-inefficiency, did not get much of a sympathetic
ear from us. They were inconsistent
with notions of equilibrium, the absence of
unexploited profit opportunities, and the
possibilities for economic arbitrage. But real
economic growth is the consequence of both
the appearance of such disequilibria and
the devising of ways of closing them. How
quickly they are eliminated depends on the
strength of incentive systems within enterprises,
and on their organizational quality.
In spite of the large growth in the literature
on organizations, we have not yet developed
useful ways of quantifying their strengths
and weaknesses. Nor are we close to having
measures of such factors as the "work ethic"
or aspects of the property-rights system
which are likely to contribute much to the
observed differences in productivity across
nations.

The "new" growth theories have various
externalities as their centerpiece (see Solow
[1991] for a recent review). It is somewhat
ironic that they have come to the fore just
when growth started declining and notions
of eternal exponential growth began to lose
their luster. Knowledge externalities are obviously
very important in the growth process,
but they do not help us to explain what
has happened in the last two decades. There
is no reason to believe that they have declined
over time. If anything, the communication
and transportation advances should

have expanded the availability of such externalities.
22 But we have no good models for

the measurement of such processes.

Knowledge is not like a stock of ore,
sitting there waiting to be mined. It is an
extremely heterogenous assortment of information
in continuous flux. Only a small part
of it is of any use to someone at a particular
point of time, and it takes effort and resources
to access, retrieve, and adapt it to
one's own use. Thus models of externalities
must perforce be models of interaction between
different actors in the economy. We
have, however, very few convincing models
of such interactions, and the identification
problems are severe (see e.g., Charles Manski,
1993). Our measurement frameworks

are not set up to record detailed origin and
destination data for commodity flows, much
less so for information flows. We do have
now a new tool for studying some of this:
citations to patents and the scientific literature
(see e.g., Jaffe et al., 1993), but anyone
currently active in the e-mail revolution and
participating in the conferences and workshops
circuit knows how small this tip is
relative to the informal-communications
iceberg itself.

21This is not a new idea. Versions of it appear in
Harry Johnson (1964), Arnold Harberger (1990), and
Theodore Schultz (1990) and presumably also elsewhere.


22The story is similar for externalities from humancapital
investments, another linchpin of the new growth
theories, but I will not pursue it here.


### ---Economics-1994-0-19.txt---
VII. The Glass Half-Full?

After a long detour I come back to the
original question: why don't we know more
about the sources of productivity growth
and the causes for its recent slowdown?
Why does it feel as if the glass is still
half-empty? First note that in a trivial sense
we are doing better: the residual is smaller.
But that is the bad news, not the good. It is
smaller not because we have succeeded in
providing a substantively fuller explanation
of output growth, but rather because measured
output growth declined, leaving some
of these explanations in the dust. But we
are also doing better substantively. We know
much more about the components of growth
and where our measures are lacking. After
decades of work and contributions by Denison,
Jorgenson, Kendrick, and many others,
the conceptual and measurement underpinnings
of the growth accounts are in much

better shape today. We now have extensive
micro data on firms, their productivity, their
R&D expenditures, and other variables. We
have more data on individual investments in
education and training, and we also have
more asset detail on capital formation. More
international data are now available, with
the OECD both collecting R&D data and
computing TFP numbers for many countries,
and with Summers and Heston (1991)

providing comparable real GNP numbers
for many countries. Finally, we have much
more computing power and better econometric
techniques and frameworks for attacking
many of the problems that arise in

the analysis of such data. So what is still
missing?

We are caught up in a mixture of unmeasurement,
mismeasurement, and unrealistic

expectations. The productivity situation is
both better than we think and also worse. It
is likely that there have been significant
unmeasured productivity advances in many
of the service sectors (Bresnahan, 1986;
Baily and Gordon, 1988). Moreover, rising
R&D investment rates in the mid-1980's
and the recent rise in the number of patent
applications augur well for the future. Also,
productivity growth rates are probably underestimated
even in the "measurable" sectors

because they are based on "book value"
estimates of physical- and human-capital
stocks and do not reflect the capital losses
-the obsolescence that occurred, first as
the result of the various energy-price shocks,
and later as the result of increased international
competition and the melting away of
much of the previously existing monopoly
rents to both types of capital. That is actually
bad news. We are not as wealthy as we
thought, but productivity growth, based on
the lower remaining levels of input, is probably
higher than we have measured it.

A cautionary remark needs to be added
here: productivity growth contributes to the
potential for welfare, but it is not the same
thing. Welfare can move in the opposite
direction if the resources released by productivity
growth do not find adequate employment
in other, economically valuable,

activities (including leisure). Also the physical,
economic, and political environments
can change, both positively and negatively,
overwhelming the productivity story.23 So
even though I have been focusing on it here
tonight, it is not the be-all of economic
welfare. But as George Bernard Shaw used
to say when he was accused of money-grubbing:
"Yes, I know that money is not happiness,
but it is a pretty good substitute."
Nevertheless, the issues I have been discussing
here tonight are important. Much

depends on whether the "truth" is closer to
the upper ("measurable") line in Figure 1,
or the lower one. The country's mood is
affected by bad data and incorrect perceptions.
Are we really not much better off

than we were in the 1960's? Would we
really like to exchange the commodity assortment
we have today for that of

yesteryear? Our health system, warts and
all? The air pollution? The civil-rights situation?
The fear of nuclear war? These are


### ---Economics-1994-0-20.txt---
not just idle intellectual curiosities. They
affect what we feel about ourselves and the
future.

Returning to the topic of technical
change, our expectations of what economics
can deliver here may also be excessive. It is
unlikely that we can have a fully "endogenous"
theory of technical change. Yes, both
the rate and direction of inventive activity
are subject to economic influences and analysis.
So also is the diffusion of innovations.
But the outcome of inventive activity is not
really predictable. True "innovation" is an
innovation. If it were knowable in advance
it would not be one, and the innovators
would not be able to collect any rents. In
that sense it is futile to expect that we could
control it fully or predict it well.24 Given the
fundamental uncertainties entailed in the
creative act, in invention, and in innovation,
there is no reason to expect the fit of our
models to be high or for the true residual to
disappear. We should, however, be able to
"explain" it better ex post even if we cannot
predict it.

The metaphor of the glass half-empty is
also misleading. As we fill it, the glass keeps
growing. A major aspect of learning is that
the unknown keeps expanding as we learn.
This should be looked at positively. It is
much better this way-especially for those
of us who are engaged in research!
 ## Economics-1995-0


### ---Economics-1995-0-02.txt---
While Aristotle agreed with Agathon that
even God could not change the past, he did
think that the future was ours to make-by
basing our choices on reasoning. The idea
of using reason to identify and promote
better-or more acceptable-societies, and
to eliminate intolerable deprivations of different
kinds, has powerfully moved people

in the past and continues to do so now. In
this lecture I would like to discuss some
aspects of this question which have received
attention in the recent literature in socialchoice
and public-choice theories. The contemporary
world suffers from many new as

well as old economic problems, including,
among others, the persistence of poverty
and deprivation despite general economic
progress, the occurrence of famines and
more widespread hunger, and threats to our
environment and to the sustainability of the
world in which we live. Rational use of the
opportunities offered by modern science and
technology, in line with our values and ends,
is a powerful challenge today.

I. Problems and Difficulties

How are we to view the demands of rationality
in social decisions? How much guidance
do we get from Aristotle's general

recommendation that choice should be governed
by "desire and reasoning directed to
some end"? There are several deep-seated
difficulties here.

The first problem relates to the question:
whose desires, whose ends? Different persons
have disparate objects and interests,
and as Horace put it, "there are as many
preferences as there are people." Kenneth
Arrow (1951) has shown, through his famous
"General Possibility Theorem" (an

oddly optimistic name for what is more
commonly-and more revealingly-called
Arrow's "impossibility theorem"), that in
trying to obtain an integrated social preference
from diverse individual preferences, it
is not in general possible to satisfy even
some mild-looking conditions that would
seem to reflect elementary demands of reasonableness.
1 Other impossibility results

have also emerged, even without using some
of Arrow's conditions, but involving other
elementary criteria, such as the priority of
individual liberty.2 We have to discuss why
these difficulties arise, and how we can deal
with them. Are the pessimistic conclusions
that some have drawn from them justified?
Can we sensibly make aggregative socialwelfare
judgments? Do procedures for social
decision-making exist that reasonably
respect individual values and preferences?


### ---Economics-1995-0-03.txt---
Second, another set of problems relates
to questions raised by James Buchanan
(1954a,b), which were partly a response to
Arrow's results, but they are momentous in
their own right.3 Pointing to "the fundamental
philosophical issues" involved in

"the idea of social rationality," Buchanan
(1954a) argued that "rationality or irrationality
as an attribute of the social group
implies the imputation to that group of an
organic existence apart from that of its individual
components" (p. 116). Buchanan was

perhaps "the first commentator to interpret
Arrow's impossibility theorem as the result
of a mistaken attempt to impose the logic of
welfare maximization on the procedures of
collective choice" (Robert Sugden, 1993 p.
1948). But in addition, he was arguing that
there was a deep "confusion surrounding
the Arrow analysis" (not just the impossibility
theorem but the entire framework used
by Arrow and his followers) which ensued
from the mistaken idea of "social or collective
rationality in terms of producing results
indicated by a social ordering" (Buchanan,
1960 pp. 88-89). We certainly have to examine
whether Buchanan's critique negates the
impossibility results, but we must also investigate
the more general issues raised by

Buchanan.4

Third, Buchanan's reasoned questioning
of the idea of "social preference" suggests,
at the very least, a need for caution in
imposing strong "consistency properties" in
social choice, but his emphasis on procedural
judgments may be taken to suggest,

much more ambitiously, that we should
abandon altogether consequence-based
evaluation of social happenings, opting instead
for a procedural approach. In its pure
form, such an approach would look for
"right" institutions rather than "good" outcomes
and would demand the priority of

appropriate procedures (including the acceptance
of what follows from these procedures)
. This approach, which is the polar
opposite of the welfare-economic tradition
based on classical utilitarianism of founding
every decision on an ordering of different
states of affairs (treating procedures just as
instruments to generate good states), has
not been fully endorsed by Buchanan himself,
but significant work in that direction
has occurred in public choice theory and in
other writings influenced by Buchanan's
work (most notably, in the important contributions
of Robert Sugden [1981, 1986]).

This contrast is particularly important in
characterizing rights in general and liberties
in particular. In the social choice literature,
these characterizations have typically been
in terms of states of affairs, concentrating
on what happens vis-a-vis what the person
wanted or chose to do. In contrast, in
the libertarian literature, inspired by the
pioneering work of Robert Nozick (1974),
and in related contributions using "gameform"
formulations (most notably, by Wulf
Gaertner, Pattanaik, and Suzumura [1992]),
rights have been characterized in procedural
terms, without referring to states of affairs.
We have to examine how deep the

differences between the disparate formulations
are, and we must also scrutinize their
respective adequacies.

Fourth, the prospects of rationality in social
decisions must be fundamentally conditional
on the nature of individual rationality.
There are many different conceptions of
rational behavior of the individual. There is,
for example, the view of rationality as canny
maximization of self-interest (the presumption
of human beings as "homo economicus,"
used in public choice theory, fits into this
framework). Arrow's (1951) formulation is
more permissive; it allows social considerations
to influence the choices people make.
Individual preferences, in this interpretation
reflect "values" in general, rather than
being based only on what Arrow calls
"tastes" (p. 23). How adequate are the respective
characterizations of individual rationality,
and through the presumption of

rational behavior (shared by most economic
3Dennis C. Mueller (1989) provides an excellent
introduction to public choice theory and its relation to
social choice theory. See also Atkinson (1987) and
Sandmo (1990) on Buchanan's contributions.
4The canonical treatise on the "public choice" approach
is Buchanan and Tullock (1962), but it is important
to note the differences in emphases between the
appendix by Buchanan and that by Tullock.


### ---Economics-1995-0-04.txt---
models), the depiction of actual conduct
and choices?

Another issue, related to individual behavior
and rationality, concerns the role of
social interactions in the development of
values, and also the connection between
value formation and the decision-making
processes. Social choice theory has tended
to avoid this issue, following Arrow's own
abstinence: "we will also assume in the present
study that individual values are taken
as data and are not capable of being altered
by the nature of the decision process itself"
(Arrow, 1951 p. 7).5 On this subject,
Buchanan has taken a more permissive position-
indeed emphatically so: "The definition
of democracy as 'government by discussion'
implies that individual values can

and do change in the process of decisionmaking"
(Buchanan, 1954a p. 120).6 We

have to scrutinize the importance of this
difference as well.

This is a long and somewhat exacting list,
but the different issues relate to each other,
and I shall try to examine them briefly and
also comment on some of their practical
implications.

II. Social Welfare Judgments and Arrow's
Impossibility Theorem

The subject of welfare economics was
dominated for a long time by the utilitarian
tradition, which performs interpersonal aggregation
through the device of looking at

the sum-total of the utilities of all the people
involved. By the 1930's, however,

economists came to be persuaded by arguments
presented by Lionel Robbins (1938)

and others (influenced by the philosophy of
"logical positivism") that interpersonal
comparisons of utility had no scientific
basis.7 Thus, the epistemic foundations of
utilitarian welfare economics were seen as
incurably defective.

Because of the eschewal of interpersonal
comparability of individual utilities, the
"new welfare economics" that emerged tried
to rely only on one basic criterion of social
improvement, the Pareto criterion. Since
this confines the recognition of a social improvement
only to the case in which everyone'
s utility goes up (or someone's goes up
and no one's goes down), it does not require
any interpersonal comparison, nor for that
matter, any cardinality of individual utilities.
However, Pareto efficiency can scarcely
be an adequate condition for a good society.
It is quite insensitive to the distribution of
utilities (including inequalities of happiness
and miseries), and it takes no direct note of
anything other than utilities (such as rights
or freedoms) beyond their indirect role in
generating utilities. There is a need, certainly,
for further criteria for social welfare
judgments.

The demands of orderly, overall judgments
of "social welfare" (or the general
goodness of states of affairs) were clarified
by Abram Bergson (1938, 1966) and extensively
explored by Paul Samuelson (1947).

The concentration was on the need for a
real-valued function W of "social welfare"
defined over all the alternative social states,
or at least an aggregate ordering R over
them, the so-called "social preference." In
the reexamination that followed the Bergson-
Samuelson initiative (including the development
of social choice theory as a discipline)
, the search for principles underlying a
social welfare function played a prominent
part.

Arrow (1951) defined a "social welfare
function" as a functional relation that specifies
a social ordering R over all the social
states for every set of individual preference
orderings. In addition to assuming-not especially
controversially-that there are at


### ---Economics-1995-0-05.txt---
least three distinct social states and at least
two (but not infinitely many) individuals,
Arrow also wanted a social welfare function
to yield a social ordering for every possible
combination of individual preferences; that
is, it must have a universal domain. A second
condition is called the independence of
irrelevant alternatives. This can be defined in
different ways, and I shall choose an extremely
simple form. The way a society ranks
a pair of alternative social states x and y
should depend on the individual preferences
only over that pair-in particular,

not on how the other ("irrelevant") alternatives
are ranked.

Now consider the idea of some people
being "decisive": a set G of people-I shall
call them a group G-having their way no
matter what others prefer. In ranking a pair
x and y, if it turns out that x gets socially
ranked above y whenever everyone in group
G prefers x to y (no matter what preferences
those not in G have), then G is decisive
over that ordered pair (x, y). When a
group G is decisive over all ordered pairs, it
is simply "decisive."

Arrow required that no individual (formally,
no single-member group) should be

decisive (nondictatorship), but-following
the Paretian tradition-also demanded that
the group of all individuals taken together
should be decisive (the Pareto principle).
The "impossibility theorem," in this version
(presented in Arrow [1963]), shows that it is
impossible to have a social welfare function
with universal domain, satisfying independence,
the Pareto principle, and nondictatorship.


The theorem can be proved in three simple
steps.8 The first two steps are the following
(with the second lemma drawing on

the first).

FIELD-EXPANSION LEMMA: If a group

is decisive over any pair of states, it is decisive.
9

GROUP-CONTRACTION LEMMA: If a

group (of more than one person) is decisive,
then so is some smaller group contained in
it.

The final step uses the Group-Contraction
Lemma to prove the theorem. By the Pareto
principle, the group of all individuals is decisive.
Since it is finite, by successive partitioning
(and each time picking the decisive
part), we arrive at a decisive individual, who
must, thus, be a dictator. Hence the impossibility.


8The strategy of proof employed here (as in Sen
[1986b]) is more direct and simpler than the versions
used in Arrow (1963) and Sen (1970) and does not
require defining additional concepts (such as "almost
decisiveness").

9For proof, take two pairs of alternative states (x, y)
and (a, b), all distinct (the proof when they are not all
distinct is quite similar). Group G is decisive over
(x, y); we have to show that it is decisive over (a, b) as
well. By unrestricted domain, let everyone in G prefer
a to x to y to b, while all others prefer a to x, and y
to b, but rank the other pairs in any way whatever. By
the decisiveness of G over (x, y), x is socially preferred
to y. By the Pareto principle, a is socially preferred to
x, and y to b. Therefore, by transitivity, a is socially
preferred to b. If this result is influenced by individual
preferences over any pair other than (a, b), then the
condition of independence would be violated. Thus, a
must be ranked above b simply by virtue of everyone in
G preferring a to b (since others can have any preference
whatever over this pair). So G is indeed decisive
over (a, b).

10For proof, take a decisive group G and partition it
into G1 and G2. Let everyone in G1 prefer x to y and
x to z, with any possible ranking of (y, z), and let
everyone in group G2 prefer x to y and z to y, with
any possible ranking of (x, z). It does not matter what
those not in G prefer. If, now, x is socially preferred to
z then the members of group G1 would be decisive
over this pair, since they alone definitely prefer x to z
(the others can rank this pair in any way). If G1 is not
to be decisive, we must have z at least as good as x for
some individual preferences over (x, z) of nonmembers
of G1. Take that case, and combine this social ranking
(that z is at least as good as x) with the social
preference for x over y (a consequence of the decisiveness
of G and the fact that everyone in G prefers x to
y). By transitivity, z is socially preferred to y. But only
G2 members definitely prefer z to y. Thus G2 is
decisive over this pair (z,y). Thus, from the FieldExpansion
Lemma, G2 is decisive. So either G1 or G2
must be decisive-proving the lemma.


### ---Economics-1995-0-06.txt---
III. Social Preference, Social Choice,
and Impossibility

The preceding discussion makes abundant
use of the idea of "social preference."
Should it be dropped, as suggested by
Buchanan? And if so, what would remain of
Arrow's impossibility theorem?

We have to distinguish between two quite
different uses of the notion of "social preference,
" related respectively to (i) the operation
of decision mechanisms, and (ii) the
making of social welfare judgments. The first
notion of "social preference" is something
like the "underlying preference" on which
choices actually made for the society by
prevailing mechanisms are implicitly based
-a kind of "revealed preference" of the
society.11 This "derivative" view of social
preference would be, formally, a binary representation
of the choices emerging from

decision mechanisms.

The second idea of "social preference"
as social welfare judgments-reflects a view
of the social good: some ranking of what
would be better or worse for the society.
Such judgments would be typically made by
a given person or agency. Here too an aggregation
is involved, since an individual

who is making judgments about social welfare,
or about the relative goodness of distinct
social states, must somehow combine
the diverse interests and preferences of different
people.

Buchanan's objection is quite persuasive
for the first interpretation (involving decision
mechanisms), especially since there is
no a priori presumption that the mechanisms
used must -or even should -necessarily
lead to choices that satisfy the requirements
of binary representation (not to

mention the more exacting demands of an
ordering representation).12 On the other
hand, the second interpretation does not
involve this problem, and even an individual
when expressing a view about social welfare
needs a concept of this kind.13 When applied
to the making of social welfare judgments
by an individual or an agency, Arrow's
impossibility theorem thus cannot be disputed
on the ground that some organic existence
is being imputed to the society. The
amelioration of impossibility must be sought
elsewhere (see Section IV). However,
Buchanan's critique of Arrow's theorem
would apply to mechanisms of social decision
(such as voting procedures).

Would the dropping of the requirement
that social choices be based on a binary
relation-in particular a transitive ordering
-negate the result in the case of social
decision mechanisms? A large literature has
already established that the arbitrariness of
power, of which Arrow's case of dictatorship
is an extreme example, lingers in one
form or another even when transitivity is
dropped, so long as some regularity is demanded
(such as the absence of cycles).'4

There is, however, cause for going further,
precisely for the reasons identified by
Buchanan, and to eschew not just the transitivity
of social preference, but the idea of
social preference itself. All that is needed
from the point of view of choice is that the
decision mechanisms determine a "choice
function" for the society, which identifies


### ---Economics-1995-0-07.txt---
what is picked from each alternative "menu"
(or opportunity set).'5

However, provided some conditions are
imposed on the "internal consistency" of
the choice function (relating decisions over
one menu in a "consistent" way to decisions
over other-related-menus), it can be
shown that some arbitrariness of power
would still survive.16 But the methodological
critique of James Buchanan would still
apply forcefully, as reformulated in the following
way: why should any restriction

whatever be placed a priori on the choice
function for the society? Why should not
the decisions emerging from agreed social
mechanisms be acceptable without having
to check them against some preconceived
idea of how choices made in different situations
should relate to each other?

What happens, then, to Arrow's impossibility
problem if no restrictions whatever
are placed on the so-called "internal consistency"
of the choice function for the society?
Would the conditions relating individual
preferences to social choice (i.e., the
Pareto principle, nondictatorship, and independence)
then be consistent with each

other? The answer, in fact, is no, not so. If
the Pareto principle and the conditions of
nondictatorship and independence are redefined
to take full note of the fact that

they must relate to social choices, not to any
prior notion of social preference, then a very
similar impossibility reemerges (see Theorem
3 in Sen [1993]).

How does this "general choice-functional
impossibility theorem" work? The underlying
intuition is this. Each of the conditions
relating individual preferences to social decisions
eliminates-either on its own or in

the presence of the other conditions-the
possibility of choosing some alternatives.
And the conjunction of these conditions can
lead to an empty choice set, making it "impossible"
to choose anything.

For example, the Pareto principle is just
such a condition, and the object of this
condition in a choice context, surely, is to
avoid the selection of a Pareto-inferior alternative.
Therefore this condition can be

sensibly redefined to demand that if everyone
prefers x to y, then the social decision
mechanism should be such that y should
not get chosen if x is available.17 Indeed, to
eliminate any possibility that we are implicitly
or indirectly using any intermenu consistency
condition for social choice, we can
define all the conditions for only one given
menu (or opportunity set) S; that is, we can
consider the choice problem exclusively over
a given set of alternative states. The Pareto
principle for that set S then only demands
that if everyone prefers some x to some y
in that set, then y must not be chosen from
that set.

Similarly, nondictatorship would demand
that there be no person such that whenever
she prefers any x to any y in that set S,
then y cannot be chosen from that set.
What about independence? We have to
modify the idea of decisiveness of a group
in this choice context, related to choices
over this given set S. A group would be
decisive for x against y if and only if,
whenever all members of this group prefer
any x to any y in this set S, then y is not to
be chosen from S. Independence would now
demand that any group's power of decisiveness
over a pair (x, y) be completely independent
of individual preferences over pairs
other than (x, y). It can be shown that there
is no way of going from individual preferences
to social choice satisfying these

15The pioneering work on choice-functional formulations
came from Hansson (1968, 1969), Thomas
Schwartz (1972, 1985), Fishburn (1973), and Plott
(1973). Mark Aizerman and his colleagues at the Institute
of Control Sciences in Moscow provided a series
of penetrating investigations of the general choicefunctional
features of moving from individual-choice
functions to social-choice functions (see Aizerman,
1985; Aizerman and Fuad Aleskerov, 1986). On related
matters see also Aizerman and A. V. Malishevski
(1981).

16A sequence of contributions on this and related
issues has come from Plott, Fishburn, Hansson, Donald
Campbell, Bordes, Blair, Kelly, Suzumura, Deb, R. R.
Parks, John Ferejohn, D. M. Grether, Kelsey, V. Denicolo,
and Yasumi Matsumoto, among others. For general
overviews and critiques, see Blair et al. (1976),
Suzumura (1983), and Sen (1986a).

17See also Buchanan and Tullock (1962).


### ---Economics-1995-0-08.txt---
choice-oriented conditions of independence,
the Pareto principle, nondictatorship,
and unrestricted domain, even without
invoking any "social preference," and without
imposing any demand of "collective rationality,
" or any intermenu consistency

condition on social choice.18

The morals to be drawn from all this for
Buchanan's questioning of "social preference"
would appear to be the following.

The "impossibility" result identified in a
particular form by Arrow can be extended
and shown to hold even when the idea of
"social preference" is totally dropped and
even when no conditions are imposed on
"internal consistency" of social choice. This
does not, however, annul the importance of
Buchanan's criticism of the idea of social
preference (in the context of choices emerging
from decision mechanisms for the society)
, since it is a valid criticism on its own
right. But the "impossibility" problem identified
by Arrow cannot be escaped by this

move.

IV. On Reasoned Social Welfare Judgments
How might we then avoid that impossibility?
It is important to distinguish the

bearing of the problem in the making of
aggregative social welfare judgments, as opposed
to the operation of social decision
mechanisms. I start with the former.
It may be recalled that the BergsonSamuelson
analysis and Arrow's impossibility

theorem followed a turn in welfare economics
that had involved the dropping of

interpersonal comparisons of utility. As it
happens, because of its utilitarian form, traditional
welfare economics had informational
exclusions of its own, and it had been
opposed to any basic use of nonutility information,
since everything had to be judged

ultimately by utility sum-totals in consequent
states of affairs. To this was now

added the exclusion of interpersonal comparisons
of utilities, without removing the

exclusion of nonutility information. This
barren informational landscape makes it
hard to arrive at systematic judgments of
social welfare. Arrow's theorem can be interpreted,
in this context, as a demonstration
that even some very weak conditions
relating individual preferences to social welfare
judgments cannot be simultaneously

satisfied given this informational privation.19
The problem is not just one of impossibility.
Consider the Field-Expansion Lemma:
decisiveness over any pair of alternatives entails
decisiveness over every pair of alternatives,
irrespective of the nature of the states
involved. Consider three divisions of a given
cake between two persons: (99,1), (50,50),
and (1, 99). Let us begin with the assumption
that each person-as homo economicus -
prefers a larger personal share of the cake. So
they happen to have opposite preferences.
Consider now the ranking of (99,1) and
(50, 50). If it is decided that (50, 50) is better
for the society than (99,1), then in terms of
preference-based information, person 2's
preference is getting priority over person l's.
A variant of the Field-Expansion Lemma
would then claim that person 2's preference
must get priority over all other pairs as well,
so that even (1,99) must be preferred to
(50, 50).20 Indeed, it is not possible, given
the assumptions, to regard (50, 50) as best of
the three; we could either have (99,1), giving
priority to person l's preference, or
(1,99), giving priority to 2's preference. But
not (50, 50). I am not arguing here that
(50,50) must necessarily be taken to be the
best, but it is absurd that we are not even
permitted to consider (50,50) as a claimant


### ---Economics-1995-0-09.txt---
to being the best element in this cakedivision
problem.

It is useful to consider what arguments
there might be for considering (50,50) as a
good possibility, and why we cannot use any
of these arguments in the information
framework resulting from Arrow's conditions.
First, it might seem good to divide the
cake equally on some general non-welfarist
ground, without even going into preferences
or utilities. This is not permitted because of
-the exclusion of evaluative use of nonutility
information, and this is what the FieldExpansion
Lemma is formalizing. Second,

presuming that everyone has the same
strictly concave utility function, we might
think that the sum-total of utilities would be
maximized by an equal division of the cake.
But this utilitarian argument involves comparability
of cardinal utilities, which is ruled
out. Third, we might think that equal division
of the cake will equate utilities, and
there are arguments for utility-centered
egalitarianism (see James Meade, 1976). But
that involves interpersonal comparison of
ordinal utilities, which too is ruled out. None
of the standard ways of discriminating between
the alternative states is viable in this
informational framework, and the only way
to choose between them is to go by the
preference of one person or another (since
they have opposite preferences).

To try to make social welfare judgments
without using any interpersonal comparison
of utilities, and without using any nonutility
information, is not a fruitful enterprise. We
do care about the size and distribution of
the overall achievements; we have reasons
to want to reduce deprivation, poverty, and
inequality; and all these call for interpersonal
comparisons-either of utilities or of
other indicators of individual advantages,
such as real incomes, opportunities, primary
goods, or capabilities.21 Once interpersonal
comparisons are introduced, the impossibility
problem, in the appropriately redefined
framework, vanishes.22 The comparisons
may have to be rough and ready and often
open to disputation, but such comparisons
are staple elements of systematic social welfare
judgments. Even without any cardinality,
ordinal interpersonal comparisons permit
the use of such rules of social judgment
as maximin, or lexicographic maximin.23 This
satisfies all of Arrow's conditions (and many
others), though the class of permissible social
welfare rules that do this is quite limited,
unless cardinality is also admitted,
along with interpersonal comparisons (see
Louis Gevers, 1979; Kevin Roberts, 1980a).
With the possibility of using interpersonal
comparisons, other classes of possible rules
for social welfare judgments (including
inter alia, utilitarianism) become usable.24
While the axiomatic derivations of different
social-welfare rules in this literature
are based on applying interpersonal comparisons
to utilities only, the analytical

problems are, in many respects, rather similar
when people are compared in terms of
some other feature, such as real income,
holdings of primary goods, or capabilities to
function. There are, thus, whole varieties of
210n different types of interpersonal comparisons,
and the relevance of distinct "spaces" in making efficiency
and equity judgments, see Sen (1982a, 1992a),
John Roemer (1986), Martha Nussbaum (1988),
Richard Arneson (1989), G. A. Cohen (1989), Arrow
(1991), Elster and Roemer (1991), and Nussbaum and
Sen (1993).

22On the other hand, Arrow's impossibility theorem
can be generalized to accommodate cardinality of utilities
without interpersonal comparisons; see Theorem
8.2 in Sen (1970).

23Maximin gives complete priority to the interest of
the worst off. It was proposed by John Rawls (1963), as
a part of his "difference principle" (though the comparisons
that he uses are not of utilities, but of holdings
of primary goods). Lexicographic maximin, sometimes
called "leximin," was proposed in Sen (1970) to
make the Rawlsian approach consistent with the strong
Pareto principle, and it has been endorsed and used in
his Theory of Justice by Rawls (1971). Axiomatic derivations
of leximin were pioneered by Peter J. Hammond
(1976), and Claude d'Aspremont and Gevers (1977),
among others. See also Edmund Phelps (1973).
24See Harsanyi (1955), Patrick Suppes (1966), Sen
(1970, 1977b), Phelps (1973), Hammond (1976, 1985),
Arrow (1977), d'Aspremont and Gevers (1977), Gevers
(1979), Eric Maskin (1978, 1979), Roberts (1980a,b),
Roger B. Myerson (1981), James Mirrlees (1982),
Suzumura (1983), Blackorby et al. (1984), d'Aspremont
(1985), and Kelsey (1987), among others.


### ---Economics-1995-0-10.txt---
ways in which social welfare judgments can
be made using richer information than in
the Arrow framework.

This applies also to procedures specifically
aimed at making social welfare judgments
and other aggregative evaluations,

based on institutionally accepted ways of
making interpersonal comparisons: for example,
in using indexes of income inequality
(see Serge Kolm's [1969] and Anthony
Atkinson's [1970] pioneering work on this),
or in aggregate measures of distributioncorrected
real national income (Sen, 1976a),

or of aggregate poverty (Sen, 1976b).5 This
links the theory of social choice to some of
the most intensely practical debates on economic
policy.26 While Arrow's impossibility
theorem is a negative result, the challenge it
provided has led, dialectically, to a great
many constructive developments.

V. On Social Decision Mechanisms

Moving from the exercise of making social
judgments to that of choosing social
decision mechanisms, there are other difficulties
to be faced. While systematic interpersonal
comparisons of utilities (and other
ways of seeing individual advantage) can be
used by a person making social welfare
judgment, or in agreed procedures for social
judgments (based on interpreting available
statistics to arrive at, say, orderings of aggregate
poverty or inequality or distribution-
corrected real national income), this is
not an easy thing to do in social-decision
mechanisms which must rely on some standard
expressions of individual preference
(such as voting), which do not readily lend
themselves to interpersonal comparisons.
The impossibility problem, thus, has
greater resilience here. While it is also the
case that the critique of James Buchanan
(and others) of the idea of "social rationality"
and the concept of "social preference"
applies particularly in this case (that of
judging social decision mechanisms), the impossibility
problem does indeed survive, as

we have seen, even when the concept of
social preference is eschewed and the idea
of social rationality in the Arrovian form is
dropped altogether (Section III). How, then,
can we respond to the challenge in this
case?

We may begin by noting that the conditions
formulated and used by Arrow, while
appealing enough, are not beyond criticism.
First, not every conceivable combination of
individual preferences need be considered
in devising a social decision procedure, since
only some would come up in practice. As
Arrow had himself noted, if the condition of
unrestricted domain is relaxed, we can find
decision rules that satisfy all the other conditions
(and many other demands) over substantial
domains of individual preference

profiles. Arrow (1951), along with Duncan
Black, had particularly explored the case of
"single-peaked preferences," but it can be
shown (Sen, 1966) that this condition can be
far extended and generalized to a much less
demanding restriction called "value restriction.
"27


### ---Economics-1995-0-11.txt---
The plausibility of different profiles of
individual preferences depends on the nature
of the problem and on the characteristics
of individual motivations. It is readily
checked that with three or more people, if
everyone acts as homo economicus in a
cake-division problem (always preferring
more cake to oneself over all else), then
value restriction and the related conditions
would all be violated, and majority rule
would standardly lead to intransitivities. 'It
is also easy to show that in the commodity
space, with each concentrating on her own
commodity basket, the Arrow conditions
could not be all satisfied by any decision
mechanism over that domain. Majority rule
and other voting procedures of this kind do
cause cycles in general in what is called
"the economic domain" (of interpersonal
commodity space), if everyone votes in a
narrowly self-interested way.

However, majority rule would be a terrible
decision procedure in this case, and its
intransitivity is hardly the main problem
here. For example, taking the most deprived
person in a community and passing on half
her share of the cake divided between two
richer persons would be a majority improvement,
but scarcely a great welfare-economic
triumph. In view of this, it is perhaps just as
well that the majority rule is not only nasty
and brutish, but also short in consistency.28
The tension between social welfare judgments
(of different kinds explored, for example,
by Meade [1976], Arrow [1977],

Mirrlees (1982), William J. Baumol [1986],
or John Broome [1991]) and mechanical decision
rules (like majority decision) with
inward-looking, self-centered individuals is
most obvious here. Also, as Buchanan
(1994a, b) has argued, the acceptability of
majority rule is, in fact, related to its tendency
to generate cycles, and the endemic
cyclicity of majority decisions is inescapable,
given the endogeneity of alternative proposals
that can be presented for consideration.
In practice, in facing political decisions,
the choices may not come in these stark
forms (there are many issues that are mixed
together in political programs and proposals)
, and also individuals do not necessarily
only look at their "own share of the cake"
in taking up political positions and attitudes.
29 The "public choice" school has

tended to emphasize the role of logrolling
in political compromises and social decisions.
While that school has also been rather
wedded to the presumption of each person
being homo economicus even in these exercises
(see Buchanan and Tullock, 1962),

there is a more general social process here
(involving a variety of motivations) that can
be fruitfully considered in examining decision
mechanisms. Central to this is the role
of public discussion in the formation of
preferences and values, which has been emphasized
by Buchanan (1954a,b).

The condition of independence of irrelevant
alternatives is also not beyond disputation
and, indeed, has led to debates-explicitly
or by implication-for a very long

time. It was one of the issues that divided
J. C. Borda (1781) and Marquis de Condorcet
(1785), the two French mathematicians,
who had pioneered the systematic

theory of voting and group decision procedures
in the 18th century. One version of
the rule proposed by Borda, based on adding
the rank-order numbers of candidates in
each voter's preference list, violates the independence
condition rather robustly, but it

is not devoid of other merits (and is frequently
used in practice).30 Other types of
voting rules have also been shown to have
different desirable properties.3'

a somewhat exaggerated title: "The General Irrelevance
of the General Possibility Theorem") and in a
definitive paper by Jean-Michel Grandmont (1978).
Fine discussions of the issues involved in the different
types of domain conditions can be found in Gaertner
(1979) and Arrow and Herve Raynaud (1986).
28The ubiquitous presence of voting cycles in majority
rule has been extensively studied by R. D.
McKelvey (1979) and Norman Schofield (1983).
29Even individual social welfare judgments (and
more generally, individual views of social appropriateness)
presumably have some influence on political preferences.


30Positional rules of other kinds have been studied
extensively by Peter Gardenfors (1973) and Ben Fine
and Kit Fine (1974a,b). On different versions of the
Borda rule, see Sen (1977a, 1982a pp. 186-87).
31For example, Andrew Caplin and Barry Nalebuff
(1988) provide a case for 64-percent majority rule. Also
see the symposium on voting procedures led by
Jonathan Levin and Nalebuff (1995).


### ---Economics-1995-0-12.txt---
In examining social decision mechanisms,
we have to take the Arrow conditions seriously,
but not as inescapable commandments.
Our intuitions vary on these matters,
and Arrow's own theorem shows that not
everything that appeals to us initially would
really be simultaneously sustainable. There
is a need for some de-escalation in the grim
"fight for basic principles." The issue is not
the likely absence of rationally defendable
procedures for social decisions, but the relative
importance of disparate considerations
that pull us in different directions in evaluating
diverse procedures. We are not at the
edge of a precipice, trying to determine
whether it is at all "possible" for us to hang
on.

VI. Procedures and Consequences

I turn now to the general issue, identified
earlier, of the contrast between relying respectively
on (i) the "rightness" of procedures,
and (ii) the "goodness" of outcomes.
Social choice theory, in its traditional
form, would seem to belong to the latter
part of the dichotomy, with the states of
affairs judged first (the subject matter of
"social preference" or "social welfare
judgements"), followed by identification of
procedures that generate the "best" or
"maximal" or "satisficing" states. There are
two issues here. First, can consequences
really be judged adequately without any notion
of the process through which they are
brought about? I shall also presently question
whether this presumption of processindependence
is the right way of seeing the

claims of social choice theory. Second, can
we do the converse of this, and judge procedures
adequately in a consequence-independent
way? This issue I take up first.

Sugden (1981, 1986), who has extensively
analyzed this dichotomy (between procedural
and consequence-based views), explains
that in the public choice approach, which he
supports, "the primary role of the government
is not to maximize the social good, but
rather to maintain a framework of rules
within which individuals are left free to
pursue their own ends" (Sugden, 1993
p. 1948). This is indeed so, but even in
judging a "framework of rules" in this way,
we do need some consequential analysis,
dealing with the effectiveness of these
frameworks in letting individuals be actually
"free to pursue their own ends." In an
interdependent world, examples of permissive
rules that fail to generate the freedom
to pursue the respective individual ends are
not hard to find (see Sen, 1982b).

Indeed, it is not easy to believe that the
public-choice approach is-or can bereally
consequence-independent. For example,
Buchanan's support of market systems
is based on a reading of the consequences
that the market mechanism tends to produce,
and consequences certainly do enter
substantially in Buchanan's evaluation of
procedures: "To the extent that voluntary
exchange among persons is valued positively
while coercion is valued negatively, there
emerges the implication that substitution of
the former for the latter is desired, on the
presumption, of course, that such substitution
is technologically feasible and is not
prohibitively costly in resources" (Buchanan,
1986 p. 22). While this is not in serious
conflict with Buchanan's rejection of any
"transcendental" evaluation of the outcomes
(p. 22), nevertheless the assessment
of outcomes must, in some form, enter this
evaluative exercise. 32

There are, however, other-more purely
procedural-systems to be found in this literature.
If the utilitarian tradition of judging
everything by the consequent utilities is
one extreme in the contrast (focusing only
on a limited class of consequences), Nozick's
(1974) elegant exploration of libertarian
"entitlement theory" comes close to the
other end (focusing on the right rules that
cover personal liberties as well as rights of
holding, using, exchanging, and bequeathing


### ---Economics-1995-0-13.txt---
legitimately owned property). But the possibility
of having unacceptable consequences
has to be addressed by any such procedural
system. What if the results are dreadful for
many, or even all?

Indeed, it can be shown that even gigantic
famines can actually take place in an
economy that fulfills all the libertarian rights
and entitlements specified in the Nozick
system.33 It is, thus, particularly appropriate
that Nozick (1974) makes exceptions to
consequence-independence in cases where
the exercise of these rights would lead to
"catastrophic moral horrors."34 Because of
this qualification, consequences are made to
matter after all, and underlying this concession
is Nozick's good sense (similar to

Buchanan's) that a procedural system of
entitlements that happens to yield catastrophic
moral horrors (we have to have some
consensus on what these are) would

be-and should be-ethically unacceptable.
However, once- consequences are

brought into the story, not only is the purity
of a consequence-independent system lost,
but also the issue of deciding on the relative
importance of "right rules" and "good consequences"
is forcefully reestablished.

I turn now to the other side of the dichotomy:
can we have sensible outcome

judgments in a totally procedure-independent
way? Classical utilitarianism does indeed
propose such a system, but it is hard
to be convinced that we can plausibly judge
any given utility distribution ignoring altogether
the process that led to that distribution
(attaching, for example, no intrinsic
importance whatever to whether a particular
utility redistribution is caused by charity,
or taxation, or torture).35

This recognition of the role of processes
is not, in fact, hostile to social choice theory,
since there is nothing to prevent us
from seeing the description of processes as
a part of the consequent states generated
by them.36 If action A is performed, then
"action A has been done" must be oneindeed,
the most elementary-consequence

of that event. If Mr. John Major were to
wish not merely that he should be reelected
as Prime Minister, but that he should be
"reelected fairly" (I am not, of course, insinuating
that any such preference has been

expressed by Mr. Major), the consequence
that he would be seeking would have procedural
requirements incorporated within it.
This is not to claim that every process can
be comfortably placed within the description
of states of affairs without changing
anything in social choice theory. Parts of the
literature that deal with comparisons of decision
mechanisms in arriving at given states
would need modification. If, in general, processes
leading to the emergence of a social
state were standardly included in the characterization
of that state, then we have to

construct "equivalence classes" to ignore
some differences (in this case, between some
antecedent processes) to be able to discuss
cogently the "same state" being brought
about by different decision mechanisms. To
make sense of such ideas as, say, "path
independence" (on which see Plott [1973]),
so that they are not rendered vacuous,
equivalence classes of this type would certainly
have to be constructed (on the concepts
of equivalence classes and invariance
conditions, see Sen [1986b]).

The contrast between the procedural and
consequential approaches is, thus, somewhat
overdrawn, and it may be possible to
combine them, to a considerable extent, in
an adequately rich characterization of states
of affairs. The dichotomy is far from pure,
and it is mainly a question of relative concentration.


330n this see Sen (1981), linking starvation to unequal
entitlements, with actual case studies of four
famines. See also Ravallion (1987), Dreze and Sen
(1989), and Desai (1995).

34See also Nozick's (1974) discussion of "Locke's
proviso."

350n this question, see Sen (1982a,b).
360n this question, see Sen (1982b), Hammond
(1986), and Levi (1986).


### ---Economics-1995-0-14.txt---
VII. Liberties, Rights, and Preferences
The need to integrate procedural considerations
in consequential analysis is especially
important in the field of rights and
liberties. The violation or fulfillment of basic
liberties or rights tends to be ignored in
traditional utilitarian welfare economics not
just because of its consequentialist focus,
but particularly because of its "welfarism,"
whereby consequent states of affairs are
judged exclusively by the utilities generated
in the respective states.37 While processes
may end up getting some indirect attention
insofar as they influence people's utilities,
nevertheless no direct and basic importance
is attached in the utililtarian framework to
rights and liberties in the evaluation of states
of affairs.

The initial formulation of social choice
did not depart in this respect from the utilitarian
heritage, but it is possible to change
this within a broadly Arrovian framework
(see Sen, 1970, 1982a), and a good deal of
work has been done in later social choice
theory to accommodate the basic relevance
of rights and liberties in assessing states of
affairs, and thus to evaluate economic, political,
and social arrangements. If a person is
prevented from doing some preferred thing
even though that choice is sensibly seen to
be in her "personal domain," then the state
of affairs can be seen to have been worsened
by this failure. The extent of worsening
is not to be judged only by the magnitude
of the utility loss resulting from this (to
be compared with utility gains of others, if
any), since something more is also at stake.
As John Stuart Mill (1859 p. 140) noted,
"there is no parity between the feeling of a
person for his own opinion, and the feeling
of another who is offended at his holding
it."38 The need to guarantee some "minimal
liberties" on a priority basis can be incorporated
in social choice formulations.

It turns out, however, that such unconditional
priority being given even to minimal
liberty can conflict with other principles of
social choice, including the redoubtable
Pareto principle. The "impossibility of the
Paretian liberal" captures the conflict between
(i) the special importance of a person'
s preferences over her own personal
sphere, and (ii) the general importance of
people's preferences over any choice, irrespective
of field. This impossibility theorem
has led to a large literature extending, explaining,
disputing, and ameliorating the result.
39 The "ways out" that have been sought
have varied between (i) weakening the priority
of liberties (thereby qualifying the minimal
liberty condition), (ii) constraining the
field-independent general force of preferences
(thereby qualifying the Pareto principle)
, and (iii) restricting the domain of permissible
individual-preference profiles. As

in the case of the Arrow impossibility problem,
the different ways of resolving this conflict
have variable relevance depending on
the exact nature of the social choice exercise
involved.

There have also been attempts to redefine
liberty in purely procedural terms. The
last is an important subject on its own (quite
independently of any use it might have
as an attempt to resolve the impossibility),
and I shall presently consider it. But as has


### ---Economics-1995-0-15.txt---
been noted by Gaertner, Pattanaik, and
Suzumura (1992), who have recently provided
the most extensive recharacterization
of liberty (in terms of "game forms"), the
impossibility problem "persists under virtually
every plausible concept of individual
rights" (p. 161).40

The decisive move in the direction of a
purely procedural view of liberty was made
by Nozick (1974), responding to my social
choice formulation and to the impossibility
of the Paretian liberal (Sen, 1970). This has
been followed by important constructive
contributions by Gardenfors (1981) and
Sugden (1981), and the approach has been
extended and developed into game-form
formulations by Gaertner et al. (1992). In
the game-form view, each of the players has
a set of permissible strategies, and the outcome
is a function of the combination of
strategies chosen by each of the players
(perhaps qualified by an additional "move"
by "nature"). The liberties and rights of the
different persons are defined by specifying a
permissible subset from the product of the
strategy sets of the different individuals. A
person can exercise his rights as he likes,
subject to the strategy combination belonging
to the permissible set.

In defining what rights a person has, or in
checking whether his rights were respected,
there is, on this account, no need to examine
or evaluate the resulting state of affairs,
and no necessity to examine what states the
individuals involved prefer. In contrasting
this characterization of preference-independent,
consequence-detached rights with the
social choice approach to rights, perhaps
the central question that is raised is the
plausibility of making people's putative
rights, in general, so dissociated from the
effects of exercising them. This is a general
issue that was already discussed at a broader
level (Section VI).

In some contexts, the idea of seeing rights
in the form of permission to act can be
quite inadequate, particularly because of
"choice inhibition" that might arise from a
variety of causes. The long British discussion
on the failure of millions of potential
welfare recipients from making legitimate
claims (apparently due to the shame and
stigma of having one's penury publicized
and recorded) illustrates a kind of nonrealization
of rights in which permission is not
the main issue at all.4' Similarly, the inability
of women in traditionally sexist societies
to use even those rights that have not been
formally denied to them also illustrates a
type of rights failure that is not helpfully
seen in terms of game forms (see Sen, 1992b
pp. 148-50). Even the questions that standardly
come up in this country in determining
whether a rape has occurred have to go
well beyond checking whether the victim in
question was "free" to defy.

Leaving out such cases, it might well be
plausible to argue that rights can be nicely
characterized by game forms in many situations.
However, even when that is the case,
in deciding on what rights to protect and
codify, and in determining how the underlying
purpose might be most effectively

achieved, there is a need to look at the
likely consequences of different game-form
specifications and to relate them to what
people value and desire. If, for example, it
appears that not banning smoking in certain
gatherings (leaving the matter to the discretion
of the people involved) would actually
lead to unwilling victims having to inhale
40The belief that the problem can be resolved
through Pareto-improving contracts, which has been
suggested by some authors, overlooks the incentiveincompatibility
of the touted solution and, perhaps
more importantly, confounds the nature of the conflict
itself, since the conflict in values keeps open the question
as to what contracts would be offered or accepted
by the persons involved. For example, in the (rather
overdiscussed) case of whether the prude or the lewd
should read Lady Chatterley's Lover, it is not at all
clear that the prude, if he has any libertarian inclinations,
would actually offer a contract by which he
agrees to read a book that he hates to make the lewd
refrain from reading a book he loves. In fact, while the
prude may prefer that the lewd does not read that
book, consistent with that he may not want to bring
this about through an enforceable contract, and the
"dilemma of the Paretian liberal" could be his dilemma
too. The lewd too faces a decision problem about
whether to try to alter the prude's personal life rather
than minding his own business. On these issues, see
Sen (1983, 1992b), Basu (1984), and Elster and Hylland
(1986).

41Stig Kanger (1985) has illuminatingly discussed
"nonrealization" of rights, and the variety of ways this
can occur.


### ---Economics-1995-0-16.txt---
other people's smoke, then there would be
a case for considering that the game-form
be so modified that smoking is simply
banned in those gatherings. Whether or not
to make this move must depend crucially on
consequential analysis. The object, in this
case, is the prevention of the state of affairs
in which nonsmokers have to inhale unwillingly
other people's smoke: a situation they
resent and which-it is assumed-they
have a right to avoid. We proceed from
there, through consequential analysis (in an
"inverse" form: from consequences to
antecedents), to the particular game-form
formulation that would not achieve an
acceptable result. The fact that the articulation
of the game-form would be

consequence-independent and preferenceindependent
is not a terribly profound assertion
and is quite consistent with the fundamental
relevance of consequences and

preferences.

The contrast between game-form formulations
and social-choice conceptions of

rights is, thus, less deep than it might first
appear (see Sen, 1992b).42 As in other fields
considered earlier (Section VI), in this area
too, the need to combine procedural concerns
with those of actual events and outcomes
is quite strong.

VIII. Values and Individual Choices
I have so far postponed discussing individual
behavior and rationality, though the
issue has indirectly figured in the preceding
discussions (for example, in dealing with
norms for social choice, individual interest
in social welfare judgments, and determination
of voting behavior). The public choice
tradition has tended to rely a good deal on
the presumption that people behave in a
rather narrowly self-centered way-as homo
economicus in particular, even though
Buchanan (1986 p. 26) himself notes some
"tension" on this issue (see also Geoffrey
Brennan and Loren Lomarsky, 1993). Public
servants inter alia are to be seen as working
for their own well-being and success.
Adam Smith is sometimes described as
the original proponent of the ubiquity and
ethical adequacy of "the economic man,"
but that would be fairly sloppy history. In
fact, Smith (1776, 1790) had examined the
distinct disciplines of "self-love," "prudence,
" "sympathy," "generosity," and

"public spirit," among others, and had discussed
not only their intrinsic importance,
but also their instrumental roles in the success
of a society, and also their practical
influence on actual behavior. The demands
of rationality need not be geared entirely to
the use of only one of these motivations
(such as self-love), and there is plenty of
empirical evidence to indicate that the presumption
of uncompromising pursuit of narrowly
defined self-interest is as mistaken
today as it was in Smith's time.43 Just as it is
necessary to avoid the high-minded sentimentalism
of assuming that all human beings

(and public servants, in particular) try
constantly to promote some selfless "social
good," it is also important to escape what
may be called the "low-minded sentimentalism"
of assuming that everyone is constantly
motivated entirely by personal selfinterest.
44


### ---Economics-1995-0-17.txt---
This does not, however, negate an important
implication of the question raised by
Buchanan and others that public servants
would tend to have their own objective
functions; I would dissociate that point from
the further claim, with which it has come
mixed, that these objective functions are
narrowly confined to the officials' own selfinterest.
The important issue to emerge is

that there is something missing in a large
part of the resource-allocation literature (for
example, in proposals of algorithms for decentralized
resource allocation, from Oscar

Lange and Abba Lerner onward) which
make do without any independent objective
function of the agents of public action. The
additional assumption of homo economicus
is not needed to point to this general
lacuna.

While this has been a somewhat neglected
question in social choice theory

(though partially dealt with in the related
literature on implementation), there is no
particular reason why such plurality of motivations
cannot be accommodated within a

social choice framework with more richly
described social states and more articulated
characterization of individual choices and
behavior. In the formulation of individual
preference used by Arrow (1951) and in
traditional social choice theory, the nature
of the objective function of each individual
is left unspecified. While there is need for
supplementary work here, this is a helpfully
permissive framework-not tied either to
ceaseless do-gooding, or to uncompromising
self-centeredness.

Even with this extended framework, taking
us well beyond the homo economicus,
there remain some difficulties with the notion
of individual rationality used here.
There is a problem of "insufficiency" shared
by this approach to rationality with other
"instrumental" approaches to rationality,
since it does not have any condition of critical
scrutiny of the objectives themselves.
Socrates might have overstated matters a bit
when he proclaimed that "the unexamined
life is not worth living," but an examination
of what kind of life one should sensibly
choose cannot really be completely irrelevant
to rational choice.45 An "instrumental
rationalist" is a decision expert whose response
to seeing a man engaged in slicing

his toes with a blunt knife is to rush to
advise him that he should use a sharper
knife to better serve his evident objective.
This is perhaps more of a limitation in
the normative context than in using the
presumption of rationality as a device for
predicting behavior, since such critical
scrutiny might not be very widely practiced.
However, the last is not altogether clear,
since discussions and exchange, and even
political arguments, contribute to the formation
and revision of values. As Frank

Knight (1947 p. 280) noted, "Values are
established or validated and recognized
through discussion, an activity which is at
once social, intellectual, and creative."
There is, in fact, much force in Buchanan's
(1954a p. 120) assertion that this is a central
component of democracy ("government by
discussion") and that "individual values can
and do change in the process of decisionmaking.
"

This issue has some real practical importance.
To illustrate, in studying the fact that
famines occur in some countries but not in
others, I have tried to point to the phenomenon
that no major famine has ever

taken place in any country with a multiparty
democracy with regular elections and with a
reasonably free press (Sen, 1984).46 This
applies as much to the poorer democratic
countries (such as India, Zimbabwe, or
Botswana) as to the richer ones.47 This is
largely because famines, while killing mil-
450n this subject, see Nozick (1989).
46See also Dreze and Sen (1989) and World Disasters
Report (1994 pp. 33-37).

In contrast, China-despite its fine record of public
health and education even before the reformsmanaged
to have perhaps the largest famine in recorded
history, during 1959-1962, in which 23-30 million people
died, while the mistaken public policies were not
revised for three years through the famine. In India, on
the other hand, despite its bungling ways, large famines
stopped abruptly with independence in 1947 and the
installing of a multiparty democracy (the last such
famine, "the great Bengal famine," had occurred in
1943).


### ---Economics-1995-0-18.txt---
lions, do not much affect the direct wellbeing
of the ruling classes and dictators,
who have little political incentive to prevent
famines unless their rule is threatened by
them. The economic analysis of famines
across the world indicates that only a small
proportion of the population tends to be
stricken-rarely more than 5 percent or so.
Since the shares of income and food of
these poor groups tend normally to be no
more than 3 percent-of the total for the
nation, it is not hard to rebuild their lost
share of income and food, even in very poor
countries, if a serious effort is made in that
direction (see Sen, 1981; Dreze and Sen,
1989). Famines are thus easily preventable,
and the need to face public criticism and to
encounter the electorate provides the government
with the political incentive to take
preventive action with some urgency.
The question that remains is this. Since
only a very small proportion of the population
is struck by a famine (typically 5 percent
or less), how does it become such a
potent force in elections and in public criticism?
This is in some tension with the assumption
of universal self-centeredness, and
presumably we do have the capacity-and
often the inclination-to understand and
respond to the predicament of others.48
There is a particular need in this context to
examine value formation that results from
public discussion of miserable events, in
generating sympathy and commitment on
the part of citizens to do something to prevent
their occurrence.

Even the idea of "basic needs," fruitfully
used in the development literature, has to
be related to the fact that what is taken as a
"need" is not determined only by biological
and uninfluencible factors. For example, in
those parts of the so-called Third World in
which there has been increased and extensive
public discussion of the consequences
of frequent childbearing on the well-being
and freedom of mothers, the perception that
a smaller family is a "basic need" of women
(and men too) has grown, and in this value
formation a combination of democracy, free
public media, and basic education (especially
female education) has been very potent.
The implications of this finding are
particularly important for rational consideration
of the so-called "world population

problem."49

Similar issues arise in dealing with environmental
problems. The threats that we

face call for organized international action
as well as changes in national policies, particularly
for better reflecting social costs in
prices and incentives. But they are also dependent
on value formation, related to public
discussions, both for their influence on
individual behavior and for bringing about
policy changes through the political process.
There are plenty of "social choice problems"
in all this, but in analyzing them, we have to
go beyond looking only for the best reflection
of given individual preferences, or the
most acceptable procedures for choices
based on those preferences. We need to
depart both from the assumption of given
preferences (as in traditional social choice
theory) and from the presumption that people
are narrowly self-interested homo economicus


### ---Economics-1995-0-19.txt---
(as in traditional public choice theory)
.

IX. Concluding Remarks

Perhaps I could end by briefly returning
to the questions with which I began. Arrow's
impossibility theorem does indeed identify a
profound difficulty in combining individual
preference orderings into aggregative
social welfare judgments (Section II). But
the result must not be seen as mainly a
negative one, since it directly leads on to
questions about how to overcome these
problems. In the context of social welfare
judgments, the natural resolution of these
problems lies in enriching the informational
base, and there are several distinct ways of
doing this (Section IV). These approaches
are used in practice for aggregative judgments
made by individuals, but they can

also be used for organized procedures for
arriving at social measures of poverty, inequality,
distribution-adjusted real national
incomes, and other such aggregative indicators.


Second, Buchanan's questioning of the
concept of social preference (and of its use
as an ordering to make-or explain-social
choices) is indeed appropriate in the case of
social decision mechanisms, though less so
for social welfare judgments (Section III).
The Arrow theorem, in its original form,
does not apply once social decision-making
is characterized in terms of choice functions
without any imposed requirement of

intermenu consistency. However, when the
natural implications of taking a .choicefunctional
view of social decisions are

worked out, Arrow's conditions have to be
correspondingly restated, and then the impossibility
result returns in its entirety once
again (Section III). The idea of social preference
or internal consistency of social

choice is basically redundant for this impossibility
result. So Buchanan's move does not
negate Arrow's impossibility. On the other
hand, it is an important departure in its own
right.

Coming to terms with the impossibility
problem in the case of social decision mechanisms
is largely a matter of give and take
between different principles with respective
appeals. This calls for a less rigid interpretation
of the role of axiomatic demands on
permissible social decision rules (Section V).
Third, Buchanan's argument for a more
procedural view of social decisions has much
merit. Nevertheless, there are good reasons
to doubt the adequacy of a purely procedural
view (independent of consequences), just
as there are serious defects in narrowly consequentialist
views (independent of procedures)

. Procedural concerns can, however,
be amalgamated with consequential ones by
recharacterizing states of affairs appropriately,
and the evaluation of states can then
take note of the two aspects together (Section
VI). This combination is especially important
in accommodating liberty and rights
in social judgments as well as social decision
mechanisms (Section VII).

Finally, there is room for paying more
attention to the rationality of individual behavior
as an integral component of rational
social decisions. In particular, the practical
reach of social choice theory, in its traditional
form, is considerably reduced by its
tendency to ignore value formation through
social interactions. Buchanan is right to emphasize
the role of public discussion in the
development of preferences (as an important
part of democracy). However, traditional
public choice theory is made unduly
narrow by the insistence that individuals
invariably behave as homo economicus
(a subject on which social choice theory
is much more permissive). This uncompromising
restriction can significantly misrepresent
the nature of social concerns and

values. But aside from this descriptive limitation,
there is also an important issue of
"practical reason" here. Many of the more
exacting problems of the contemporary
world-varying from famine prevention to
environmental preservation-actually call
for value formation through public discussion
(Section VIII).

On the rationality of social decisions,
many important lessons have emerged from
the discipline of social choice theory as well
as the public choice approach. In fact, we
can get quite a bit more by combining these
lessons. As a social choice theorist, I had


### ---Economics-1995-0-20.txt---
not, in fact, planned to be particularly evenhanded
in this paper, but need not, I suppose,
apologize for ending up with rather
even hands.
 ## Economics-1996-0


### ---Economics-1996-0-02.txt---
Interest in health economics has soared over
the past three decades, stimulated by intellectual
innovations, greater availability of data,
and, most importantly, a surge in health care
spending from 6 to 14 percent of GDP.' An
11 -fold increase2 in the number of Ph.D.s has
enabled many professional schools, government
agencies,3 and research institutes to add
health economists to their staffs. Nevertheless,
the health care debate of 1993-1994 benefited
much less than it could have from the results
of their research.

In this lecture I identify the primary sources
of modern health economics and describe interactions
between the discipline and the field
of health, drawing heavily on my personal experience.
I then turn to the question of why

economists did not have more impact on
health care reform. I report and analyze the
answers of health economists, economic theorists,
and practicing physicians to a survey I
conducted in 1995. My principal conclusion is
that value differences among economists, as
well as among all Americans, are a major barrier
to effective policy-making. I discuss the
implications of the importance of values for
economics and conclude the lecture with my
recommendations for health care reformrecommendations
based on my values as well

as my understanding of health economics.
I. The Past

In 1963 a seminal paper by Kenneth Arrow
discussed risk aversion, moral hazard, asymmetrical
information, philanthropic externalities,
and numerous other topics that have since
played major roles in health economics research.
4 He saw that uncertainty about health
status and about the consequences of care was
the key to understanding the health sector from
both positive and normative perspectives. As
Arrow wrote, "Recovery from disease is as
unpredictable as its incidence" ( 1963 p. 951 ).
At the same time that Arrow was depicting
the theoretical landscape, Martin Feldstein
was pioneering in the application of quantitative
methods such as 2-stage least squares,
principal component analysis, and linear programming
to the estimation of production

functions and other important economic aspects
of medical care. His numerous papers
analyzing the British National Health Service
formed the basis for his Ph.D. thesis at Oxford
University (Feldstein, 1967).

A third line of work that has had a significant
influence on health economics also began
in the early 1960's with the National
Bureau of Economic Research Conference
on Investment in Human Beings (1962) and
Gary S. Becker's treatise on human capital


### ---Economics-1996-0-03.txt---
(1964). The NBER conference volume included
Selma Mushkin's (1962) paper,

"Health As an Investment," and a few years
later the application of the human capital
model to health was given its fullest development
by Michael Grossman (1972).

Predating and postdating the theoretical and
econometric innovations of the 1960's is a
stream of research that focuses on health care
institutions, technology, and policy. As early
as 1932, Michael M. Davis and C. Rufus
Rorem (1932) were writing about the crisis in
hospital finance. Significant contributions to
this genre have been made by Henry Aaron,
Alain Enthoven, Rashi Fein, Eli Ginzberg,
Herbert Klarman, Dorothy Rice, Anne Scitovsky,
Anne and Herman Somers, Burton Weisbrod,
and many others. Although they are all economists,
much of their work does not appear in
economics journals, but rather in books and in
publications such as the New England Journal
of Medicine, Journal of the American Medical
Association, Milbank Memorial Fund Quarterly,
and Health Affairs.

In recent decades several leading health
economists have addressed theoretical, empirical,
and policy questions in various aspects of
their research (e.g., Joseph Newhouse, Mark
Pauly). Health economics has also been enlivened
and enriched by contributions from economists
who are primarily specialists in other
fields such as industrial organization, labor, finance,
and public economics (e.g., Sherwin
Rosen, Richard Zeckhauser). There has also
been a welcome infusion from another direction,
namely physicians who have earned

Ph.D.s in economics and who now contribute
to the economics literature (e.g., Alan Garber,
Mark McClellan).

Parenthetically, all this name-dropping has
a point. I want to underscore the varied intellectual,
methodological, and ideological sources
that have contributed to the health economics
enterprise. Research has often been described
as lonely work, and in one sense it is. But in
another sense it is the most collective of all
human activities. The philosopher Susan Haack
(1995) sees scientific research as analogous to
an attempt by many participants to fill out a
huge crossword puzzle. We have clues; we try
out possible answers; we check to see whether
they fit together. Occasionally, an Arrow or a
Becker comes up with one of the really big
answers that runs across the puzzle and makes
it easier to discover the smaller words that intersect
it. If several of the small answers don't
fit, however, we may have to modify or even
reject the larger one. It is good to remember
that all answers are provisional until the puzzle
is completed-and it never will be.5
Although I have mentioned only American
economists, note should be taken of many fine
health economists in England, Canada, and
other high-income countries. There is, however,
less of a global intellectual community
in this field than in some other branches of
economics'-or in other fields of health7-
because most health economics research is
applied and is (or is perceived to be) country
specific. More than 60 years ago Walton
Hamilton (1932) noted that "The organization
of medicine is not a thing apart which can
be subjected to study in isolation. It is an
aspect of culture whose arrangements are
inseparable from the general organization of
society" (p. 190). On the whole I agree
with Hamilton; there are, however, important
economic questions concerning technology
assessment and disease prevention that are
common to all high-income countries. This
type of research does not receive support commensurate
with its importance because funding
sources, both public and private, tend to
focus on national problems.

My involvement in health economics grew
out of my research on the service industries
(Fuchs, 1968, 1969). It was motivated in part
by a desire to gain a better understanding of
the postindustrial society that was emerging in
the United States and other developed coun-
' In an extension of the crossword puzzle analogy suggested
by Richard J. Zeckhauser in a 1995 personal communication,
it seems that economics might make more
progress if theorists didn't tend to concentrate on the lower
left-hand corner of the puzzle while empiricists work the
upper right-hand corner.

6 The relatively new International Health Economics
Association will hold its inaugural conference in Vancouver
in May 1996.

7 The Journal of the American Medical Association has
twenty international editions published weekly in eleven
languages, with 40 percent more recipients than the regular
U.S.-based edition (George D. Lundberg and Annette
Flanagin, 1995).


### ---Economics-1996-0-04.txt---
tries (Fuchs, 1966, 1978a). The growth of the
service economy and improved methods of
contraception were bringing women into paid
employment and dramatically changing gender
roles and relationships. Lower fertility and
longer life expectancy were transforming the
age distribution of the population, and this
transformation, along with the fragmentation
of the family and the declining influence of
traditional religion, were creating new social
and economic conditions. The health sector,
with its nonprofit institutions, professional
dominance, sharply skewed distribution of demand,
and the critical importance of the consumer
in the production process, seemed like
a fruitful area for investigation. I was particularly
interested in trying to understand the determinants
of health and the determinants of

health care expenditures.

With regard to health, my research has led
me to emphasize the importance of nonmedical
factors such as genetic endowment, the
physical and psychosocial environment, and
personal behaviors such as cigarette smoking,
diet, and exercise. Over time, advances in
medical science contribute significantly to reductions
in morbidity and mortality; at any

given point in time, however, differences in
health levels within or between developed
countries are not primarily related to differences
in the quantity or quality of medical
care.8

With respect to expenditures on medical
care, my research has led me to emphasize the
importance of supply factors, especially technology
and the number and specialty mix of
physicians.9 To be sure, conventional demand
factors such as price, income, and insurance
play significant roles, but in my judgment concentration
on them to the exclusion of (partly
exogenous) supply factors misses a big part of
the expenditures story. Despite many attempts
to discredit it,'? the hypothesis that fee-forservice
physicians can and do induce demand
for their services is alive and well.11
My views about health and health care expenditures
have been formed not only through

research but also through close interaction
with medical scientists, practicing physicians,
and other health professionals. Since 1968 I
have maintained a regular medical school faculty
appointment in addition to my appointment
in economics, and have participated
every year in a wide variety of health-related
activities. This dual life would have gained approval
from John Stuart Mill who, in The Principles
of Political Economy (1848, reprinted
1987), wrote, "It is hardly possible to overrate
the value ... of placing human beings in contact
with persons dissimilar to themselves, and
with modes of thought and action unlike those
with which they are familiar ... Such communication
has always been ... one of the primary
sources of progress" (p. 581).

The proposition that the discipline of economics
has a great deal to contribute to health
and medical care is not one likely to require
elaborate defense before this audience. (I have
had audiences that were less receptive to this
notion.) It might, however, be useful to report
briefly just what it was in economics that I
found to be most relevant in the invasion of
alien turf. (To avoid undue suspense, let me
say at once that it was not game theory.)
In my experience, the most important contribution
we make is the economic point of

view, which may be summed up in three
words: scarcity, substitutability, and heterogeneity.
This economic point of view stands

in stark contrast to the romantic and monotechnic
points of view that I found prevalent
among health professionals and health policymakers.
The romantic point of view refuses to
accept the notion that resources are inherently
scarce; any apparent scarcity is attributed to
some manmade problem, such as capitalism or
socialism, market failure or excessive government
interference. In the 1960's and 1970's,
many physicians said that there was no need
to limit expenditures for medical care if only


### ---Economics-1996-0-05.txt---
we would cut defense spending. In 1996, when
health care expenditures are almost four times
as large as the defense budget, this argument
is not heard as often. Because it denies the
inevitability of choice, the romantic point of
view is increasingly seen as impotent to deal
with the problems of health care."2
To be sure, it is not clear whether economic
research or the force of circumstances is bringing
about the change in point of view. I suspect
that there is a synergistic relationship in which
the former provides the language to give
expression to the latter. Or, as Max Weber
(1915; reprinted 1946) wrote, material and
ideal interests are the tracks on which society
rides, but ideas throw the switches (p. 280).
The monotechnic point of view, found frequently
among physicians, engineers, and others
trained in the application of a particular
technology fails to recognize the diversity of
human wants, or acknowledge the difference
between what is technically best and what is
socially desirable." "Optimal" care is defined
as the point where the marginal benefit is zero,
ignoring the fact that resources used for health
care have alternative uses that might yield
greater benefit. The "production" of health is
viewed narrowly as a function of inputs of
medical care, and the appropriate input mix
is assumed to be determined by technology
without regard to relative prices, explicit or
implicit. For example, Feldstein found that average
lengths of stay in British hospitals were
uniform across regions despite large regional
differences in the pressures for admission.'4
The monotechnic view often fails to consider
the heterogeneity of preferences, even
though for many health problems there are alternative
interventions: one drug versus another,
drugs versus surgery, or even "watchful
waiting" versus any intervention. Under the
influence of economists and other behavioral
scientists, physicians are now making such
choices with more attention to patient differences
in time preference, attitudes toward risk,
tolerance of pain, functional needs, and other
characteristics.

Among our specific tools, one of the most
useful is the idea of the margin. The key to
gaining acceptance for this principle is to have
people realize that most decisions involve a
little more or a little less, and that they will
make better decisions if they look at the costs
and benefits associated with having a little
more or a little less. This formulation is more
effective than postulating "maximization,"
which economists find useful for classroom or
research purposes, but sounds unreal to most
noneconomists.

David M. Eddy's research on the frequency
with which women should get Pap smears provides
a fine example of the use of marginal (or
incremental) analysis to assist in medical
decision-making. This screening test for cervical
cancer is of proven safety and effectiveness,
and before Eddy's work appeared most
experts recommended that women obtain this
test annually. Using mathematical models and
clinical studies of the natural history of the disease,
Eddy (a physician with extensive training
in operations research and economics)
calculated the incremental cost of 1 additional
year of life expectancy with screening regimes
ranging from once every 6 months to once
every 5 years. The results were striking. Some
screening has a high yield at low incremental
cost, but as the frequency of screening is increased
from once every 2 years to once a year
the incremental cost rises to close to $1 million
per additional year of life expectancy (Eddy,
1980, 1987, 1990).15

The impact of Eddy's research on health
policy is worth noting. The American Cancer
Society accepted his conclusions and the Society'
s recommendation to screen once every
3 years made the front page of the New York
'2 As a sign of the times, Sweden, Norway, Finland,
and the World Health Organization are sponsoring the first
international conference on priorities in health care in October
1996.

' Economists fall into their own monotechnic trap
when they offer policy advice under the assumption that
efficiency is society's only goal.

'4 See Feldstein (1967).

'5 To put this in perspective, consider the choice between
tissue plasminogen activator (TPA) and its cheaper
alternative, streptokinase, as the treatment to dissolve a
clot during a heart attack. The latest studies suggest that
the incremental cost of TPA rather than streptokinase is
$33,000 per year of life extended (D. B. Mark et al., 1995).
In the United States TPA is usually the treatment of
choice, but Canadians use streptokinase.


### ---Economics-1996-0-06.txt---
Times. The U.S. Surgeon General, the U.S. Preventive
Services Task Force, and the American
College of Physicians supported this position,
and many individual physicians changed their
practice accordingly. Intense opposition came
from the American College of Obstetricians and
Gynecologists and the American Society of Cytology.
The contending groups finally negotiated
a compromise along the following lines: "Pap
smears should be done annually; after two or
more negative examinations the frequency can
be decreased." 16

The economist's distinction between movement
along a function and a shift in the function
is a very useful one. It is particularly applicable
in discussing the relationship between medical
care and health. At any given time in developed
countries the effects of additional medical care
on health are usually small, but over time advances
in medical science have had significant
effects on health."7 Or consider the relationship
between infant mortality and per capita income.
At any given time income is a good predictor of
infant mortality, especially post-neonatal mortality
(28 days to one year). In log-log regressions
across the 48 states in 1937 and 1965, the
income elasticity of post-neonatal mortality was
-0.53 (0.11) and -0.49 (0.12) respectively.18
The decline in post-neonatal mortality between
1937 and 1965, however, was consistent with an
elasticity of -2.00. There was undoubtedly a
shift in the function associated with the introduction
of antibiotics and other advances in
medical science (Fuchs, 1974b). In 1991 the
elasticity was -0.73 (0.12) but the change from
1965 to 1991 was consistent with an elasticity
of -1.08, suggesting a further shift in the function,
but not nearly so large as the shift between
1937 and 1965.

Economists have much to contribute to the
health field. What can they expect in exchange?
The most immediate benefit to me was the pressure
to make my lectures and research results
accessible, relevant, and credible to intelligent
but untutored and often unsympathetic audiences.
I was obliged to write clearly and simply
and to reconsider assumptions and conclusions
in economics that I might otherwise have accepted
too readily. My experience was in accord
with that of Thomas Henry Huxley (1863) who
wrote, "Some experience with popular lecturing
has convinced me that the necessity of making
things plain to uninstructed people was one of
the very best means of clearing up the obscure
corners in one's own mind."

For example, one of the questions that troubled
me for a long time is why there is such
a strong correlation between health and years
of schooling. I originally believed that this
was another manifestation of the productivityenhancing
effect of education. Schooling could
increase an individual's knowledge about the
health effects of personal behavior and medical
care options or could enable a person to better
process and act upon information about health
(Grossman, 1975). Or schooling could increase
an individual's ability to develop strategies
of self control (Richard A. Thaler and
H. M. Shefrin, 1981). I began to doubt the
schooling-causes-health hypothesis, however,
when it was observed that the favorable effect
of an additional year of schooling on health
does not diminish with increased years of
schooling. It is just as strong for those with
more than a high school education as for those
with less and continues right through graduate
school on up to the doctoral level (Grossman,
1975).19 I began to suspect that perhaps the correlation
was the result of some underlying difference
among individuals that affects both
schooling and health.

To explore this question I examined survey
data on smoking behavior collected

by colleagues in the Stanford Heart Disease
Prevention Program as part of a health


### ---Economics-1996-0-07.txt---
education experiment designed to alter smoking
and other risks for heart disease (Nathan
Maccoby and Douglas S. Solomon, 1981).
Identical regressions of smoking on schooling
were estimated at age 17 and at age 24,
with schooling measured in both cases as the
number of years the individual would eventually
complete. The most striking result was
the absence of any increase in the size of the
schooling coefficient between the ages of 17
and 24. The additional schooling could not
be the cause of the differential smoking behavior
(and by extension the differential

health associated with smoking) at age 24
because the differences in smoking were already
evident at age 17, before the differences
in schooling had emerged (Philip

Farrell and Fuchs, 1982) .20

In my judgment, the most likely explanation
for the high correlation between health and
schooling is that both reflect differences in
time preference (Fuchs, 1982). Both health
and schooling are aspects of investment in human
capital; differences among individuals in
time preference that are established at an early
age could result in different amounts of investment
in health and education.21

Although I believe there have been many
fruitful interactions between economics and
health, the political debate over health care
reform in 1993-1994 benefited much less
than it could have from the insights of economists.
Possible explanations for the failure
of health economics research to have more
impact on policy are explored in the next
section.

II. The Present

George Stigler's Presidential Address to the
American Economic Association in December
1964 was distinctive in its emphasis on prophecy
over preaching. To be specific, Stigler predicted
that economics was "at the threshold of
its golden age" (Stigler, 1965 p. 17) because
"the age of quantification is now full upon us"
(p. 16). The growth of empirical estimation
was, for Stigler, "a scientific revolution of the
very first magnitude" (p. 17). He believed
that empirical research would have an impact
on policy far beyond anything possible from
theory alone because "a theory can usually be
made to support diverse policy positions. Theories
present general relationships, and which
part of a theory is decisive in a particular context
is a matter of empirical evidence" (p. 13).
With regard to health care, Stigler's prediction
of a vast expansion in empirical research
has been amply fulfilled. During the past 30
years economists have published thousands of
empirical articles on various aspects of health
and medical care. But the shallow and inconclusive
debate over health policy in 1993-

1994 contradicts his expectation that this
research would narrow the range of partisan
disputes and make a significant contribution to
the reconciliation of policy differences.22 What
went wrong?

One possibility is that the research was inconclusive.
If health economists cannot agree

among themselves, why should their research
have a salutary effect on public policy? Second,
even if the research were conclusive, it
would not be of much help to policy if the
results were not adequately disseminated to a
wider audience. A third possible explanation
is that the policy debate foundered on differences
in values, differences which could not
be reconciled by empirical research, however
conclusive and however well disseminated.
To gain some insight into these matters,
I prepared a 20-question survey concerning
health economics and health policy and sent it
to health economists, economic theorists, and
practicing physicians. The health economists
were those whom I considered to be the lead-
20 It is worth noting that the negative relation between
schooling and smoking is only evident for cohorts that
reached age 17 after the information about the effects of
smoking on health became available. It is also of interest
that the relationship has not diminished for more recent
cohorts even though the information about the negative
consequences of smoking has become more widely
available.

21 There are alternative or complementary "third variable"
explanations possible; compare Albert Bandura's
(1991) concept of self-efficacy.

22 Stigler's optimism regarding the impact of empirical
research on policy may have had more vindication in other
fields, but my research into family issues (Fuchs, 1983)
and gender issues (Fuchs, 1988a) do not lead me to such
a conclusion.


### ---Economics-1996-0-08.txt---
ing people in the field, plus some of the more
promising recent Ph.D.s. There were 46 respondents
(response rate 88 percent). The theorists
were also leaders in the field; I was
assisted in selecting them by two eminent theorists.
" There were 44 respondents (response
rate 63 percent). The practicing physicians
were reached through my personal contacts,
and include colleagues and friends of those
contacts. Nearly all are in private practice, not
teaching, research, or administration. They are
located on both the east and west coasts in
small towns and large cities. The practice settings
vary from solo to a group of over 100
physicians, and in organizational form from
traditional fee-for-service to capitation. They
include generalists, surgical specialists, and
nonsurgical specialists. There were 42 physician
respondents (response rate 89 percent).
The participants were asked to indicate
whether they agree or disagree with each of 20
relatively short statements; they were also
given the option of answering "no opinion."
Ten percent of the health economists' replies
were "no opinion"; the theorists used that option
19 percent of the time, and the physicians
11 percent. Participants were also invited to
qualify any of their replies by jotting comments
on the back of the survey. The percentage
of replies that were qualified was 8, 5, and
3 for the health economists, theorists, and physicians,
respectively. Participants were told to
assume that the statements refer to the United
States in 1995, other things held constant. For
statements with more than one part, "agree"
would indicate that the respondent agreed with
all parts of the statement. The order of the
questions was determined randomly, and respondents
were guaranteed anonymity.

Three experts24 from three different universities
who were not participants in the survey
were asked to identify which of the 20 questions
were relatively value-free ("positive"
questions) and which had substantial value aspects
("policy-value" questions). Their independent
replies were almost unanimous in

identifying seven of the questions as "positive,
" and thirteen as "policy-value." Table 1
shows the percent agreeing for each question,
with the two types of questions grouped separately.
Question numbers refer to the ordering
of the questions in the survey. The policyvalue
questions are presented in three groups:
four that pertain directly to national health insurance,
three that pertain directly to health
insurance company underwriting, and all others.
Questions for which the percentage agreeing
differs significantly from a 50-50 split (by
a chi-square test) are identified with asterisks.
We see in Table 1 that the degree of consensus
on positive questions among health

economists is extremely high.25 In six of the
seven cases the hypothesis that the observed
split differs from a 50-50 split simply by
chance is rejected with p < 0.01 and the seventh
with p < 0.05. There is also a high degree
of consensus among economic theorists, but
for two of the questions (12 and 13) the majority
of theorists gave replies opposite to
the majority of health economists. Consensus
among the physicians on the positive questions
was more rare. In no case did the split differ
from 50-50 with p < 0.01, and in only three
cases was the split significant at p < 0.05. For
one question (4) the majority of physicians
gave replies opposite to the majority of health
economists.26

When we turn to the policy-value questions,
agreement among the health economists drops
sharply. For example, in replies to the four questions
dealing with support for national health insurance,
the health economists never depart

significantly from a 50-50 split. On question 8,


### ---Economics-1996-0-09.txt---
Survey Health Economic Practicing

question economists theorists physicians
number' Question (n c 46) (n c 44) (n c 42)
A. Positive Questions:

4 The high cost of health care in the United States makes U.S. firms 9** 17** 64
substantially less competitive in the global economy.
9 Third-party payment results in patients using services whose costs 84** 93** 73*
exceed their benefits, and this excess of costs over benefits
amounts to at least 5 percent of total health care expenditures.
10 Physicians have the power to influence their patients' utilization 68* 77** 67
of services (i.e., shift the demand curve), and their propensity
to induce utilization varies inversely with the level of demand.
12 Widespread use of currently available screening and other 11** 83** 37
diagnostic techniques would result in a significant (more than
3%) reduction in health care expenditures (from what they
would otherwise be) 5 years from now.
13 The primary reason for the increase in the health sector's share of 81** 37 68*
GDP over the past 30 years is technological change in
medicine.

18 Differential access to medical care across socioeconomic groups 0** 17** 34*
is the primary reason for differential health status among these
groups.

19 In the long run employers bear the primary burden of their 13** 8** 43
contributions to employees' health insurance.
B. Policy-Value Questions:

National health insurance questions:
3 The U.S. should now enact some plan that covers the entire 62 65* 68*
population.

7 The U.S. should seek universal coverage through a broad-based 54 56 56
tax with implicit subsidies for the poor and the sick.
14 The U.S. should seek universal coverage through mandates, with 38 29* 46
explicit subsidies for the poor and the sick.
15 Given a choice between the Clinton health care plan or no federal 36 33* 28**
health care legislation for at least 5 years, the Clinton plan
should be approved.

Insurance company underwriting questions:
8 Insurance companies should be required to cover all applicants 51 29** 69*
regardless of health condition and not allowed to charge sicker
individuals higher premiums.

17 Health insurance premiums should be higher for smokers than for 71** 90** 85**
nonsmokers.

20 Health insurance premiums charged to individuals born with 14** 20** 13**
genetic defects (that result in above average use of medical
care) should be higher than those charged to individuals
without such defects.

All other policy-value questions:

I It is inequitable for the govemment to vary subsidies for health 62 36 86**
insurance by size of firm.

2 "Any willing provider" legislation (that requires health plans to 12** 12** 39
include any physician who wants to be included) is desirable
for society as-a whole.

5 National standardized health insurance benefit packages should be 42 51 63
established.

6 It is inefficient for the govemment to vary subsidies for health 66* 42 73*
insurance by size of firm.

11 Expenditures on medical R&D are greater than is socially 27* 29* 16**
optimal.

16 All health insurance plans should be required to offer "point of 30** 55 83**
service" options (that allow patients to obtain care outside the
basic plan at additional cost).

'Of those who agree or disagree. * Significantly different from 50 percent at p < 0.05.
Question numbers refer to order of questions in original survey. ** Significantly different from 50 percent at p < 0.01.


### ---Economics-1996-0-10.txt---
which would require insurance companies to
cover all applicants regardless of health condition
with no premium surcharge for the sick, the
health economists are evenly divided: 51 percent
agree and 49 percent disagree. Among economic
theorists there is slightly more agreement on
policy, but not as much as among practicing
physicians who, contrary to both groups of economists,
show more agreement on policy-value
than on positive questions.

The contrasts between the replies by group
and type of question are brought more sharply
into focus in Table 2, which shows the average
absolute difference between the percentage
agreeing and the percentage disagreeing.
Among health economists the extent of consensus
for the positive questions is significantly
larger than for the policy-value

questions regardless of whether the comparison
is between means or medians. Although
the sample sizes are very small (7 and 13 ), the
differences by type of question are so large we
can reject the null hypothesis with considerable
confidence.27

It is also worth noting that the extent of
agreement among health economists on the
positive questions is much higher than is usually
found in surveys of economists covering
a wide variety of fields. For example, in a
survey conducted by Richard M. Alston et al.
(1992) the authors identify ten questions as
"micro-positive" and seven as "micronormative.
"28 In order to achieve comparability
between their survey and mine, I

combined their "agree, with provisos" with
their "agree," and then calculated the mean
absolute difference between percentage agreeing
and percentage disagreeing.29 This difference
(22 percentage points) was much smaller
(and less statistically significant) than the difference
I found for the health economists.


### ---Economics-1996-0-11.txt---
Why is there so little agreement among economists
regarding policy-value questions when
there is so much agreement on the positive
questions? One possible explanation is differences
in values. Most health policy decisions
have significant implications for freedom, efficiency,
justice, and security. Health economists
(like other Americans) probably desire all these
goals, but (again like other Americans) they
probably differ in the values they attach to
them, or in the way they define them,3' and
these differences could lead to sharply different
views about policy.

Another possible explanation is that there
are positive questions embedded in the policyvalue
questions and that health economists disagree
with respect to those positive questions.
This is the view taken by Milton Friedman in
195332 although he subsequently modified his
position in 1966 and 1995.33 In order to gain
some insights concerning the roles of values
and embedded positive issues in policy differences
I take a closer look at the policy-value
questions bearing on national health insurance
(3, 7, 14, 15) and on insurance company underwriting
(8, 17, 20).

Consider, for instance, question 3 which
calls for some national plan to cover the entire
population. The 62-38 percent split among
health economists may well reflect differences
in values, with those who agree placing a high
value on providing all Americans with the
right to have access to health care. On the other
hand, it is readily apparent that there are many
positive questions embedded in this policyvalue
question. For instance, most economists
see a loss in efficiency from requiring everyone
to have the same health insurance, but they
probably differ in their estimates of the extent
of the loss. Some may even believe there is a
net gain in efficiency because of imperfections
in the private market for health insurance.
Strongly held differences about this positive
question could produce different answers to
question 3 even among respondents with similar
values.

Some of the positive questions embedded in
question 3 may be beyond the scope of conventional
economics. For instance, Professor

A may favor national health insurance in part
because she believes it will contribute to a
more stable and harmonious society.34 Professor
B may disagree with that prediction, and
is therefore less inclined to support national
health insurance.

The role of embedded positive questions
can also be easily discerned in the three questions
(8, 17, 20) dealing with insurance company
underwriting. Health economists strongly
support charging higher premiums for smokers
than for nonsmokers, but are strongly opposed
to charging higher premiums to individuals
born with genetic defects. On question 8, dealing
with requiring insurance companies to insure
the sick with no premium surcharge, the
health economists are evenly split. One of the
positive questions embedded in question 8 is
the reason for people's illness. If a respondent
thought that most illness was the result of genetic
differences, the reply would presumably
be consistent with the answer to question 20.
On the other hand, if most illness was assumed
to be the result of personal behaviors like cigarette
smoking, the reply would probably be
consistent with the one given to question 17.
Inasmuch as leading medical scientists have
strongly divergent views about the importance
of genetic factors in disease, it is hardly
surprising that health economists are unable
to reach agreement. The state of knowledge
about the links between genes and disease is
the same as for the health economists' policy-value questions
(0.77).

" For a discussion of alternative conceptions of justice,
see Amartya Sen (1987).

32 In Essays in Positive Economics, Friedman (1953)
wrote "Differences about economic policy among disinterested
citizens derive predominantly from different
predictions about the economic consequences of taking
action ... " (p. 5).

33 See Dollars and Deficits (1966 p. 6); personal communication
in 1995.

34 In 1974 I recommended universal comprehensive insurance
for several reasons, one of which was the speculation
that "a national health insurance plan to which all
(or nearly all) Americans belong could have considerable
symbolic value as one step in an effort to forge a link
between classes, regions, races, and age groups." I also
thought it important to add "It will be more likely to serve
that function well if not too much is expected of it-if it
is not oversold-particularly with respect to its probable
impact on health" (Fuchs, 1974a p. 150).


### ---Economics-1996-0-12.txt---
constantly changing. Thus, if cigarette smoking
were found to be determined primarily by
genetic factors, the answers to question 17
would probably change even in the absence of
any change in values.

Positive economic questions are also embedded
in the insurance company underwriting
issues. Most economists realize that requiring
health insurance companies to charge healthy
people the same premium as those with a genetic
disease will deter healthy individuals
from purchasing insurance. But economists
may well differ as to how large that effect will
be and how large a welfare loss it implies.
It is easy to see that there are positive questions
embedded in the policy-value questions,
but it is more difficult to believe that disagreement
over them, rather than differences in
values, explains the low level of consensus
among health economists with respect to the
policy-value questions. Note that the physicians
have a higher level of consensus about
the policy-value questions than do the health
economists. This probably reflects more homogeneous
values among physicians rather

than agreement about the embedded positive
questions. (Note the low level of agreement
among physicians on the explicit positive
questions.)

It may be that it is not so much disagreement
among health economists about the embedded
positive questions as it is uncertainty about
them that make differences in values the
driving force in replies to the policy-value
questions. Many psychologists and economists
have observed that uncertainty about a datum
causes most individuals to give it less weight
when making choices.:

Uncertainty among health economists concerning
the positive questions that are embedded
in the policy-value questions is suggested
by their use of the "no opinion" option. Unlike
the theorists, who chose "no opinion"
twice as often for the positive questions as for
the policy-value questions (28 percent versus
15 percent), the health economists chose "no
opinion" less often for the positive questions
than for the policy-value questions (8 percent
versus 11 percent).36 The role of uncertainty
was mentioned by Milton Friedman in 1966 as
a reason for qualifying his position about the
relative imeportance of scientific judgment and
value differences (Friedman, 1966 p. 6).
In order to investigate further the relationship
between policy-value and positive questions,
I developed two indexes based on the
answers to the national health insurance and
insurance underwriting questions. The first index
measures each respondent's support for
national health insurance. It is constructed by
assigning a value of 1 to agreement with each
of questions 3, 7, 14, and 15, a value of 0 for
disagreement with those questions, and a value
of 0.5 for no opinion. The sum of the values
was divided by 4, giving a range for the index
of 1 (indicating agreement with all four questions)
to 0 (indicating disagreement with all
four questions). The "actuarial" 37 model index
was based on answers to questions 8, 17,
and 20. In the case of question 8, "disagree"
was given a value of 1, and for questions 17
and 20 "agree" was given a value of 1. The
total score for each individual is divided by 3,
again yielding a range for the index from 1 to
0 (indicating complete support or complete rejection
of the actuarial approach).

The results are presented in Table 3. We
see that with respect to national health insurance
the support among the three groups is virtually
identical. There is considerable variation around
the mean for each group, and the amount of variation
is similar across thOe groups. Thirteen percent
of all respondents had an index value of 1,
while 15 percent completely rejected the notion
of national health insurance with an index value
of 0. Not surprisingly, there is a negative correlation
between the national health insurance
index and the actuarial model index. But there


### ---Economics-1996-0-13.txt---
TABLE 3-INDEXES OF SUPPORT FOR NATIONAL HEALTH INSURANCEa AND FOR AN ACTUARIAL MODEL
OF PRIVATE INSURANCE UNDERWRITINGb

Health, Economic Practicing

economists theorists physicians All
National health insurance index:

Mean 0.48 0.48 0.49 0.48

Standard error of the mean 0.05 0.05 0.05 0.03
Coefficient of variation (percent) 71 70 67 69
Percentage with index = 1 15 9 14 13
Percentage with index = 0 13 18 14 15
Actuarial model index:

Mean 0.46 0.61 0.44 0.50

Standard error of the mean 0.05 0.04 0.04 0.03
Coefficient of variation (percent) 71 42 64 60
Percentage with index = 1 7 16 7 10
Percentage with index = 0 22 5 14 14
Coefficient of correlation between

the two indexes -0.37t -0.34t -0.37t _0.35**
a National health insurance index is based on answers to survey questions 3, 7, 14 and 15.
'Actuarial model index is based on answers to survey questions 8, 17 and 20.
t Significant at p < 0.02.

** Significant atp < 0.01.

is a significant difference between the groups in
the extent of support for the actuariai model index.
The economic theorists have a value of
0.61, compared with 0.46 for the health economists
and 0.44 for the practicing physicians. The
theorists are as supportive of national health insurance
as are the other groups, but if insurance
is to be provided through the private market, the
theorists are more inclined than the other two
groups to have premiums reflect expected loss.
One reasonable interpretation of this result is that
the theorists give more weight to the efficiency
aspects of the actuarial model, whereas the
health economists and the practicing physicians
give more weight to the distributional aspects.
Is there a close relationship between the respondents'
scores on the indexes and their

responses to the positive questions? The correlation
coefficients presented in Table 4 show
that the answer is overwhelmingly in the negative.
For the national health insurance index
there is only one positive question (10) for one
group (the health economists) that reaches statistical
significance with p < 0.05. For the actuarial
model index, only questions 9 and 10
show a significant relationship for the health
economists, and questions 10 and 12 for all
groups taken together. Whatever it is that is determining
the respondents' positions with regard
to national health insurance or the actuarial approach,
it is not their views on the seven positive
questions.

Correlations between the indexes and the six
policy-value questions not utilized in their
construction also are typically low, with one
striking exception. Respondents agreeing with
question 5, which calls for national standardized
health insurance benefit packages, also
support national health insurance and just as
clearly reject the actuarial approach for private
insurance underwriting. The actuarial model
index is also negatively correlated with agreement
with question 1.

The weak relationship between the positive
questions and the two indexes is also revealed
in Table 5, which presents the results of regressing
the indexes on the positive questions.
38 In the national health insurance
38 The reliability of the OLS regressions was checked
in several ways: values for each respondent were predicted
from each regression and found to be always between 0
and 1; regressions run with the dependent variable transregression


### ---Economics-1996-0-14.txt---
the only statistically significant coefficient
is for question 10 for health economists.
Other things being equal, those who
agree with the induced-demand hypothesis are
more supportive of national health insurance
than those who disagree, but the effect on the
index (0.239) is less than changing one of the
four answers from disagree to agree. The actuarial
model regressions result in a few additional
significant coefficients but, in general,
the respondents' replies to the explicit positive
questions do not explain their position with respect
to such major policy issues as national
health insurance or insurance company underwriting
changes. It seems unlikely, then, that
their position on these policy issues can be explained
by differences in the embedded positive
questions.

Although I believe that differences in values
lie at the heart of the disagreement about
policy-value questions, I recognize that there
is scope for work on the embedded positive
questions and this work could contribute to a
narrowing of policy differences. One indication
of where research is needed is the percent
of health economists answering "no opinion"
on the individual policy-value questions. This
option was chosen most frequently (35 percent
of the time) for question 11 concerning the
optimality of expenditures on medical R&D.39
Given the importance of technologic change
in medicine both from the point of view of
health outcomes and of expenditures, this is
clearly a high-priority area for research. Two
other questions elicited a "no opinion" response
from one fifth of the health economists.
They are question 1 concerning the subsidies
for health insurance by size of firm (a key part
of the Clinton plan) and question 20 (about
differential premiums for persons born with
genetic defects). In the latter case the high percentage
responding "no opinion" may reflect
uncertainty regarding the magnitudes of the
efficiency and distributional implications of


### ---Economics-1996-0-15.txt---
TABLE 5-RESULTS OF ORDINARY LEAST SQUARES REGRESSIONS OF THE NATIONAL HEALTH INSURANCE INDEX
AND THE ACTUARIAL MODEL INDEX ON SEVEN POSITIVE QUESTIONS
Survey question Health Economic Practicing
number economists theorists physicians All groups
National health

insurance index:

4 0.206 -0.007 0.048 0.022 0.029

(0.165) (0.163) (0.158) (0.079) (0.088)
9 0.053 0.138 0.046 0.056 0.052

(0.139) (0.195) (0.162) (0.084) (0.086)
10 0.239* 0.032 -0.104 0.079 0.077

(0.112) (0.157) (0.151) (0.074) (0.075)
12 -0.167 0.221 -0.100 -0.053 -0.043
(0.154) (0.196) (0.128) (0.076) (0.084)
13 -0.169 0.027 -0.121 -0.088 -0.093
(0.124) (0.133) (0.123) (0.066) (0.071)
18 -0.776 -0.031 0.087 0.007 0.012

(0.699) (0.162) (0.133) (0.093) (0.094)
19 0.231 0.049 -0.016 0.087 0.089

(0.141) (0.198) (0.145) (0.080) (0.083)
ET dummy' -0.026

(0.086)

PP dummy' -0.024

(0.089)

Constant 0.402 0.201 0.598 0.438 0.454
(0.189) (0.198) (0.178) (0.099) (0.112)
R 2 0.287 0.066 0.080 0.052 0.053

Adjusted R2 0.156 -0.116 -0.110 -0.001 -0.017
F 2.18 0.36 0.42 0.98 0.76

Actuarial model

index:

4 -0.102 0.079 -0.029 -0.069 -0.029
(0.160) (0. 1 19) (0.131) (0.068) (0.073)
9 0.373** 0.027 -0.102 0.146* 0.142*
(0.135) (0.142) (0.135) (0.072) (0.072)
1 0 -0.224* -0.211 -0.013 -0. 187** -0. 190**
(0.108) (0.115) (0.125) (0.063) (0.062)
12 0.026 0.216 0.023 0.146* 0.091

(0.149) (0.143) (0.106) (0.065) (0.070)
13 0.094 0.149 0.090 0.041 0.090

(0.120) (0.097) (0.102) (0.056) (0.059)
18 0.432 0.068 0.113 0.114 0.109

(0.678) (0.118) (0.111) (0.079) (0.079)
19 -0.010 0.080 -0.075 -0.062 -0.028
(0.137) (0.145) (0.120) (0.068) (0.070)
ET dummy' 0.130

(0.071)

PP dummy' -0.033

(0.074)

Constant 0.234 0.491 0.454 0.446 0.391
(0.184) (0.144) (0.148) (0.085) (0.093)
R 2 0.279 0.166 0.114 0.145 0.182

Adjusted R2 0.146 0.004 -0.068 0.096 0.122
F 2.10 1.02 0.63 3.00 3.02*

Notes: Standard error in parentheses.
Health economist is the excluded class (ET = economic theorists and PP = practicing physicians).
* Significant at p < 0.05.

** Significant at p < 0.01.


### ---Economics-1996-0-16.txt---
eliminating premium differentials. Or, it may
reflect a reluctance to choose between conflicting
values.

Before leaving the survey it is worth considering
what it reveals about the ability of
health economists to disseminate their conclusions
about the positive questions to a wider
audience. Overall, one must conclude that they
have not been very successful, as revealed by
the political debate of 1993- 1994 and the media
coverage of policy issues. Consider, for
example, question 19 concerning whether in
the long run employers bear the primary burden
of their contributions to their employees'
health insurance. Although 87 percent of the
health economists disagreed with that statement,
politicians on both sides of the debate
assumed, erroneously, that it was correct.
Moreover, nearly all of the media made the
same error. Most of the politicians and most
of the media also showed little understanding
of questions 4, 12, 13, and 18.

I am as ready as the next economist to
criticize politicians and journalists, but the
survey results suggest that their poor understanding
of health economics is not entirely
their fault. First, the economic theorists
and the practicing physicians, two groups with
above-average ability and opportunity to absorb
the conclusions of the health economists,
did not show good command of the positive
questions. In my judgment the health economists
answered 80 percent correctly, but the
average theorist answered only 52 percent correctly
and the mean score for the physicians
was only 53 percent. The differences in the
distributions of scores is striking: 45 of the
46 health economists had more correct answers
than the average theorist or the average
physician.

A second possible reason for the poor understanding
of health economics displayed by

the politicians and the media in 1993-1994 is
the wide disagreement among health economists
over the policy-value questions. When
health economists interact with politicians and
journalists, their discussions probably focus on
the policy-value questions; in the absence of a
professional consensus on many of these questions,
it is not surprising that politicians and
journalists fall back on their own values to
shape their positions.

Returning to the question posed at the beginning
of this section about why economic

research failed to result in a more informed
and productive health care policy debate, the
survey results provide some provisional answers.
First, although health economists are
in substantial agreement about the positive
questions, they have major disagreements
about policy-value questions. Second, health
economists were not successful in getting
their conclusions on positive questions accepted
by the politicians or the media, and
even had difficulty in communicating their
results to economic theorists and practicing
physicians. Third, the health economists'
disagreements over policy probably reflect
differences in values, although it is clear that
there are many positive questions embedded
in the policy-value questions. In my judgment
the problem is not so much that the
health economists disagree about the embedded
questions as that they are uncertain
about them. In the face of such uncertainty,
they tend to let their values drive their policy
recommendations.

III. The Future

If values play such an important role in policy
disputes, what are the implications for
economics and economists? First, we should
endeavor to make explicit the differences in
values, and seek ways to resolve them. Value
differences can take many different forms.
Economists are most familiar with the distinction
between efficiency and distributional issues,
especially greater equality of income
versus greater total income.40 But comprehensive
changes in health policy can have other
important distributional effects. Even for individuals
at the same income level, the costs
and the benefits of care could change along
many dimensions: rural areas versus central
cities, the elderly versus the young, smokers
versus nonsmokers, savers versus nonsavers,
men versus women, and so on. Health economists
who are unanimous in approving gains
in efficiency might have very different views


### ---Economics-1996-0-17.txt---
regarding the desirability of the distributional
changes and might also differ in the weights
they give to the changes in efficiency versus
the distributional consequences.

Second, greater openness about value differences
should force economists to make explicit
the positive questions that are embedded
in most policy-value questions. This would
point the way to productive research. If the
embedded questions are identified and studied,
it should be possible to reduce the uncertainty
about them and thus provide a basis for narrowing
differences on policy-value questions.
A third agenda item for economists is to undertake
research on the formation of values,
especially insofar as they are the consequences
of policy. Economists are understandably reluctant
to prescribe values or to make normative
judgments about them. But when

economic policies affect values and preferences,
and these in turn affect behavior, it is
incumbent on economists to analyze the links
between policies and values, and to examine
the economic and social consequences of alternative
value systems. I believe there is an
analogy between the economics of values and
the economics of technology. Over the past
several decades some economists have begun
to treat technology as at least partly endogenous.
4" Now, a similar effort must be undertaken
for values (Henry J. Aaron, 1994;

Becker, 1996; Albert 0. Hirschman, 1986;
Assar Lindbeck, 1994).

Finally, economists must develop more selfawareness
of how our values color our judgment
about policy, and more candor in making
clear to others the respective roles of positive
research and of values in our policy recommendations.
Alice M. Rivlin, in her AEA presidential
address in December 1986, warned

economists against letting "their ideological
position cloud their judgment about the likely
effects of particular policies" (p. 4). She
urged us "... to be more careful to sort out, for
ourselves and others, what we really know
from our ideological biases" (p. 9). In my
view, there is a vast difference between a researcher
and a reformer, between an analyst

and a player in the policy arena. They are all
socially valuable occupations, and the same
individual may successfully wear different
hats at different times. What is not likely to
work well, either for economics or for policy,
is trying to wear two hats at the same time.
In the remainder of this paper I present a
summary of my policy recommendations for
health system reform. The use of the bully pulpit
by an AEA president to push personal policy
choices has ample precedent, but I also
want to use this opportunity to show how those
choices are shaped by the interaction between
my values and my understanding of health
economics. Finally, I identify aspects of my
policy recommendations that are problematic
and which would clearly benefit from additional
research.

My three major recommendations are:
(i) a broad-based tax earmarked for health
care to provide every American with a
voucher for participation in a basic plan;
(ii) provision of care through integrated health
systems that include hospitals, physician
services, and prescription drugs. These
systems would be led by physicians, would
be reimbursed by capitation plus modest
co-payment from patients at the time of
use, and would be required to offer a wide
variety of point-of-service options to be
paid for by patients with after-tax dollars;
(iii) a large private center for technology assessment
financed by a small industrywide

levy on all health care spending.

My desire to see all Americans insured for
a basic health plan is clearly driven in part by
values. Although medical care is often not a
crucial factor in health outcomes, it is nearly
always a source of utility through its caring
and validation functions. In my judgment, it
fully meets Adam Smith's 1776 definition of
a necessary: "By necessaries I understand not
only the commodities which are indispensably
necessary for the support of life but whatever
the custom of the country renders it indecent
for creditable people, even of the lowest order,
to be without" (1776; republished 1937 p.
821). To achieve universal coverage there
must be subsidization for those who are too
4' For example, Kenneth Arrow, Zvi Griliches, Ed
Mansfield, Richard Nelson, Nathan Rosenberg, and Jacob
Schmookler.


### ---Economics-1996-0-18.txt---
poor or too sick to acquire insurance, and there
must be compulsion for the "free riders" 42 to
pay their share.

There are only two ways to achieve systematic
universal coverage: a broad-based general
tax with implicit subsidies for the poor and the
sick, or a system of mandates with explicit
subsidies based on income. I prefer the former
because the latter are extremely expensive to
administer and seriously distort incentives;
they result in the near-poor facing marginal tax
rates that would be regarded as confiscatory if
levied on the affluent.43

Both theory and experience show that integrated
health care systems are usually the best
way to deliver cost-effective care. The primary
reason is the physician's central role in medical
decision-making. Under any approach to
care, it is the physician who admits patients to
hospitals, orders tests and other procedures,
and decides when to discharge. It is the physician
who prescribes drugs and who refers
patients to other physicians for consultation
and treatment. Thus physicians' decisions
are the major determinant of the cost of care.
Only in an integrated system, however, do
physicians have the incentive, the information,
and the infrastructure needed to make
these decisions in a cost-effective way. Integrated
systems also have an advantage in

avoiding excess capacity of high-cost equipment
and personnel.

Given the central importance of physicians
to medical care, I believe the integrated systems
should be led by them and other health
care professionals. At a minimum, health care
professionals should have a prominent place
in the govermance of the systems. One of the
greatest errors of health policy-makers today
is their assumption that market competition or
government regulation are the only instruments
available to control health care. There
is room for, indeed need for, a revitalization
of professional norms as a third instrument of
control.' The patient-physician relationship
often is highly personal and intimate, similar
in many ways to relationships within families
or between teachers and pupils or ministers
and congregants. This relationship is, in part,
what economist Kenneth Boulding (1968)
called an integrative system, one that depends
on mutual recognition and acceptance of rights
and responsibilities, enforced by traditional
norms as well as market pressures and government
regulations. As long as physicians control
the use of complex technology in life and
death situations, and as long as we expect them
to perform priestly functions, they must be endowed
with certain privileges and held to certain
standards of behavior different from those
assumed by models of market competition or
government regulation.45

Comprehensive government control of medical
care has not worked well in any setting.
The essence of good care is an informed patient
working cooperatively with a health professional
who provides personalized attention
and concern. The rules, regulations, and bureaucratic
controls that almost always accompany
governmental activities are inimical to
high-quality cost-effective care. It is revealing
that countries such as England and Sweden
with deep government involvement in the
financing of medical care have bent over backwards
to leave physicians with a great deal of
professional autonomy-indeed more autonomy
than is possessed by many American physicians
working in a "private" system.

Market competition also has its problems. It
assumes a preoccupation with the bottom line
and governance by a corporate mentality that
judges the success of each division by its profit
growth. Physician-led systems will also have


### ---Economics-1996-0-19.txt---
to pay attention to costs, and physicians will
also be interested in making a good income,
but there is a vast difference between a profitmaximizing
corporation and physicians who

strive to balance their obligations to patients,
the organization, and themselves.46
Reimbursement of these integrated systems
should be primarily by capitation, adjusted
for patient characteristics. In addition, patients
should be required to make modest copayments
at the time of use (e.g., $15 for each
visit and $5 for each prescription). Such payments
will generate some income but, more
important, will help to discourage wasteful use
of health care. The payments could be waived
for patients living below the poverty level,
and for essential preventive services such as
vaccination.

The earmarked tax would provide every
American with a voucher for a basic health
care plan. Each integrated system would be
required to offer the basic plan, plus a variety
of options. These options are not alternative
insurance plans; they are services to be paid
for at time of use with after-tax dollars.47 The
options could take many forms: a private room
in the hospital; a wider choice of physicians
and hospitals than is available through the basic
plan; or access to new experimental technologies
or older technologies not included in
the basic plan because they have a low benefitto-
cost ratio.48

These options would accommodate the demands
of patients with higher incomes or

those who choose to spend more of their income
on medical care. The options would not
constitute establishment of different plans. Everyone
would be in the same plan and most

persons would stick to the basic plan most of
the time. An option would be exercised only
when the patient desired and was willing to
pay for it. This is the quintessential American
approach to balancing equality and freedom.
On the one hand, this approach avoids the
egalitarianism of the English and Canadian
systems in which only a small elite have an
escape valve. On the other hand, it does not
create a separate plan for the poor while the
great majority of Americans obtain care from
a different system. The experience with Medicaid
shows that a separate system limited to
the poor is not likely to function well.
Where feasible, the integrated health care
system would engage in managed competition.
49 Having advocated policies similar to
such an approach to health care for more than
20 years (Fuchs, 1974a), I am not unmindful
of its virtues. We cannot, however, rely on
managed competition alone to contain costs.
In most rural areas, population density is too
low to support several health care systems.
Even in some urban areas, competition is impossible
or undesirable because of economies
of scale. For instance, only one hospital is
needed to serve a population of 100,000 efficiently.
Similar constraints apply to competition
in physician specialty care, especially if
the physicians work full time at their specialties.
A population of 1 million would probably
not justify enough independent maternity services
or open-heart surgery teams to create
competitive conditions. Moreover, the public
interest is not best served by insisting that
health professionals always maintain rigorous
arm' s-length competition with one another.
Patients can benefit from cooperation among
physicians and hospitals, both in reduced costs
and better service. Managed competition alone
will not be enough to contain costs; it must be
46 The effects on television network news departments
of the subordination of professional norms to the pursuit
of profits shows what could happen in medical care.
4 Readers whose values lead them to prefer a more
egalitarian system might consider how individuals now
have options to use their income to live in safer neighborhoods,
drive safer cars, avoid unhealthy occupations,
and make other choices that have larger and more predictable
effects on health than the options available in my
recommendation for health care.

48 Many advances in medicine do not spring full-blown
from the test tube. They require long periods of development
through trial and error and incremental improvements.
In my judgment it is desirable to have a system in
which technologic opportunities can be explored on a reasonably
large scale with the cQsts borne by those patients
who are most willing and able to pay for a chance at unproven
benefits. Government- or industry-financed randomized
clinical trials with small samples of selected
patients treated in selected environments are not always a
satisfactory substitute for larger scale efforts to establish
the effectiveness, and especially the cost-effectiveness of
a medical technology.

4 See Alain Enthoven (1986, 1988).


### ---Economics-1996-0-20.txt---
supplemented by constraints on the supply
side, especially with respect to technology and
the specialty mix of physicians.

In 1995 Americans spent about $1 trillion
for health care, broadly defined. If, during the
past 30 years, health care spending had grown
at the rate of the rest of the economy, the
health care bill in 1995 would have been only
a little more than $400 billion. What accounts
for this extraordinary excess of almost $600
billion in annual spending? There has been a
small increase in physician visits per capita,
but use of acute care hospitals has decreased
sharply. Patient-days per 1000 population are
less than three fifths the level of 30 years ago.
By far the most important factor accounting
for the increase in health care's share of the
GDP is the change in technology.50 Physician
visits and hospital-days cost more than they
used to because the content has changed-the
technologies used for diagnosis and treatment
are more expensive than in the past. Much of
this technological change is welcome; it contributes
to enhancing the length and quality of
life. Some of the change is less desirable because
it adds more to cost than to patient benefit.
Unfortunately, there is great uncertainty
regarding the merits of many technologies.
Moreover, even when the advantages and disadvantages
are known, there are often significant
barriers facing physicians who would
like to practice in a cost-effective manner.
To deal with this problem, I propose the creation
of a large, private center for technology
assessment. Financing for this center would
come from a small levy (less than one tenth of
1 percent) on all health care spending. A centralized
approach is necessary, because health
care is highly fragmented. Individual physicians
and health plans lack the incentive and
ability to commit the resources needed to assess
-new technologies. Even the largest insurance
companies individually account for only
a small percentage of the health care market;
they are, therefore, understandably reluctant to
pay for large-scale assessments that would
benefit all.51 Government agencies try to fill
the void, but the scale of effort is too small,
and a private center would be able to avoid the
political interference that often intrudes on
government-run agencies.52 Health care providers
would fund and set the agenda for the
center, much as the electric power companies
do for the Electric Power Research Institute.
This institute is financed by a small levy on
every public utility bill.

A health care technology assessment center
would have two primary functions. First, it
would help to develop and disseminate systematic
knowledge about the cost-effectiveness of
medical technology through support of research
and through a comprehensive program of
publications and conferences. The center would
have some intramural research capability, but
most of the research would be conducted extramurally
at medical schools, hospitals, and research
institutes throughout the country. It would
provide health professionals with essential information
to evaluate and improve their clinical
practices and offer a rational basis for deciding
what services should be included in the basic
plan.

The second important function would be to
provide legitimacy for the cost-effective practice
of medicine. Currently, many directors of
health plans and many individual physicians
know they could be practicing in a more costeffective
way, but they are inhibited from doing
so because they do not practice in a
vacuum. Physicians are influenced by peers
who have been trained in settings that emphasized
the use of the latest technologies regardless
of cost. Patients come with particular sets
of expectations based on what they read or
hear in the media and what their relatives and
friends tell them has been their experience.
The threat of malpractice suits lurks in the
background. A major function of the center


### ---Economics-1996-0-21.txt---
would be to give legitimacy and a stamp of
authority to physicians who practice in a more
cost-effective way.

My policy recommendations seek to achieve
a balance among the diverse values of efficiency,
justice, freedom, and security. The link
between the earmarked tax and the basic plan
would create a healthy tension between the desire
to increase benefits and the need to pay
for the increase in a responsible and equitable
manner. Competition among health care systems
in highly populated areas would widen
choice and foster cost-effective practice. The
private technology assessment center would
help to contain costs without the imposition of
controls or caps that might stifle innovation
and progress.

Are these recommendations politically saleable?
In the short run, certainly not. But neither
are any other proposals for comprehensive
reform. Indeed, for more than 20 years it has
been my view that the United States would not
enact comprehensive health care reform except
in the wake of a major war, a depression,
large-scale civil unrest, or some other event
that completely changed the political climate.
Why is the United States the only major
industrialized nation without national health
insurance? Many observers focus on the opposition
of "special interests," and that certainly
is a factor, but I do not find it a
completely satisfactory explanation. After all,
special interests are not unknown in Sweden,
England, Canada, and other countries that do
have national health insurance.

In 1976 I suggested four reasons for its absence
in the United States: distrust of government,
heterogeneity of the population, a weak
sense of noblesse oblige, and strong private
voluntary organizations such as nonprofit hospitals
and Blue Cross and Blue Shield plans
that carry out quasi-governmental functions
with respect to the financing and delivery of
health care (Fuchs, 1976). Upon revisiting
this question (Fuchs, 1991), I concluded that
the first three reasons were stronger than ever,
but the fourth had weakened considerably. It
is ironic that "the competition revolution"
(Fuchs, 1988b), which erodes the ability of
not-for-profit health care institutions to provide
a modicum of social insurance through
community rating and cost shifting, may in the
long run push the country toward national
health insurance.

My plan is certainly not a panacea; it would
be difficult to implement and others might
seek a different balance of values. Several aspects
require additional research. For example,
what should be the content of the basic plans?
How should the content change over time?
How should the plans be reimbursed from the
funds raised by the earmarked tax, and especially
how should reimbursement be risk adjusted
to take account of differences in plan
populations? Another problem is how to encourage
competition among plans where it is
feasible, while recognizing that a competitive
approach will not be desirable or possible in
areas of low population density. Considerable
research is needed on how the out-of-plan options
should be priced53 and how the providers
of such care should be reimbursed. Finally,
much thought should be given to how to reinvigorate
professional norms as a third instrument
of control, along with market competition
and government regulation.54

I conclude this tour of health economicspast,
present, and future-on a mildly optimistic
note. In the past three decades economics
has made a positive contribution to health
and medical care, and I believe that future contributions
will be even greater. Now that the

basic ideas of economics are gaining acceptance,
it will be more important than ever for
economists to master many of the intricacies
of health care institutions and technologies.
We will also have to consider the problems of
dissemination in order to insure that when we
agree on research results, these results are understood
and accepted by all relevant audiences
including the media, politicians, and
health professionals. Moreover, we must pay
more attention to values than we have in the
past. Through skillful analysis of the interactions
between values and the conclusions of
positive research, we will be able to contribute
more effectively to public policy debates. And,
13 For an interesting discussion of the "topping off"
problem, see Robert H. Frank (1996).
5 This would undoubtedly require research to uncover
the reasons for the erosion of professional control. See, for
example, Steven Brint (1994).


### ---Economics-1996-0-22.txt---
if health economists are successful in this demanding
assignment, we can lead the way toward
progress in areas such as child care and
education that face similar problems of reconciling
multiple goals and heterogeneity in
values. To be useful to our society while deriving
pleasure from our work-in the words
of the old Gershwin tune, "Who could ask for
anything more?"
 ## Economics-1997-0


### ---Economics-1997-0-02.txt---
The improvement in living standards, life
expectancy, and economic growth prospects in
developing countries ranks among the most
important economic success stories since the
Second World War. Growth in some has been
dramatic, and while progress has been far from
uniform, there are grounds for optimism that
future growth prospects can be even better
than performance to date.

One factor accounting for that success has
been improved understanding and adoption of
economic policies much more conducive to satisfactory
economic growth than was the case in
the 1950's and 1960's. That better understanding,
in turn, resulted from a combination and
interaction of research and experience with development
and development policy.

Ideas with regard to trade policy and economic
development are among those that have
changed radically. Then and now, it was recognized
that trade policy was central to the overall
design of policies for economic development.
But in the early days, there was a broad consensus
that trade policy for development should
be based on "import substitution." By this
was meant that domestic production of importcompeting
goods should be started and increased
to satisfy the domestic market under incentives
provided through whatever level of protection
against imports, or even import prohibition, was
necessary to achieve it. It was thought that import
substitution in manufactures would be synonymous
with industrialization, which in turn
was seen as the key to development.
The contrast with views today is striking. It
is now widely accepted that growth prospects
for developing countries are greatly enhanced
through an outer-oriented trade regime and
fairly uniform incentives (primarily through
the exchange rate) for production across exporting
and import-competing goods.' Some

countries have achieved high rates of growth
with outer-oriented trade strategies. Policy reform
efforts removing protection and shifting
to an outer-oriented trade strategy are under
way in a number of countries. It is generally
believed that import substitution at a minimum
outlived its usefulness and that liberalization
of trade and payments is crucial for both industrialization
and economic development.

While other policy changes also are necessary,
changing trade policy is among the essential
ingredients if there is to be hope for improved
economic performance.

And, while there are still some disagreements
over particular aspects of trade policy
both among academic researchers and policy
makers,2 the current consensus represents a
distinct advance over the old one, in terms
both of knowledge and of the prospects it
offers for rapid economic growth. While it
will no doubt be further refined in light of


### ---Economics-1997-0-03.txt---
experience, a changing world economy, and
research, there is no question of "going back"
to the earlier thinking and understanding of the
process.

A number of interesting questions arise
about this change in thought and policy.
How could it happen that a profession, for
which the principle of comparative advantage
was one of its key tenets, embraced such
protectionist policies? What was the contribution
of economic research to the sea

change in thinking, policy prescriptions, and
politicians' acceptance of the need for policy
reform? What sorts of economic research
best informed the policy process? In a nutshell,
how did we learn? And what was the

contribution of economists and their research
to the process?

Attempting to answer these questions is
the subject of this lecture. Even with a focus
limited to trade and development, analysis of
the role of research and its usefulness is at
least somewhat conjectural. The issue, however,
of what types of research inform good
policy is an important one. I suspect that the
tentative conclusions I draw here may be relevant
for other areas of research-informing
policy, but leave that to others to demonstrate
or refute.3

In what follows, I first sketch the initial approach
to trade policy in early development
research and thought. Next, consideration is
given to the evolution of thought, research,
and experience with respect to trade and development
over the next several decades, and

to the "conventional wisdom" of the 1990's.
Thereafter, I consider the role of research and
the sorts of research that proved most fruitful
in guiding policy and changing the consensus.
Before proceeding, two caveats are necessary.
First, it is very difficult to disentangle
views of the proper role for trade policy in
development from views about the appropriate
role for the state. Partly as a legacy of the
Great Depression, partly because of the belief
that the Soviet Union had succeeded in its developmental
and industrial aspirations through

central planning, and partly because of the perceived
success of wartime controls, there was
widespread agreement- in developed and developing
countries alike-that the state should
play a major role in economic activity, not
only in affecting aggregate demand, but also
in regulating private markets and indeed augmenting
or supplanting them with state-owned
enterprise production of manufactured and
other goods. Quite clearly, early views about
the necessity for a leading role for the state in
guiding resource allocation were incompatible
with an open trade policy or outer-oriented
trade strategy. Yet to attempt to consider the
evolution of both views is well beyond the
scope of this paper, and focus here is confined
to trade policy.

Second, to focus on research that influenced
thinking about economic policy is not to denigrate
the importance of research that does not
appear to have had immediate policy relevance.
First of all, basic research often informs more
applied research. Second, in some cases of research
that provided little of lasting value, that
outcome could not be known at the time. Perhaps
some of that research served to demonstrate
the infeasibility of certain policy paths,4 or to
demonstrate the futility of further explorations.
Nonetheless, ex post it is clear that some
lines of research served to hasten the day
when policy makers would accept the desirability
of removing high walls of protection,
while others were irrelevant or served largely
to reinforce prejudices and perpetuate the "old
wisdom." Perhaps that is inevitable in the
"marketplace of ideas" as new paradigms are
brought forth to replace old ones.

'To name just one example from another field, con-
sider the pioneering work of Theodore W. Schultz ( 1964),
challenging the view that irrational peasants were unresponsive
to incentives. Once his work was accepted, it was
no longer possible to maintain low prices for agricultural
commodities and believe that there would be little or no
output effects.

4 An example is the line of research, which continued
into the 1970's, improving methodology for planning
models. This research certainly contributed greatly to understanding
both the functioning of the economy and also
to one aspect of what would be necessary in order for the
planning approach to succeed. Without those research
contributions, it is possible that many would claim that
planning failed because it was incorrectly done (rather
than, as most would now believe, it was misconceived).


### ---Economics-1997-0-04.txt---
I. Evolution of Theory, Understanding,
and Policy

A. The Early Years

As developing countries gained independence
from their former colonial rulers, 5 their
leaders had a political mandate to achieve
higher living standards and rapid economic
growth.6 It is difficult in the 1990's to recall
the extent to which it was then plausible to
view the world economy as split into the industrialized
countries and the underdeveloped

countries, or "first world" and "third world,"
as they were often called. Underdeveloped
countries had markedly lower average educational
attainments (including a great deal of
illiteracy and a high fraction of the population
with no schooling), poor health conditions,
and very little infrastructure. They were heavily
specialized in the production and export of
primary commodities and imported most of
their manufactured goods. While differences
among the underdeveloped countries were acknowledged,
these seemed minor contrasted

with the overwhelming realities of their common
attributes and widespread poverty.

The new field of development economics
was regarded by many as covering underdevelopment
because "conventional economics"

did not apply (see Albert Hirschman, 1982).
Focus on how the developing countries should
shape policies for accelerating growth and
raising living standards was the central issue.7
B. Accepted Stylized Facts and Premises
Early trade and development theories and
policy prescriptions were based on some
widely accepted stylized facts and premises
about the underdeveloped countries. These
were a mixture of touristic impressions, halftruths,
and misapplied policy inferences. In
hindsight, it is surprising how some thenaccepted
stylized "facts" were so uncritically
accepted and held sway for so long. However,
it is not possible to understand what thinking
about trade and development was except in
light of those premises. Indeed, it can be argued
that improved understanding of trade and
development came about in large part through
research which effectively demonstrated the
falsity of these premises.

A first premise was based on the fact-then
certainly true-that developing economies'
production structures were heavily oriented toward
primary commodity production. The dependence
on foreign trade was believed to be
extreme, as there was virtually no production
capacity for manufactured goods outside a few
light mass-consumed commodities. However,
many observers went further and attributed the
low living standards in developing countries
to dependence on primary commodity production
and export.

A second "fact," or premise, was that if
developing countries adopted policies of free
trade, their comparative advantage would forever
lie in primary commodity production. It
followed that industrialization and, hence, development
would not take place if free trade

policies were adopted.

A third premise-termed "export pessimism"
-was that both the global income and
price elasticities of demand for primary commodities
were low. Consequently, it was anticipated
that export earnings would not grow
very rapidly, if at all.8


### ---Economics-1997-0-05.txt---
A fourth premise was that the labor force
in developing countries, predominantly engaged
in agricultural activities as it was, had
a marginal product of labor that was "negligible,
zero, or even negative," to quote W.
Arthur Lewis (1954 p. 141). The stylized
"fact" that there was "surplus" labor, or
disguised unemployment in less developed
countries (LDCs) was widely accepted.9 In
many analytical formulations, it was explicitly
or implicitly assumed that labor was a
free good while capital was the scarce factor
of production.'0

Related to the fourth premise was a fifth
premise: that capital accumulation was crucial
for growth, and in early stages of development
it could occur only with the importation of
capital goods. Since it was expected that the
demand for capital goods imports, and imports
of other products used in the production
process, would grow rapidly while foreign
exchange earnings would not, it appeared that
growth could follow only if domestic production
of import-competing goods could expand
rapidly.

Yet a sixth widely accepted premise was
that there was very little response to price
incentives in developing countries: peasants
were "traditional" in their behavior, and
there were "structural" problems within the
economy." '

Based on these stylized facts and premises, it
was a straightforward step to believe that the
process of development was that of industrialization,
by which was essentially meant the

accumulation of capital for investment in manufacturing
industry and related infrastructure.
Moreover, since most manufactured goods were
imported, it seemed to follow logically that, as
stated by Chenery (1958 p. 463) among many
others: "Industrialization consists primarily in
the substitution of domestic production of manufactured
goods for imports."

C. Initial Policies

Policy prescriptions were derived from
these propositions, or stylized facts. Since it
was thought that industrialization was necessary
for development and that free trade would
leave underdeveloped countries specialized in
primary commodity production, it followed
that there had to be investment in new manufacturing
industries whose output would substitute
for imports. Further, it was widely
believed that new industries in poor countries
could not possibly compete with their established
counterparts in the developed world.
Therefore, industry would have to be protected
during its initial phase. Import-substitution
policies therefore became the hallmark of development
strategies for manufacturing and

the underlying rationale for trade policy."2
The case for import substitution was based
both on the premises outlined above and also
on received doctrine: the infant industry argument.
The notion that dynamic considerations
and externalities might imply that an
industry, although economic, would not be established
by private agents had been accepted
by economists as a legitimate exception to the
case for free trade since Hamilton and List."3
tended to demonstrate that at the very least the deterioration
had been much less than was believed. John Spraos
(1980) provided a classic review of the evidence.
9 A modem interpretation would be that there are many
people in developing countries with very low marginal
products of labor. While they are too poor to remain unemployed,
the process of development entails equipping
people with the capabilities (partly through education)
and opportunities to increase their productivity.
' To be sure, all analysts recognized the importance of
increased provision of education and health services. But
for purposes of analyzing trade policy, emphasis was almost
exclusively on investment.

" This gave rise to a great deal of literature based on
"structuralism." According to some, it was the absence
of responsiveness to price that made developing countries
"different." Structuralism was also used as an argument
that inflation was necessary in order to achieve growth.
See Hollis B. Chenery ( 1975) for a fuller description.
2 There were many important subthemes that are not
elaborated here, since they are not essential to the main
argument. It should, however, be noted that there were
many who believed that the situation of developing countries
was "structural" and that marginal changes would
not matter. It was then concluded that a "big push" was
needed, with many new investments simultaneously generating
additional demand and then becoming profitable.
Ragnar Nurkse's ( 1958) "balanced growth" prescription
reflected the same viewpoint.

'" See Robert E. Baldwin's (1969) classic analysis of
the argument, which not only sets up the conditions under
which there might be an infant industry, but also carefully


### ---Economics-1997-0-06.txt---
It was stipulated that a low-cost producer or
producers were already in operation abroad;
then, the argument proceeded, a potential entrant
in a developing country would be faced
with an initial period of high costs, but could
in the longer run compete. However, in the
presence of dynamic externalities (presumably
internal to the industry), it was believed
that no individual producer would find it profitable
to start production. In these circumstances,
the infant industry argument could

justify temporary intervention to make entry
into the new industry privately profitable provided
that, over the longer term, its costs
would decline below the imported cost by
enough to yield an economic return on the intervening
loss, which could be viewed as an

investment.

Although the infant industry argument was,
in a first-best world, an argument for a production
subsidy (which would presumably

equal the unit value of the externality and
might apply as well for production for exports
as for the domestic market), it was combined
with the appeal for import substitution14 to
yield a justification for protection of newly established
manufacturing industries in developing
countries.

However, combining the assumptions that
industrialization would have to take place
through substituting for imports, that there
were infant industries requiring initial intervention,
and that export earnings were unlikely
to increase, the stage was set for trade
and industrialization policies.

The premises underlying import-substitution
policies were so widely accepted that developing
country exceptions were even incorporated
into the General Agreement on Tariffs
and Trade (GATT) articles. Article XVIII
explicitly protected the developing countries
from the "obligations" of industrialized countries
and permitted them to adopt tariffs and
quantitative restrictions. They also were entitled
to "special and differential treatment" in
other regards under GATT. That the GATT,
the upholder of an open international trading
system, would accept an "exception" for
developing countries shows how deeply entrenched
the views supporting import substitution
were. It is arguable that the very

existence of this exception not only legitimized
developing countries' inner-oriented
trade policies, but also removed pressures
that might otherwise have been brought to
bear earlier for them to adopt trade and payments
regimes more conducive to economic

growth. "

D. Resulting Evolution of Policies

In one way or another, provision was made
in country after country that, once domestic
production became feasible, imports would be
restricted. In Brazil, a "Law of Similars" provided
that firms importing goods that were
similar to those available domestically would
lose their government privileges, which included
not only access to credit and tax treatment,
but also eligibility to bid on government
contracts and a variety of other valuable rights.
In India, imports were licensed, and in the
event that there was domestic production, any
would-be importer was required to obtain letters
from any supplier government officials
thought might be capable of producing the
good to the effect that the supplier could not
meet the specifications. In Turkey, goods were
removed from the list of items for which import
licenses could be granted once domestic
production capacity was available. Similar
provisions, or very high tariffs, were used to
encourage import substitution in most developing
countries."

and critically scrutinizes the various circumstances in
which those conditions might hold. Baldwin's article was
an important contribution to better understanding of the
empirical relevance of the theory, as I shall argue below.
'4 It was also believed that there was a revenue constraint,
making the first-best production subsidy infeasible.
More recent analyses would also point to the greater potential
for corruption inherent in production subsidies as
yet another reason why protection might be preferable.
'" See Kenneth Dam (1970 Ch. 14) for a full discussion.
16 In Argentina, an effort was made to liberalize the
trade regime by lowering tariffs in the late 1970's. To the
surprise of officials, there was no apparent effect of the
first round of tariff cuts. Subsequent investigation revealed
that the tariffs in question had been between 500 and 1,000
percent, and that they had been above the rates at which
domestic producers could compete.


### ---Economics-1997-0-07.txt---
In some countries and industries, the trade
regime was used as the key policy instrument
to provide incentives for import-substituting
investment and production by private firms. In
other circumstances, state-owned enterprises
were established, and investments were made
directly by the state sector in new manufacturing
activities. In that case, the trade regime
provided protection to the state-owned enterprises,
although their budget constraints were,
in any event, very soft. None of these policies,
as adopted, provided means of identifying
where dynamic externalities were largest, nor
was there any provision for reduction of protection
after an initial period. Indeed, protection
was virtually automatic for any new
import-substitution industry.

A final aspect of early policies also contributed
to high and indiscriminate levels of protection.
That is, as countries embarked on

ambitious development plans, inflation rates
rose to levels significantly above those in industrial
countries (although far below inflation
rates prevailing in many developing countries
today). Demand for foreign exchange was rising
rapidly in response to the development
plans, rising incomes, and domestic inflation.
Nonetheless, policy makers in most developing
countries chose to maintain their fixed nominal
exchange rates. In part, this reflected the perception,
noted above, that there was little response
to prices and that, indeed, maintaining
the nominal exchange rate "taxed" agriculture
while simultaneously subsidizing capital goods
imports. In part, exchange rates were held fixed
because it was believed that so doing made imports
of capital goods cheaper and thus increased
investment. The net result was, of

course, real appreciation of the exchange rate,
which further intensified ex ante payments
imbalances, reduced foreign exchange availability,
and induced greater restrictiveness in
import licensing.

It will be recalled that the 1950's and
1960's were a time of unprecedented economic
growth for the industrial countries and
for world trade. Buoyed in part by international
markets, and in part by the stimuli of
increased investment and other aspects of development
programs, the rates of growth of per
capita incomes rose markedly relative to historical
levels in most developing countries,
although they remained below those in industrial
countries with few exceptions. Even the
growth of industry itself was fairly rapid, as
the "easy" import-substitution opportunities
were by and large undertaken first.'7
However, with real exchange rate appreciation
and the pull of resources into newly
profitable, import-competing industries, the
growth of foreign exchange earnings inevitably
slowed. It is not widely appreciated that
developing countries, which had a 44 percent
share of world exports of agricultural commodities
in 1955, lost share to the point where
they had only 31 percent by 1970.18
With acceleration in the growth of demand
for foreign exchange, and deceleration in the
growth of supply, foreign exchange difficulties
were inevitable. The export pessimism premise
had been self-fulfilling, given the policies
that were followed. The drop in primary commodity
prices in the early 1950's accentuated
the phenomenon, but affected the timing more
than the actuality of the result. The initial
response by most policy makers was to impose
rationing of scarce foreign exchange
(and require the surrender of foreign exchange
from exports) on imports, and the resulting
system had little to do with encouraging infant
industries.

Although initial rationing of imports was
usually on a relatively uniform and across-theboard
procedure, controls over foreign trade
generally became more restrictive and complex
over the next two decades, both in


### ---Economics-1997-0-08.txt---
response to growing "foreign exchange shortage,
" in reaction to the "unfairness" of the
undifferentiated controls, and in response to
evasion of the regimes."9 Periodic balance of
payments crises arose in reaction to overvaluation
of the real exchange rate, increased indebtedness,
and the failure of export earnings

to grow.

International Monetary Fund (IMF) "stabilization"
programs were undertaken, under

which import regimes were simplified and rationalized
(as import licensing was, in those

years, not abolished). The nominal exchange
rate was normally altered (but usually to a
new fixed exchange rate in the face of continuing
inflation).20 Even in IMF programs,
however, it was seldom intended that the
underlying trade policies related to import
substitution be changed: the intent, rather,
was to rationalize the trade regime and find
ways to induce more foreign exchange earnings
to finance the capital goods that would
be imported to undertake additional importsubstitution
investments. Growth proceeded

in "stop-go" fashion, as periods of foreign
exchange crisis were followed by tight(er)
monetary and fiscal policies, a consequent reduction
in excess demand for imports, and an
increase in foreign exchange earnings. When
the trade regime was again relaxed, growth
resumed and the demand for imports again
mushroomed until the next crisis.21
E. Research Directions and Contributions
Most research in the 1950's and 1960's was
based on the premises outlined above, and supported
the basic thrusts of policy. It needs only
brief mention here. Some focussed on the possible
existence of externalities and the need for
"balanced growth," as it was assumed that
expansion of any one industry alone would not
be feasible because of the limited size of the
market.22 This prescription, of course, was
based on the premise that development of
manufactured exports was not feasible. Another
line of supportive research focussed on
planning models, concentrating in large part
on interindustry flows and linkages.23 Empirical
research on pattems of development began,
focussing on the structure of economies
and their growth performance. For more than
a decade, the growing disparity between theory
and practice was all but ignored.

There was also research providing a rationale
for protection of new industries and import
substitution. These results demonstrated that
domestic distortions could warrant trade intervention
24 in a number of situations. Everett E.
Hagen (1958), in perhaps the best known of
these, set up a model assuming that urban
wages exceeded rural wages exogenously, and
demonstrated that a tariff could improve welfare
by inducing resources into the (artificially)
higher-cost urban industries.

Work also continued on structuralist models,
as a number of authors found reasons
why developing countries' economic structures
were "different" and why, therefore, the
usual economic analysis would not apply.25
Chenery and Michael Bruno (1962), Chenery
and Alan Strout (1966), and Chenery and
many other coauthors developed the "twogap"
model, using the stylized fact that
foreign exchange was "scarce" in developing
countries. In this model, export earnings
were exogenously given and growing more
slowly than the demand for foreign exchange.
'9 For a description, see Jagdish N. Bhagwati (1978).
20 See William R. Cline and Sydney Weintraub ( 1981 )
for analyses of some of these episodes.
21 See Carlos Diaz-Alejandro ( 1976) for an analysis of
the "stop-go" cycle in Colombia.

22 For a modem presentation of the "big push" need
for balanced growth, see Kevin M. Murphy et al. ( 1989).
The notion of balanced growth and "big push" in the
1940's and 1950's was associated with such analysts as
Paul Rosenstein-Rodan (1943) and Nurkse (1958),
among others.

23 See Chenery and Paul Clark (1959) for an exposition.
Economists in India probably carried planning models
the furthest into practice. The Indian Second Five-Year
Plan was explicitly based on the P. C. Mahalanobis ( 1955)
model, and contained estimates of output levels for the
subsequent five years which were used as a basis for granting
investment licenses. No licenses were issued once
the increased capacity already had been allocated. See
Bhagwati and Padma Desai (1970) for an account.
24There was a huge literature on this subject. See
Bhagwati ( 1971 ) for a synthesis of many of the papers.
25 See Christopher Bliss (1989 p. 1194) for a modem
statement of the proposition that if demand and supply are
sufficiently inelastic, prices do not matter.


### ---Economics-1997-0-09.txt---
Investment was limited by the more binding
of two linear constraints: the available savings
and the available foreign exchange. There
were thus two "gaps" -between savings and
investment, and between demand for, and
supply of, foreign exchange. Growth was
constrained either by savings or by foreign
exchange availability, and the model demonstrated
the high potential productivity of foreign
aid (in providing foreign exchange),
enabling otherwise redundant domestic savings
to be used in capital formation. The
model, reflecting the views of the day, had little
role for the price mechanism.26

An example of an analytical effort to clarify
circumstances under which one of the stylized
facts could be realized was Bhagwati's ( 1958)
and Harry G. Johnson's (1967) demonstration
of the possibility of "immiserizing growth,"
under which a country might increase its output,
only to find the price of exports falling so
much that the country was worse off. As
Bhagwati showed, the conditions under which
that might happen were fairly extreme.
An important development was the theory
of shadow pricing, which was an offshoot of
programming and planning models. It was initially
used to demonstrate how reliance on
market prices might yield an inappropriate resource
allocation. Quickly, however, analysts
pointed to the distortions between domestic
prices of import-competing and exportable
goods because of the trade regime. There is
little doubt that cost-benefit techniques improved
project selection and enabled improved
governmental decision-making with, inter alia,
the insistence on use of border prices. The
publication of the I. M. D. Little and James A.
Mirrlees (1969) volume marked a milestone,
after which there was almost no question about
the appropriateness of using border prices in
project evaluation.

In a related and important development, the
theory of effective protection was developed
by Johnson (1965a), W. M. Corden (1966),
Bela Balassa (1965), and others, providing a
framework for analyzing the protection accorded
to industries engaged in light processing
and much higher value-added activities on
a comparable basis. The notion of domestic
resource costs (Bruno, 1965; Krueger, 1966),
showing the uneven allocation of resources to
earning and saving a unit of foreign exchange
across activities, was developed to meet the
argument that market prices failed to reflect
opportunity cost. This research provided a tool
with which economists could measure the
wide disparities in protection accorded to different
import-competing industries.

Recognizing that these estimates were based
in part on partial equilibrium analysis,27 a
number of researchers began work on developing
techniques for computing general equilibrium
results. Based on newly developed

solution algorithms, techniques were developed
for models which endogenized prices,
and thus moved away from the linear models
earlier used for analysis.28

By the late 1960's and 1970's, there were significant
contributions which undermined some
of the premises on which import-substitution
strategies were based. At an analytical level,
one line of research focussed on whether the
stylized facts of "market failure" in fact warranted
the imposition of trade restrictions.
Bhagwati and V. K. Ramaswami (1963),
Johnson (1965b), Bhagwati (1969), and others
demonstrated that a trade instrument (tariff
or quota) was usually not a first-best, nor often
even second-best, instrument for achieving the
objectives in the name of which protection had
been granted. The equivalence of tariffs and
quotas, an old result in international economics,
was revised and refined, as quotas became
more frequently used.29

Research also began analyzing other aspects
of the ways in which protection actually
worked. Here, attention focussed on rentseeking
(Krueger, 1974) as a by-product of

protection (and, indeed, as a user of resources


### ---Economics-1997-0-10.txt---
as lobbyists sought protection-see Bhagwati
and T. N. Srinivasan, 1980), as resources
were used to obtain valuable import licenses,
thereby incurring deadweight costs. This, in
turn, showed that protection was more costly
than earlier, area-under-the-triangle estimates
had indicated. It further enabled insights as to
the buildup of vested interests that is likely to
arise once any policy is undertaken. When policy
reforms were attempted, it was clear that
those administering earlier policies were in
the forefront of those opposing change, alongside
the beneficiaries of protection (or other
policies).

Related to work on rent-seeking and the tendency
for vested interests to spring up around
the policies that were adopted, others worked
on the theory of overinvoicing and underinvoicing
(see Bhagwati, 1974) and smuggling

(see Munir A. Sheikh, 1974; Mark Pitt, 1981),
again focussing on some of the flaws of the
system of protection as practiced in most developing
countries.

As trade regimes became more chaotic,
empirical work began to document these problems,
bolstered by the development of the
measurement tools embodied in the concepts
of effective rates of protection and domestic
resource costs. Researchers focussing on Pakistan
discovered that there was actually negative
value added in some circumstances,

suggesting that it would have been cheaper to
pay workers to stay home and import the final
product.3"

The Organization for Economic Cooperation
and Development (OECD) sponsored a

series of country studies on industrialization
led by Little et al. The three synthesized
(1970) the results and provided estimates of
effective rates of protection in a number of developing
countries. These showed how high

and indiscriminate protection levels were and
demonstrated the extent to which import substitution
had failed to achieve many of the objectives
set for it. A later series of country
studies undertaken under the auspices of the
National Bureau of Economic Research, synthesized
in works by Bhagwati (1978) and

Krueger (1978), provided further systematic
empirical evidence of the economic wastefulness
and irrationality of the inner-oriented
trade regimes.

F. East Asian Experience

At the same time as evidence of the high
costs of import-substitution regimes was accumulating,
another important development

occurred. Starting first in Taiwan, several East
Asian economies began growing rapidly under
policies diametrically opposite those prevalent
under import substitution. Interestingly, the
Taiwanese government seems to have listened
carefully to the views of S. C. Tsiang,3 1a professor
at Cornell University specializing in international
economics. Following the precepts

of comparative advantage, Tsiang advocated
growth through industrialization, but with industrialization
taking place through increased

capacity for exports, as well as for the domestic
market. Taiwan's transformation from a
high-inflation, inner-oriented, aid-dependent
economy to a major exporting economy is well
known.

Korea, whose initial conditions appeared, if
anything, even less conducive to growth than
those of Taiwan, followed the same pattern. In
the late 1950's, Korea's exports had averaged
only 3 percent of gross domestic product
(GDP) and were growing slowly, if at all,
while imports represented 13 percent of GDP.
The current account deficit was financed
largely by foreign aid, and the domestic savings
rate was virtually zero. Major policy reforms
took place in Korea in the early 1960's,
which greatly increased the return to exporters.
There were fairly uniform incentives to
all exporters and assurances that the real
exchange rate would not appreciate to their
detriment. Reforms also reduced the protection
to import-competing producers and permitted
exporters duty-free importation of needed
intermediate goods and raw materials.
The Korean economic performance was
transformed, as growth rates entered the
double-digit range and living standards
30See Corden (1971 p. 51) for a summary of that
literature.

" For an account of Taiwan's turnaround, see Tsiang
(1985).


### ---Economics-1997-0-11.txt---
improved rapidly. Hong Kong and Singapore
also became part of the East Asian "miracle"
through policies designed to encourage exporting.
Growth rates exceeded those previously
thought to represent an upper bound on
attainable performance.32

It was not until the 1980's, however, that
the importance of the differences became
unarguable. After the second oil price increase
of 1979, the worldwide recession of 1980-
1982, and the accompanying "debt crisis,"
the East Asian net importing countries (NICs)
rapidly resumed growth, whereas other heavily
indebted countries were unable to service
their debts and were hard hit by events in the
international economy. Research undertaken
in attempting to understand the impact of the
debt crisis on the developing countries made
it abundantly evident that the debt-GDP ratios
were not significantly different between the
two groups of countries. What was significantly
different was the debt-export ratios, as
the East Asian countries were able to maintain
debt servicing and resume growth because of
the greater flexibility of their economies.33 It
also emerged that, even prior to the debt crisis,
the rates of growth of inner-oriented developing
countries had not increased despite substantial
increases in their savings rates.:

This is not the place to enter into the debate
as to the factors contributing to the success of
the East Asian "tigers." For, while there is
debate about whether government intervention
in "picking the winners" was a key component
of the growth strategy, 35 all recognize that
the reversal from an import-substitution strategy,
the opening up of the economy, and the
relative uniformity of incentives across the
board were necessary, if not sufficient, for success.
Indeed, there is an irony in the fact that
the East Asian experience has stimulated some
to attempt to identify the "dynamic" factors
in exporting that are absent from production
for the domestic market. Thus, we have a complete
turnaround: in the 1950's and 1960's, the
neoclassical argument for an open trade regime
was rejected on the grounds that it was
"static" and ignored "dynamic considerations"
; in the 1990's, there appears to be widespread
agreement that the benefits of an open
trade regime are largely "dynamic" in nature,
and go well beyond the gains from trade under
"static" models of an open economy. Just as
was the case with the infant industry argument,
however, there is a question as to how to identify
and measure these "dynamic" gains.

II. How Did Economists and Researchers
Go Wrong?

The "Washington consensus" is very different
from the policy consensus that led to the
adoption of import-substitution policies in the
1950's and 1960's. While there will no doubt
be refinements in that consensus with further
experience and research, it is highly unlikely
that the ideas of the 1950's and 1960's will be
revived.

One can raise three questions about the
change in viewpoints. First, how could it be
that the economics profession, whose consensus
on the principle of comparative advantage
was at least as great as that on any other policy
issue, endorsed a highly protectionist policy
stance?36 Second, what factors contributed to


### ---Economics-1997-0-12.txt---
changing the entrenched views of the 1950's
and 1960's? Finally, what types of research
were most (and least) productive in bringing
about better understanding of the role of trade
and trade policy in development? I address
these questions in turn.

The first is the issue of how the principle of
comparative advantage could have been so
blithely abandoned. With hindsight, it is almost
incredible that such a high fraction of
economists could have deviated so far from
the basic principles of international trade.
What led them to do so? Can any lessons be
drawn to avoid (or shorten the duration of)
similar mistakes in other applied fields when
new policy problems arise?

But, recall the stylized facts that were
widely accepted. People were thought not to
respond to incentives; exports earnings were
thought to be predetermined and slowly growing
at best; industrialization was necessary for
development; supply response was lacking;
and so on. These stylized facts, which were at
best simplistic and in most instances simply
wrong, permitted economists to conclude that
developing economies were "different."
However, it took theory to support these
conclusions. Here, one can distinguish several
failures. First, there was misapplication of
good theory. Second, there was what I shall
call the "theory of negative results," which
essentially could be used to provide a rationale
for virtually any trade intervention. Third,
there was good theory harnessed to erroneous
stylized facts.

A. Misapplication of Good Theory

Misapplication of good theory was significant.
37 The identification of comparative
advantage with the two-factor, two-good
model, and the assumption that free trade
would imply that developing countries would
forever specialize in primary commodities,
was an important misapplication. One of the
puzzling aspects of the evolution of thinking
about policy is the degree to which proponents
of open trade regimes failed to refute the allegation
that free trade would forever leave developing
countries specialized in production
of agricultural commodities.38

It was not until the 1970's (see Ronald W.
Jones, 197 ib; Krueger, 1977) that modelsmotivated
in part by the East Asian experiencewere
developed in which three factors of
production (land, labor, and capital) were allocated
among sectors, each of which could

produce many commodities. As the threefactor
models demonstrated, comparative advantage
lies within manufacturing and within
agriculture, and not between them. Thus, poor
unskilled, labor-abundant countries have a
comparative advantage in labor-intensive agricultural
and unskilled labor-intensive manufactured
commodities, while countries with

a much higher land-labor ratio have a comparative
advantage in more land-using agricultural
commodities and their comparative

advantage in manufacturing lies more in goods
with higher capital-unskilled labor ratios. In
these models, the overall trade balance in manufactures
is a function of the size of the manufacturing
sector, itself a function of past

capital accumulation and the land-man ratio.
A second serious misapplication of good
theory arose because of the nonoperational
nature of the theory itself, and the failure to
identify circumstances under which policy implementation
might be incentive compatible

and potentially increase welfare. A key culprit
in this case was the interpretation of the infant
industry argument. As I already discussed, it
was widely touted as a basis for import substitution,
and generally recognized as a "legitimate"
case for a departure from free trade.
One can hardly argue with the proposition
that the presence of a positive externality gives
rise to a basis for intervention; if the externality
is dynamic and temporary, then temporary
3 Another example of misapplication of good theory
was the early defense, such as that of Hagen (1958), of
protection because of a domestic distortion. But it took the
development of the theory of domestic distortions to correct
that, as is discussed below.

38 Some of Johnson's (1958) research on trade and
growth went some way toward refuting this proposition,
but still in a 2 x 2 framework. Moreover, Johnson's work
implied that labor-abundant countries would, while accumulating
capital, undergo "ultra anti-trade biassed"
growth, which seemed to support import substitution.


### ---Economics-1997-0-13.txt---
intervention, such as infant industry protection,
can be called for.

The problem with the argument, as a basis
for policy, is that it fails to provide any guidance
as to how to distinguish between an infant
that will grow up and a would-be producer
seeking protection because it is privately profitable.
It is not even clear how one could begin,
empirically, to identify the domain of the
externality. Moreover, even if there were a
producer or producers whose increased production
would generate dynamic externalities,
it does not follow that any level of protection
is warranted. And there is nothing in the infant
industry argument to provide guidance for
quantifying or estimating the likely magnitude
of the externality.

Indiscriminate protection in developing
countries was defended on infant industry
grounds with arguments of capital market failure,
labor market failure (as the costs of training,
presumably, would be borne by first
entrants into industries and then not recouped
as others hired workers away), costs of investments
in technology, and uncertainty all

used. It was not until Baldwin's (1969) seminal
article that it was demonstrated that, even
when the presumed imperfection existed, it
was unlikely that infant industry protection
would help correct it. As Baldwin cogently argued,
later entrants to an industry might speed
up their investments if protection made domestic
production more profitable, and the first
entrant might even be worse off! It was only
after critical examination of these circumstances
that the defenders of the infant industry
case for import substitution became less
vehement.

The infant industry argument also is an
excellent example of a theory that is nonoperational
because criteria for bureaucrats to identify
cases have not been put forward. Quite
aside from the unpredictability and immeasurability
of the future time path of costs in new
factories and the moral hazard associated with
asking individual entrepreneurs to indicate
how much protection they need, there is nothing
to my knowledge in the literature specifying
how the policy maker might instruct a
bureaucrat to identify (much less measure) a
dynamic externality if it were present, how an
incentive-compatible mechanism might be devised
for improving welfare, how the bureaucrat
might measure the height of warranted
protection, nor how policy makers might credibly
commit to temporary protection. Even ex
post, it is not entirely clear how one might
identify an industry as a successful infant: simply
because a firm became profitable and exported
does not prove that there was either an
externality or a dynamic process at work! 9
B. Negative Results

Much of the theorizing that took place was
concerned with what I call "negative results."
That is, analysts sought to find reasons why,
for example, an exception to free trade should
be made. Once the principle of comparative
advantage was laid down as a basis for policy,
there was little left for theorists to prove supporting
an open trading system, so the challenge
to theorists was to find conditions under
which the free trade precept did not hold. As
theory, these findings were significant, but for
policy they were unhelpful, and probably
served to perpetuate inappropriate policies.
In most real-world circumstances, one
strongly suspects that protection exists where
theoretical exceptions do not justify it, and that
moves to first-best policies would on average
lower, and not raise, protection. Judged by that
metric, research output relevant for policy
would consist more of attempts to measure the
costs of these excess levels of protection. In
practice, it would be interesting to review the
literature and ascertain how many articles, or
pages, or other measures of research output
were devoted to finding exceptions to the
proposition that comparative advantage should
form the basis for trade policy, contrasted with
those focussing on circumstances where protection
was too high! In undergraduate international


### ---Economics-1997-0-14.txt---
economics courses, sections on trade
policy spend considerable time addressing national
defense exceptions, the optimum tariff
argument, the infant industry argument, secondbest
arguments, and other arguments for protection.
While attention is paid to the reasons
why these arguments may not be correct, focus
nonetheless centers on the exceptions to the
case for free trade, rather than on the reasons
for it. While this may be inevitable as a way
of reasoning, the temptation to draw inappropriate
inferences seems high.

An example will illuminate the argument.
Whereas theory suggests criteria for departures
from laissez-faire free trade which normally
would result in different levels of
protection for different industries, a widely
used prescription for policy makers is that, if
there is to be protection, a uniform tariff is
usually preferable to any alternative structVre.
This proposition rests on several considerations.
First, only a uniform tariff can generate
a uniform rate of effective protection in the
import-competing sectors and, if different
goods are subject to different rates of tariff,
the resulting differences in effective rates of
protection will lead to resource misallocation
even within the import-competing industries
and have no relation to underlying "dynamic"
or market-failure considerations. Second, a
uniform tariff simplifies customs administration,
making evasion and/or bribery of customs
officials more difficult than a varying rate
structure. Third, a uniform tariff greatly reduces
the opportunities for resource losses in
rent-seeking and lobbying. Fourth, given international
prices, international value added is
more likely to be maximized under a uniform
tariff structure than under a variable one.
None of these arguments is sufficient to
prove that a uniform tariff is optimal. And, indeed,
it is straightforward to develop models
in which a uniform tariff is nonoptimal, especially
in the presence of income-distribution
considerations. In theory, the costs of protection
can be minimized by imposing higher
tariffs or taxes on goods whose supply and demand
is relatively more price inelastic.
Those arguments, as put forward, are all
couched in terms of demonstrating the "falsity"
of the proposition that a uniform tariff is preferable
to variable tariff rates and that there is a
departure from uniformity that can potentially
improve welfare. But the difficulty with that formulation
is that it does not provide a criterion
for which departures from uniformity might
improve welfare, because a model considering,
for example, income-distribution considerations,
cannot simultaneously address issues of corruption
and administration. And, the fact that
income-distribution considerations can warrant a
nonuniform tariff structure does not prove that
any nonuniform tariff structure is preferable to
a uniform one! As such, a negative result gives
little or no guide for policy. Nonetheless, it arms
lobbyists and others with ammunition to discredit
technocrats' efforts to maintain a less irrational
structure of protection.

Some good theoretical papers would have
done less damage, or at least given less aid and
comfort to policy positions that were clearly
not those intended in the analyses, if the authors
had taken greater pains to note the limitations
to their analyses, and the other factors
that would have to be taken into account, before
their results were applied to policy.
In that regard, it is often overlooked that most
policy implementation is carried out by government
officials who cannot be expected to have
advanced degrees, and sometimes even undergraduate
degrees, in economics. In many instances
(including formulae for optimal tariff
differentiation), the degree of sophistication
needed to interpret research results is well beyond
that which most bureaucrats will have. As
pointed out by Johnson (1970 p. 101):
...The fundamental problem is that, as
with all second-best arguments, determination
of the conditions under which a

second-best policy actually leads to an
improvement in social welfare requires
detailed theoretical and empirical investigation
by a first-best economist ... it is
therefore very unlikely that a second-best
welfare optimum will result based on
second-best arguments.

C. Good Theory Assuming

Counterfactual Situations

The final abuse of theory was primarily a
fault of inappropriate stylized facts. Nonetheless,
in many instances, analysts assumed
signs of variables that were certainly questionable,


### ---Economics-1997-0-15.txt---
modelled the situation neatly, and then
drew policy conclusions that could hold only
if the posited signs were valid. Yet their claims
often went beyond the assertion that "if these
facts ... then" variety.

As an example to illustrate the point, I have
deliberately chosen a good, widely cited paper,
because the paper represents good theory, but
interprets it, for policy purposes, with dubious
"stylized facts." Sudhir Anand and Vijay Joshi
(1979) considered a world, such as that envisaged
by Hagen ( 1958), in which workers in the
advanced sector receive a higher wage than in
the rest of the economy due to unions or other
(presumably unalterable) circumstances. They
then asked whether maximizing international
value added for given employment of domestic
resources is an appropriate criterion when
income-distribution considerations cannot be separated
from productive-efficiency considerations.
In their setup, the clear answer is no, because
tradeables are produced by the advanced (presumably
unionized) sector, and hence maximizing
international value will pull more

resources into that sector at the cost of a deteriorating
income distribution. Interestingly,
they do not address the question of whether the
advanced sector is labor or capital intensive. If,
as is true for outer-oriented developing countries,
the exportables are labor intensive relative
to import-competing activity, removing protection
to induce a move of more workers to the
"advanced" high-wage sector would presumably
increase wages of those workers and also
those in the rest of the economy: a more equal
income distribution would be obtained at the
expense of lower real wages for all. Without
regard to factor intensity, however, Anand and
Joshi (1979 p. 350) conclude that:

The motivation behind the theory of distortions
has been to criticise and to guide

trade and industrialisation policies ...
Our analysis emphasises the need for
caution ... Departures from technical efficiency
may be called for as part of the

rational response by governments to the
limitations they face in carrying out desirable
income distribution policies ...40

Anand and Joshi ( 1979) assumed that moving
toward economic efficiency in tradeables
requires paying higher wages because of a distortion.
Yet, in fact, the evidence suggests that
it has been the highly protected, importcompeting
industries which have been able to

pay above-average wages; removing protection
has led to rapid expansion of employment
in labor-intensive industries. If the latter stylized
fact is correct, and if income-distribution considerations
are important, it would suggest that
the policy implications of the Anand-Joshi
analysis are the opposite of what they suggest-
namely, that policy makers should encourage,
even beyond the optimum, a shift of
resources out of protected industries (presumably
by removing protection) and into exportable
industries.4"

III. What Research Contributed

to Improved Policies

Policies that were not consistent with policy
makers' growth objectives were cloaked in respectability
in the 1950's and 1960's by theory

and stylized facts of the type I have already
described. I have so far discussed properties of
some theories that made them susceptible to
misapplication or misuse.

A second question is equally important,
however. That is, how did the change in economists'
policy prescriptions come about? What
led to the reversal to recognition of the importance
of an open economy after the conversion
to advocacy of import substitution in
the 1950's and 1960's? I can address this question
more rapidly because much of the answer
was implicit in the description of the evolution
of developing countries' trade policies.


### ---Economics-1997-0-16.txt---
Three sets of research efforts can be singled
out as having been particularly useful in informing
changes in policy, although others,
no doubt, also contributed.42 First, there was
research analyzing how import-substitution
policies were actually working. Second, and
not unrelated to the first, there was the refinement
and more appropriate interpretation of
theory. Third, there was research demonstrating
the feasibility of the alternative.
A. Challenging the Stylized Facts and
Understanding How Import-Substitution
Regimes Worked

Analyses of the evidence regarding the key
stylized facts were in hindsight important steps
in undermining the intellectual consensus.
Demonstration that there were significant responses
to incentives undermined the policy
case for ignoring prices. Proof that the terms
of trade had deteriorated very little, if at all,
began to undermine export pessimism.
Empirical work on the ways in which
import-substitution regimes functioned was
crucial. Comparative analyses such as those
of Little et al. (1970), Bhagwati (1978),
Krueger ( 1978, 1983 ), and Michael Michaely
et al. ( 1991 ) clearly contributed significantly
to awareness that the effects of importsubstitution
policies were not idiosyncratic to

individual countries. The comparative studies
provided a great deal of evidence as to the
shortcomings of reliance on import substitution.
Evidence that protection was not temporary,
that protection levels were high and
idiosyncratic, that there was very great discrimination
against exports, and that "foreign

exchange shortage" was a function of policies
and not an exogenously given datum, were all
important in challenging the protectionist trade
policies still prevailing in most developing
countries in the 1980's.

If one considered the evidence regarding the
workings of trade policies in any one country
taken alone, there were ample grounds for criticism
of inner-oriented trade policies, with the
monopoly positions they conferred on domestic
producers, the high costs of doing business,
rent-seeking low quality of products, and so
on. It was possible, however, to recognize that
and nonetheless conclude that policy makers
in that particular country had been inept, or
had simply failed to implement policies appropriately.
As evidence mounted across countries,
the similarity in the evolution of regimes
and their consequences was striking. It was increasingly
difficult to dismiss the evidence

from a particular country as being sui generis
or the failing only of the particulars of policy
execution in that country.

But, underpinning the analyses of individual
country situations, either in the comparative
studies or individually, were agreed-upon
measurement tools. The empirical studies
could not have had their impact without the
development and use of measurement tools.
As cost-benefit techniques were used, it became
increasingly difficult to justify some
highly uneconomic projects. And, as measurement
of effective rates of protection was undertaken
in country after country, the high and
erratic nature of protection became evident.
Techniques for cost-benefit analysis and measurement
of effective rates of protection were
important, first of all, in providing analysts
with tools with which to demonstrate the chaotic
nature of import-substitution policies. In
addition, even before the policy consensus
changed, there is little doubt that some of the
earlier extreme irrationalities of policy were
curbed through use of these tools. It became
extremely difficult to defend the high average
of, and wide variance in, effective rates of
protection.

At an empirical level, it seems clear that early
demonstrations of the great range of variation in
rates of effective protection were useful both in
demonstrating some of the problems with trade
regimes and also in preventing at least a few of
the worst excesses that might otherwise have occuffed.
More generally, recognition and reintroduction
of the proposition that there is a

42 The ideas and events influencing policy makers in
one country may not have been precisely the same as those
in the next one. And, of course, no precise measurement
of the relative influence of research results and of experience
is possible. There is also the question of the role of
research, relative to the role of experience itself. However,
in fact, the ways in which people in other developing
countries learned about the East Asian experience was
largely the consequence of research efforts.


### ---Economics-1997-0-17.txt---
response to incentives that cannot be overlooked
in policy formulation, combined with the evidence
on the erratic and arbitrary nature of incentives
provided by trade regimes, forced a
reexamination of the premises on which importsubstitution
policies were based.

Yet another contribution of empirical research
was to focus upon the actual workings
of policy implementation. In early policy prescriptions,
there had been something of a naive
tendency to assume that enunciating a desired
outcome was itself sufficient to achieve it.
This naivete was dispelled, as the theories regarding
bureaucratic behavior, rent-seeking,
smuggling, and overinvoicing and underinvoicing
all enabled observers to examine more
critically the ways in which alternative policy
prescriptions might have side effects that had
earlier been unanticipated.

B. Refinement and More Appropriate

Interpretation of Theory

As already seen, some of the intellectual underpinning
of import-substitution policies was
provided by inappropriate interpretation of
theory, or the failure of theory to take into account
key institutional or behavioral variables.
Analytical developments focussing on conditions
under which these interpretations were
valid, or examining the ways in which results
had to be modified to take into account these
institutional and behaviorial aspects, were
clearly important in improving understanding.
The entire literature on optimal interventions
in the presence of domestic distortions is
one important example of a demonstration that
earlier interpretations of theory had failed to
examine the relevant alternatives. It was invaluable
in demonstrating clearly that in most
circumstances, the presence of a distortion
warranted a first-best policy intervention other
than a tariff.43 For example, in the case of
Hagen's (1958) employment-generating case
for protection, the optimal intervention literature
demonstrated clearly that a first-best intervention
would be in the labor market, and

that a tariff or quota could not achieve a firstbest
outcome.

Similarly, developments showing that the
comparative advantage results were not the
simple "specialize forever in primary products"
precept proved significant in enabling
policy makers to contemplate alteration in
trade strategy. Baldwin's (1969) critical examination
of the infant industry argument provides
yet another example of an analytical
contribution that was important in making
those concerned with policy consider carefully
the effectiveness of the policies they had
adopted in achieving their desired goals.
Finally, there was theory that was developed
in response to the functioning of importsubstitution
regimes. Here again, the theory of

rent-seeking, as it pointed to the ways in which
bureaucrats and others made protection very
costly, was important. Further, when it was
recognized that bureaucrats, businessmen, and
others attempted to capture or thwart policy
initiatives not in their self-interest and that
they acquired an interest in maintaining the
system, once established, and that resources
were expended in operating the system, it had
to be recognized that changing the system
would be politically difficult.

Development of a better understanding of
the incentives for underinvoicing and overinvoicing
of exports and imports and for smuggling
under exchange-control regimes worked
in the same direction: not only could these activities
prove costly to the exchequer and in
terms of resource drains, but the very recognition
of their presence served to remind

policy makers of the limitations of their
instruments.

Finally, good analyses demonstrating how
individual import controls actually worked
contributed to understanding and made empirical
work more effective. The further

refinement of theory showing tariff-quota
equivalence has already been mentioned.
Rent-seeking again comes to mind. But, in
addition, individual mechanisms for encouraging
import substitution each had their

own, often idiosyncratic, incentive effects. A
good example is Gene Grossman's (1981)
classic analysis of domestic content regulations
and their effects.


### ---Economics-1997-0-18.txt---
C. Demonstration of the Viability of
Alternative Trade Policies

Research on the contrast between East
Asian and other developing countries and reasons
for it obviously turned out to be a major
contributing factor in influencing thinking
about policy. In a way, research on East Asian
experience provided a final blow to the earlier
uncritical acceptance of the stylized facts. For,
the East Asian experiences demonstrated, as
nothing else could have, the feasibility and viability
of alternative trade policies: it was no
longer possible to associate comparative advantage
with reliance on primary commodity

exports, and the East Asian experience certainly
put an end to the belief that developing
countries could not develop rapidly when relying
on integration with the international
economy.'

The experience of the East Asian exporters
did several things. Most important, it provided
concrete evidence that a developing country
could achieve industrialization without relying
on domestic markets to absorb almost all
additional output. That demonstrated the fallacy
of the earlier view that industrialization
could take place only through import substitution.
45 Also, the East Asian trade regimes offered
significant opportunities for empirical
research, and the evidence mounted that properties
formerly thought to be those of all developing
countries were, in fact, properties
resulting from inner-oriented trade and payments
regimes.

It cannot be said that either research results
or the contrast in economic performance alone
led to the change in policies in other developing
countries.46 Both research (especially
that which brought the sharply contrasting experiences
of the East Asian exporters and the
import-substituting countries into focus) and
experience contributed.

Whether one should regard the East Asian
experience as entirely separate from economic
theory, however, is an interesting question. As
already mentioned, Tsiang (1985) was himself
an international economist, and it was in
significant part his efforts that led the Taiwanese
authorities to abandon inner-oriented
policies and attempt to develop through exports.
The theory of comparative advantage
was, at least in that instance, a pillar on which
policy was built. And, while a variety of factors
no doubt contributed to the Korean adoption
of outer-oriented trade policies after 1960,
the favorable experience of Taiwan undoubtedly
facilitated the willingness of decision
makers to try the new approach.

The East Asian exporters put to rest the
mistaken belief that developing countries relying
on the international market would forever
be specialized in the production of primary
commodities. They also showed that rates
of growth well above those realized even in
the most rapidly growing import-substitution
countries such as Brazil and Turkey could be
realized.

IV. What Lessons Can Be Learned for Research
in New Applied Fields?

It is difficult to draw generalizations based
on the evolution of analysis, empirical research,
and policy in one applied field. Nonetheless,
in the hope that insights from other
applied areas may reinforce or amend the list,
the effort seems worthwhile.

Perhaps the most obvious generalization
from the various factors that have been discussed
is that empirical research which tests
for the presence and order of magnitude of
stylized facts which are used in modelling and
" To be sure, there are still doubters. Some claim that
South Korea and Taiwan were major recipients of foreign
aid, which is said to account for much of their rapid growth
(although the announcement that foreign aid would diminish
was what triggered policy reform in Korea). The
status of Hong Kong and Singapore as city-states is alleged
by some to render their experience of little relevance.
Even today, those resisting policy changes assert
that conditions in the 1950's and 1960's were conducive
to export expansion in ways in which the world market of
the 1990's is not-despite the rapid expansion of exports
from China and Southeast Asian countries.
45 Some have argued that the East Asian outer-oriented
trade strategy might not have succeeded without an earlier
stage of import substitution. In that view, East Asia moved
away from import substitution at the "right time,"
whereas other countries stayed with the strategy too long.
See Gustav Ranis (1984) for one such argument.
46 For that matter, trade policy reform is still resisted in
many countries, notably most of Sub-Saharan Africa.


### ---Economics-1997-0-19.txt---
policy formulation can be invaluable. If the
right stylized facts can be used as a basis for
theory, and theorists have good indications of
the relative quantitative importance of various
phenomena, it is clearly far more likely that
the theory itself can make a useful contribution.
In the case of trade policy and development,
the demonstrations that there were responses
to incentives and that developing countries
could expand export earnings and did have
comparative advantage in other than primary
commodities, were clearly crucial to improved
understanding of the relationship of trade to
development.

For that reason, high marks must go to the
analytical research that pointed to measurement
techniques such as effective protection
and cost benefit, which enabled policy makers
and their analysts to obtain empirical quantification,
however rough, of the relevant

magnitudes.47

In like manner, the empirical demonstration
of the similarity of policy responses across developing
countries, and of the wide and largely
irrational variation in incentives for importcompeting
industries, increased understanding
of what was wrong with existing policies.
Overturning, or more accurately interpreting,
the accepted stylized facts, therefore, was
a first prerequisite for developing a better theory
of trade policy for development. But theory
was important in many ways, in addition
to pointing to appropriate measurement tools.
First of all, good policy-relevant theory provided
blueprints for those windows of opportunity
in which governments genuinely sought
to improve economic performance, as was the
case in Taiwan and Korea in the early 1960's,
and in Chile, Mexico, and India in later decades,
to name just a few.48 Having the "blueprints"
on hand from good theory is obviously
a major contribution. As already noted, however,
that theory is often relatively dull -such
as comparative advantage-rather than the
more exciting and refined results of complex
models.

Second, theory was invaluable when it
showed why simple interpretations of received
doctrine were in fact wrong. This was the case
with the theory of first-best intervention in the
case of domestic distortions, and in the case
with comparative advantage as interpreted to
mean developing countries would specialize in
the production of primary commodities, and
with the infant industry argument.

These considerations suggest that research
results, in order to be most likely to be amenable
to policy relevance, should be interpretable
into phenomena that are observable,
hopefully quantifiable, and recognizable by
the policy maker. A negative result, such as
that theory does not always tell us, can be
counterproductive precisely because the policy
maker is informed only that a certain generalization
(such as comparative advantage

and the value of free trade) is not without
exception; the generalization can then be
ignored.

A more general statement of the problems
inherent in theorems which show that major
propositions are "not generally true" would
encompass all of that theory which is cast in
terms of "anything can happen." While it is
certainly true that there are conditions under
which a wide range of outcomes (Paretoinferior,
a bad equilibrium, Pareto-superior,
etc.) are possible from the same policy instrument,
it would have challenged the skills of
even the most superb theorist to attempt to develop
a case for the sorts of chaotic policies
prevalent in Turkey in 1957, in Ghana in 1983,
and in Argentina in the late 1980's. It is far
too easy for analysts to ignore the fact that "an
exception" does not rationalize all possible
policy alternatives to free trade.

There is a criterion for efficient resource allocation,
equating domestic and international
marginal rates of transformation. Even if there
are "dynamic" factors which contravene part
of the static efficiency criterion, they too are
measurable. Yet the "anything can happen"
theories do not provide guides as to how the


### ---Economics-1997-0-20.txt---
phenomena under examination may be quantified,
and thus provide rationalizations (admittedly
for those who want them) for policies
that cannot by any realistic test pass muster.
Perhaps the lesson is that there is a significant
danger that economic theory will be misinterpreted
in the policy arena, and researchers
could productively take more pains to distance
themselves from policy conclusions that are
not warranted by their analysis. Theoretical
papers which end with "it has been shown
that, under conditions x and y, policy z may
no longer represent an optimum ... Therefore
policy should ..." are obviously overstepping
their bounds when the empirical relevance of
x and y are not yet established, and even more
so when conditions other than x and y also may
be important (as, for example, with rentseeking)
.

But many good theory papers are written
where the authors assume that their audience
will consist entirely of other theorists. In such
instances, good theory may be misused, and it
certainly will be in the self-interest of some to
harness it to their own ends. It behooves applied
economists, as well as the theorists, to
be careful to interpret the policy relevance of
results in ways which minimize the scope for
misinterpretation. This is as true for those
seeking to find "dynamic" aspects of exporting,
or endogenous aspects of a "big push,"
as it should have been for those developing the
infant industry or optimum tariff arguments.
Complex results, such as those noted by
Johnson (1970), are particularly suspect in
that they can be interpreted in whatever ways
suit the decision maker or lobbyist.
Finally, there is theory which provides no
guidance as to when or how to observe the
phenomenon. In such instances, it is difficult
to find policy implications that will not be captured.
One possible challenge for theorists
might well be to ask for at least one plausible
incentive-compatible mechanism under which
the inefficiencies they identify might be improved
upon by policy makers and bureaucrats.
The existence of infant industries, of
cases in which there are rents that might be
captured by appropriate strategic trade policy,
and of informational asymmetries and other
market imperfections cannot be doubted. But
until the magnitude of these phenomena can
somehow be measured, or incentive-compatible
mechanisms for correcting them can be devised,
theorists asserting their presence are
simply providing a carte blanche for policy
makers and bureaucrats to intervene in whatever
ways they like, and this will simultaneously
be seized upon by special interests to
bolster their causes.

No matter how careful economists are,
special interests always will seize their research
results in supporting their own objectives.
And, no matter how sophisticated and
careful research findings are, there always
will be politicians formulating, and noneconomists
administering, policies. Recognition
of these propositions could do much
to increase the degree to which economists'
research results can contribute (positively)
to policy formulation.

I
 ## Economics-1998-0


### ---Economics-1998-0-01.txt---
One of the great pleasures of belonging to
my generation of economists is that we were
able to witness the birth and the subsequent
evolution of the modern approach to the
analysis of economic growth. The centerpiece
of that approach is probably growth
accounting, but we should never forget that
growth accounting is firmly rooted in economic
theory.

My way of telling the story goes like this:
Many, maybe even most, economists expected
that increments of output would be explained
by increments of inputs, but when we took our
best shot we found that traditional inputs typically
fell far short of explaining the observed
output growth. Our best shot consisted in attributing
to each factor a marginal product

measured by its economic reward. Thus:
(1) ffAy = WAL + (p + 6)AK + R.

Here:

Ay = change in output (GDP);

AL = change in labor input;

p = initial general price level;

w = initial real wage;

T = initial real rate of return to capital;
8 = rate of real depreciation of capital;
AK = change in capital stock; and

R = "the residual" of growth unexplained
by increases in traditional inputs.
Many economists are probably more familiar
with a variant of (1)

(1') (Ay/y)-= (wL/py)(LL/L)

+ [(+ 6)Klfy ] (AK/K)

+ (Rly) = se(AL/L)

+ Sk(AK/K) + (Rly).

In whichever form, the measured residual
typically accounted for an important fraction
of the observed output growth, quite often half
or more.


### ---Economics-1998-0-02.txt---
| | e46; 2~~~~~. . . . . . . . . . . . . .. . .
~~~~~~~~~~~.. . . . ......... s

% ~ ~ ~ ~ ~ ~ ~ . .. .. . .. .. .. .. .. .. .. .. .. .. .. .. ..
.: . ::'.'.':. ::R . .'.. . '- : -, ' ,:: .:. ..,.. . . .. . . . . .... .. . . """....
:::: : :::::::::':' : '.:-'::. ..: .: : :::: ::::. . . . . . . . . . . . . . . . . :. : -. : . . . . ..
.. . .. . . .. ... ...: : . ::: ::::::: : .: :-::: :::..:.. .: ...:.. . ....: . . ........:': ::b -::
i, -' .: :: .:':::..,::. ...... . .. ....; .. .... .. .. ::. :-:. : ...: : ,... .. .. . :.: ::: : @ .::-:
| .,, '"':.-:u:."..... ...... , . ... .......
tei. . . . . . . . . . . . . |\ - .s
X ;.... :... .. .: - . .--:~ i:; - : -: - :-...."-.> ||s.|::':: Z: :...' :: S
''''.'".''.'-.~~~~~~~~~~~.'..'...

|k:-..'.-..^.;:-'-.'..:'..''..... .........
'.~~~~~~~~~~~~~~~~~~~~.. . .. ...

_S .>.?............;"'.

f>~~~~~~~~~~~~~~~~~.. ..........

_ | ~ ~~~ ~~~~~~~~ .. .....

__ _. . . . . . . . . .. . . . . . . . . . . _
_E,,.. ,~~~~~~~~~~.. ...,..

...... .........

.............. . ...

. . . . . . . . . . . .

..... ... ... ......... ..... .. ....
____........ . .. . ...

___ ...._

_ _......

_ _ _ _ _~~~~~~~..........

_ ~~~~~~~~~~~~~~.........


### ---Economics-1998-0-03.txt---
This result came as a surprise to the profession,
though perhaps less so to those who
reached it, or something very like it, by an altemative
route. They were the people who

came at the problemn out of a tradition of measuring
labor productivity, and at some point
complemented output per worker with a measure
of output per unit of capital, and finally
joined the two to create a measure of total factor
productivity (TFP). The idea of total factor
productivity increasing through time was less
a shock to these people than the "growth residual"
was to those who approached its measurement
along the lines of equation ( 1) or
(1'). See Moses Abramovitz (1952, 1956)
and Solomon Fabricant (1954).

In any case, as the newly discovered residual
loomed large in our professional thinking, our
discussion centered on two potential explanations:
"human capital" and "technical advance.
" (See Robert M. Solow, 1957.) These
can be thought of as complementary explanations,
at least up to a point, with technical advance
representing truly new ways of doing
things, and the accumulation of "human capital"
representing increases in the "quality" of
the typical human agent. It was not long before
attempts were made to quantify the contribution
of improved labor quality. These came as part
of a general move toward disaggregation of the
two factors, which can be represented by:
(2) ffAy=ZiwViLi

+ z9(Pj + 6j)AKJ + R'.

Here the index i can vary over all sorts of education
and skill groups as well as categories
like gender, age, occupation, region, etc. All
these are items that may signal a different market
wage. In a similar vein, the index j would
appropriately vary over categories like the corporate,
noncorporate, and housing sectors

where, for tax if for no other reasons, different
(gross-of-tax) rates of return would presumably
prevail, even in a full equilibrium.
In an equation like (2), the presumed marginal
product of each category of labor is measured
by the wage wi . Average quality can be
measured by Q, = iwi Lil i L,, and the contribution
of change in quality to Ay, between
t and t + 1 can be calculated as Liw( i(A Q, + I /
Qt). Thus, the contribution of quality change
is already built into the first summation in (2),
but can be separately identified if we so
choose.

A focus on human capital could lead us to
a slightly different way of breaking down
1wiALi. Here we could choose some "basic
wage" w*, ideally the wage of some welldefined
category of relatively unskilled labor.
Then we could divide the remuneration wi of
any given category into a part w* which was
a reward for "raw labor" and another part
(wi - w *) which we would identify as the
reward to the human capital of a typical
worker of type i.

Using a framework like (2) has long been
the standard for careful professionals. Pioneered
by Zvi Griliches (1960, 1963), it was
utilized by Edward F. Denison (1967) and
John W. Kendrick (1973, 1976, 1977), among
others. This approach has been further developed
and carried to a high art by Dale W.
Jorgenson and Griliches ( 1967), Jorgenson et
al. (1987), and Jorgenson (1995).

The main point to be made here is that once
the residual is measured using a framework
like (2) or its equivalent, the direct, measured
contribution of human capital is captured in
the labor term 1wiALi. By direct contribution
I mean what people are paid for. Doctors earn
more than nurses, and engineers more than
draftsmen. These and similar differences are
captured in D;FiALi, which can be positive
even if XALi is zero, just from an upward reshuffling
of the same labor force. A truly accurate
measurement of type (2) would capture
all the subtle differences of quality that exist
in a modem labor force and would give each
a weight corresponding to the (gross-of-tax)
earnings that demanders are observed to pay.
We may do this imperfectly, but, in concept at
least, the residual R' as measured by (2) does
not contain any elements of quality change or
any direct contributions of human capital to
growth. This is a quite important point for it
permits us to zero in on the residual as representing
"technical change," "TFP improvement,
" and "real cost reduction."

There is no analytical reason to prefer one
of the above three terms over another, in referring
to the residual R'. But I am going a bit
out on a limb to say that a term like "technical
change" leads most economists to think of inventions,
of the products of research and development


### ---Economics-1998-0-04.txt---
(R&D), and of what we might call

technical innovations. On the other hand, TFP
improvement, once purged of the changes in
the quality of labor and/or the direct contributions
of human capital, makes one think of
externalities of different kinds -economies of
scale, spillovers, and systematic complementarities.
And finally, real cost reduction, to my
mind, makes one think like an entrepreneur or
a CEO, or a production manager.

I think it would be perfectly fair to characterize
my presentation today as a paean in
praise of "real cost reduction" as a standard
label for R'. Labels do not change the underlying
reality, but they may change the

way we look at it and the way we think about
it. They also can lead us to understand it better.
Thinking in terms of real cost reduction
has certainly done all this for me, as I have
tried to sort out the many puzzles and complexities
that surround the process of economic
growth.

Let me try to take you down the path I traveled.
In the first place, real cost reduction
(RCR) is probably on the mind of most business
executives, production managers, etc., at
some point or another in any given week, let
alone in any given month or year. It is a major
path to profit in good times, and a major defense
against adversity in bad times. Most U.S.
firms that have downsized in recent years did
so with RCR in mind. So, too, did the firms
that computerized their payrolls and other accounts.
And so also did those who shifted to
what they considered more modern management
techniques. I recall going through a
clothing plant in Central America, where the
owner informed me of a 20-percent reduction
in real costs, following upon his installation of
background music that played as the seamstresses
worked. And then there is the story of
two Chilean refrigerator firms that ended up as
parts of a single conglomerate at one point.
The new management reduced the number of
models from something like 24 to two, making
agreements to import other models while exporting
these two. The end result was that output
more than doubled, while the labor force
was cut to less than half, and even the capital
stock (at replacement cost) was significantly
reduced. This sounds like (alnd is really) economies
of scale, but they would not be detected
by our usual measures, as both labor force and
capital stock went down. And we all have seen
cases where, say, an office's real costs were
reduced when a martinet of a manager was replaced
by someone more reasonable. But we

have also seen cases where real costs were reduced
when a very lax manager was replaced
by someone more strict.

It has long been my song that there are at
least 1001 ways to reduce real costs and that
most of them are actually followed in one part
or other of any modem complex economy,
over any plausible period (say, a decade).
Once one accepts this proposition as true, the
question then arises: Why would anybody try
to settle on just one underlying cause of real
cost reduction? The answer, I think, is mindset-
the framework in which one is thinking
at the moment. The pioneer writings of the recent
endogenous growth literature can, I think,
be said to reflect a kind of annoyance at something
like R or R' being considered exogenous.
There was an urge to surmount that inelegance
by somehow making the residual endogenous.
And in a simple growth model that meant generating
a feedback from the rest of the model
to the residual. A 1001 feedbacks would be out
of the question, but one feedback would work
just fine. Thus Paul Romer (1986) focused on
a feedback through "knowledge," with the
stock of knowledge shifting production functions
all over the economy; Robert E. Lucas,
Jr. (1988) focused on "human capital," not
on its direct and remunerated productivity, but
on the externalities that each increase in the
stock of human capital were presumed to generate.
These single feedbacks achieved the
limited purpose of endogenizing R or R'
within a specified model, but they did not represent
very well the multifaceted nature of real
cost reduction as we observe it in actuality.
And, in point of fact, both the cited authors in
their more recent writings display a deep recognition
of the subtlety and complexity of the
growth process, not really capable of being
captured through a simple feedback mechanism.
(See Romer, 1990, 1994a, b; Lucas,

1993.)

So, real cost reduction is multifaceted and
everywhere around us. Where does that get us?
Or how can we get anywhere in the face of
such complexity? The next step is to recognize
that in spite of its complexity, real cost reduction
can be reduced to a single metric, and can


### ---Economics-1998-0-05.txt---
be made additive. For a quick appreciation of
this, assume that total factor productivity grew
by 80 percent in one industry over a decade,
by 60 percent in another industry, and by 50
percent in a third. If their initial value added
amounted to $100 billion, $200 billion, and
$300 billion, respectively, then the real cost
reduction of the first was $80 billion, that of
the second was $120 billion, and that of the
third $150 billion. So we can say that, measured
at initial prices, the real cost reduction
of the three together was $350 billion over the
decade in question. I truly think that the notion
of real cost reduction being additive in this
way came to my mind, and is easily seen by
others, just as a consequence of the label. The
idea of additivity does not follow nearly so
easily from the labels "technical advance"
and "total factor productivity."

Anyway, this vision of the growth process
opens up many new vistas and gives us many
new challenges. To me, it gives life to the residual,
viewed as real cost reduction, in a way
that remote macroeconomic externalities
never did. It gives the residual body, in the
sense that the number of dollars saved by real
cost reduction is a tangible and measurable
quantity. It gives the residual a name (real cost
reduction), an address (the firm), and a face
(the face of the entrepreneur, the CEO, the
production manager, etc.) And, finally, we
shall see that there can be vastly different expressions
on that face, even as we move from

firm to firm in a given industry, as the TFP
experience of a period moves from sharply
positive to devastatingly negative.
I. Yeast versus Mushrooms: Part I

Table 1 is based on the numerical example
just given, plus the information that the remaining
industries (say, in the economy) together
had an initial value added of $1,400
billion and experienced real cost reduction of
$150 billion over the period. Setting out data
in the format of Table 1 allows us to make
statements like "15 percent ($300 b./$2,000
b.) of the industries (measured by initial value
added) accounted for 40 percent ($200 b./
$500 b.) of the real cost reduction (RCR) of
the period" and "30 percent ($600 b./$2,000
b.) of the industries accounted for 70 percent
($350 b./$500 b.) of the period's RCR."
I stumbled on this way of presenting data on
real cost reduction in the course of writing a
background paper (Harberger, 1990) for the
World Bank's World Development Report of
1991. Once I saw it, I immediately embraced
it, because it helped me communicate to others
what I call the "yeast versus mushrooms" issue.
The analogy with yeast and mushrooms
comes from the fact that yeast causes bread to
expand very evenly, like a balloon being filled
with air, while mushrooms have the habit of
popping up, almost overnight, in a fashion that
is not easy to predict. I believe that a "yeast"
process fits best with very broad and general
externalities, like externalities linked to the
growth of the total stock of knowledge or of
human capital, or brought about by economies
of scale tied to the scale of the economy as a
whole. A "mushroom" process fits more
readily with a vision such as ours, of real cost


### ---Economics-1998-0-06.txt---
TABLE 2-CONCENTRATION OF TFP GROWTH AMONG U.S. INDUSTRIES 1958-1967
[COLUMNS (2) TO (5) IN BILLIONS OF 1958 DOLLARS]
Absolute amount

of real cost Cum. GDP by Cum.

TFP growth over period reduction sum of industry sum of
(1.0 = 100 percent) [(1) X (4)] (2) 1958 (4)
(1) (2) (3) (4) (5)

Lumber & Wood Products 0.72 2.51 2.51 3.50 3.50
Railroad Transport 0.63 5.52 8.03 8.70 12.20
Textile Mill Products 0.61 2.49 10.52 4.10 16.30
Electrical Machinery 0.55 5.10 15.66 9.30 25.60
Transport Equipment 0.46 7.05 22.71 15.20 40.80
Chemicals 0.44 3.97 26.68 9.10 49.90
Public Utilities 0.42 4.65 31.33 11.00 60.90
Petroleum and Coal 0.41 1.27 32.60 3.10 64.00
Rubber and Products 0.41 1.23 33.83 3.00 67.00
Mining 0.41 5.20 39.03 12.60 79.60

Communication 0.40 3.61 42.64 9.00 88.60
Trade 0.33 24.93 67.57 76.40 165.00
There follow 18 more industries

the combined results of which are 0.03 7.53 75.10 239.80 404.80
Notes: Top 10 percent (these percentages are contributions to GDP of industries ranked according to their present rate of
TFP growth over period) of industries account for 30 percent of total TFP contribution.
Top 22 percent (these percentages are contributions to GDP of industries ranked according to their percent rate of TFP
growth over period) of industries account for 52 percent of total TFP contribution.
Top 40 percent (these percentages are contributions to GDP of industries ranked according to their percent rate of TFP
growth over period) of industries account for 70 percent of total TFP contribution.
Sources: Kendrick and Grossman (1980). GDP data from U.S. national accounts.
reductions stemming from 1001 different
causes, though I recognize that one can build
scenarios in which even 1001 causes could
work rather evenly over the whole economy.
Personally, I have always gravitated toward
the "mushrooms" side of this dichotomy. I
remember being impressed, when I first saw
some early industry estimates of TFP improvement,
by their tendency to industry concentration.
For years I told my students that the
1920's were the decade of cars and rubber
tires, the 1930's the decade of refrigerators,
the 1940's that of pharmaceuticals (especially
antibiotics), and the 1950's that of television,
with telecommunications anid computers taking
over in recent decades. But these were just
impressions, not based on any systematic approach.
My real turnaround came in the course
of writing my 1990 paper, where I presented
a series of tables based on Kendrick and Elliot
S. Grossman's (1980) work. Table 2 is an
example.

Table 2 has the same format as Table 1. Column
(1) presents the familiar measure of the
percentage by which TFP grew, or real costs
were reduced, during the period in question
(note that the percentages apply to the period
1958-1967 as a whole; they are not annual


### ---Economics-1998-0-07.txt---
rates). To turn these percentages into dollar
amounts of real cost saving over the period,
one multiplies them by base-period real GDP
[col. (4)]. The results are shown in colunm
(2). Columns (3) and (5) are the cumulative
sums of columns (2) and (4), respectively.
Working with these figures one can make
statements like those at the bottom of the
table-i.e., the top 10 percent of industries
accounted for 30 percent of total real cost reduction;
the top 22 percent of industries (measured
by initial value added) accounted for
more than half of total real cost reduction.
Readers will notice that at the foot of each
column in the table is an entry refering to 18
additional industries, which together accounted
for only 10 percent of the total TFP contribuition,
while their combined share of initial output was
almost 60 percent of the total.

Using the analogy with yeast and mushrooms,
the results of my calculations using the
Kendrick-Grossman data pointed very clearly to
a "nmushrooms" interpretation. Not only were
the contributions to RCR highly conicentrated in
a relatively few industries, these industries also
were very different as one shifted from decade
span to decade span. The top four branches in
percentage of real cost reduction during 1948-
1958 were Communications, Public Utilities,
Farming, and Miscellaneous Manufacturing. In
1958-1967 they were Lumber, Railroad Transport,
Textile Mills, and Electrical Machinery. In
1967-1976 they were Finance, Insurance &
Real Estate, Apparel, Communications, and
Chemicals. Only Communications appears twice
among these 12 listings.

Now to my mind, this already brings
evidence to bear on a number of possible hypotheses
concerning the nature of TFP

improvement. Certainly some ways of interpreting
a generalized externality due to improved
education would be hard to justify

using evidence like this. Strong links of the
residual term to R&D expenditures' would
suggest a high degree of persistence among the
leaders in TFP improvement. So also (probably)
would economies of scale associated with
the scale either of the firm or of the industry.
Such economies are not likely to jump wildly
around from one industry to the next, from period
to period. One would expect them to embody
characteristics of the productive process
that would be relatively stable over time;
hence they should show a reasonably high degree
of persistence, over time, in terms of the
TFP experience of particular industries.
No economist can look at Table 2 without
thinking of its close analogy with a Lorenz
curve. That, indeed, was the next step I took
in trying to represent the degree of concentration
of real cost reduction. Figure 1 (drawn
from Edgar Robles, 1997), shows the quasiLorenz
curves for a 20-industry breakdown of
the U.S. manufacturing sector over four successive
five-year periods.

What strikes one immediately about Figure
1 is the characteristic "overshooting." I have
marked withi the first vertical line the point
where the rising curve crosses 100 percent on
the vertical axis. The interpretation is that in
1970-1975 the cumulative real cost reduction
of just 25 percent of manufacturing industries
(measured by initial value added) was equal
to the total RCR for manufacturing as a whole.
After that there are other industries producing
another 40 percent of the total, but their contribution
is offset by still other industries with
negative RCR during the period.

Corresponding to the 25-percent figure for
1975, we have around 12 percent for 1975-
1980, 48 percent for 1980-1985, and 40 percent
for 1985-1991. These are the fractions of
manufacturing industry which by themselves
were able to account for the full amount of real
cost reduction during the respective period, in
manufacturing industry as a whole.

The second vertical line in each panel of
Figure 1 marks the maximum point of the
curve. The interpretation is that about 64 percent
of industries enjoyed real cost reduction
during 1970-1975, with the remaining 36 percent
suffering real cost increases (declining
TFP). For the subsequent periods, the corresponding
figures are 65(35) percent, 78(22)

percent, and 82 ( 18 ) percent. Here the first figure
is the percent of industries enjoying real
cost reductions; the figures in parentheses represent
those experiencing declining TFP.

Some interest attaches to the ordinate of the
maximum point on each curve. In the first period,
TFP growth ended up accounting for close


### ---Economics-1998-0-08.txt---
Percentile 300

of Real

Cost

Reduction

170 ______________ 240,

Percentile

of Real

Cost

Reduction

100 1

100* _

0 ____

, | | , , ~~~~~~~~~~~~~~~~~PercentFieof Initial Value Added
0 Percentile of Initial Value Added
A. 1970-1975 B. 1975 -1980

150 - 150

127 - E

118 ___--

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 1 0 0 _ _ _ _ _ _
Percentite

Percentile of Rest

of Restaos

Cost ~~~~~~~~~~~~~~Reduction

Reduxtion

0 _ _ _ _ _ __0 _ __ _

0 Percentile of Initial Vatue Added 10 Percentite of Initial Vatun Added
C. 1980-1985 D. 1985-1990

FIGURE 1. PROFILES OF TFP GROWTH AMONG U.S. MANUFACTURING BRANCIHES
to 170 percent of the RCR for total manufacturing.
In 1975-1980 this figure was about 240
percent, in 1980-1985 only about half that, and
in 1985-1991 a little more than 125 percent.
The trouble is that when the aggregate TFP contribution
is relatively small, the cumulative total
of the positive contributions is a large multiple
of that aggregate, while when the aggregate is
large, this multiple tends to be smaller. Thus, for
1970-1975 and for 1975-1980, the total RCR
in manufacturing as a whole was only about 2.3
percent of initial manufacturing value added. In
contrast, the total RCR for all manufacturing was
almost 10 percent of initial manufacturing value
added in 1980-1985, and about 7.5 percent in
1985-1991.

The problem obviously becomes greatly compounded
if the real cost reduction for the aggregate
(in this case total manufacturing) turns out
to be negative. Special conventions would have
to be established to make clear the interpretation
of Loreinz-like diagrams in such cases.
I believe I have hit on a felicitous way of
solving all these problems, and at the same
time creating an even better, clearer visual representation
of the degree of concentration or

dispersion of real cost reduction among the
components of an aggregate. The idea is simply
to relabel the vertical axis of the Lorenzlike
diagram, making it represent an annual
growth rate. For simplicity, think of a 30-
degree line as representing 1 percent per annum
of TFP growth. The rest of the vertical
axis would be calibrated accordingly. Thus, by
looking at the slope of a simple chord, we
could visually assess how rapid was the TFP
growth of the aggregate in question.
Figure 2 is presented simply for didactic
purposes. Here we have a hypothetical industrial
branch made up of four industries, A, B,


### ---Economics-1998-0-09.txt---
C, and D. First, we order the industries in descending
order, according to their rates of TFP
increase in the period. Then we calculate cumulative
real cost reduction (a real dollar

amount) and plot it against cumulative initial
real value added. Then we scale the vertical
axis so as to comply with whatever metric we
have decided upon for the TFP growth rate (in
the example, a 30-degree line representing a
1-percent annual TFP growth rate), and the
horizontal axis so as to add up to 100 percent.
In the lower panel of Figure 2 1 give examples
to show how these diagrams cope with
the problems of a low TFP growth rate (the
overshoot for the case of 0.25-percent growth
would show up peaking at over 400 percent in


### ---Economics-1998-0-10.txt---
U.S. wnodacturinp 1948-53,32 Sectors 1J.S. Wnufactuip 1953-57, 32 SectorsC
Rase of 01 TFRo

TFP TFP

.0085 Growth Growth

CumoSu Gun Sum

Roat 0ea .007

Cost ___ _Cost

Roduction -0037 Reduction

O- j 2 A ,, .2 A O O -8 1

Percentile ot Value Added Percentle ot Value Added
US.Mantacturin195760 32 Sectors LU-.nn Cu.S.ManCacturnp196066 32Sectors ? Cons
S015 Rateot .015 Rate ot

TFP TFP

Growth Grwth

Cuns Sms .008 Cuon. S .005

Coot sos cost .0056

Reducttnot Reduction

________________________________ .015 .015
O .2 .4 .6 ;8 t 0 .2 .4 .6 .8

Percentile ot Vaoue Added Percent0le ot Value Added
U.S. Manutactuoly 1966690 32 Sectors GO. Monulac18t
C 9 . 33Cum. tn ' t Curn.

.01 5 Rosate .015S Raob at

TFP TFP

Growth G

Cuon Sua CunmSum .0073

Reotal. 05 Reoal ~ a - ~ 07

cost 009Cost

Reduction Reduction

o- ----- - 9006

O~ / T _ iS, -?.0014

._ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .015 .015
0 .2 .4 .6 .8 1 . .4 6 .8 1

Percentie ot Vatue Added Percentile o at Vou Added
U.S. MoWuacturod 1973-79 32 Sectors U.S. Manuactuorin 1979-85 32 Sectows
.015 Ra at .015 Rateot

TFP TFP

Growth Growth

Cum Sum Cum Sun

Reaot Root

Coot - .0051 Cost . 0052

Reduction -Il>_0Reduction = - _

0010 .- Rot to

ZZ ~ ~ ~ ~ E ~~~ ~~ 0093 i

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .015o s
i' -.Ot5 i g . -.~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~015
o .2 04 .6 .8 t

Percentile ot Value Added Percentile of Value Added
U0S. Manuaotausp 1948-85 32 Sectors
L 015 Rate oa

Growth

v _ ~~~~~~~~~~~~~~~~~.0063

Cum Suo --r-- .0055

Real

Reduction

FIGURE 3. TF GROWTHPROFILESFORUS.MANUFACTUR015
0 2 .4 .6 .8 t

Percentl,e of Value Added

FIGURE 3. TFP GROWTH PROFI1.ES FOR UJ.S. MANUFACTURING


### ---Economics-1998-0-11.txt---
a Lorenz-type diagram) and of negative TFP
growth (where it is hard to even conceptualize
a Lorenz-type picture).

I first presented these diagrams before a
large audience at the Western Economic Association
meetings in Seattle (July 1997), and
for that presentation coined the label of "sunrise
diagrams" on their analogy with the sun
rising over a hill. That same evening Yoram
Barzel suggested that where the aggregate
slope is negative, we apply the term "sunset
diagrams," which I immediately accepted.
Figure 3 presents a set of sunrise-sunset
diagrams based on Jorgenson et al. (1987 pp.
188-90). These cover 32 industrial sectors
(their 35 minus Agriculture, Trade, and
Government Enterprises). I think the utility
of sunrise-sunset diagrams needs no further
championing once these pictures are examined
and digested. Practically all variants are
represented in these real-world cases: low
TFP growth with a huge overshoot (1953-
1957 and 1969-1973); negative growth
with large and moderate overshoots (1966-
1969 and 1973-1979); moderate growth
with small (1979-1985), medium (1960-
1966), and large (1948-1953) overshoots.
One striking fact that emerges from this set
of pictures is how variable across periods is
the negative contribution of the losers. If the
losers had only contributed zero change in
TFP, we would have had cumulative TFP contributions
of about 0.8 percent per annum in

1948-1953, in 1957-1960, and in 1960-
1966. And the other periods would not have
been much different: about 0.7 percent in
1953-1957 and in 1969-1973, 0.6 percent in
1966-1969, and 0.5 percent in 1973-1979
and 1979-1985. Instead of this narrow range
of cumulative contributions, we have an actual
distribution that goes from -0.9 percent in
1973- 1979 through around 0.1 percent in
1953-1957 and 1969-1973 to over 0.5
percent in 1960-1966 and 1979-1985.
Does this not suggest that we make a major
research push trying to improve our understanding
of the phenomenon of negative TFP

growth? What syndromes characterize the firms
and industries experiencing it? How much of it
stems from external shocks like international
prices? How much of it from competition
within the industry? How much of it represents
firms struggling to survive, yet experiencing
output levels well below their previous peaks
(and presumably below installed capacity)?
How much of it represents things like "labor
hoarding" as firms go through periods of
adversity?

IL. Yeast versus Mushrooms: Part II
I hope that in the previous section I have
made a convincing case concerning: (a) the
usefulness of sunrise-sunset diagrams, (b) the
aptness of the "yeast versus mushrooms" dichotomy,
and (c) the pervasiveness with

which the mushroom side of that dichotomy
seems to come out ahead when the GDP is
broken down into industries or industrial
branches for TFP analysis. The grand design
that emerges from the studies reported here,
and from just about all the other industry
breakdowns that I recall having seen, is that:
(i) a small-to-modest fraction of industries
can account for 100 percent of aggregate real
cost reduction in a period; (ii) the complementary
fraction of industries contains winners
and losers, the TFP contributions of
which cancel each other; (iii) the losers are
a very important part of the picture most of
the time, and contribute greatly to the variations
we observe in aggregate TFP performance;
and (iv) there is little evidence of
persistence from period to period of the leaders
in TFP performance.

The above results are, I think, very interesting
(in the sense of piquing our curiosity)
, very strong (in terms of their implications
about the nature of the growth

process), and very robust (in the sense that
they have wide applicability over different
data sets analyzed by different authors using
at least somewhat different methods). But
these results, so far, are quite compatible
with what I might call an "industry view"
of the TFP story. This is the way I, myself,
looked at the growth process until quite
recently-a vision that was reflected in my
stories about rubber tires and autos in the
1920's, refrigerators and other household
appliances in the 1930's, pharmaceuticals in
the 1940's, etc. The image that I had in
mind was one of yeast within each industry
and mushrooms between industries-a

commonality of TFP experience by firms within'
an industry, depending on that industry's


### ---Economics-1998-0-12.txt---
.06

Cum. Sum Cum.

Real Rate of

Cost TFP

Reduction Growth

.03 -

0 ~~~~~~~~~~~~~~~~~0

-.03

. - --II ' I -I

0 .2 .4 .6 .8 1

Percentile of Initial Value Added

FIGURE 4. TFP GROWTH PROFILE IN MEXICAN MANUFACTURING SECTOR (1892 ESTABLISHMENTS, 1984- 1994)
luck in the technological draw, side by side
with highly diverse experience between industries
because the distribution of technical advances
had wide dispersion, even for periods
as long as a decade.

Getting access to data at the firm level permits
one to explore whether this view is compatible
with the actual experiences of firms and
industries. We are just in the early stages of this
exploration, but I think the result is quite clear
already; namely, the "mushrooms" story prevails
just as much among firms within an industry
as it does among industries within a
sector or broader aggregate. I will present here
only a taste of the evidence from the United
States (on which our systematic work just recently
got started). Our massive evidence

comes from the Mexican manufacturing sector,
for which Leonardo Torre ( 1997) has analyzed
data from a sample of over 2,000 firms. A small
fraction of these firms were lost owing to missing
data, but some 1,900 firms remained in the
sample that Torre finally worked with. These
firms were divided into 44 branches of industry,
so that on average we have about 43 firms per
branch.

There are really too many ways to present
such a mass of information as is contained in
Torre's study. What I will do here is give the
aggregate picture in Figure 4, and then show
in Figures 5A-C three fast-growing branches,
three of around median growth, and three from
among the slowest-growing branches.
To complement these figures, I finally present,
in Figures 6A-D, certain summary statistics
from the sunrise-sunset diagrams of the 44
branches that Torre studied. Here Figure 6A
gives the distribution of average rates of TFP
growth among the 44 industries. Figure 6B
shows the distribution of peak cumulative contributions,
i.e., what the TFP contribution

would have been had all the negatives been
zeros. Figure 6C displays the percentile of
firms (by initial value added) marking the borderline
between positive and negative TFP

growth. And finally, Figure 6D shows, for
branches with positive TFP growth, the percentile
of firms which, by themselves, account
for 100 percent of the industry's TFP growth.
This evidence almost seems to replicate, for
firms within an industry, what was found in
the previous section for industries within the


### ---Economics-1998-0-13.txt---
economy-rampant overshooting of sunrisesunset
diagrams, great influence of firms with
negative TFP growth in determining the TFP
outcome for an industry, and a small or moderate
fraction of firms accounting for 100 percent
of the TFP growth of an industry (when
that growth is positive), with the complementary
fraction being winners and losers whose
efforts end up just offsetting each other. It remains
to try to give some interpretation to
those results.

III. "Just Errors" or "It's a Jungle
Out There?"

The first question that will enter the mind
of many economists on looking at the evidence
presented so far is: how much of what


### ---Economics-1998-0-14.txt---
Mexican Manufacturing: Medicines [65 Establishments]
.06 -

Cum.

Rate .03 .-. .............0253........
of TFP ____________.0253

Growth

0 0

-.03

-.06 _ _ ._._,_L_t

64

Percentile of lnitiA ualue AddeA

Mexican Manufacturing: Paper [90 Establishments]
.06 -

Cum.

Rate .03 -.0157

of TFP ........

Growth _ 0

0

-.0076

-.03

-.06 -, - --,-t1r-- -1

- Percentile of lnitialalue Adcd

Mexican Manufacturing: Toiletries [39 Establishments]
.06-

Cum.

Rate .03 -

of TFP ... . _.0132

Growth

0 - 0

-.008

-.03

-.06 , _ ,_,_,__ _

0 Percentile of InitiaIalue Add 1

FIGURE 5B. TFP GROWTH PROFILES FOR MEDIUM-GROWING BRANCHES (MEXICAN MANUFACTURING, 1984-1994)
we have seen and emphasized might simply
be the result of errors of observations? This
is by no means a frivolous question. For one
can actually create frequency distributions
of rates of TFP increase which contain exactly
the same information as the sunrisesunset
diagrams previously presented. The

only trick is to count as the unit of frequency
not one firm (out of an industry aggregate)
or one industry (out of some larger aggregate)
but, instead, say, 1 percent of the total
value added of the aggregate. Thus a firm
with 20 percent of the value added of an industry
would appear with 10 times the

weight of a firm accounting for 2 percent of
the value added of that industry. In such a
chart, the cumulative frequency (say, 68 percent)
above ATFP = 0 would represent the

projection on the horizontal axis of the
maximum point on a sunrise diagram. Its


### ---Economics-1998-0-15.txt---
complement (32 percent) would represent
the initial value added associated with negative
TFP performance during the period.

If, then, all the information could be generated
by a properly designed frequency distribution
of rates of TFP growth, could it not
all be the result of chance alone-more specifically,
of errors of measurements? I really
think not-my favorite quip on this is that
"white noise does not sing a tune." That is, if
we can rationalize what we see in terms of an
analytical framework which embodies wellestablished
economic principles and sensible

presumptions about underlying relationships
and facts, this is itself strong evidence against
the white noise hypothesis.

Nonetheless, we have to face the fact that
errors of observation of some magnitude certainly
do exist, and we must recognize that
they can cloud our perceptions and bias our
results. What I am going to do here is consider
frequency distributions of firms. TFP is meaFrequency


### ---Economics-1998-0-16.txt---
12

10

8

6

2H

0

I ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ I I II

-6 -4.5 -3 -1.5 0 1.5 3 4.5 6

TFP Growth Rate

FIGURE 6A. AVERAGE ANNUAL TFP GROWTH RATE: MEXICAN MANUFACTURING 1984-1994
FREQUENCY DISTRIBUTION, 44 INDUSTRIAL BRANCHES
sured in two ways-one using value added by
a set of firms on the one hand, and the other
using "output" by those same firms, measured
through dividing value added by separate
estimated firm-by-firm price (of value
added) indexes pj. For these purposes we can
conveniently think in terms of logarithms, so
let:

V = observed value added of firm;

pj = estimated firm-level price index;
y, = v, - p, = estimated output;

Vj = Tj + ej [Tj =true value added];
p= = 7rj + Uj [7r, = true price index]; and
qj = T- 7r, [true output of firm].

We would like to have data on cj and its
variance

2 2 +

If we simply work with observed value added
as our quantity variable, we get

a?2 = a2 + a? (assuming at0 = ?)

If we worked with the measured yj, we get
2= ao + a? + a2 + u2- 2at

(assuming a and e to be strictly random).
My presumptions are as follows:

(i) We can estimate value added quite accurately
at the firm level. Hence the presumption
that ao is small.

(ii) In most industries, there is considerable
variety among the firms and their products.
Hence, except in cases of industries
with very homogeneous products, we

should not expect a2 to be small. Hence,
I expect a > a?

(iii) Finally, we have the presumption that, at
least at the level of firms within an industry,
o7 < 0. We know that firms

choose to operate in regions of the demand
curve where they consider the elasticity
facing them to be greater than one.
But also, in an analysis of the growth
process, one would expect the big gains
in value added to accrue to those firms
in an industry which were passing along
to consumers some of the fruits of current
or past real cost reductions.

These three presumptions lead me to the conclusion
that ao is likely to understate the true
variance of output ao (because - 2JT > 0 and
ao2 > sa?), and that ao is likely to overstate
0o2 (only the covariance terms with e and u,
which were assumed to be zero, could make it
otherwise). And since Torre worked with real


### ---Economics-1998-0-17.txt---
value added as his quantity variable, this suggests
that, if anything, the substitution of the
"true quantity variable" q for observed value
added v would have given results with
greater dispersion of TFP, and consequently
greater overshooting in his sunrise-sunset
diagrams.

The above demonstration should be taken as
merely suggestive. It is not important to me
that Torre' s results underestimate the variability
of the different firms' TFP experience. It is
only important that measurement error should
not be the principle determinant of those results.
On this I feel very confident. In my view,
it really is "a jungle out there," with winners
and losers in every period-good as well as
bad.

As I have noted earlier, we are only just beginning
a systematic study of TFP among U.S.
firms, so I can offer no display comparable to
Torre' s.

However, Robles (1997) did examine the
experience of 12 firms in the U.S. oil industry.
His results are summarized in Figure 7. But
Robles tells basically the same story as Torre.
Three firms out of the 12 were more than sufficient
to generate the real cost reduction experienced
by the total group. Half (or almost
half) of the firms had negative TFP growth in
each period. And the cumulated amount of this
negative TFP growth was sizeable when measured
against the total TFP performance of the
industry.

What I see in TFP performance is quite analogous
to what I see in the stock market pages of
the newspaper. There are winners and losers
eveiy day, every month, and every year. The
gains and losses come from all sorts of causes.
World price shocks can drive firms into negative
TFP performance if the consequent output reductions
are greater than the reductions of inputs.
So, too, can cyclical or secular declines in
demand, including those caused by the successful
actions of competitors.

When firms are under stress, they typically
fight to stay alive. Maybe they fight for too
long in some cases, in the sense that less of
society's resources would be wasted if they


### ---Economics-1998-0-18.txt---
12 -

10 -

8

C-)

CT

U-

4

2

0

0 12.5 25 37.5 50 62.5 75 87.5 iQO

Percentiles

FIGURE 6C. PERCENTAGE WITH POSITIVE TFP GROWTH: MEXICAN MANUFACTURING 1984-1994
FREQUENCY DISTRIBUTION OF PERCENTILES, 44 INDUSTRIAL BRANCHES
were to quit earlier in response to a challenge
that turns out to be deadly. But they do not
recognize the challenge as deadly, so they
keep struggling to survive. I believe this is part
of the nature of entrepreneurs, CEOs, and
business leaders in general. They would not be
where they are, doing what they are doing, if
they were ready to quit at the first sign of a
challenge. They are fighters by nature, and
they probably would not have achieved success
if they were not.

Firms with negative TFP growth may even
be innovators. New challenges come and different
firms think of different ways to respond
to them. Some (like Intel and Microsoft) end
up winners; others (Montgomery Ward and
Apple?) end up losing. But it may not be that
they just waited passively and tried to fight to
survive in the face of negative shocks. They
may have had quite innovative ideas, with decent
prior probabilities of success, but in the
end success did not come. Thus, negative TFP
performance can, and I believe often does,
come simply from "backing the wrong horse."
To me, Joseph A. Schlumpeter's vision
(1934) of "creative destruction" captures
much of the story. What he is saying is, yes,
it's a jungle out there, but the processes of that
jungle are at the core of the dynamics of a
market-oriented economy. They are what got
us to where we are, and they hold the best
promise for further progress in the future.2
In my opinion, Schumpeter saw through to
the essence of the problerrm, but it is not wise
for us to be fatalistic in accepting his vision.
We cannot lose by making a major effort to
understand the process of TFP improvement
where it happens-at the level of the firm.
This is all the more true because of the
2 The idea of creative destruction has come up in recent
literature, in a context of formal modeling as distinct from
this paper's focus on growth accounting and the intuitive
economic interpretation of its results (see Gene M.
Grossman and Elhanan Helpman, 1991, 1994; Philippe
Aghion and Peter Howitt, 1992). For an econornetric study
emphasizing the variability of performance among firns,
see Jacques Mairesse and Griliches (1993 pp. 200-04).


### ---Economics-1998-0-19.txt---
pervasiveness of negative as well as positive
TFP performance among the components of
almost any aggregate. By learning more about
this aspect of the aggregate picture, we may
stumble upon ways to "accentuate the positive
and eliminate the negative" parts of the TFP
story. But that is too quixotic a goal to take as
the point of focus right now. To me, the present
task is simply to get hold of the huge mass
of information that is available at the firm
level and squeeze it hard enough to wring out
as much understanding and as much insight as
we can.

IV. Some Observations on Methods

and Research

What I am about to say in this section is not
meant to consist of direct implications of what
has gone before. Instead, I think of the earlier
parts of this paper as building a case for a certain
vision of the economy, and of how the
forces of growth work within it. This vision in
turn leads one to think in different ways not
only about the growth process itself but about
how we, as economists, might best advance
our study and understanding of it, and how
policies might be molded so as better to promote
it.

(a) It is always wise to study the components
of growth separately. The rate of investment,
the rate of return on capital, the rate
of growth of the labor force in numbers or in
hours worked, the contribution of human capital
or of the increment in average quality of
labor, and the residual representing real cost
reduction-all these are sufficiently different,
and potentially sufficiently disjoint from each
other, to merit their being treated separately. I
would give special emphasis to the following
three points.

(i) The worthwhileness of measuring the
rate of return and emphasizing its role in
the growth process.

(ii) The importance of focusing on investment
rather than saving in studying the


### ---Economics-1998-0-20.txt---
-.15

Cum.

Cum. Sum - Rateof

Real Cost Grot

Reduction Growth

.08862

.05

0 .5 1

Percentile of Initial Value Added

A. 1970-1981

I.. . ...... ........................ ............... . ........................ ....... ......................... _ . ........
.08

Cum. Sum Cum.

Real Cost Rate of

Reduction TFP

Growth

.06

.04

... ... ... ... . ...... ...... . .................... . . ... .0241.................... .............48...........
0

0 .5 1

Percentile of Initial Value Added

B. 1982-1994

FiG,URE 7. TFP GROWTH PROFILES FOR U.S. OIL FIRMS


### ---Economics-1998-0-21.txt---
process of growth. Saving is an interesting
topic in its own right, but the more
''open economy" is the situation being
studied, the less saving has to do with
investment. Saving takes on great importance
in closed-economy models focused

on aggregate growth, in which

case it is equal to investment. It gets to
be almost meaningless as one focuses on
the growth of cities and regions, or on
firms and industries.

(iii) The importance of viewing the residual
as an umbrella covering real cost reductions
of all kinds, and of recognizing that
we are closer to home thinking that RCR
takes 1001 forms than that it can be well
represented by one or two or three

aggregate-style variables.

(b) In principle, the accumulation of human
capital by the laborforce should be represented
in the labor contribution of the

growth equation, or in a bifurcation of this
contribution into one due to raw labor, the
other to human capital. It is in a term like
iwiALi that one captures the shifting skill
composition of the labor force. In particular,
we capture here the higher wages that are the
fruits of investment in education and training,
which are the benefits that the workers themselves
perceive. These should be kept separate
from any externalities education might
have.

It is important to try to keep this internalized
part of the story out of the residual, so that we
can straightforwardly interpret the residual as
real cost reduction.

(c) To study externalities due to education,
training, or human capital, we should not be
content with broad generalizations such as
"TFP growth is higher in entities with lots of
human capital per worker." We should try to
figure out how this externality works. Is it
higher forfirms with high incidence of human
capital? Is it higher for industries or sectors?
Or are human capital externalities more spatial
in nature, making more efficient the economic
life of the cities, provinces, states, or nations
which have high concentrations of human capital?
And if this is a fruitful trail to pursue, at
what type and size of geographical units do
these externalities typically work?
(d) The same goes for economies of scale.
We should not be satisfied with vague attributions
of economies of scale, say, at the level
of the national economy. Instead, we should
pursue the matter. If the economies of scale
are national, through what channels do they
work, and what evidence do we have to look
at to see them in operation? In particular, what
is their connection to real cost reductions
where they really happen-i.e., at the level of
the firm? Economies of scale at the levels of
the firm and the industiy are easier to visualize.
Here, too, however, the task is to check
them out-to see if the real cost reductions of
firms are linked to the initial sizes of those
firrns themselves, or of the industries in which
they operate, and of the direction (up or down)
in which output is moving.

(e) Perhaps most important of all, we
should really try to take full advantage of evidence
at the firm level. I think particularly of
identifying considerable numbers of outstanding
cases of TFP improvements and TFP decline,
and studying them one by one to try to
ferret out the sources of their big real cost reductions
and real cost increases. You can be
pretty sure, if there have been big real cost
reductions in a firm, some people in that firm
have a pretty good idea of where those reductions
came from and how they were accomplished.
By capturing this grassroots evidence,
we can put some added discipline into our ruminations
about the nature of TFP at the aggregate
level. We must follow up on the sort
of work pioneered by Jacob Schmookler
(1966) and Edwin Mansfield (1995). In general,
our aggregate story should be compatible
with, and comfortably contain, what we see at
the grassroots level. In particular our overall
picture of TFP improvement should comfortably
accept the overwhelming evidence of the
"'mushroomss" rather than "yeast" nature of
the process.

(f ) Special urgency applies to the study of
declining total factor productivity at both the
firm and the industry levels. The pervasiveness
of declining TFP is perhaps the most profound
conclusion to emerge from the empirical links
that I have reported here. As a profession, we
obviously have been aware of its existence at
the industry level for virtually all studies that


### ---Economics-1998-0-22.txt---
give a breakdown by industry reveal it. Yet to
my knowledge, we have barely scratched the
surface in studying it. I find it hard to think of
more fertile soil for future research on the process
of economic growth.

(g) I do not think that we gain much by
trying to express the relation between policies
and economic growth by a series of regressions.
Cross-country growth regressions seem
hopelessly naive to longtime observers of the
growth process like myself. To us, there is too
mutch to question in regression lines that draw
much of their slope from the differences between
Sudan and Switzerland, between Bangladesh
and Brazil, or between Ceylon and

Canada. In contrast, it seems much more sensible
to look at episodes within individual
countries and to search for common elements
that characterize the passage from bad to good
growth experiences within each of the number
of countries, and for those elements that seem
to describe the good growth experiences on the
one hand and the bad ones on the other. I think
we can reach in this way a good appreciation
of the nature of the growth process, without
resorting to the straitjacket of regression lines
that seem to draw from comparisons among
very disparate countries, lessons that are supposed
to be meaningful for countries like
Bangladesh, Ceylon, and the Sudan-as well
as others at different levels-as each strives
to take the next upward step in the climb toward
modernization.

My view of cross-country growth regressions
is somewhat less negative to the extent
that they focus on the components of growth
(rate of investment, rate of return, and real
cost reduction in particular) rather than on the
overall growth rate. There is also a subtle distinction
to be drawn between two ways of

presenting cross-country regressions- (i) as
"explaining" why and how some countries
grow faster than others (not recommended),
and (ii) as simply summarizing a series of
"stylized facts" describing the experience we
observe (far preferable, and not just for its being
more modest in its claims).

V. Some Policy Implications

In approaching the question of the influence
of policy on real cost reduction in particular,
and, to a degree even on economic growth in
general, I believe that the key words are "obstructing"
and "enabling." We know from

sad experience how easy it is for governments
to adopt policies that get in the way of economic
growth and even turn it negative. We
know, too, that there is no "silver bullet," no
single magic key that by itself opens the door
to a paradise of prosperity and growth.
Broadly speaking, the easiest starting point for
a successful growth experience is a onceprosperous
economy that has suffered from

bad policies. Releasing that economy from its
trammels, correcting an accumulation of past
mistakes, can sometimes set in motion a prolonged
episode of astounding growth. A shift
from policies that obstruct to policies that enable
growth seems to lie at the heart of growth
"miracles" like those of Taiwan, Spain, Korea,
Brazil, Indonesia, Malaysia, and China
(among others).

The springboard for the following listing of
policy implications is the interpretation of the
growth residual as representing real cost reduction
and the ready acceptance that in the
real world RCR comes in 1001 different
forms.

(a) The first key observation is that people
must perceive real costs in order to reduce
them. Hence, policies that impede the accurate
perception of real costs are ipso facto inimical
to growth. Inflation is the most obvious, probably
the most pervasive, and almost certainly
the most noxious of such policies. If I have any
expertise based on experience in economics, it
has to be in the first-hand observation of processes
of serious inflation. So I ask you to
take my word for it: the most serious cost of
inflation is not a triangle or a trapezoid under
the demand curve for real cash balances, nor
is it the inflation tax. The most serious cost
of inflation is the blurring of economic
agents' perceptions of relative prices. This
happens because individual prices adjust in
different ways and at very different rates. A
high product price and a low input cost normally
is an invitation clamoring for new investments
to be made. This is not so during a
serious inflation, when such a signal can easily
turn out to be "here today, gone next
month" as both product and input prices continue
on their separate paths of adjustment to


### ---Economics-1998-0-23.txt---
the ongoing inflation. Without exception, in
my own observations, the higher the rate of
inflation, the worse is its effect in blurring
agents' perceptions of relative prices. In an
inflation at, say, 20 percent to 50 percent per
year, people see prices as in a morning haze;
in one of 20 percent to 50 percent per month,
they see them as in a London fog. Many empirical
studies exist showing that serious inflations
are seriously inimical to growth. (See
William Easterly, 1996.) The clouding of
perceptions of relative prices is an important
reason why-for it gets in the way of successful
real cost reductions at the level of the
individual firm.

Inflation also inhibits growth in other, perhaps
more obvious ways:

(i) by diverting energies from more productive
activities to the search for mechanisms
of inflation protection;

(ii) by reducing (often very drastically) people'
s real monetary balances, thus impacting
negatively on the real amount of

credit the banking system provides to the
productive sector; and

(iii) somewhat related to both (a) and (b),
by causing people (both "here"' and
abroad) to invest abroad some of the
funds they would otherwise have invested
"here," or (what is very close

to the same thing) by accumulating

hoards of hard currencies as an inflation
hedge.

(b) A second policy implication is, in the
words of my friend and longtime collaborator
Ernesto Fontaine, avoid "prices that lie"
(precios mentirosos). Talking about inflation,
we focus on the blurring of the signals
that the price system gives; here we focus on
its giving wrong signals due to distortions
that have been introduced, usually as a direct
consequence of government policies. No
good can or did come, in terms of economic
efficiency, from tariffs of 50 percent and 100
percent and more, giving effective protection
often of 200 percent and 300 percent
and more. Nor can growth be fostered by
heavy-handed price controls and interventions
in credit markets.

I am not being a religious purist here-just
as big distortions have big costs, small distortions
typically have small costs, and all economies
are distorted to some degree. The

message here is that economies have to pay
the price for the level of distortions they
choose to have, and that one of the important
components of that price is that distortions
create situations where what is truly a saving
of private costs is not a genuine saving of costs
from the point of view of the economy as a
whole.

(c) Just as bad, and often even worse than
direct distortions, are the excess costs imposed
on an economy by ill-conceived regulations
and bureaucratic hurdles. Hermando DeSoto
(1989) has made the exposure of these trammels
in Peru into what has become virtually
his life's work. Clear rules of the game are
an essential and integral part of a wellfunctioning
market economy, but all too easily

these get supplemented by others that make
investment, production, marketing, sales, new
product development, etc., more costly. Labor
laws have been particularly troublesome, often
adding artificially to the cost of labor and giving
firms a strong incentive to avoid hiring
new workers, simply because of the high costs
associated with any later dismissal of them.
But there are loads of other items-the need
for approvals, sometimes a dozen or more,
before undertaking some investment or some
new venture; regulations that one way or another
impede new entry, so as to protect strong
vested interests ( small retailers being protected
against supermarkets in many countries)
; and the complexity of tax codes and
their enforcement, which imposes large compliance
costs on business firms and individuals.
Somehow, countries interested in

promoting growth should find ways of paring
their regulatory frameworks down to those
rules and requirements that are really justifiable
in terms of their costs and benefits to the
economy and society at large.

(d) Although international trade distortions
(tariffs, quotas, licenses, prohibitions,
etc.) might be subsumed under points (b) and
(c), their importance merits a separate heading.
The move to openness (from a protectionism
that sometimes bordered on autarchy)
has been one of the main hallmarks of the
growth miracles of the past half-century [see


### ---Economics-1998-0-24.txt---
Sebastian Edwards (1993) and Anne 0.
Krueger (1985, 1997)]. Just as inflation has
costs beyond the area under the demand curve
for real cash balances, so protectionism seems
to have burdens that go beyond the standard
triangle-trapezoid-rectangle measures of the
costs of trade distortions. There are at least two
quite natural explanations: first, that openness
helps grease the wheels of the international
transfer of more modern technologies, and
second, that firms that may once have relaxed
in ease and comfort behind high protective
barriers end up having to sink or swim once
forced to compete in a much more openeconomy
setting. Under either explanation,

trade liberalization opens up new paths of real
cost reduction, thus providing additional impetus
to economic growth.

(e) The recent wave of privatizations
among both developed and developing economies
may have important effects in enabling
real cost reductions that otherwise might have
been delayed, or not have happened at all. It
is, I believe, fair to say that in most countries
state-owned enterprises operate under a series
of constraints that seriously get in the way of
real cost minimization in a comparative-static
sense and of real cost reduction in a dynamic
sense. These constraints sometimes limit the
salaries of executives, sometimes impose
onerous conditions on the firm as it employs
lower-skilled workers, often limit the capacity
of the firm to shut down inefficient lines of
production, and almost always make it difficult
to fire workers, etc. To my mind, however,
perhaps the worst attribute of state-owned enterprises
is the ethos that often evolves inside
of them-an ethos where middle managers
are well advised to "leave well enough
alone," "not rock the boat," and "not invite
trouble." This ethos flies in the face of a vision
of the growth process that gives a huge role to
the search for real cost reductions at the grassroots
level, and that recognizes the tumult that
accompanies "creative destruction" in all its
forms. I thus must applaud the contemporary
trend toward privatization. If I harbor any
qualms in this connection, they concern the
degree to which many privatizations have been
carried out in too much haste and with too little
care, often motivated by purely fiscal considerations
rather than by a general search for
economic efficiency. This nlay have led to gratuitous
transfers of wealth in some instances
and to the planting of newly private enterprises
in soil that was not properly prepared (e.g.,
still lacking a sound regulatory framework for
electricity rates, or intelligent rules promoting
competitiveness in at least some aspects of
telecommunications, etc.)

(f) One cannot complete a list like this
without mentioning something that most of us
simply take for granted-a sound legal and
institutional framework in which individuals
are protected against arbitrary incursions on
their property and other economic rights. This
very basic point-recently much emphasized
by Douglass C. North ( 1990), Robert J.
Barrow and Xavier Sala-i-Martin (1994),
Mancuir Olson, Jr. ( 1996), and Barro ( 1997 )
is at least potentially a vital element for a sustained
process of successful economic growth.
If it is true that spurts of growth have sometimes
occurred in the absence of such a framework,
it is also true that most cases of

sustained growth over long periods of time
have benefitted from a sound institutional and
legal environment.

(g) Somewhat related to the above is the
element of political consensus concerning the
broad outlines of economic policy. We have
learned from experience that very admirable
policy reforms can take place, yet end up having
little effect. This can happen because a
new government comes in and reverses the reform.
But it can also happen because people
fear that a new government will come in and
reverse the reforms later on. At the moment,
the Chilean economy is one of the jewels of
economic growth (and general economic success)
in Latin America. Many people point to
the thoroughness and pervasiveness of Chile's
economic reforms over the last two decades or
so. But not so many point to the fact that the
reform package has remained essentially intact
through several changes of ministers, and even
more i-mportant, through two presidential elections
in which the winners came from the opposite
side of the political fence from the
government that initiated the reforms. The
confidence in the economic order of things instilled
by this sequential endorsement of the
basic framework of economic policy has to be


### ---Economics-1998-0-25.txt---
one of the important reasons for Chile's continued,
very impressive economic performance.
And it is important, also, in the context
of this paper. Living in a world in which real
cost reductions are a key dynamic force producing
economic growth, we must look to the
motivations and preoccupations of those who
take the critical decisions at the level of the
firm. For these decisions, it is not only important
that the policy framework be good now;
the expectation that it will stay good in the
future is also important. Otherwise, investments
will tend to be limited to those with
short horizons and payment periods, and much
soil, fertile with longer-term economic opportunities,
will go unplowed.

VI. A Vision of the Growth Process

Let me now try to summarize my own vision
of the growth process-the major elements
of which have been presented here. In
the first place we have the five standard pillars
of growth-the rate of increase in the labor
force, the rate of increase in the stock of human
capital, the increase in the capital stock
(net investment as a fraction of value added),
the rate of return which that investment will
yield (or can be expected to yield) and, last
but not least, real cost reductions stemming
from 1001 different sources.

Commenting on these in turn, I would note
that increases in the labor force have taken on
new meaning in many countries as labor force
participation rates (particularly those of
women) have increased. Whereas with a constant
participation rate, the growth rate of the
labor force is just a proxy for the growth rate
of population, important increases in labor
force participation can lead, just by themselves,
to significant increases in measured
real income per head.

Concerning increases in the stock of human
capital, my conviction is that most of their
contribution to growth is, on the whole, well
measured by market wages, as in the expression
XiwiALi. This does not deny the existence
of externalities due to an increased
human capital stock; it simply judges their influence
on the growth rate to be modest in

comparison with the effects of education,
training, learning-by-doing, etc., that can be
(and typically are) internalized by those who
receive them. We therefore look for the effects
of human capital accumulation mainly in the
term YjwjALj, and only (via externalities) as
one of many elements underlying the growth
residual R'.

The rate of investment is a veteran on the
stage of growth analysis. What I would emphasize
here is the importance of maintaining
a clear separation between the rate of investment
and the rate of saving. Models (like
those of the representative consumer) in
which saving and investment are always
equal are not much use even for analysis at
the national level in our modern, interdependent
world. They are even less useful as
one goes down to smaller geographical
regions, and simply cease to make sense as
one studies the growth process at the levels
of the industry and the firm.

The rate of return to investment has in
many ways been the orphan of our growth
analysis, having been masked from view by
our typical representation of capital's contribution
to the growth rate as Sk (AK/K). Here
the rate of return (p + 6) is totally hidden
from view. I deeply urge that more of us get
into the habit of representing this same term
as (p + 6)(AKIy). I want to see more attention
paid to the rate of return because it plays
such a central role in the motivation of economic
agents, and also because changes in it
are such an important element in understanding
and explaining the growth residual, R'.
Table 3 helps explain why I feel this way.
This table is adapted from Harald Beyer's
(1996) work. He carried out an analysis of
the growth experiences of 32 countries ranging
from Sri Lanka to the United States on
the income scale, and from Iceland to Australia
on the geographic scale. In Table 3 we
present results for his ten countries with the
highest and for his ten countries with the lowest
GDP growth rates from 1971-1991. In the
second column the calculated average annual
rate of return is shown. In the third column
we have capital's contribution to the growth
rate [z (p + 6) (AKly) ], and in the final column
the estimated average annual rate of TFP
growth, all over the same time period.
Table 3 shows an unequivocal tendency for
fast-growing countnes to be experiencing high
rates of return as well as high capital contributions
and high rates of TFP improvement.


### ---Economics-1998-0-26.txt---
TABLE 3-GROWTH RATES, RATES OF RETURN, AND RATES OF TFP IMPROVEMENT
(SELECTED FROM A SAMPLE OF 32 COUNTRIES, 1971-1991)
Ten fastest- GDP Rate of Capital cont. TFP
growing countries growth rate return to growth rate growth rate
Taiwan 8.83 15.0 3.81 3.68

Korea 8.47 13.2 4.30 2.38

Thailand 7.65 12.5 3.68 2.96

Hong Kong 7.91 20.0 3.56 2.28

Ecuador 5.58 14.0 2.70 0.36

Cyprus 5.12 10.6 2.99 1.92

Zimbabwe 4.62 13.6 2.42 0.97

Colombia 4.43 11.3 1.99 0.74

Iceland 4.35 9.4 1.95 1.77

Ireland 4.12 6.7 1.70 0.36

Median 5.35 12.85 2.84 1.83

Mean 6.10 12.63 2.91 1.74

Ten slowest- GDP Rate of Capital cont. TFP
growing countries growth rate return to growth rate growth rate
Austria 2.87 5.1 1.13 1.29

France 2.80 6.1 1.21 (.99

Germany 2.60 6.3 0.97 1.29

Belgium 2.56 6.8 1.06 1.60

Netherlands 2.52 7.0 1.12 0.83

United States 2.52 9.1 1.20 0.23

South Africa 2.16 7.5 1.58 -0.97

Denmark 2.15 7.5 1.01 0.82

United Kingdom 2.12 9.6 0.95 0.22

Sweden 1.84 4.3 0.66 0.24

Median 2.52 6.90 1.09 0.825

Mean 2.41 6.93 1.09 0.661

Source: Beyer (1996), Tables 111.1.1 through 111.1.32; also Appendix I for rates of return.
This is all the more interesting because in the
calculation of TFP a higher level of the rate of
return operates to reduce the calculated TFP
(i.e., Ap is a positive component of R' and
should presumably be positively correlated
with it,3 but R' is found by subtracting p AK
from Ay; hence, in a sense, the level of p
should presumably be negatively correlated
with R'). What we are seeing here, in my
opinion, is a genuine syndrome in which all
sorts of good things go together. Strong real
3 The standard expression for the residual, R = pdy -
iwIdL - (p + 5)dK has a "dual" representation, which is:
R - Ldw + Kd(p + 6) - y-dp. T'his form simply says
that the fruits of real cost reduction have to go somewhere-
either to workers (Ldw) or to owners of capital
[Kd(p + 6)] or, in the form of lower prices to the activity'
s customers (-y7dp).


### ---Economics-1998-0-27.txt---
cost reductions and high rates of return create
attractive investment opportunities which,
when acted upon, bring about a high capital
contribution to growth. It should be no surprise
that under such circumstances the GDP growth
rate itself tends to be high. It should likewise
be no surprise that the opposite syndrome
with weak real cost reductions and low rates
of return producing fewer interesting investment
opportunities-should end up being associated
with a low capital contribution and a
low GDP growth rate.

Finally, we come to the residual R' itself.
To me, the biggest message here is to recognize
the multiplicity of sources from which it
can (and I believe does) come, and the additivity
that nevertheless is its attribute. I think
that the term real cost reduction very neatly
captures both these aspects in a way that renders
it preferable to terms like TFP improvement
and technical advance-preferable not
in the sense of a mechanical definition (for
which all three are equally good), but in the
sense of better conveying the underlying nature
of the process to one's listeners or readers.
The next step is to recognize that of the five
main pillars, at least three (the rate of investment,
the rate of return, and real cost reduction)
are key foci of decision-making

processes at the level of the firm. I cannot escape
the conclusion that the great bulk of the
action associated with the growth process
takes place at the level of the firm. Hence, I
feel we should focus much more study than
we have in the past on what happens at this
level. And when we are not working at the firm
level, we should pay a lot of attention to what
happens at lesser levels of disaggregation like
industries and industrial branches.
Key insights flow from taking this kind of
focus. Few economists are aware of the pervasiveness
with which sunrise-sunset diagrams

are characterized by overshooting, or of
the importance that firms or industries with
real cost increases (i.e., reductions in TFP)
play in determining the aggregate rates of real
cost reduction that we see in such diagrams.
Here we have only scratched the surface in
digesting the evidence. But I find impressive
the degree to which the data of Table 3 seem
to point to a growth syndrome in which high
rates of return, high rates of investment, high
rates of real cost reduction, and high rates of
output growth all go together. I see in this result
the likelihood that real cost reductions are
the big driving force, generating high rates of
return and calling forth high rates of investment
and high output growth. This interpretation
is compatible with many exercises that
I have perforned over the years in which I
have tried to contrast high-growth with lowgrowth
experiences. In such exercises, as in
Table 3, the difference in rates of real cost reduction
has typically been a major "source"
of the difference in growth rates.

Also impressive in the analysis of the
Jorgenson data is the degree to which the varying
experiences of U.S. manufacturing in different
decades derives from different degrees
of bad experience (real cost increases) rather
than different degrees of good experience (real
cost reduction). It is as if the creative part of
Schumpeter's "creative destruction" was
more steady (for these decades in U.S. manufacturing)
than the destructive part, whetting
our (or at least my) appetite to look deeper,
inquiring into why this was so.

The Mexican data at the firm level were
somewhat more recalcitrant than the Jorgenson
data by industrial branch, but they nonetheless
give us a clear picture of lots of winners and
lots of losers, with the losers being strongly
characterized by falling real value added and/
or by falling real rates of return.4


### ---Economics-1998-0-28.txt---
The role of policy in this vision of the
growth process is an "enabling" one. By creating
the circumstances where firms can

quickly and accurately predict opportunities
for real cost reduction and act on them, governments
can guide the economy toward an

enhanced contribution of RCR to growth. By
rationalizing and/or eliminating barriers and
controls, they may also lead to an increased
pace of investment and increased rates of return.
In this view, the connection between
good policy and growth is not mechanical, and
thus cannot easily be captured in regressiontype
analysis, but that does not stop it from
being of vital importance.

I want to give special weight to another role
of "growth-enabling" policy actions, which
has to do with how policies, the effects of
which might typically be considered to be
comparative static, can nonetheless turn out to
influence economic growth rates over extended
periods of time.

This story begins with a recognition that
most developing countries have typically used
production techniques that were "backward"
in relation to those used by the advanced economies.
One way to verify this is to imagine
integrally replicating, say, a U.S. factory in India,
and manning it there with Indian workers
equivalent in skill to their U.S. counterparts.
The combination of lower construction costs
and much lower operating costs (mainly
wages) would permit this hypothetical new Indian
firm to undercut the prices of both the
U.S. firm of which it was a copy and the typical
Indian firm currently active in the same
industry. This says that the typical Indian firm
is operating on an "inferior" production function.
If, as I believe, the difference in efficiency
between U.S. and developing-country
firms is typically large, there is much room for
quite rapid improvements in the developing
countries as they learn how to "adopt and
adapt" already-known techniques from the
advanced countries.

I would assume that the incentives for "convergence"
are always present, but that they

have typically run into barriers and trammels
of many kinds in the poor countries of the
world. Reducing the barrfers and loosening the
trammels permits the more rapid convergence
to techniques that are closer to the frontier of
knowledge.

The way I see the influence of policy in
growth, it is simply not true that implementing
enabling policies typically permits a quantum
jump from the old to the truly modem. It is
more accurate to describe it as speeding up
what will in any case be a very lengthy process.
Personally, I like the analogy to a hydraulic
system in which a large vessel with a
high water level and lots of water is connected
to a much smaller and narrower vessel with a
much lower level of water. Physical laws dictate
a tendency for the water levels to equalize
in the end. But this can take a very long time
if the tube connecting the two vessels is tiny,
or is clogged by extraneous matter.
The policies that we consider good for
growth have the attribute, in this analogy, of
removing the extraneous matter and/or enlarging
the connecting tube. But even with the best
modernizing policies, the tube remains small
enough so that it takes many decades for a
country to pass from poor to middle income or
from middle income to rich. If somehow the
hydraulic connection could be made so large
as to bring about an almost instantaneous full
adjustment of the water level, then we would
say that good policies mainly represent level
adjustments. But observing even the best of
real-world growth experiences, I think we
have to conclude that the adjustment is going
to be extended over a lengthy period in any
event, thus causing the big observable result
of better policies to be a higher growth rate
over an extended period rather than a discrete
jump to a totally different level.

This way of looking at the world also leads
to some observations on the current literature
dealing with convergence. I have long been
mystified by allusions to an ultimate convergence
of growth rates among countries, or an
ultimate convergence of levels of output per
head. To me, the natural convergence is product
by product, not country by country. And
ing the rate of return (p) imputed to new investments.
Arguments can be developed for using arbitrary but "sensible"
rates (like 10, 15, or 20 percent), or for applying
to investments by individual firms and/or industries rates
of return T that are obtained not from those specific firms
or industries but from broad sectoral (e.g., manufacturing)
or economywide averages. I am pursuing these avenues in
work that will be reported at a later time.


### ---Economics-1998-0-29.txt---
among products there may be some where
current techniques will never be further
improved. Those products will have no real
cost reductions or TFP improvements in the
future, while others will enjoy huge advances
of productivity. My guess is that unlucky
countries (Bhutan, Nepal, Mongolia?) may
always lag way behind the pack, while luckier
ones (Taiwan, Argentina, Brazil?) may one
day hope to be among the world's leaders. So
convergence comes through as a general tendency,
and quite surely a general possibility,
for the production techniques used in making
any given product to improve as enterprises
using "backward" techniques learn of better
ones, and even more important, learn about
how to put them into practice. I do not believe
that much more than this can be said of convergence
as a real-world force. Wage rates for
given types of labor tend toward a rough
equality across regions in a country because
of the ease of migration in response to perceived
wage differences. The forces at work
internationally are both weaker and more
complex, but the big message here is that
the improvement of technique in any one industry
does nothing to improve the wages of
labor in just that industry. After an improvement,
the industry may end up hiring more or
less labor, but will presumably choose the
amount of labor so as to bring marginal productivity
into correspondence with market

wages. So technological improvement has an
effect on wages via supply and demand in the
national labor market, not through any direct
link from technical improvement to wages
(for given skills, etc.) at the level of the industry
or the firm.

As my final point, let me return to the
thought that the justification for perfecting the
functioning of the market system does not lie
only in reducing the efficiency costs associated
with each period's operation of the economy.
Perfecting a country's economic policy does
not only cause it to move from a path at
around, say, 90 percent of its potential output
to another equal to 95 percent of potential,
with the time path of potential output being
somehow given in advance. That gain would
certainly be a worthwhile gain, and it would
amply justify a lot of hard work involved in
achieving it. But that gain is still fundamentally
comparative static in nature.

What I hope to have evoked in this paper is
a sense that the perfecting of economic processes
can also in nearly all cases be justified
as greasing the wheels of the constant search
for new avenues of real cost reduction. To the
extent that economic reforms do so, they become
vehicles for bringing an economy to a
point where, year after year, new, cheaper, and
better ways are found of doing things, not just
in so-called "production" but also in such
mundane areas as merchandising, sales, finance,
insurance, and many more.

Some years ago, in a book that I edited called
World Economic Growth (1984), I wrote an
essay called "Economic Policy and Economic
Growth," in which I listed "13 lessons" that
I thought followed from the papers presented
in the volume, recounting the growth experiences
of countries as disparate as Ghana and
Taiwan, or Japan and Sweden. These lessonsbasically
focused on thinking about policies in
terms of their economic costs and benefitscould
easily be read as a reprise of the old
comparative-static story. But they were not
meanit as such-it is quite relevant that they
appeared in an essay concerned with world economic
growth. The point was that these sensible
policies emerged as part of a consensus of serious
economists, each an expert in his particular
country's history, focusing attention on the
process of economic growth.

A few years later, and as the theme of a
different concept, John Williamson (1990)
coined the term, the "Washington consensus.
" Williamson listed ten points, covering
territory very similar to my 13 lessons. He also
produced a pithy summary that captures the
essential thrust of both his and my listings:
"Macroeconomic prudence, outward orientation,
and domestic liberalization." He, too,
and the members of the Washington professional
establishment whose apparent consensus
led to Williamson's list, were not just
thinking of comparative-static gains as they
reached their conclusions about policy. They
were thinking about ways to move economies
from slow growth, stagnation, and even in
some cases negative growth, to a healthy,
prosperous flowering of economic progress.
Similar views were more recently affirmed by
Stanley Fischer (1993).

To me, the dynamics of real cost reduction
are at the very least an important piece of what


### ---Economics-1998-0-30.txt---
people have in mind when they list efficiencyoriented
policies as essential ingredients of a
program promoting economic growth. It is
policies of this type that give the right signals
to the CEOs and the managers down the line,
that take away trammels that impede their
quest for real cost reductions, and that create
an environment in which Schumpeter's process
of "creative destruction" can work its
wonders.

APPENDIX ON METHODOLOGY

The vision of the growth process presented
in this paper leads one almost inevitably to
some methodological twists-twists which, if
they are not new, at least differ in significant
ways from what I take to be the most common
practice in breaking economic growth down
into components.

(a) To measure the real rate of return to
capital, one must express the numerator (real
dollars of retum) and the denomiinator (the
capital stock) in the same units. The most efficient
way to do this is to measure both output
(value added) and the capital stock in units of
the GDP deflator. That way one is sure that the
outputs of all the subaggregates in the economy
add up to the GDP, and one also satisfies
the requirement that the capital and return be
measured in the same units. This is also the
way cash flows would be deflated in a standard,
ex post project evaluation. When this is
done at the aggregate level the contribution of
capital to growth is (p + 5) AK where p is an
aggregate rate of return to capital, 6 the depreciation
rate (including obsolescence), and AK
the net increment to the capital stock in the
period in question. At a subaggregate level, the
contribution of capital to growth in activity j
is (pj + bj) AKj. At both levels, we find that
high rates of return are an important component
of most successful growth episodes.
(b) To capture the great diversity of the labor
factor, we would like to have a very fine
breakdown of labor into categories (indexed
by i). The labor contribution is then liwiALi,
where wi represents the real wage of category
i and ALi the change in hours worked by category
i. Since the number of relevant categories
of labor is huge, any such breakdown is
difficult, and gets more difficult as one disaggregates
from economy to sector to branch to
industry and to firm. This Gordian knot can be
cut by a simple assumption, similar to what is
done in most countries to convert residential
construction to real terms. There, one builds a
price index of a "standard house" p*, and
then obtains a quantum of construction C* by
dividing total construction outlays by the price
of the standard house. In the resulting aggregate,
each individual residence (i) gets attributed
a quantum of housing equal to Pi Ip */ In
this work I define a standard wage w*, which
I assign to "standard labor" or "raw labor."
The excess of anybody's actual wage over w *
is attributed to human capital. The returns to
natural ability, as well as to formal education,
training, and experience belong there, under
this interpretation. High returns due to a distorted
wage structure are not appropriately
attributable to human capital, but the methodology
would nonetheless be correct in attributing
to the affected labor a marginal

productivity that is measured by the distorted
high wage.

The "labor contribution" as measured by
w*AL* is equal to w* (Q S(wi /w *) AL1 +
iLi A(w /w*). The second term will be zero
if the structure of relative wages remains constant,
or even if the weighted average premium
does not change. Any changes in the weighted
average premium will cause the calculated residual
to be different from those calculated by
other methods.

The "two-deflator method" is characterized
by the use of a single numeraire-deflator (say,
the GDP deflator), by the treatment of the
quantum of output as value added divided by
the numeraire-deflator, and by the use of a
standard wage w * and a quantum of labor L *
equal to the wages bill divided by w *. This is
the method used by Beyer (1996), Robles
(1997), and Torre (1997) in their work reported
in this paper.

It goes without saying that the two-deflator
method is rough. But it is also tremendously
robust and easily applied. I think of it as being
really designed for use at the firm level, where
very commonly we can get data in value
added, on gross investment, and on the wages
bill, but know nothing (from standard sources)
about the quantum of output or about the number
of total hours worked (or even the total
number of employees used). This opening of
wide new vistas, of huge new data sets, is what


### ---Economics-1998-0-31.txt---
I consider the strongest argumnent for the twodeflator
approach.

But there are other pluses as well. First, at
the aggregate economy level, the two-deflator
approach comes very close to the traditional
approach. In rate-of-growth terms, we have
(R */y) = gy S- skgk * SL gL *, compared with
(Rly) =g - Skgk -SLgN, where g refers to
growth rate, and y to GDP. K* differs from K
in being built up from gross investment deflated
by the GDP deflator, while K is built up
from gross investment deflated by the investment
deflator of GDP. L * is in principle much
more refined than N (number of workers), but
its measurement can be influenced by a widening
or narrowing skill premium. (R */y)
likewise differs from Jorgenson's residual
(R'/y) = gy - Yjskjgkj - 1ise gf mainly in his
use of different capital deflators for different
categories of capital. (The implicit labor
breakdown in L * is much finer even than
Jorgenson's, but Jorgenson does not have to
contend with the possibilities of widening or
narrowing skill premiums -at least not
among the labor categories he works with,
which are typically broken down by gender,
age, education, occupation, industry, and employmnent
status.)

The bottom line is that when Beyer compares
his two-deflator results, at the national
aggregate level, with those of others using different
methods, he finds on the whole only
modest differences. [See Beyer (1996);
Harberger (1998).]

When one uses the two-deflator method at
the industry level, one often has the possibility
to adjust the quantity variable so as to make it
correspond with that of the more traditional
approach. Thus, we may start by using dy)e (=
pjdyj + yjdpj) as the quantity variable, and calculate
a residual Rj* using that concept. Then
we may get an adjusted residual by taking
R yjdpj. This is easy to do so long as we
have decent data on dpj, the relative price of
j, which we often do at the branch or industry
level. When Jorgenson's residuals are compared
with the two-deflator residuals, with and
without price adjustment, I find the differences
without adjustment to be small enough to be
quite acceptable. With adjustment, the degree
of agreement between the two approaches is
quite notable (85 percent of differences less
than one percentage point of per annium
growth). [See Robles ( 1997); Ilarberger
(1998).]

When one gets down to the firm level, the
two-deflator method is in its element. Rarely
do we have decent time series on the price index
of a firm' s output. Treating many firrns in
one industry, one might then give all of them
the same price index-that of the industry. At
that point the distribution of adjusted TFP residuals
among the firms would end up differing
from the original two-deflator distribution
only by a constant. When expressed in percentage
terms we vould have (R)' ly)) = g4i -
*kj j ge*, for each fim j without adjustment,
while with adjustment we would have
R]*Iy1 = the same expression minus go, the
rate of growth of the industry's relative ptice
index (the same for all firms).

So in the great bulk of cases one ends up
with something quite like the two-deflator
method when working at the individual firm
level. The consolation is that the residual
terms of individual firms, calculated for the
whole economy, add up to a residual term
for the aggregate-in the sense that outputs
sum to GDP, the L]* sum to L * for the economy,
the Kj* to K* for the economy, etc. For
more details on methodology, see Harberger
(1998).
 ## Economics-1999-0


### ---Economics-1999-0-01.txt---
In his Presidential Address five years ago,
Zvi Griliches ( 1994) called attention to the severe
difficulties that beset current attempts to
measure the growth of labor productivity in
the American economy. Because of these difficulties,
it is likely that the true rate of economic
growth is substantially underestimated.
The root of the problem is the difficulty in
measuring output in the service sector which
now represents two-thirds of the economy. In
such sectors as health care and information
services, the contribution to gross domestic
product (GDP) is measured by inputs rather
than outputs, a procedure that makes it impossible
to gauge accurately improvements in the
quality of output. Thus, in the case of computers,
which are transforming American society,
economists have been unable, so far, to
find a measurable contribution of computers to
the rise in labor productivity-an astonishing
paradox.

I want to follow up on this problem of mismeasurements.
My thesis is that the profession

is lagging behind the economy more than it
has to. We are, to some extent, entangled in
concepts of the economy and in analytical
techniques that were developed during the first
third or so of the century, when economics
emerged as a modem discipline. The range of
the discipline did not expand greatly during
the middle decades of the century, due partly
to a concentration on the reformulation of the
previous analytical concepts and techniques in
more sophisticated and more general mathematical
models. Although the dividends from
these efforts were high and have contributed
to the flexibility and capacity of economics,
they did not encourage a reconsideration of
some of the received assumptions about the
scope and focus of economic analysis. There
has been a significant broadening of the scope
of economics during recent decades, with the
emergence of such fields as the new household
economics, the new institutional economics,
the economics of aging, and medical economics,
but much remains to be done.

The balance of this address is divided into
four sections. I begin with the inadequate
attention to the accelerating rate of technological
change, the implications of this acceleration
for the restructuring of the economy,
and its transforming effect on human beings.
I then consider the neglect of the nonmarket
sector of the economy, the implication of that
neglect for the measurement of consumption,
and for the analysis of economic growth. The
third section deals with the need to shift the
focus of economic analysis from crosssectional
to life-cycle and intergenerational
data sets, especially in connection with forecasting.
The final section points to the impact
of cultural lag in the treatment of material inequality,
and the neglect of the more severe

problem of spiritual inequality. I use the word
spiritual not in its religious sense but as a reference
to commodities that lack material form.
Spiritual or immaterial commodities make up
most of consumption in the United States and
other rich countries today.


### ---Economics-1999-0-03.txt---
6000- Genome Project PCs

Man on Moon \ /

\ Man on MoonNuclear Energy

High-Speed Computers \ Discovery of DNA
5000- War on Malaria Penicillin

Invention of Airplane

4000- Discovery of New World Invention of Automobile
Population 01 Black Plague~

(millions) 0 Peak of Rome

3000- 0A Peak of Greece:

'V~~~~ & 10 ~~~~~~Invention of Telephone
\$w '/ Germ T eory

6% Os 1ttttt>,% s ' ', . BeginningofRailroads
1000- Invention of WattEngine

4; e$;tNtttH ' ~~~~~~~Beginning of Industrial Revolution
%V. ~~~~~Beginning ofRalod

YIL..YY Y _ 82nd Agricultural Revolution
0 --7 t 1. ...r TT T -- - - - _

-9000 -6000 -4000 -2000 0 2000

-5000 -3000 -1000 1000

Time (years)

FIGURE 1. THE GROWTH OF THE WORLD POPULATION AND SOME MAJOR EVENTS IN THE HISTORY OF TECHNOLOGY
Notes: There is usually a lag between the invention of a process or a machine and its general application to production.
"Beginning" means the earliest stage of this diffusion process.
Sources: Carl W. Bishop, 1936; T. K. Derry and T. I. William, 1960; Graham Clark, 1961; B. H. Slicher von Bath,
1963; Stuart Piggott, 1965; Glenn T. Trewartha, 1969; William McNeill, 1971; Jacob Bronowski, 1973; Carlo M.
Cipolla, 1974; B. M. Fagan, 1977. See also E. A. Wrigley, 1987; Robert C. Allen, 1992, 1994.
I. Some Implications of Technophysio Evolution
The rapid acceleration in technological
change is apparent in the decline in mortality
rates during the twentieth century. Study of the
causes of this decline point to the existence of
a synergism between technological and physiological
improvements that has produced a

form of human evolution that is biological but
not genetic, rapid, culturally transmitted, and
not necessarily stable. This process, which is
still ongoing in both rich and developing
countries, has been called "technophysio
evolution."

Unlike the genetic theory of evolution
through natural selection, which applies to the
whole history of life on earth, technophysio
evolution applies only to the last 300 years of
human history, and particularly to the last century.
Despite its limited scope, technophysio
evolution appears to be relevant to forecasting
likely trends over the next century or so in longevity,
the age of onset of chronic diseases,
body size, and the efficiency and durability of
vital organ systems. It also has a bearing on
such pressing issues of public policy as the
growth in population, in pension costs, and in
health-care costs (Fogel and Dora L. Costa,
1997).

Technophysio evolution implies that human
beings now have so great a degree of control
over their environment that they are set apart
not only from all other species, but also from
all previous generations of Homo sapiens.


### ---Economics-1999-0-04.txt---
This new degree of control has enabled Homo
sapiens to increase its average body size by
over 50 percent, to increase its average longevity
by more than 100 percent, and to improve
greatly the robustness and capacity of
vital organ systems.'

Figure 1 helps to point out how dramatic the
change in the control of environment after
1700 has been, and it highlights the astounding
acceleration of technological change over the
past two centuries. The advances in the technology
of food production after the Second
Agricultural. Revolution (which began about
1.700 A.D.) were far more dramatic than those
associated with the First Agricultural Revolution
since they permitted population to increase
at so high a rate that the line of

population appears to explode, rising almnost
vertically. The new technological breakthroughs
in manufacturing, transportation,

trade, communication, energy production,
leisure-time services, and medical services
were in many respects even. more striking than
those in agriculture. Figure 1. emphasizes that
prior to 1600, centuries elapsed between major
technological advances and the process of diffusion
was even more extended, continuing

over several millennia. Studies of the origin
and diffusion of the plow, for example, show
how little improvement there was in its design
between its original development in the Mesopotamian
valley around 4000 B.C. and its

diffusion across the Mediterranean Sea and
northward in Europe down to the beginning of
the second millennium (Bishop, 1936; E.
Cecil Curwen, 1953).

To my mind nothing better illustrates this
amazing acceleration in technological change
than the realization in the twentieth century of
humankind's ancient desire to fly. The first
successful motor-driven flight took place in
1903, but the fragile aircraft of Wilbur and
Orville Wright traveled only a few hundred
feet. Just 66 years later, an astronaut was
standing on the moon, talking to another astronaut
on earth, and hundreds of millions of


people around the world overheard and
watched that conversation.

Worldwide, the most important aspect of
technophysio evolution is the continuing conquest
of chronic malnutrition, which was virtually
universal three centuries ago.2 Table 1
shows that in rich countries today some 1,800
to 2,000 or more kilocalories (kcal) of energy
are available for work daily per equivalent
adult male, aged 20-39.3 At the beginning of
the eighteenth century, however, France produced
less than one-fifth of the current U.S.
amount of energy available for work. And England
was not much better off. Only the United
States provided potential energy for work
equal to or greater than late-twentieth-century
levels during the eighteenth and early nineteenth
centuries, although some of that energy


### ---Economics-1999-0-05.txt---
was wasted due to the prevalence of diarrhea
and other conditions that undermined the
body's capacity to utilize nutrients (Fogel and
Floud, 1999).

One implication of these estimates of caloric
availability is that mature adults of the eighteenth
and much of the nineteenth century

must have been very small by current standards
and less physically active. Today the average
American male in his early thirties is
about 177 cm (70 inches) tall and weighs
about 78 kg (172 pounds). Such a male requires
daily about 1,800 kcal for basal metabolism
and a total of 2,300 kcal for baseline
maintenance. If either the British or the French
had been that large during the eighteenth century,
virtually all of the energy produced by
their food supplies would have been required
for personal maintenance, with little available
to sustain work. To have the energy necessary
to produce the national products of these two
countries circa, 1700, the typical adult male
must have been quite short and very light
(Fogel, 1997).

Recent studies have established the predic
tive power of height and weight at early ages
with respect to onset of chronic diseases and
premature mortality at middle and late ages.
Variations in height and weight appear to be
associated with variations in the chemical
composition of the tissues that make uip vital
organs, in the quality of the electrical transmission
across membranes, and in the functioning
of the endocrine system and other vital
systems. Nutnitional status thus appears to be
a critical link connecting improvements in
technology to improvements in human physiology
(Fogel and Costa, 1997).

So far I have focused on the contribution of
technological change to physiol-ogical improvements.
The process has been synergistic,

however, with improvement in. nutrition and
physiology contributi-ng significantly to economic
growth and technological progress in a
manner described elsewhere (Fogel., 2000).
Here I merely want to point out the main conclusion.
Technophysio evolution appears to

account for about half of British economic
growth over the past two centuries. Much of
this gain was due to the improvement in human
thermodynamic efficiency. The rate of
converting human energy input into work output
appears to have increased by about 50 percent
since 1790 (cf., Partha Dasgupta, 1993).
Technophysio evolution calls into question
a number of easy and frequent assu-mptions in
economic analyses such as: tastes are fixed;
needs are fixed or exogenously determined;
existing life tables are adequate to forecast future
pension costs in. 2030 or 2075; and the
rate of aging is genetically controlled and unchanging
from one generation to another.

Technophysio evolution also calls into
question such frequently used theoretical assumptions
as fixed utility functions, fixed rates
of time preference over the life cycle, and the
related assumption that, except for risk differentials,
economic phenoomena should generally
be subject to the same rate of discount.
Yet the rate of discount varies over the life
cycle; children, after all, have much higher
time preferences than adults. It may also vary
by religion. Some individuals are prepared to
wait for their reward in heaven while others
want it here and now. Moreover, these differences
may be evolving and multiplying more
rapidly than is now presunmed, as is indicated
by the debates over the impact of greenhoiuse
gases. Rethinking of the issue is now underway
(see William D. Nordhaus, 1997; cf.,
Peter Koslowski, 1992; Nazli Choucri, 1.993;
Marc Fleurbaey and Philippe Michel, 1994;
Michael Toman, 1994) but the profession can
benefit from an increased allocation of resources
to this problem.

Technophysio evolution requires not just
marginal adjustments, but majtor leaps in economic
theory. We are slow in pondering such
grand questions as the inmplications of the Human
Genome Project, which is now nearing
completion, and the emergence of molecular
medicine for the future of economic life. We
have entered an era in which p-urposeful intervention
in evolutionary processes is passing
beyond plant and animal breeding. The new
growth economics needs to incorporate at least
some aspects of directed, rapid human evolution.
Endogenous technological change needs
to extend to the fundamentals of human behavior.
Theorists also need to grapple with the
ethical implications of technological changes
that, whatever their positive aspects, threaten
to undermine the mystery of human life by
transforming people into "material" that is


### ---Economics-1999-0-06.txt---
transplanted, cloned, arbitrarily altered in external
appearance, artificially changed in personality
and intelligence, and otherwise

manufactured in ways that challenge the definition
of a human being (cf., Zbigniew Brzezinski,
1996).

It. The Growth of the Nonmarket Sector'
One aspect of technophysio evolution has
been a change in the structure of consumption
and in the division of discretionary time between
work and leisure. Perhaps the best index
of the growth of the nonmarket sector is the
change in the use of time. Changes in hours of
work and in the average division of the day
have paralleled the changes in the structure of
consumption. Table 2 shows the remarkable
reduction in the workyear that has occurred for
males in the U.S. labor force over the past century.
Sleep, meals, and essential hygienie,
which are biologically determined, required
about 10 hours of the day in 1880, as they do
today. The remaining 14 hours represent "discretionary"
time.

The most notable feature of Table 2 is the
large increase in leisure available to the typical
male worker. His leisure time has tripled over
the past century, as his workyear declined
from about 3,100 hours to about 1,730 hours
today. Table 2 also forecasts the division of
the average day in 2040, indicating that by that
date more than half of the discretionary day
will be devoted to leisure activities. The forecast
is for a reduction of the workyear from
the current average of about 1,730 hours to just
1,400 hours, with the average workweek down
to 30 hours.

The pattern of chainge anmong women was
similar to that among men (cf., Claudia
Goldin, 1990; Stanley Lebergott, 1993). Th'e
workday of wotnen in 1.880 was somnewhat
longer, and in some respects may have bexen
more arduous, than that of men. In households
of working farmners, artisans, and manual laborers,
wives rose before their husbainds and
continued working until bedtimle at 10 or I1
P.M. That routine suggests a workday that


may have run about 15 niinutes longer than
that of males, implying an annual workyear of
perhaps 3,200 hours.

As a result of the mechanization of the
household, smaller families per household,
and the marketing of prepared foods, the typical
nofiemployed mnarried woman today

spends about 3.4 hours per day engaged in
housework; and if she is enmployed the figure
for housework drops to 2.2 hours. However,
women in the labor force average about 4.4
hours per day as emnployees. Hence comlbining
"4work" with " chores," men and women
work roughly equal amounts per day, and both
enjoy much miore leisure than they used to.
The principal difference is that the gains of
women have comrie exclusively from the reduction
in hours of housework, while the gains
of men have come from the reduction in the
hours of employed work (cf., John P.
Robinson and Geoffrey Godbey, 1997).
I have so far retained the common distinction
between work and leisure, although these
terms are already inaccurate and may soon be
obsolete. The distinction was invented when
most people were eingaged in manual labor for
60 or 70 hours per week and was intended to
conitrast with the highly regarded activities of
the English gentry or their American equivalent,
Thorsten Veblen's (l1899) "leisure

class." However, it should not be assumed
that members of the leisure class were indolent.
In their youth they were students and athletes.
In young adult years they were warriors.
In middle and later ages they were judges,


### ---Economics-1999-0-07.txt---
TABLE 3-ESTIMATED TREND IN THE LIFETIME
DISTRIBUTfION OF DiSCRETIONARY TIME
C. 1880 c. 1995 c. 2040

Lifetime discretionary

hours 225.900 298&500 321,900

Lifetinle earnwork

hours 182,100 122,400 75,900

Lifetime volwork

hours 43,800 176,100 246,000

bishops, merchant princes, and patrons of the
arts. Whatever they did was for the pleasure it
gave them since they were so rich that earning
money was not their concern.

Hence, leisure is not a synonym for indolence,
but a reference to desirable forms of effort
or work. As George Bernard Shaw (1928)
put it, "labor is doing what we must; leisure
is doing what we like; and rest is doing nothing
whilst our bodies and our minds are recovering
from their fatigue." In order to avoid confusion,
in the balance of this address I reserve
the word "work" for use in its physiological
sense, an activity that requires energy, over
and above the basal metabolic rate (BMR).
Activity aimed primarily at earning a living I
will call " earnwork. " Purely voluntary activity,
even if it incidentally carries some payment
with it, I will call "volwork "

Why have hours of earnwork declined so
much in recent years? The answer to that question
is suggested by the fact that it is not just
daily and weekly hours of earnwork that have
declined. The share of lifetime discretionary
hours spent in earnwork has declined even
more rapidly. Table 2 only dealt with the hours
of earnwork of persons in the labor force. It
did not reflect the fact that the average age of
entering the labor force is about five years later
today than it was in 1880, or that the average
period of retirement for those who live to age
50 is about 11 years longer today than it was
in 1880 (cf., Lee, 1996). A century ago only
one out of five males aged 65 and older were
retired. Today, six out of seven are retired.
All in all the lifetime discretionary hours
spent earning a living have declined by about
one-third over the past century (see Table 3)
despite the large increase in the total of lifetime
discretionary time. In 1880, four-fifths of
discretionary time was spent eamning a living.
Today, the lion's share (59 percent) is spent
doing what we like. Moreover, it appears probable
that by 2040 over three-quarters of discretionary
time will be spent doing what we

like, despite a fuLrther substantial increase in
discretionary time due to the continuing extension
of the life span (cf., Jesse H. Ausubel and
A. Griibler, 1995).

Why this deep desire for volwork? Why do
so many people want to forgo earnwork which
would allow them to buy more food, clothing,
housing, and other goods'? The answer tumrs
partly on the extraordinary technological
change of the past century, which has not only
greatly reduced the nunmber of hours of labor
the average individual needs to obtain his or
her -food supply, but has also made housing,
clothing, and a vast array of consurner durables
so cheap in real terms that the totality of
material consumption requires miuch fewer
hours of labor today than was required over a
lifetine for food alone in 1880.

Indeed, we have become so rich that we are
approaching saturation' in the consumption
not only of necessities, but of goods recently
thought to be luxuries, or which were only
dreams or science fiction during the first third
of the twentieth century. T'oday there is an average
of two cars per household in the United
States. Nearly every household with a person
competent to drive a car has one. On some
items such as radios, we seem to have reachied
supersaturation, since there is now more than
one radio per ear (5.6 per household). The
point is not merely that we are reaching saturation
in commodities that once defined a high
standard of living aiid quality of life but also
that the hours of labor required to obtain those
comnmodities have drastically declined. The
typical household in 1875 required 1,800
hours of labor in the marketplace to acquire
the annual food supply, but today it takes jus.t
260 hours. All in all, the comfrmodities that
used to account for over 80 percent of household
consumption can now be obtained in

greater abundance than previously, with less
"Saturation" in this context tneans that niost purchases
are for replacement rather than for new use.


### ---Economics-1999-0-08.txt---
than a third of either the market or the household
labor once required (Fogel, 2000).

Table 4 shows how sharply the U.S. distribution
of consumption has changed over the
past 120 years. Food, clothing, and shelter,
which accounted for about three-quarters of
consumption in 1875, accounted for just 12
percent in 1995. Leisure, on the other hand,
has risen from 18 percent of consumption to
67 percent.6 As Table 4 shows, the long-term
income elasticities of the demand for food and
clothing are below 0.5 and the elasticity of the
demand for shelter is closer to, but still below,
1.0. On the other hand, the long-term income
elasticities for leisure, education, and for medical
services are over 1.O.7

Table 4 differs from current official tabulations
of household consumption in two principal
respects. Official tabulations, with minor
exceptions, are limited to the out-of-pocket expenditures
of households. Table 4, however,

adds expenditures for education and health
care consumed by households but paid by government,
employers, and other third parties.
My procedure does little to change the distribution
of consumption in 1875 but significantly
increases the education and health-care
shares of consumption in 1995.8

Table 4 also differs from official tabulations
by adding to the value of out-of-pocket expenditures
on leisure, the value of the time devoted
to leisure (volwork). That time is priced
at the average hourly rate of compensation of
labor. The procedure increases the share of leisure
in consumption in both 1875 and 1995.
While leisure (volwork) remains a relatively
small share of expanded consumption in 1875,




it accounts for two-thirds of expanded consumption
in 1995.'

The failure to include the value of volwork
in the national income and product accounts is
perhaps the most glaring example of the cultural
lag to which I referred at the beginning
of this address. That omission also leads to a
significant underestimate of the long-term
growth rate of per capita income. Before adjustments
for the increased quality of volwork,
the growth rate is increased by eight-tenths of
a point (from 1.8 to 2.6 percent per annum).
But adjustments for improvements in the quality
of volwork might substantially increase
that figure, since leisure-time activities in 1875
were limited largely to church on Sundays and
carousing in bars during the rest of the week
(cf., Fogel, 2000). Despite some important
contributions to the development of an economics
of leisure (cf., Gary S. Becker, 1991;
Costa, 1998a, c; Daniel S. Hamermesh, 1998;
John Pencavel, 1998), the profession has far
to go before it catches up with the economy.
I1I. Shifting to Life-Cycle Data Sets for
Successive Cohorts and to Intergenerational
Data Sets

It is no secret that cross-sectional data sets
are cheaper to construct and more abundant


### ---Economics-1999-0-09.txt---
than longitudinal data sets. But whatever their
usefulness for other problems, cross-sectional
data are often highly misleading guides to
trends on such critical economic issues of the
new millennium as the prevalence rates of
chronic disabilities, expenditures on health
care, and pension costs.

Research initiated during the past decade
and a half has called into question previous
views on the length and fixity of the life span,
on the shape of the Gompertz curve (which
relates the log of the age-specific probability
of dying to age), on the theory of the epidemiological
transition, 10 and on the related

proposition that longer life expectancy implies
worse health among the survivors. It appears
that some of the earlier propositions were the
consequence of attempts to infer life-cycle behavior
from cross-sectional data sets. Such efforts
were thwarted by changes in the

sampling design of successive cross sections
and by changes in technology that led to earlier
diagnosis of preexisting conditions (Timothy
Waidmann et al., 1995).

The new research also accumulated evidence
on the outward movement of the survivorship
curve. Vaino Kannisto ( 1994) (cf.,
Kannisto, 1996) has shown that, in 14 countries
for which the data are adequate, mortality
over age 80 has been declining by about 1 percent
per annum for about half a century with
a significant acceleration in recent decades.
John R. Wilmoth (1995, 1997), who examined
extreme longevity in five countries,
concluded that the right-hand end of the survivorship
curve has been shifting outward for
two centuries. Moreover, there is additional
evidence that the Gompertz curve either levels
off or declines at old old ages (James W.
Vaupel, 1997; cf., S. Jay Olshansky and Bruce
A. Cames, 1997).

Another development is the continued accumulation
of evidence linking events early in
life, and reflected in height, weight, and body
mass index (BMI), to the onset of chronic con-
ditions. " A number of longitudinal studies that
were launched in the 1950's and 1960's have
recently been extended to cover the entire period
of human growth and early adulthood,
through follow-up studies. These have confirined
the persistence of central nervous systemr
defects induced by malnutrition in early
childhood (Nevin S. Scrimshaw, 1995). Numerous
other follow-up studies have shown

that malnutrition and smoking in adolescence
and in middle ages are risk factors for the early
onset of chronic conditions and premature
mortality, especially due to coronary heart disease,
non-insulin-dependent diabetes, and respiratory
diseases (Avita Must et al., 1992;

Vincent J. Carey et al., 1997; N. K. Chin et
al., 1997; Joan M. Dom et al., 1997; K. Kotani
et al., 1997; Dan S. Sharp et al., 1997; Ralf
Bender et al., 1998). These relationships have
been established in American, Asian, Australian,
European, and Latin American populations-
rich and poor. Economists have also
discovered a close link between later productivity
and height, BMI, and protein consumption
(after controlling for caloric intake)
(John Strauss and Duncan Thomas, 1995;
Thomas and Strauss, 1997).

There has also been an expansion of research
into the connection between intrauterine
and infant growth and the onset of chronic
diseases (or premature mortality). The strongest
evidence for such a link that has emerged
thus far is with respect to hypertension, coronary
heart disease (CHD), and non-insulindependent
diabetes. A review of 32 papers

dealing with the relationship between birth
weight and hypertension by Catherine M. Law
and Alistair W. Shiell (1996) showed a tendency
for middle-aged blood pressure to increase
as birth weight declined. Evidence of a
connection between birth size and later coronary
heart disease has been found in England,
Wales, Sweden, India, and Finland.12
"? This theory holds that the prevalence of chronic diseases
is unrelated to the prevalence of acute infectious
diseases but depends only on the natural process of senescence
that precedes mortality.

" BMI is measured by the ratio of weight in kilograms
to height in square meters (BMI = kg/m2).
i2 See Stephen Frankel et al., 1996; Law and Shiell,
1996; C. E. Stein et al., 1996; Swen-Olef Andersson et al.,
1997; D. J. Barker, 1997; Barker and C. N. Martyn, 1997;
J. L. Cresswell et al., 1997; T. Forsen et al., 1997; Jan A.
Henry et al., 1997; Ilona Koupilova et al., 1997; ScrimThe


### ---Economics-1999-0-10.txt---
accumulation of historical and current
biomedical studies on the trends in health, longevity,
and human physiology, combined with
controlled studies of animal populations, are
leading some evolutionary biologists to place
increasing emphasis on plasticity in human aging
(Michael R. Rose, 1991; Caleb E. Finch,
1997; Hillard Kaplan, 1997). The term "plasticity"
refers to the widely varying pattern of
aging within species and the ability of the
length of the life span to be affected by environmental
factors, some of which may operate

over several generations (cf., Kenneth W.
Wachter and Finch, 1997).

The search for better ways to forecast longterm
trends in health-care costs and in pension
costs has made it essential for economists to
understand the underlying physiological, nutritional,
and epidemiological factors that affect
the changing demand for health care and
retirement. Forecasts about health-care costs
are obviously related to assumptions about
changes in both life expectancy and agespecific
morbidity rates. If current morbidity
rates at older ages are presumed to remain constant
or to increase over the next 30 to 50
years, and if life expectancy is also presumed
to increase, then national health-care costs become
astronomic. However, if one takes account
of the recent declines in age-specific
disability rates, then the share of health-care
costs in GDP might remain constant (Kenneth
G. Manton et al., 1997b; Manton et al., 1998),
provided the demand for health services
(given disability rates) does not increase. If,
however, the income elasticity of the demand
for health is greater than one, then as income
increases, the share of income that is spent on
health care could increase even if disability
rates decline (see Fogel, 1997). It is quite possible
for the demand for medical interventions
to increase, even though age-specific morbidity
rates decline, for one or more of the following
reasons: shifts in the age structure of
the population toward ages with high morbidity
rates; improvements in inedical interventions
for a wide range of chronic conditions
which alleviate symptoms but do not cure; the
replacement of current disease-specific standards
of care by new ones based on improved technologies
that are much more expensive than

those replaced; the elimination of current age restrictions
on such expensive procedures as organ
replacements (cf., David M. Cutler and Ellen
Meara, 1998; Alan M. Garber et al., 1998).
The complexity of the underlying issues in
the economics and biodemography of aging
has led to a significant shift away from crosssectional
data sets and to a concentration on
longitudinal data sets. The earliest of the major
longitudinal studies sponsored by the National
Institutes of Health (NIH), the Framingham
Heart Study, began in 1950 and was focused
on the causes and treatment of coronary heart
disease rather than on the economics and biodemography
of aging. The Retirement History

Survey (RHS) followed a cohort of men and
women aged 58 to 63 from 1969 to 1979 and
then was suspended. Although not initially
planned as a longitudinal study, it became the
mainstay of retirement research during the
1980's. Linked to the social security file on
covered earnings, it provided important insights
into the labor-force and retirement behavior
of older workers.

In 1990, the National Institute of Aging
launched a new Health and Retirement Survey
(HRS) focused on the cohort born between
1931 and 1941, and aimed at collecting a much
wider set of variables than covered by the
older RHS. The new HRS collected numerous
measures of health (including cognitive ability)
, taxable earnings, job characteristics,
hours of work, job turnover, family characteristics,
pensions, health insurance, and ownership
of an array of assets including real estate,


### ---Economics-1999-0-11.txt---
consumer durables, and financial securities.
The most ambitious social science project ever
undertaken, the first wave of HRS cost
$14,000,000 and the second wave was budgeted
for $17,000,000. A third wave is currently
under way collecting information on the birth
cohorts of 1942-1947 (F. Thomas Juster and
Richard Suzman, 1995).

NIA launched a second longitudinal survey
aimed at cohorts born in 1924 or earlier years.
This sample, which is called Asset and Health
Dynamics. Among the Oldest Old (AHEAD),
consists of persons aged 70 years and older in
1994, when the first wave was undertaken. Although
the questions in AHEAD overlap with
HRS, functional ability is investigated intensively
and labor-market activity is covered
lightly.

Both HRS and AHEAD have made it possible
to examne a wide range of issues with
more reliable and more detailed evidence than
previously available. James P. Smith and
Raynard Kington (1997), for example, have
used AHEAD to examine disparities in functional
status and to relate them to socioeconomic
status (SES variables). They

discovered strong feedback effects not only
from health to SES variables but from SES
variables to health. To disentangle these effects
they broke household income and wealth
into various categories made possible by the
data and examined the effects by age. They
were also able to make use of a range of variables
bearing on the health and socioeconomnic
status of relatives in three generations (parents,
siblings, and children). Their analysis indicated
that cuffent-period health and income
are attributes of past, concurent, and future
generations. Measuring the full extent of these
influences, the direction of causation, and
complex interactions requires the study of lifecycle
histories from birth to very old age. It
would be inappropriate, they concluded, to use
SES variables to explain variations in late-age
health without taking account of the feedback
mechanisms or identifying within-period innovations
in the stock of health (cf. John

Bound et al., 1998).

Such findings point to the usefulness of creating
a prospective life-cycle sanple for an extinct
cohort (the veteratns of the Union Army)
by utilizing military, pension, and census records
in archives. This procedure not only creates a
longitudinal data base in a small fraction of the
time required to trace a living cohor, but cm be
done at less than a tenth of the normnal cost. Such
a project was launched by NIA in 1996 (cf,
Clayne L. Pope and Larry T. Wimmer, 1998).
Based on 11 different data sets, the fully
linked life histories contain over 10,000 variables
on each recruit, including socioeconomic,
ecological, and health variables. Called "Early
Idicators of Later Work Levels, Disease, d
Death," this project provides a conmprehensive
data set on the life course of over 39,000 Union
Amy veterans. Bor mainly between 1835 and
1845, these men represented the first cohor to
reach age 65 in the twentieth cent , d can
be compared with the veterans of World Wa IL1.
The preliminary comparisons revealed that at
the same ages the prevalence of chronic diseases
was much higher among elderly Uniion Any
veterans tha among veteras of World War IL
Musculoskeletal and respiratoy diseases were
16 times as prevalent, heart diseases were 2.9
times as prevyent, and digestive diseases were
4.7 times as prevalent among elderly veterans in
1910 than in the niid-1980's (cf., Sven E.
Wilson and Louis L. Nguyen, 1998). Moreover,
young adults bom during the second quaer of
the nineteenth centr who survived the deadly
contagious diseases of childhood and early adolescence
were not freer of degenerative diseases
than persons of the samne ages today, as
propounded by the theory of epidemiological
transition, but were more afflicted. Heria rates
at ages 35-39, for exanmple, were more than
three times as prevalent in the 1860's as in the
1980's.

Although the analysis of the data in the Un
ion Army sample is still in progress, some of
the initial findings have a bearing on forecasts
of long-term trends in health status. Costa
( 1998b) has reported that early-age socioeconomiic
and biomedical stress had a substantial
impact on the likelihood that Union Army vet
erans would have disabling chronic health
conditions by age 60. Thus, veterans raised in
a county with high mortality rates were, half a
century later, at elevated risks of suffering
from disabling respiratory disease, circulatory
disease, and musculoskeletal problems (cf.,
Lee, 1997). Episodes of acute diseases experienced
as youtng adults, such as respiratory


### ---Economics-1999-0-12.txt---
infections, work-related injuries, and extended
bouts of diarrhea, also increased the odds of
suffering from chronic disabilities by age 60.
Costa concludes that about 15 percent of the
decline in the prevalence of joint problems and
75 percent of the prevalence of back problems
between 1910 and 1980 was due to shifts in
the occupational structure from manual to
white-collar jobs. Moreover, a comparison of
rates of decline in disabilities before and after
1980 indicates that disabilities are declining at
an accelerating rate (cf., Manton et al., 1997a;
Cutler and Elizabeth Richardson, 1998).
The mounting evidence of substantial interactions
over the life cycle that influence the
process of aging, the acceleration of technological
change which has profoundly affected
the context in which aging occurs, and the increasing
evidence that environmental influences
on the aging process begin in utero, has
led to the initiation of a new life-cycle project
called "Fetal, Infant, and Later Aging Markers,
Cohort b. 1910-35." The acronym for
this project is FILAM.

The central objective of FILAM is the creation
of a life-cycle sample of persons born
between 1910 and 1935 that would make it
possible to compare changes in the aging process
over the period of 70 years that separate
this new sample from the aging experiences of
the Union Army cohort. The FILAM cohort is
important not only because it studies individuals
currently between ages 63 and 89, but also
because of the dramatic environmental and
early life-style changes they experienced.
These changes include the rise and partial decline
of smoking, the decline and the new rise
of alcohol consumption, the replacement of
horses by internal combustion engines as the
main source of urban vehicular power, the
cleaning up of the water and milk supplies, and
the emergence of a wide range of effective
medical interventions.

The sample will be drawn from the birth
records of 10,000 men and women of differing
ethnicities and races who were born in Boston,
New York City, Baltimore, Chicago, Iowa
City, and San Francisco. Approximately half
of the neonates will not have survived to the
present day and these men and women will be
linked to their death certificates. The survivors
will be traced and interviewed to determine the
presence of chronic conditions, socioeconomic
status, and family and own health history.
Survivors will be linked to social

security, census, military, and tax records, subject
to their written consent.

FILAM will make it possible to investigate
the predictive power of fetal, neonatal, and
early childhood measures of retarded growth
such as weight for gestational age, ratio of placental
weight to birth weight, infant weight
gain, thinness at birth, arid shortness at birth
relative to head size, on the risk of developing
specific chronic conditions at mid-adult and
late ages. Among the chronic conditions that
will be examined are coronary heart disease,
hypertension, stroke, obstructive lung disease,
non-insulin-dependent diabetes, and autoimmune
thyroiditis. FILAM will also make it
possible to investigate how the interaction of
fetal and infant developments with various risk
factors at later childhood, young adult, and
middle ages may intensify or moderate risks
of chronic conditions and early death after age
65. Another objective is the analysis of differences
in the health histories, occupational histories,
and retirement pattern between FILAM
and the Union Army sanmple, with special emphasis
on the similarities and differences of
predictors of later work levels, morbidity, and
waiting time until death.

The HRS, AHEAD, Union Army sample,

and FILAM all contain information on the
parents and children of the individuals under
study. Hence, they make it possible to address
some factors that may be transmitted
intergenerationally. An effort to utilize archival
data to study intergenerational processes
is also under way. Pope (1992) is

linking a large sample of genealogies to the
life-cycle sample of Union Army recruits.
There are at least 60,000 published family
histories that contain information on over
100,000,000 people who ever lived in North
America. Pope is drawing a subsample of
these histories containing 10,000 men of
military age at the time of the Civil War. It
is estimated that 40 percent of these men
served in the Union Army. This new sample
(called ILAS, for intergenerationally linked
aging sample) will be linked to the military,
pension, medical, and census data sets previously
discussed.


### ---Economics-1999-0-13.txt---
WlAS will make it possible to control for the
effect of wartime stress by comparing subsequent
morbidity and mortality among those who
served in the Union Arny with relatives who
did not. It will also be possible to measure family
effects on aging and mortality experience. This
will be done by including parents' occupation,
wealth, residential history, number of children,
place or region of birth, and migration histoiy.
Brothers in ILAS share a common prewar environment
as well as a common genetic heritage.
They may also share a common war experience
(see D. S. Lauderdale and P. J. Rathouz, 1998).
This common genetic heritage and environment
is not fully captured by the aforementioned intergenerational
variables because heritability is

composed of the many different dimensions included
in genetics and environment. However,
conmmon family effects may be measured by using
independent variables to "sweep out" the
effects of observed variables on death age. The
covariance of the errors in that regression with
brothers' estimated death ages is then a measure
of the common family effect on death age. As
an alternative to the residual-covariance approach,
a kiindred-frallty model (Vaupel, 1990)
could be used where brothers share a level of
frailty. Then, with assumptions about the structure
of the frailty distribution, the parameters of
a hazard function and a frailty distribution could
be estimated (cf., James J. Heckman and
Christopher R. Taber, 1994).

IV. The Production and Distribution
of Spiritual Assets

There is, finally, the issue of spiritual or immaterial
assets. A good place to begin is with
Socrates' question: What is the good life? That
was a critical question not only for the sons of
rich Athenians but for sons of the landed rich
throughout history. Freed of the need to work
in order to satisfy their material needs, they
sought self-realization in public service, military
adventures, philanthropy, the arts, theology,
ethics, and moral philosophy. Their
preoccupation with immaterial commodities
led Adam Smith to argue that the landed aristocracy
ignored their property and lacked interest
in advancing methods of cultivation.
"The situation of such a person," he wrote,
"naturally disposes him to attend rather to ornamenit
which please his fancy, than to profit
for which he has so little occasion" (Smith,
1937 pp. 364, 891-892).

In a world in which all but a small percenitage
are lacking in adequate nutrition and other
necessities of life, self-realization may indeed
seem like a mere ornanent, but not in a country
where even the poor are rich by past or
Third World standards. That is the case in
America today since the poverty line is at a
level of real income that was attained by only
those in the top 10 percent of the income distribution
a century ago'3 (Fogel, 2000). Technophysio
evolution has made it possible to

extend the quest for self-realization from a
minute fraction of the population to almost the
whole of it. Although those who are retired
will have more time to pursue self-realization,
even those still in the labor force will have
sufficient leisure to seek it either within their
professional occupations or outside of them
(Peter Laslett, 1991; Hans Lenk, 1994),
Some proponents of egalitarianism insist
on characterizing the material level of the
poor today as being harsh. They confound
current and past conditions of living. Failure
to recognize the enormous material gains
over the last century, even for the poor, impedes
rather than advances the struggle

against chronic poverty in rich nations, the
principal characteristic of which is spiritual
estrangement from the mainstream society.
Although material assistance is an important
element in the struggle to overcome spiritual
estrangement, such assistance will not be
properly targeted if one assumes that iinprovement
in material conditions naturally

leads to spiritual improvement.

That proposition, so widely emnbraced by the
nore secular of the economic reformers of the
twentieth century, did more to promote the
consumerism of the 1920's and 1930's than to
produce spiritual regeneration. The middle and
working classes became preoccupied with the
acquisition of automobiles and those household
appliances made possible by electricity:
" This calculation neglects a variety of goods and services,
such as heart bypass operations, that are available
today through Medicaid, but which were unavailable at
any income a century ago.


### ---Economics-1999-0-14.txt---
irons, lamps, telephones, toasters, refrigerators,
radios, and washing machines. It was this
consumerism that led such progressive critics
of the era as Vernon Louis Parrington, pioneer
in the development of intellectual history, to
decry the "cash-register" mentality of modern
urban life (Parrington, 1930 p. 81).
The economist's traditional measures of income
inequality are inadequate measures of
both egalitarian gains and egalitarian failures
(cf., Amartya K. Sen, 1996). They focus on a
variable- money income-that currently accounts
for less than half of real consumption
and which in a generation may slip to just a
quarter of real consumption. The most serious
threats to egalitarian progress-certainly the
most intractable forms of poverty-are related
to the unequal distribution of spiritual (immaterial)
resources (cf., William Julius

Wilson, 1996).

Realization of the potential of an individual
is not something that can be legislated by the
state, nor can it be provided to the weak by the
strong. It is something that has to develop
within each individual. Moreover, which aspect
of one's potential an individual chooses
to develop most fully, such as choosing a profession,
is purely an aesthetic consideration.
John Dewey and one of his chief disciples,
Richard Rorty of the University of Virginia,
contend that in a democracy self-realization is
"4a particularized creative project of individual
growth" (Richard Shusterman, 1994 pp. 396-
97). The emphasis on individual choice does
not mean that other individuals and institutions
play no role in shaping those choices. Quite
the contrary, the quality of the choices and the
range of opportunity depends critically on how
well endowed an individual is with spiritual
resources. But the spiritual resources needed
for personal development are unequally distributed
among young and old, among men

and women, among various ethnic groups, and
among rich and poor. Those rich who are continuously
preoccupied with sensual gratifications
are as likely to fail in self-realization as
the poor who share that preoccupation. Although
the rich may have the wealth to buy
the treatment needed from highly trained professionals,
inexpensive character and religious
counseling may serve as well, if not better, for
the addicts of all classes.

The full list of maldistributed spiritual resources
is too long to discuss adequately here
but I have in mind such vital assets as a vision
of opportunity and a work ethic. A common
characteristic of such assets is that they are
transferred from one individual to another
mainly very early in the life of the recipient.
Self-esteem and a sense of family solidarity
begin to be transferred to children along with
mother's milk and with pabulum. Other spiritual
resources begin to be transferred during
the toddler and toilet-training stages, including
a sense of discipline, a capacity to resist or
control impulses, and a sense of community.
Telling nursery rhymes such as "TThis little
piggy went to market," recounting the autobiographies
of the mother and father, and family
histories going back two or three

generations convey such spiritual resources as
a work ethic, a sense of the mainstream of
work and life, an ethic of benevolence, a vision
of opportunity, and a thirst for knowledge.
Although these early transfers of spiritual
resources are enriched and expanded by primary,
secondary, and college education, and
by occupational and other later-life experiences,
the salience of these later transfers depends
in no small measure on what happens at
home before formnal education begins. It is,
therefore, necessary to remedy the maldistribution
of spiritual resources early in life, because
the most spiritually deprived infants will
often be born to single, teenaged mothers who
are themselves spiritually deprived.
Some young mothers are too deprived, or
too young, to call on their own life experiences
to transmit a sense of discipline and of opportunity,
a work ethic, a family ethic, a sense of
self-esteem, and a knowledge of the mainstream
of work and life. The deprivation can
be addressed by promoting a system of mentoring,
taking advantage of the increasingly
large number of retired men and women who
have abundant -spiritual resources. Such mentoring
programs would be useful, not only for
the toddlers and their mothers and fathers, but
also for the elderly who are looking for ways
to enrich their retirement years.

Despite the improvements in their material
conditions of life, including comfortable
stocks of consumer durables, the elderly today
suffer from a maldistribution of immaterial


### ---Economics-1999-0-15.txt---
resources that traces back to the conditions of
their youth. Persons aged 80 today were borm
in 1918 or 1919. Only 43 percent of that cohort
graduated from high school and less than 15
percent entered college. Even among the
youngest cohort of the elderly, those born in
1933 and 1934, only half graduated from high
school and about 20 percent entered college
(U.S. Bureau of the Census, 1975 p. 379).
These cohorts also suffered from high infant
death rates, poor nutrition in early infancy, and
early onset of chronic diseases, as compared
with cohorts born since World War II.
Hence, despite their relatively high levels of
income and stocks of consumer durables, the
maldistribution of spiritual resources is substantial.
Depression, alienation, and substance
abuse are common (S. C. Samuels, 1997).
Those who are most afflicted are lonely, have
few communal contacts, live in retirement
homes rather than in their own households,
and sense a loss of control over their personal
lives (W. L. Fletcher and R. 0. Hansson,
1991; K. Pahkala et al. 1992; K. Yamashita
et al., 1993). Recent studies also indicate that
those who lacked immaterial resources early
in life have difficulty in attaining selfrealization
after retirement (J. C. HIenretta,

1997; J. E. Mutchler et al., 1997).
I have emphasized the level of education because
recent studies indicate that the capacity
of the elderly to engage effectively in physical
activity was strongly correlated with education
early in life. Education also affects cognitive
ability and the rate of illness (J. W. Rowe and
R. L. Kahn, 1997). Consequently, individuals
who were deprived of adequate education in
youth are, for that reason among others, relatively
deprived of both physiological and spiritual
resources in late life.

Despite the long reach of youthful deprivation,
there are enough other factors affecting
the quality of elderly life to pennit redistributions
that compensate for previous deficits.
On the physiological side, for example, there
are effective medical interventions that can increase
the quality of life and longevity. Although
the elderly are eligible for Medicare to
pay for treatment, the quality of treatment is
variable, and many individuals may be
shunted to low-quality care. Moreover, some
interventions are denied, or are more reluc-
tantly ordered, for the elderly than for the middle
aged.

Because spiritual resources are so unequally
distributed among the elderly, different
programs are needed for different

strata. The mninority of the current elderly
who are well educated, the 14 percent with
at least bachelor's degrees, most of whom
had professional careers, have developed
some innovative programs (U.S. Bureau of
the Census, 1997 p. 160). In Great Britain
one of these is called the "University of the
Third Age." This educational program is not
aimed at providing credentials for those
about to embark upon new careers, but at
satisfying the thirst for knowledge. It is
based on the proposition that education, and
the acquired knowledge and skills, are a
source of self-satisfaction, even if they do
not enhance an individual's employability.
As Laslett (1991 pp. 171-71) put it:
Reading in a literature, mastering a language,
unraveling a point in logic or philosophy,
understanding the objectives

set for themselves by poets, painters,
novelists or architects, these things extend
your appreciation and your miastery
of your world, your objective and your
subjective world as well. They are fulfilling,
and adding to other people's

knowledge is the most fulfilling of all.
Programs su'h as the University of the
Third Age will become increasingly important
as the baby boomers and others of the more
highly educated cohorts of the post-World
War :I era begin to retire. However, today and
for the next decade, the bulk of the elderly
lacks the skills to create and participate in such
high-level programs as the University of the
Third Age. A recent survey of adult literacy
revealed that more than half of the elderly population
suffers from functional illiteracy.
These individuals may be able to sign their
name or read very simple material, but they
cannot follow instructions for taking medicines
or cope with a variety of documents encountered
in daily living (R. Boling, 1,998).
Those who suffer from low levels of literacy
are educable. Engaging them in intellectual activities
has a significant influence on their
physiological perormance. Recent studies reveal


### ---Economics-1999-0-16.txt---
more physiological plasticity than was
previously suspected. The capacity for selfimprovement
continues into old age and appropriately
designed programs can return

diminished individuals to earlier levels of
functioning (Rowe and Kahn, 1997; cf.,
Wachter and Finch, 1997).

Peer tutoring has a two-way effect, since
it is beneficial both to the learner and to the
tutor. Both gain from involvement in social
networks that enhance mood, combat depression,
and reduce the risk of suicide. For
widowed men, the benefits are physiological
as well as psychological. Men in situations
that provide higher social support have significantly
lower losses of cortisol, epinephrine,
and norepinephrine (hormones that

reduce pain, stimulate the functioning of the
heart, and improve electrical transmission
across cells). Statistical analysis indicates a
positive relationship for both men and
women between social support and physical
performance. For the tutors, being active in
such productive and emotionally rewarding
activities serves to retain a sense of relative
youthfulness. Thus volwork, because it is effective,
because it is emotionally rewarding,
and because it is what the tutors want to do,
adds significantly to national product.
Use of fiscal policy to correct the maldistribution
of income is based, explicitly or

implicitly, on the ethical proposition that
those households at the top of the income
distribution have more income than they
ought to have. What about the case of spiritual
redistributions? Are spiritual resources
maldistributed because virtue is too heavily
concentrated? Government cannot legislate
the transfer of virtue as it does with money
income. Even if they desired to do so, those
rich in virtue or in the family ethic, or in
benevolence, could not transfer spiritual resources
by writing out checks denominated

in virtue, benevolence, or family solidarity.
Those poor in these spiritual resources acquire
more of them only through the process
of self-realization, through a concerted effort
to develop as fully as possible the virtuous
aspects of their nature.

Those rich in spiritual resources can help
those who are spiritually deprived by counseling
them, by providing spiritual companionship
and moral support, by informing and
teaching those who are deprived about existing
opportunities and procedures, and by helping
to raise their self-esteem. But this process of
correcting the maldistribution of spiritual resources
not only leaves those who are deprived
better off, it also increases the spiritual resources
of those who have virtue in abundance.
In contrast to income redistribution,
spiritual redistribution is not a fixed-sum game
in which some people can become better off
only if other people are made worse off. It is
a game in which total resources increase and
the share of the deprived in this larger total
may also increase without in any way diminishing
those who have a superabundance of

spiritual resources.

Some economists may be astonished by my
claim that in some respects the discipline has
fallen seriously behind the economy. After all,
if it were so, who would know about it before
them? The answer lies in the subtext of this
address. To understand where the economy is
and how it is evolving one needs to study not
only the present but the past. In the 1940's,
Kuznets ( 1941 ), musing about some of the analytical
mistakes made in the aftermath of the
Great Depression said: "A broader historical
background might have prevented some economists
from ignoring the dependence of their
generalizations upon transient historical conditions.
"' That advice is as good today as it
was a half century ago.
 ## Economics-2000-0


### ---Economics-2000-0-01.txt---
People today have more adequate nutrition
than ever before and acquire that nutrition at the
lowest cost in all human history, while the
world has more people than ever before-not by
a little but by a lot. This is an achievement that
many have argued could not be realized.
Throughout history there have been those who
believed that food shortages and famine were
the fate of humanity and that the world's population
was restricted not by human decisions
on fertility but by limitations imposed by nature.
Unfortunately for nearly all of human history
and for the vast majority of the world's
people, this pessimism was justified. In the last
two centuries, and especially in the twentieth
century, all has changed to a remarkable degree.
The twentieth century can be remembered as the
century in which hunger could have been eliminated
and, to a significant extent, has been.
I. Food and Population Growth

Thomas Robert Malthus, publishing the first
edition of his famous An Essay on the Principles
of Population in 1798, is usually credited
with the pessimistic view that population had a
tendency to outrun the available food supply
and was held in check by vice and misery--war,
disease, or starvation-but he was not the originator
of the idea. At least two millennia earlier
it was written in the Bible: "When goods increase,
those who eat them increase." (Ecclesiastes
5.)

Quintus Septimus Florence Tertillianus wrote:
Indeed it is certain, it is clear to see, that
the earth itself is currently more cultivated
and developed than in earlier times.
Now all places are accessible, all are docurnented,
all are full of business. The

most charming farms obliterate empty
places, ploughed fields vanquish forests,
herds drive out wild beasts, sandy places
are planted with crops, stones are fixed,
swamps drained, and there are such great
cities where formerly hardly a hut ... everywhere
there is a dwelling, everywhere

a multitude, everywhere a government,
everywhere there is life. The greatest evidence
of the large number of people: we

are burdensome to the world, the resources
are scarcely adequate to us; and

our needs straiten us and complaints are
everywhere while already nature does not
sustain us. Truly, pestilence and hunger
and war and flood must be consider as a
remedy for nations, like a pruning back of
the human race becoming excessive in
numbers. (Bart K. Holland, 1993 pp.
328-29.)

This was written about 200 A.D. when the
world's population was approximately 200 million.
Note that the quotation includes nearly all
the modern complaints about the effects of excessive
population on the environment-deforestation,
loss of biological diversity, farming
unsuitable land, drainage of the natural refuges
for wildlife-as well as the massing of people in
cities.

While Malthus was neither the first, nor the
last, to claim population growth carried with it
the seeds of disaster for humanity, he may have
been the first to significantly modify his view
that population growth would inevitably press
against the food supply. Five years after the
gloomy first edition, in the second edition of An
Essay on the Principles of Population he significantly
modified his major conclusion. After
noting the recent growth of European population,
he wrote:

.. fewer famines and fewer diseases arising
from want have prevailed in the last
century than in those that preceded it. On
the whole, therefore, though our future
prospects respecting the mitigation of the


### ---Economics-2000-0-03.txt---
evils arising from the principle of population
may not be so bright as we could

wish, yet they are far from entirely disheartening
and by no means preclude

gradual and progressive improvement in
human society which, before the late wild
speculations on the subject, was the object
of rational expectations. (Malthus, 1992
pp. 330-31.)

Unfortunately, the Malthus of the first edition
provided a model of population growth that
accurately depicted the experience of nearly all
of human history and was generally valid up to
the time he wrote. But he was also correct in his
view that during the century following the publication
of the essay that there would be gradual
improvement in the well-being of people in the
part of the world where he lived, namely Europe.
However, the progress was neither uniform
nor without interruption, as witness the
Irish famine of the 1840's and other famines
and food shortages that occurred in several European
countries during the nineteenth century.
In the second edition he recognized a third
factor that affected population growth in addition
to vice and misery-namely, the desire for
self-improvement. In other words, families
were willing and capable of influencing the
number of children by changing the age of
marriage, for example.

What made it possible for the world to escape
from what could be called the Malthusian trap?
The answer is simple: the creation of knowledge.
1 While there had been improvements in
agriculture for many millennia through knowledge
gained from practical experience-learning
by doing-there was an explosion of

knowledge over the past two centuries that
made possible an unparalleled increase in per
capita well-being, not just in terms of food but
in all aspects of life. Fundamentally, new technologies
have been developed at a rate unprecedented
by historical standards.

Consider the following (Angus Maddison,
1995):

1. The increase in the world's population in the
decade of the 1980's of 844 million was
nearly as large as the world's total population
in 1800 of 900 million.

2. During the decade of the 1980's the increase
in the world's gross domestic product per
capita equaled the estimated per capita gross
domestic product in 1820 (Maddison, 1995
p. 228).

Measured in 1990$, in the 1980's the per
capita GDP increased by $661; the per capita
figure for 1820 was $651.

3. The physical world-the land, the water, the
air, the sun-was basically the same in the
1980's as it was in 1820 or 1020 or 109000
years ago. Some might argue that the physical
world was less valuable than in the past.
The magnitude of the increase in the world's
output since 1820 is much greater than is directly
implied by the comparability of the increase
in GDP during the 1980's and in all
history up to 1820. The increase in real world
output during the 1980's was more than 10
times the output in 1820 and the world output in
1990 was 40 times that of 1820. How could
these enormous changes have occurred? They
occurred because we have found ways to offset
the limitations that natural resources imposed
on the world's output in times past as well as
improving greatly the amount and productivity
of human capital. We have not found how to
repeal the principle of diminishing marginal
returns. But we have found low cost and abundant
substitutes for natural resources important
in the production process.

As I will show later, the improvement in
well-being of the world's population goes far
beyond the enormous increase in the value of
the world's output. The improvements are evident
in fewer famines, increased caloric intakes,
reduced child and infant mortality, increased
life expectancy, great reductions in time
worked, and greatly increased percentage of the
population that is literate.

1 I have an enormous intellectual debt to many people for
the ideas that I have tried to put together in this paper.
Rather than interrupt the flow of the text with the numerous
citations that could be made, I wish to specify the individuals
and publications that have had the greatest influence:
Theodore W. Schultz (1964), Simon Kuznets (1966, 1979),
Paul M. Romer (1986, 1990), Paul Bairoch (1988), Zvi
Griliches (1988, 1998), Robert E. Lucas, Jr. (1988, 1993),
Gary S. Becker (1991), Malthus (1992), and Robert W.
Fogel (1996, 1999). I have not provided a full list of papers
that have influenced my thinking but only the one or two
that had the most influence. My debts to many others for
data are indicated at the appropriate points.


### ---Economics-2000-0-04.txt---
This lecture proceeds in the following way. I
show that for most of human history, life was
both short and difficult for the vast majority of
the world's people, that food supply was a major
factor affecting population size, and that
consumption-nonfood as well as food-was
very limited. I shall then turn to the question of
how the developed world escaped from the
Malthusian trap during the nineteenth century
and how the developing world did so more than
a century later.

I emphasize three major factors that I consider
responsible for the remarkable period of
economic growth that has occurred over the past
two or three centuries that permitted breaking
free from the limits imposed by the food supply.
The first factor is the significant advances in
agricultural productivity in the eighteenth and
nineteenth centuries. The increase in agricultural
productivity made possible the development
of cities as the major focus of further
economic development and growth. The second
factor is the enormous increase in knowledge
over the past two centuries made possible by
increasing population and rising real per capita
incomes resulting from the economic growth
from the mid-eighteenth century. The increase
in real incomes permitted the allocation of substantial
resources to the creation of knowledge.
This reallocation was associated with the rapid
development of two institutions- universities
and research institutes. The third factor, contrary
to what is often assumed, is that the response
of families to the removal of restraints
on their well-being imposed by limited food
supplies was not significantly increased fertility;
population growth resulted primarily from
mortality declines. Population growth was not
limited by the supply of food but by the decisions
of families.

The three reasons do not fully explain why
population growth did not spoil everything for
the developed world in the nineteenth century
and for the developing world more recently.
One reason the growth of food output in the
nineteenth century may not have been overwhelmed
by population growth was that knowledge
and technology required for the rapid
reduction of mortality did not become generally
available until near the end of the nineteenth
century and, further, the rapid increase in the
population of cities limited the decline in mortality.
The decline in mortality in the developing
world in the twentieth century was far more
rapid and resulted in a much higher rate of
population growth than the experience of the
nineteenth century even though fertility declined
significantly.

II. Agriculture and Food Before the
Nineteenth Century

Agriculture is a relatively recent inventionthe
transformation from hunting and gathering
to planting and growing crops and domesticating
animals probably occurred about 10 millennia
ago. At that time the world's population was
about 4 million and a large fraction of all resources
were devoted to obtaining food, and a
very poor lot it was.

As of 1800 it is estimated that '75 to 80
percent of the working population in the developed
world was engaged in agriculture (Bairoch,
1988 p. 287). In the rest of the world, with
nearly 80 percent of the world's population, the
percentage of workers engaged in agriculture
was certainly higher-of the order of 85 to 90
percent. In 1891, 90 percent of the population of
India was rural (Adna Ferrin Weber, 1899 p.
124) and as late as 1949, 89 percent of China's
population was rural.2 Unfortunately we do not
have evidence that permits -us to directly determine
the amount of food available in ancient
times. But if life expectancy in Roman times
were 25 years (Donald J. Bogue, 1969 p. 566),
it is highly probable that the available food per
capita was very limited. Fogel has estimated the


### ---Economics-2000-0-05.txt---
daily caloric supplies at the beginning of the
eighteenth century of 2,095 for Great Britain
and 1,657 for France (Fogel, 1996 p. 10). Life
expectancy for England in 1725 is estimated to
be 32 years and in France in 1750, 26 years.
Over the next century per capita calories increased
by approximately 10 percent-to 2,237
in Great Britain and to 1,846 in France and by
1800 life expectancy in England was 36 years
and in France 32 years (Fogel, 1996 p. 2).
Obviously other factors had a role in the increase
in life expectancy, but it is unlikely that
these increases and those that followed would
have occurred in the absence of improved nutritional
intake.

A life expectancy of between 25 and 30 years
was probably the fate of most of humanity
throughout recorded history until about 1650
(Bogue, 1969 p. 566). It was not until the seventeenth
century that there is evidence that life
expectancy increased significantly beyond what
it was in Roman or earlier times. As noted,
England and France, two of the wealthiest nations
of the world, had life expectancies at the
beginning of the eighteenth century that were
not much above what had prevailed throughout
human history.

It is probable that the per capita calorie supplies
for the world prior to the seventeenth
century were in the range found in England and
France at the beginning of the eighteenth century-
perhaps from 1,650 to less than 2,000.
These are in the range of calorie intakes in many
developing countries in 1934-1938, the earliest
date for which we have estimates for several
countries. Calorie intakes in India, the Philippines,
Peru, Colombia, and Mexico were in the
range of 1,800 to 2,000 calories (M. K. Bennett,
1976 p. 199). By 1934-1938 these countries
had significant population growth rates and at
earlier times consumption was probably rather
less.

Important evidence that the productivity of
agricultural resources in Europe remained relatively
constant and low was that in Europe,
excluding Russia, there was almost no change
in the percentage of the total urban population
between 1300 and 1800. Bairoch (1988 pp. 177,
216) estimates that in 1300 the urban population
of Europe was 10.4 percent of the total; five
centuries later in 1800 it was only 12.1 percent
and most of this increase occurred in England in
the eighteenth century. The nineteenth century
saw a major increase in urbanization; by the end
of the nineteenth century the urban population
was 37.9 percent of Europe's population, Russia
excluded. The urban population increased
almost five times in the nineteenth century after
little more than doubling in the previous five
centuries (Bairoch, 1988 pp. 177, 216). The
development of cities as a significant share of
the total population became possible only after
farmers increased production relative to their
own consumption.

Further evidence that per capita output of
agriculture increased very little throughout history
was the slow growth of world population
until nearly the beginning of the nineteenth century.
During the first millennium of the current
era, the annual rate of growth was 0.04 percent,
a doubling time of 1700 years. In the 700 years
ending in 1700, the rate of population growth
was 0.12 percent, a doubling time of about 580
years. The rate of population growth did increase
in the eighteenth century-to 0.41 percent
annually. But even at that rate it would take
179 years to double.

Europe's population during the eighteenth
century increased from 102 million to 154 million
and this increase was made possible by a
significant increase in food production occurred.
However, except in England, there was
no increase in urbanization so it is reasonable to
infer that in the rest of Europe the growth of
food production increased at approximately the
same rate as population during the eighteenth
century.

I[I. Agriculture and the Industrial Revolution
What was agriculture's contribution to the
Industrial Revolution? The Industrial Revolution
is generally considered to have started
3 England was the exception; it had a significant increase
in urbanization during the eighteenth century. Bairoch
(1988 p. 215) estimates that the percentage of the population
that was urban in 1700 was 13-16, in 1750, 17-19, and in
1800, 22-24. For Europe, excluding England, there was no
increase in urbanization as a percentage of the total population
in the eighteenth century. England was also an imnportant
exception because it had a significant growth rate of
population in the last half of the eighteenth century of 0.82
percent annually compared to 0.5 percent for Europe (excluding
Russia).


### ---Economics-2000-0-06.txt---
about 1750 in England and up to a century later
in the rest of Europe. As I have noted, the share
of cities in Europe's population had remained
nearly constant for the previous five centuries,
and that this meant that the available food supply
had not increased significantly faster than
population and, equally important, that the productivity
of labor in agriculture also had not
increased enough to permit labor to transfer out
of agriculture and migrate to cities. The midpoint
of the eighteenth century marks a striking
dividing point in the demographic and agricultural
history of England. In the century up to
1750 England's population was static. It actually
declined in some periods and increased at
an annual rate of only 0.1 percent or by 10
percent in the entire century (E. A. Wrigley and
R. S. Schofield, 1981 pp. 528-29). Life expectancy
may have actually declined. But between
1751 and 1801 its population grew at an annual
rate of 0.81 percent and the total increased by
50 percent. In the next half century the population
nearly doubled. Nearly all of the increase in
the United Kingdom's population in the nineteenth
century was urban (Bairoch, 1988

p. 290).4

What increased productivity in agriculture so
strikingly after 1750? Exact causes are unknown,
but many changes were involved, including
the spread of two high-yielding crops
from the Americas-corn (maize) and potatoes,
the enclosure movement, the elimination of fallow,
improved drainage, and increased availability
of animal manure made possible by the
cultivation of turnips as feed for cattle (David S.
Landes, 1969 p. 76). There was a major increase
in England and the Netherlands in the grain-toseed
ratio to more than ten in 1750-1820 compared
to seven in the two centuries prior to 1700
(B. H. Slicher van Bath, 1963).5 Such a large
increase in yield was almost certainly associated
with a significant increase in labor productivity.
While urbanization increased very little in England
during the last half of the eighteenth century,
there was a significant expansion of
industrial activity in rural areas, especially in
the production of textiles.

Significant increases in per capita food production
and in labor productivity in agriculture
were necessary conditions for the Industrial
Revolution which was associated with, and may
well have been advanced by, rapid population
growth. The increase in food production was
necessary to sustain the rapid population
growth; the growth in agricultural labor productivity
was required to permit a reduction in the
share of labor devoted to farming and to permit
the transfer of labor to the cities. I do not argue
that the improvements in food supply and labor
productivity were sufficient conditions for the
Industrial Revolution. It is quite probable that
the agricultural and industrial revolutions had
the same sources and each was affected by
developments in the other.

IV. The Mechanical Revolution

The improvements in labor productivity in
agriculture occurring in the eighteenth century
and the early years of the nineteenth century
were insignificant compared to the changes that
occurred in the rest of the century. Throughout
history for most of the world's population the


### ---Economics-2000-0-07.txt---
major source of calories has been grain-of the
order of 75 to 80 percent (Bennett, 1976 p. 206).
Until the early nineteenth century the serious
bottleneck in the production of grain was harvesting.
The plow was introduced several millennia
before, and it saved labor, but at a time of
the year when labor was not scarce. Plowing
could be done over an extended period of time,
but harvesting in most areas had to be done in a
brief period to prevent the crop being harmed or
destroyed by wind, rain, or frost.

At the beginning of the nineteenth century
grain was harvested by the same methods as in
the fourteenth century and probably much earlier-
the sickle, the scythe, and the cradle. T'he
invention and introduction of the reaper in
America in the second quarter of the nineteenth
century changed all that. The reaper was soon
followed by the binder, which was a reaper with
an attachment that brought the grain straw together
in a bundle and tied it with twine. The
binder was complemented by the thresher that
saved a great deal of labor, though at a time less
critical than the savings made possible by the
reaper and binder. In turn the binder and
thresher were largely replaced by the combine,
but not until well into the twentieth century.
Many other machines and tools were part of
the mechanical revolution. Very important
was harnessing the internal combustion engine
to create the tractor. The labor savings of
the mechanical revolution were enormous. It
is estimated that the direct labor input used to
produce a ton of grain in the United States
declined by 70 percent in the nineteenth century
(Martin R. Cooper et al., 1947). Consequently
in the developed countries after the
mid-nineteenth century the transfer of labor
from agriculture to nonagricultural pursuits
was more likely limited by the rate of growth
of nonagricultural employment than by the
labor requirements of agriculture.

V. Land Was Not the Scarce Resource
While today many give emphasis to the limited
supply of land of good quality as a major
impediment to further increases in food production,
throughout nearly all of human history
land has not been an important factor limiting
production. It had to have been something else
when the world's population was 500 million,
as it was in the sixteenth century, or perhaps
even when the population first reached one billion,
early in the nineteenth century. Given the
state of knowledge that existed until quite recently,
the primary limiting factor was labor.
Labor limited the amount of food that could be
produced by a family and, as noted, for much of
human history, farm families were barely able
to produce enough for their own consumption
with little surplus for trade with others. Until
quite recently this surplus was hardly more than
a quarter or a fifth of what they produced. A
good indicator that land was not the limiting
factor is that until the beginning of the nineteenth
century yields were calculated per unit of
seed, not per unit of land (Slicher van Bath,
1963).

Ester Boserup (1965) makes a convincing
case that labor and not land was the limiting
factor in agricultural output until quite recently.
She showed how farmers adapted to increasing
population by modifying the ways that land was
utilized, shifting from slash and burn and long
fallow to shorter periods of fallow and in Western
Europe eliminating fallow entirely. They
found ways other than fallow to maintain the
fertility of the soil-the use of manure and
legumes, for example. These changes were the
result of new knowledge, knowledge largely
derived from experience of the farmers themselves
and were a response to the growing population
and the need to expand food production.
VI. The Increased Role of Knowledge
As noted, the enormous increase in the
world's output over the past two centuries has
been due in large part to the advancement of
knowledge combined with the increase in human
resources, both in number and capabilities,
and savings translated into physical capital. We
do not have more natural resources than existed
in the distant past, yet output has increased
many fold. What is the source of the increased
knowledge? Two factors have been importantone
is simply the growth of population and the
other is that rising real per capita incomes have
made it possible for specialization in the production
of knowledge and for devoting a significant
share of our resources to that effort.
Michael Kremer (1993) makes a convincing
case for the conclusion that a larger population


### ---Economics-2000-0-08.txt---
leads to greater creation of knowledge. First, the
larger the population, the greater the benefit
from a given improvement in productivity resulting
from new knowledge. Second, with a

larger population, there are more individuals
capable of making a significant discovery or
adding to knowledge. It is not that today we are
smarter or more intelligent than populations a
century ago, two centuries ago, or a millennium
ago. Presumably the distribution of talents or
intelligence is the same today as at any past
time. But there are many, many more of us and
if the distribution of talents has not changed,
there are many more individuals capable of
advancing knowledge.

But it is not only that there are more of us
available to add to the world's knowledge, but
with the improvements in agricultural productivity,
the expansion of the cities, and the very
large increases in real per capita incomes that
have occurred over the past two centuries, institutions
have been created specifically to advance
and transmit knowledge. I refer to

universities and research institutes, including
both public and private ones. It was not that
prior to the nineteenth and twentieth centuries
that there were no individuals who had the
intelligence, time, curiosity, and energy for the
creation of knowledge. But their numbers were
limited. Our lives, however, are greatly influenced
by those who developed the reaper and
the binder, the internal combustion engine, the
steam engine, the railroad, electricity, the telephone,
and by those who discovered the small
pox vaccine and the germ theory of disease. But
by the beginning of the twentieth century their
effects on the lives of individuals were limited
compared to the effects of the recent increases
in knowledge and their applications.
When as many as 80 to 85 percent of the
world's labor force was engaged in farming, a
small percentage of a much smaller world population
had the time and resources to devote to
producing nonfood products, such as clothing,
tools, roads, and housing, let alone acquiring
new knowledge and technology. In 1990 in the
developed world no more than 10 percent of its
labor force was engaged in agriculture and in
the developing world approximately 60 percent
(World Bank, 1997 p. 220). Not only are there
about seven times as many people as there were
in 1800, but a significant percentage of this
much larger population specializes in the creation
of knowledge compared to the very small
number who could do so just two centuries ago.
The modern university, with many faculty devoting
their time to research in science and
graduate education, is a very recent creationsuch
institutions hardly existed before the middle
of the nineteenth century. German

universities dominated the world' s graduate education
in the nineteenth century. Yet as of
1900 in all the German universities there were
only 38,000 students and 1,830 faculty
(Friedrich Paulsen, 1908 p. 193); these are totals
for all colleges and universities, not just those
engaged in graduate education.

In 1869-1970, only a single Ph.D. was
awarded in the United States (U.S. Bureau of
the Census, Department of Commerce, 1960).
The development of colleges and universities
after 1869-1970 was quite remarkable. In that
year there were 563 colleges and universities
with a total faculty of 5,553 and 52,000 students.
The contribution to new knowledge had
to be limited; there were approximately ten faculty
members per institution, including, I assume,
the president who probably spent much
of the available time trying to find enough financial
resources to keep the institution open.
Sixty years later, for example, there were
82,000 faculty, 1.1 million students, and 2,299
doctorates awarded. Further rapid expansion of
higher education came after World War II and
by 1994/1995 there were an estimated 915,000
faculty, 14.3 million students, and 43,000 doctorates
awarded (Thomas I). Snyder, 1993).

In part, as a result of World War II, the
governmental support of research in universities
and federal research laboratories was greatly
expanded and many private research institutes
were created and developed. Prior to World
War II federal support of research was largely
concentrated in agriculture and the military.6


### ---Economics-2000-0-09.txt---
As the twentieth century ends, both the share
and the absolute amount of the world's resources
devoted to the development of new

knowledge are vastly greater than at the beginning
of the century. Equally important is that
the share of resources devoted to the wide distribution
of that knowledge has also increased
greatly.

VII. Population Growth-Roles of Fertility
and Mortality

My third point is that population growth in
the developed countries in the nineteenth century
and in the developing countries in the twentieth
century was due almost entirely to

mortality declines and not to fertility increases.
In other words, the response of men and women
to improved circumstances-improved nutrition
and higher incomes-was not to increase
fertility significantly. For the nineteenth century
the picture is clear. For the three European
countries, both fertility and mortality declined.
In England and Sweden there was an increase in
fertility during the latter half of the eighteenth
century but the increase was small (less than 10
percent) and lasted less than 50 years before
declining throughout the nineteenth century.
The increases in fertility had little or no effect
on population growth in Europe, with a modest
positive effect in Sweden and England from
perhaps 1750 to 1800, but with declines
throughout the nineteenth century.

The total fertility rate (average number of
children per woman) for Sweden increased from
4.21 in 1750 to 4.68 in 1800 but then declined
continuously throughout the nineteenth century,
reaching a level of 1.90 in 1990 (Massimo LiviBacci,
1992 p. 122). Life expectancy in the last
20 years of the eighteenth century was 34 years,
increasing to 39 years by 1835 and to 54 years
in the first decade of the twentieth century
(Nathan Keyfitz and Wilhelm Flieger, 1968 pp.
36-37).

Total fertility rates for England increased
from 5.28 in 1750 to 5.87 in 1775 and then
declined to 1.96 in 1900 (Livi-Bacci, 1989 p.
122). In the early period of the Industrial Revolution
there seems to have been a small positive
response in fertility to the improved
circumstances that lasted less than half a century
and had only a modest effect on the rate of
population growth. In England life expectancy
was 32 years in the last fifth of the seventeenth
century and remained at that level in the years
before 1750. It increased to 36 years by the end
of the eighteenth century and to 41 years by the
middle of the nineteenth century (Wrigley and
Schofield, 1981 pp. 528-29) and continued to
increase thereafter. In France the fertility and
mortality trends are very clear-the total fertility
rate was low in 1825 at 3.42 and fell continuously
reaching 2.14 in 1900 (Livi-Bacci,

1992 p. 122). Life expectancy increased from
about 28 years in 1760 to 40 years in 1840 and
to 46 years at the end of the century (Wrigley,
1987 pp. 274).

The data on fertility and mortality available
for the developing countries since 1960 prove
that the source of the rapid population growth in
these countries was the decline in mortality
rather than an increase in fertility. In fact, both
mortality and fertility fell much more rapidly in
the developing countries in the twentieth century
than in the developed countries in the nineteenth
century prior to 1875.7 Excluding China,
which has had coercive restraints on fertility,
the decline in fertility in the 31 lowest-income
countries from 1960 to 1995 was 38 percent
(United Nations Development Program
[UNDP], 1998). Over the same period of time,
life expectancy increased from 42 years to 59
years for the same countries. But the fertility
decline lagged behind the mortality decline by a
decade or more and high rates of population
growth occurred in the 1960's and the 1970's.
For example, between 1960 and 1978, in the 38
research. In addition, then and for at least the next century,
the results of agricultural research were public goods- once
a discovery was made, it became available to all. The
private sector cannot afford to significantly invest in public
goods. Today a significant fraction of agricultural research
is undertaken in the private sector due to legal protection for
intellectual property rights.

7The declines in total fertility rates in several European
countries from 1875 to 1900 were very rapid-Germany, 48
percent; Sweden, 46 percent; England and Wales, 42 percent;
Italy, 30 percent. In the United States the decline was
30 percent (Livi-Bacci, 1992 p. 122). These declines were
actually greater than what occurred in a similar period in the
low-income developing countries as a group but the declines
occurred much later in the demographic transition.


### ---Economics-2000-0-10.txt---
low-income economies the crude death rate declined
31.5 percent while the crude birth rate
declined 14.4 percent (China excluded) (World
Bank, 1980). The annual population growth rate
for the same countries was 2.5 percent for
1960-1970 and 2.2 percent for 1970-1978
(World Bank, 1980).

Why has fertility declined as real per capita
incomes have increased? At low levels of income,
with agriculture as the major occupation,
children have a positive benefit in increasing the
level of income of the parents as well as providing
security against illness and old age. Children,
and their growth and development, enter
into the utility functions of parents positively.
Parents realize satisfaction both in terms of the
quantity and quality of their children (Becker,
1991). As real per capita incomes increase, the
structure of the benefits from children change.
The direct contribution of children to the income
and material welfare of their parents diminishes
and in urban communities becomes

negative; even in agricultural communities
where incomes and the level of mechanization
are high, children make modest contributions to
current incomes and fertility in rural and urban
areas in the United States are now the same.
However, the utility that parents derive from
their children's growth and development increases
as their incomes increase and the emphasis
on the quality is reflected in increased
investment in their children. Children have increasingly
become a consumption good as real

per capita incomes have increased. Thus the
desired number of children is negatively related
to real income and as contraceptive knowledge
and technology have improved, families now
have the ability to achieve the number of children
desired to a greater degree and at lower
cost than in decades past.

VIII. Why the Nineteenth Century Had Lower
Population Growth

The differences in the population growth
rates between Europe in the nineteenth century
and in the developing countries in the twentieth
century are very great. During the first half of
the nineteenth century, the annual growth rate
for Europe (excluding Russia) was 0.55 percent;
from 1850 to 1880, 0.60, and for the last two
decades, 0.80 percent. The annual rate of increase
for the developing regions for 1950 to
1995 was 2.0 percent. In 1900 Europe's population
(excluding Russia) was 285 million. If
Europe's population during the nineteenth century
had grown at the developing countries' rate
from 1950 to 1995, its population would have
exceeded a billion in 1900, more than three
times its actual population. Could Europe have
accommodated such a large population in 1900
without a significant reduction in its real per
capita income at the end of the century? Obviously
we will never know, but it seems very
unlikely that it could have unless many of the
technological developments of the twentieth
century had occurred much, much earlier. While
it was true that the rate of economic growth, as
measured by changes in real GDP per capita,
was much slower in Europe in the nineteenth
century than in the developing countries in the
twentieth century (Maddison, 1995), it is worth
exploring why population growth was relatively
slow in Europe.

One factor responsible for the slow growth of
population in Europe was the significant increase
in the percentage of the population living
in cities-in 1800 the percentage was 12.1 and
in 1900 it was 37.9. Throughout the nineteenth
century, cities had much higher rates of mortality
than rural areas. Migration from rural areas
was the source of the growth of cities. Not only
did death rates in cities exceed birth rates, but
their death rates were significantly higher than
in rural areas.

Bairoch (1988 p. 230) reports that in Western
Europe throughout the nineteenth century infant
mortality rates in urban areas exceeded the rates
in rural areas by 30 to 60 percent. In Sweden in
the early nineteenth century infant deaths accounted
for approximately 25 percent of all
deaths (Keyfitz and Flieger, 1968). But the difference
in mortality was not confined to infants;
life expectancy at age 15 in Sweden was more
than four years higher in rural than in urban
areas in 1881-1890 (Bairoch, 1988 p. 235).8


### ---Economics-2000-0-11.txt---
The difference between population growth rates
in the nineteenth and twentieth centuries was
due primarily to the advancement of knowledge
and technology that permitted much more rapid
reductions in mortality in the twentieth century,
even in the world's poorest countries. There
was rather limited progress in reducing death
rates until the midpoint of the nineteenth century
in Sweden and significantly later for England.
The basic environmental problems of
unclean water, inadequate sanitation, and childhood
infectious diseases still took a major toll
until the early years of the twentieth century.
The infant mortality rate in New York City in
1890 was 264 per 1,000 births, more than double
the rate of 121 in rural areas (Weber, 1899).
The rates of decline in fertility in the developing
world were greater than in the developed
world for the periods under consideration. However,
the crude birth rates in the developing
world started their decline from a much higher
level than existed in the developed world in the
eighteenth and nineteenth centuries. The crude
birth rates in Sweden and England in the eighteenth
and early nineteenth centuries were in the
range of 35 to 37 per thousand population while
the averages for low-income countries in 1960
was 48 and the average for middle-income
countries was 40 (World Bank, 1980). Consequently,
the birth rate in the low-income countries
needed to fall by a third just to reach the
level prevailing in Western Europe at the beginning
of the Industrial Revolution. The more
rapid growth of population in today's developing
world was not due to increases in fertility
but to the combined effects of high initial rates
of fertility and rapid declines in mortality.
IX. Wide Distribution of Benefits of Knowledge
In recent years a great deal of concern has
been expressed about the lack of convergence of
per capita income among countries and increasing
inequality within countries. The emphasis
on increased income inequality has left the impression
that most measures of well-being have
become much more unequal as well. The use of
differences in per capita incomes as measures of
either satisfaction or well-being assumes that
these measures are proportional to income, a
conclusion that cannot be supported.
Contrary to views that are widely held, for
several important measures of well-being there
has been great improvement, both absolutely
and relatively, in the lives of the people of the
low-income developing countries.9 The benefits
of the growth of knowledge have not been restricted
to the countries responsible for the advances
in knowledge but have spread

throughout most of the world. And they would
have spread more quickly and more widely if
the policies of many governments had been
more supportive of economic growth and
development.

Improvements in the conditions of life in
terms of nutrition, infant mortality, and life expectancy,
have occurred at a much faster pace in
the developing countries in the twentieth century
than in the developed countries in the nineteenth
century. These improvements have

occurred with much larger populations and
greater population densities. The population of
the developing countries at the end of the twentieth
century is 4.84 billion, an increase of 350
percent in a century. This compares to the increase
in Europe in the nineteenth century of 85
percent. But not only did the improvements in
well-being occur more rapidly during similar
periods of economic development, but with respect
to several very important variables the
gaps narrowed significantly during the twentieth
century and especially during the last half of
that century.

century at a time of rapidly improving real wages is
certainly due in part to the very rapid growth in the
percentage of the population living in towns where
the death rates were high. ... Better wages in the
same economic and social environment may reduce
mortality, but since better wages may also mean
moving to a less healthy environment, there was a
negative rather than positive relationship between
wealth and health.

9 For the countries that moved from low- to middle- or to
high-income levels in the past half century, the improvements
in terms of nutrition, infant mortality rates, and life
expectancies have been significantly greater than what has
occurred in countries that have remained low income. South
Korea, Singapore, and Hong Kong are examples. Their
infant mortality rates ranged from 4 to 11 per thousand in
1995 and life expectancy at birth from 72 to 79 years
(World Bank, 1997), approaching the levels in Europe and
North America. In 1950 their incomes were in the range of
the current low-income countries.


### ---Economics-2000-0-12.txt---
A striking difference between the developing
countries at the end of the twentieth century and
the developed countries at the end of the nineteenth
century is that the lowest-income countries
have achieved rates of infant mortality and
life expectancy that are significantly superior to
those attained at the end of the nineteenth century
in the developed countries.

The infant mortality rate for 30 low-income
developing countries, including China, in
1960 was 157 per thousand births and it declined
by 62 percent to 62 in 1996 (IJNDP,
1998). The infant mortality rate in 1900 in
nine European countries ranged from a low of
121 in Denmark to a high of 216 in Austria
(Bairoch, 1988 p. 231). The rate in the United
States was 160. The teeming cities of the
developing world are often viewed negatively
by observers from the developed world. A
recent publication has the title "The Poverty
of Cities in the Developing World" (Martin
Brockerhoff and Ellen Brennan, 1997). Yet
the study revealed that cities with a population
of a million or more included in their
sample had an infant mortality rate of 60 per
thousand births in the 1990's (Brockerhoff
and Brennan, 1997 p. 24). Two comparisons
are relevant. Estimates of infant mortality for
the first decade of the twentieth century were
500 to 600 for Bombay and 350 to 400 in
Singapore (Bairoch, 1988 p. 450). The infant
mortality rate in New York City at the beginning
of the twentieth century was 264 (Weber,
1899 p. 346).

While infant mortality rates in cities in the
developed world at the beginning of the twentieth
century were higher than in rural areas, in
the developing world the rates in the cities are
now below those in rural areas (Brockerhoff and
Brennan, 1997 p. 24). These data indicate both
the large magnitude of the declines and the
extent to which the improvements have been
widely shared, even among many of the lowestincome
families in the world.

Significant increases in life expectancy were
achieved between 1900 and mid-century in the
developing countries, though much greater absolute
increases came later. The best available
long-term data are for India, at least one benefit
of being a British colony. Life expectancy in
India in 1900 was 23 years, increasing to 32
years in the 1940's (Bogue, 1969 p. 572) and to
43 years in 1960 and 62 years in 1996.10 Life
expectancy increased by 170 percent in a century-
-it is now almost three times what it was a
hundred years ago. Life expectancy in the
world's poorest countries has increased since
1940 at a far more rapid rate than achieved in
any country in the developed world in the nineteenth
century. This is an area where there has
been convergence between the rich and the poor
countries over the past several decades. At the
end of the nineteenth century life expectancy in
seven industrial countries ranged from 46 to 51
(Bogue, 1969). In 33 low-income countries in
1996 life expectancy was 64 years, compared to
44 years in 1960, an absolute increase of 20
years (World Bank, 1998). Furthermore, the
improvements in infant mortality and life expectancy
have been achieved at lower levels of
real per capita incomes than those prevailing in
the developed countries at the beginning of the
twentieth century (Maddison, 1995). The
knowledge about the benefits of clean water and
sanitation has been widely distributed and investments
have been made to make those benefits
widely available."

Since the late 1940's there has been greater
improvement in the world's availability of food
than had occurred in all previous history. The
evidence is the increase in per capita food supplies
that occurred in the developing countries,
with nearly 80 percent of the world's population.
There are reasonably reliable estimates of
daily per capita supply of calories for 19611963


### ---Economics-2000-0-13.txt---
when the daily per capita supply was
1,940 k/cals. In 1994-1996 the supply was
2,580 (Nikos Alexandratos, 1999 p. 5908)-a
remarkable increase of 33 percent, given that
population doubled during the period. Based on
the increase in per capita grain production from
1948-1952 to 1961-1963, it can be estimated
that the per capita calorie supply was about
1,700 in 1948-1952. The increase in calories
per capita available in the developing countries
from 1948-1952 to 1994-1996 was of the order
of 50 percent.12 Since developing countries
produce at least 90 percent of the food they
consume, this means that food production almost
trebled in four decades! This could not
have happened prior to the last half of the twentieth
century; the knowledge that made it possible
did not then exist.

Alexandratos (1999 p. 5908) provides a picture
of the improvement in food supplies that
adds another dimension to the increase in the
per capita availability in the developing countries:
" ... the part of the world population living
in countries where per capita food supplies are
still very low (under 2,200 k/cal/day) decreased
considerably to only 10% in the mid-1990s,
down from 56% 30 years earlier." Note that the
2,200 calories per day that is now defined as
very low is somewhat higher than was available
in England and significantly higher than in
France in 1800, just two centuries ago. True, the
average citizen of England and France as of
1800 was stunted and/or wasted-as short in
height and low in weight-just as the average
person in the developing countries with significantly
less than 2,200 calories per day would
be similarly designated today. What is important
is that this level of consumption applies to
countries with only a tenth of the world's population
while 200 years ago it applied to nearly
all of the world's population.

It is estimated that in 1990 approximately
780 million persons (19 percent of the developing-
country population, down from 36 percent
in 1969-1971) were malnourished (Alexandratos,
1995 p. 33). There is an adequate quantity
of food now produced to provide these people
with sufficient calories. However, as Adam
Smith taught us, policies are an important factor
determining how well a nation utilizes its resources.
The majority of the malnourished people
live in rural areas and most of them live in
countries that have had policies that discriminated
against agriculture and rural people for all
or part of the last three decades. The most
effective means to eliminate such malnutrition
is to increase the incomes of farm people in
these countries, something that would occur
with more appropriate policies (Johnson, 1999
p. 52).

In my opening sentence I stated that not only
are people better fed than ever before, but they
acquire their food at the lowest cost in all history.
There is no way to prove that the real cost
per calorie is the lowest in all history, but we do
know that nearly all people are now devoting a
smaller percentage of their consumption expenditures
to the acquisition of food than was true
at times past. As recently as 1955 families in the
United States allocated 23 percent of their consumption
expenditure to food; today it is about
10 percent. For Japan the reduction was from 54
to 20 percent and for South Korea from 50 to 36
percent. Between 1960 and 1990 the food share
in Thailand declined from 47 to 23 percent
(United Nations, National Account Statistics). It
is not unreasonable to assume that at the beginning
of the nineteenth century in the developed
countries that 70 percent or more of the consumption
expenditures went for food and that
the percentage was even higher at the beginning
of the twentieth century for the developing
countries. 13

12 Grain provides 75 to 80 percent of calories consumed
in the low-income developing countries. From 1961-1963
to 1988-1990, per capita grain production increased by 25
percent, almost the same as the per capita increase in per
capita calorie supplies. Data are available on grain production
for 1948-1952. From that period to 1961-1963 per
capita grain production increased by 14 percent. Thus one
may estimate that per capita calorie supplies in 1948-1952
were approximately 1,700 (1,940/1.14 = 1,701).
13In discussing the benefits of the increase in knowledge
and substantially higher incomes generated by that knowledge,
I have said nothing directly about the possibility of
environmnental degradation. While in some and, perhaps,
many respects the environment has been degraded in the
twentieth century, in terms of the effects on life and overall
health, the net effect has been strikingly positive. The declines
in infant mortality and increases in life expectancy
have been due largely to improved nutrition and environmental
improvements in terms of sanitation and clean water.
True, some features of the twentieth century, such as air


### ---Economics-2000-0-14.txt---
X. Concluding Comments

During the last two centuries, and especially
in the twentieth century, there has been an enormous
increase in knowledge that has been transformed
into technology and ways of utilizing
resources more efficiently. It is not only that
knowledge has increased rapidly but the means
of communicating that knowledge in an effective
way have been markedly improved and the
knowledge has become much more accessible
throughout the world.

The rapid growth of knowledge has resulted
both from the growth of the world's
population and the increase in the percentage
of that population that is now able to devote
time and energy to the creation of knowledge.
It was not so long ago that farmers accounted
for 80 percent of the world's labor force and
they barely produced enough for themselves
with little left for exchange. As productivity
in agriculture increased, the rapid growth of
cities occurred and the growth in real per
capita incomes exceeded what had ever been
achieved before. The fact that during the
1980's the increase in the world's output was
ten times what world output was in 1820
illustrates how great has been the growth of
output in a very brief period of time.
But perhaps the greatest achievement of the
twentieth century is that the majority of the
poor people of the world have shared in the
improvements in well-being made possible by
the advancement of knowledge. Three measures
show how great these improvements

have been-infant mortality rates, life expectancy,
and per capita food supplies. The large
cities of the developing world now have infant
mortality rates about a quarter of those of
New York City in 1890. True, there is much
more that can be done to share more fully the
benefits of the knowledge base. And I am
confident that whoever speaks from this platform
just 25 years from now could point to
further dramatic reductions in worldwide inequalities
in well-being.
 ## Economics-2001-0


### ---Economics-2001-0-03.txt---
The resurgence of the American economy
since 1995 has outrun all but the most optimistic
expectations. Economic forecasting models
have been seriously off track and growth projections
have been revised to reflect a more
sanguine outlook only recently.! It is not surprising
that the unusual combination of more
rapid growth and slower inflation in the 1990's
has touched off a strenuous debate among economists
about whether improvements in America'
s economic performance can be sustained.
The starting point for the economic debate is
the thesis that the 1990's are a mirror image of
the 1970's, when an unfavorable series of "supply
shocks" led to stagflation-slower growth
and higher inflation.2 In this view, the development
of information technology (IT) is one of a
series of positive, but temporary, shocks. The
competing perspective is that IT has produced a
fundamental change in the U.S. economy, leading
to a permanent improvement in growth
prospects.

The relentless decline in the prices of information
technology equipment has steadily enhanced
the role of IT investment as a source of American
economic growth. Productivity growth in ITproducing
industries has gradually risen in importance
and a productivity revival is now under way
in the rest of the economy. Despite differences in
methodology and data sources, a consensus is
building that the remarkable behavior of IT prices
provides the key to the surge in economic growth.
In the following section I show that the foundation
for the American growth resurgence is
the development and deployment of semiconductors.
The decline in IT prices is rooted in
developments in semiconductor technology that
are widely understood by technologists and
economists. This technology has found its
broadest applications in computing and communications
equipment, but has reduced the cost of
a wide variety of other products.

A substantial acceleration in the IT price decline
occurred in 1995, triggered by a much
sharper acceleration in the price decline of
semiconductors in 1994. Although the decline
in semiconductor prices has been projected to
continue for at least another decade, the recent
acceleration could be temporary. This can be
traced to a shift in the product cycle for semiconductors
from three years to two years that

took place in 1995 as the consequence of intensifying
competition in markets for semiconductor
products.

In Section II I outline a framework for analyzing
the role of information technology in the
American growth resurgence. Constant quality
price indexes separate the change in the performance
of IT equipment from the change in price
for a given level of performance. Accurate and
timely computer prices have been part of the
U.S. National Income and Product Accounts
(NIPA) since 1985. Unfortunately, important
information gaps remain, especially on trends in
prices for closely related investments, such as
software and communications equipment.
The cost of capital is an essential concept for
capturing the economic impact of information
technology prices. Swiftly falling prices provide
powerful economic incentives for the substitution
of IT equipment for other forms of capital and for
labor services. The rate of the IT price decline is a


### ---Economics-2001-0-04.txt---
key component of the cost of capital, required for
assessing the impacts of rapidly growing stocks
of computers, communications equipment, and
software.

In Section III I analyze the impact of the 1995
acceleration in the information technology price
decline on U.S. economic growth. I introduce a
production possibility frontier that encompasses
substitutions between outputs of consumption
and investment goods, as well as inputs of capital
and labor services. This frontier treats IT
equipment as part of investment goods output
and the capital services from this equipment as
a component of capital input.

Capital input has been the most important
source of U.S. economic growth throughout the
postwar period. More rapid substitution toward
information technology has given much additional
weight to components of capital input
with higher marginal products. The vaulting
contribution of capital input since 1995 has
boosted growth by nearly a full percentage
point. The contribution of IT accounts for more
than half of this increase. Computers have been
the predominant impetus to faster growth, but
communications equipment and software have
made important contributions as well.
The accelerated information technology price
decline signals faster productivity growth in
IT-producing industries. In fact, these industries
have been the source of most of aggregate productivity
growth throughout the 1990's. Before
1995 this was due to the decline of productivity
growth elsewhere in the economy. The ITproducing
industries have accounted for about
half the surge in productivity growth since 1995,
but faster growth is not limited to these industries.
I conclude that the decline in IT prices will
continue for some time. This will provide incentives
for the ongoing substitution of IT for
other productive inputs. Falling IT prices also
serve as an indicator of rapid productivity
growth in IT-producing industries. However, it
would be premature to extrapolate the recent
acceleration in productivity growth in these industries
into the indefinite future, since this depends
on the persistence of a two-year product
cycle for semiconductors.

In Section IV I outline research opportunities
created by the development and diffusion of
information technology. A voluminous and rapidly
expanding business literature is testimony
to the massive impact of IT on firms and product
markets. Highest priority must be given to a
better understanding of the markets for semiconductors.
Although several models of the

market for semiconductors already exist, none
explains the shift from a three-year to a twoyear
product cycle.

The dramatic effects of information technology
on capital and labor markets have already
generated a substantial and growing economic
literature, but many important issues remain to
be resolved. For capital markets the relationship
between equity valuations and growth prospects
merits much further study. For labor markets
more research is needed on investment in information
technology and substitution among different
types of labor.

I. The Information Age

The development and deployment of information
technology is the foundation of the
American growth resurgence. A mantra of the
"new economy"-faster, better, cheaper-captures
the speed of technological change and
product improvement in semiconductors and
the precipitous and continuing fall in semiconductor
prices. The price decline has been transmitted
to the prices of products that rely heavily
on semiconductor technology, like computers
and telecommunications equipment. This technology
has also helped to reduce the cost of
aircraft, automobiles, scientific instruments, and
a host of other products.

Modem information technology begins with
the invention of the transistor, a semiconductor
device that acts as an electrical switch and encodes
information in binary form. A binary digit
or bit takes the values zero and one, corresponding
to the off and on positions of a switch. The
first transistor, made of the semiconductor germanium,
was constructed at Bell Labs in 1947
and won the Nobel Prize in Physics in 1956 for
the inventors-John Bardeen, Walter Brattain,
and William Shockley.4

The next major milestone in information technology
was the coinvention of the integrated circuit
by Jack Kilby of Texas Instruments in 1958
4 On Bardeen, Brattain, and Shockley, see: http://www.
nobel.se/physics/laureates/l956/.


### ---Economics-2001-0-05.txt---
and Robert Noyce of Fairchild Semiconductor in
1959. An integrated circuit consists of many, even
millions, of transistors that store and manipulate
data in binary form. Integrated circuits were originally
developed for data storage and retrieval and
semiconductor storage devices became known as
memory chips.5

The first patent for the integrated circuit was
granted to Noyce. This resulted in a decade of
litigation over the intellectual property rights.
The litigation and its outcome demonstrate the
critical importance of intellectual property in
the development of information technology.
Kilby was awarded the Nobel Prize in Physics
in 2000 for discovery of the integrated circuit;
regrettably, Noyce died in 1990.6

A. Moore's Law

In 1965 Gordon E. Moore, then Research
Director at Fairchild Semiconductor, made a
prescient observation, later known as Moore's
Law.7 Plotting data on memory chips, he observed
that each new chip contained roughly
twice as many transistors as the previous chip
and was released within 18-24 months of its
predecessor. This implied exponential growth
of chip capacity at 35-45 percent per year!
Moore's prediction, made in the infancy of the
semiconductor industry, has tracked chip capacity
for 35 years. He recently extrapolated this
trend for at least another decade.8
In 1968 Moore and Noyce founded Intel Corporation
to speed the commercialization of

memory chips.9 Integrated circuits gave rise to
microprocessors with functions that can be programmed
by software, known as logic chips.

Intel's first general purpose microprocessor was
developed for a calculator produced by Busicom,
a Japanese firm. Intel retained the intellectual
property rights and released the device
commercially in 1971.

The rapidly rising trends in the capacity of
microprocessors and storage devices illustrate the
exponential growth predicted by Moore's Law.
The first logic chip in 1971 had 2,300 transistors,
while the Pentium 4 released on November 20,
2000, had 42 million! Over this 29-year period the
number of transistors increased by 34 percent per
year. The rate of productivity growth for the U.S.
economy during this period was slower by two
orders of magnitude.

B. Semiconductor Prices

Moore's Law captures the fact that successive
generations of semiconductors are faster and better.
The economics of semiconductors begins with
the closely related observation that semiconductors
have become cheaper at a truly staggering
rate! Figure 1 gives semiconductor price indexes
constructed by Bruce T. Grimm (1998) of the U.S.
Bureau of Economic Analysis (BEA) and employed
in the U.S. National Income and Product
Accounts since 1996. These are divided between
memory chips and logic chips. The underlying
detail includes seven types of memory chips and
two types of logic chips.

Between 1974 and 1996 prices of memory
chips decreased by a factor of 27,270 times or at
40.9 percent per year, while the implicit deflator
for the gross domestic product (GDP) increased
by almost 2.7 times or 4.6 percent per year! Prices
of logic chips, available for the shorter period
1985 to 1996, decreased by a factor of 1,938 or
54.1 percent per year, while the GDP deflator
increased by 1.3 times or 2.6 percent per year!
Semiconductor price declines closely parallel
Moore's Law on the growth of chip capacity,
setting semiconductors apart from other products.
Figure 1 also reveals a sharp acceleration in
the decline of semiconductor prices in 1994 and
1995. The microprocessor price decline leapt to
more than 90 percent per year as the semiconductor
industry shifted from a three-year product
cycle to a greatly accelerated two-year
cycle. This is reflected in the 2000 Update of
the International Technology Road Map for
Semiconductors,'0 prepared by a consortium of
industry associations.


### ---Economics-2001-0-06.txt---
100,000.0

10,000.0

1,000.0

_0E

i 100.0

0

1.0

0.1

0.0

1959 1964 1969 1974 1979 1984 1989 1994 1999
-u-Computers - Memory- Logic

FIGURE 1. RELATIVE PRICES OF COMPUTERS AND SEMICONDUCTORS, 1959-1999
Note: All price indexes are divided by the output price index.
C. Constant Quality Price Indexes

The behavior of semiconductor prices is a
severe test for the methods used in the official
price statistics. The challenge is to separate observed
price changes between changes in semiconductor
performance and changes in price

that hold performance constant. Achieving this
objective has required a detailed understanding
of the technology, the development of sophisticated
measurement techniques, and the introduction
of novel methods for assembling the
requisite information.

Ellen R. Dulberger (1993) of IBM introduced
a "matched model" index for semiconductor
prices. A matched model index combines price
relatives for products with the same performance
at different points of time. Dulberger
presented constant quality price indexes based
on index number formulas, including the
[Irving] Fisher (1922) ideal index used in the
U.S. national accounts. 1 1 The Fisher index is the
geometric average of the familiar Laspeyres and
Paasche indexes.

W. Erwin Diewert (1976) defined a superlative
index number as an index that exactly
replicates aflexible representation of the underlying
technology (or preferences). A flexible
representation provides a second-order approximation
to an arbitrary technology (or preferences)
. A. A. Konus and S. S. Byushgens

(1926) first showed that the Fisher ideal index is
superlative in this sense. Laspeyres and Paasche
indexes are not superlative and fail to capture
substitutions among products in response to
price changes accurately.

Grimm (1998) combined matched model
techniques with hedonic methods, based on an
econometric model of semiconductor prices at
different points of time. A hedonic model gives
the price of a semiconductor product as a function
of the characteristics that determine performance,
such as speed of processing and storage
capacity. A constant quality price index isolates
the price change by holding these characteristics
of semiconductors fixed.

Beginning in 1997, the U.S. Bureau of Labor
" See J. Steven Landefeld and Robert P. Parker (1997).


### ---Economics-2001-0-07.txt---
Statistics (BLS) incorporated a matched model
price index for semiconductors into the Producer
Price Index (PPI) and since then the national
accounts have relied on data from the
PPI. Reflecting long-standing BLS policy, historical
data were not revised backward. Semiconductor
prices reported in the PPI prior to
1997 do not hold quality constant, failing to
capture the rapid semiconductor price decline
and the acceleration in 1994.

D. Computers

The introduction of the Personal Computer
(PC) by IBM in 1981 was a watershed event
in the deployment of information technology.
The sale of Intel's 8086-8088 microprocessor
to IBM in 1978 for incorporation into the PC
was a major business breakthrough for Intel.
12 In 1981 IBM licensed the MS-DOS

operating system from the Microsoft Corporation,
founded by Bill Gates and Paul Allen
in 1975. The PC established an Intel/
Microsoft relationship that has continued up
to the present. In 1985 Microsoft released the
first version of Windows, its signature operating
system for the PC, giving rise to the
Wintel (Windows-Intel) nomenclature for this
ongoing collaboration.

Mainframe computers, as well as PC's, have
come to rely heavily on logic chips for central
processing and memory chips for main memory.
However, semiconductors account for less
than half of computer costs and computer prices
have fallen much less rapidly than semiconductor
prices. Precise measures of computer prices
that hold product quality constant were introduced
into the NIPA in 1985 and the PPI during
the 1990's. The national accounts now rely on
PPI data, but historical data on computers from
the PPI, like the PPI data on semiconductors, do
not hold quality constant.

Gregory C. Chow (1967) pioneered the use of
hedonic techniques for constructing a constant
quality index of computer prices in research
conducted at IBM. Chow documented price declines
at more than 20 percent per year during
1960-1965, providing an initial glimpse of the
n'k ~~~~~~~~~13

remarkable behavior of computer prices. In
1985 the Bureau of Economic Analysis incorporated
constant quality price indexes for computers
and peripheral equipment constructed by
Rosanne Cole et al. (1986) of IBM into the
NIPA. Triplett (1986) discussed the economic
interpretation of these indexes, bringing the
rapid decline of computer prices to the attention
of a very broad audience.

The BEA-IBM constant quality price index
for computers provoked a heated exchange between
BEA and Edward F. Denison (1989), one
of the founders of national accounting methodology
in the 1950's and head of the national
accounts at BEA from 1979 to 1982. Denison
sharply attacked the BEA-IBM methodology
and argued vigorously against the introduction
of constant quality price indexes into the national
accounts.14 Allan Young (1989), then Director
of BEA, reiterated BEA's rationale for
introducing constant quality price indexes.
Dulberger (1989) presented a more detailed
report on her research on the prices of computer
processors for the BEA-IBM project. Speed of
processing and main memory played central
roles in her model. Triplett (1989) provided an
exhaustive survey of research on hedonic price
indexes for computers. Robert J. Gordon (1989,
1990) gave an alternative model of computer
prices and identified computers and communications
equipment, along with commercial aircraft,
as assets with the highest rates of price
decline.

Figure 2 gives BEA's constant quality index
of prices of computers and peripheral equipment
and its components, including mainframes,
PC's, storage devices, other peripheral
equipment, and terminals. The decline in computer
prices follows the behavior of semiconductor
prices presented in Figure 1, but in much
attenuated form. The 1995 acceleration in the
computer price decline parallels the acceleration
in the semiconductor price decline that
resulted from the changeover from a three-year
product cycle to a two-year cycle in 1995.


### ---Economics-2001-0-08.txt---
10,000 -

1,000 X

100

10 -

1948 1953 1958 1963 1968 1973 1978 1983 1988 1993 1998
- Computers Communications Software Services
FIGURE 2. RELATIVE PRICES OF COMPUTERS, COMMUNICATIONS, SOFTWARE, AND SERVICES, 1948-1999
Note: All price indexes are divided by the output price index.
E. Communications Equipment and Software
Communications technology is crucial for the
rapid development and diffusion of the Internet,
perhaps the most striking manifestation of information
technology in the American economy.
15 Kenneth Flamm (1989) was the first to
compare the behavior of computer prices and
the prices of communications equipment. He
concluded that the communications equipment
prices fell only a little more slowly than computer
prices. Gordon (1990) compared Flamm's
results with the official price indexes, revealing
substantial bias in the official indexes.
Communications equipment is an important
market for semiconductors, but constant quality
price indexes cover only a portion of this equipment.
Switching and terminal equipment rely
heavily on semiconductor technology, so that
product development reflects improvements in
semiconductors. Grimm's (1997) constant quality
price index for digital telephone switching
equipment, given in Figure 3, was incorporated
into the national accounts in 1996. The output
of communications services in the NIPA also
incorporates a constant quality price index for
cellular phones.

Much communications investment takes the
form of the transmission gear, connecting data,
voice, and video terminals to switching equipment.
Technologies such as fiber optics, microwave
broadcasting, and communications satellites
have progressed at rates that outrun even the dramatic
pace of semiconductor development. An
example is dense wavelength division multiplexing
(DWDM), a technology that sends multiple
signals over an optical fiber simultaneously. Installation
of DWDM equipment, beginning in

1997, has doubled the transmission capacity of
fiber-optic cables every 6-12 months.'6
15 A general reference on the Internet is Soon-Yong Choi
and Andrew B. Whinston (2000). On Internet indicators,
see: http://www.internetindicators.com/.
16 Rick Rashad (2000) characterizes this as the "demise"
of Moore's Law. Jeff Hecht (1999) describes DWDM technology
and provides a general reference on fiber optics.


### ---Economics-2001-0-09.txt---
Both software and hardware are essential for
information technology and this is reflected in
the large volume of software expenditures. The
eleventh comprehensive revision of the national
accounts, released by BEA on October 27,
1999, reclassified computer software as investment.
17 Before this important advance, business
expenditures on software were treated as current
outlays, while personal and government expenditures
were treated as purchases of nondurable
goods. Software investment is growing rapidly
and is now much more important than investment
in computer hardware.

Robert P. Parker and Grimm (2000) describe
the new estimates of investment in software. BEA
distinguishes among three types of softwareprepackaged,
custom, and own-account software.

Prepackaged software is sold or licensed in standardized
form and is delivered in packages or
electronic files downloaded from the Internet.
Custom software is tailored to the specific application
of the user and is delivered along with
analysis, design, and programming services required
for customization. Own-account software
consists of software created for a specific application.
However, only price indexes for prepackaged
software hold performance constant.
Parker and Grimm (2000) present a constant
quality price index for prepackaged software,
given in Figure 3. This combines a hedonic
model of prices for business applications software
and a matched model index for spreadsheet
and word-processing programs developed
by Steven D. Oliner and Daniel E. Sichel
(1994). Prepackaged software prices decline at
more than 10 percent per year over the period
1962-1998. Since 1998 the BEA has relied on a
matched model price index for all prepackaged
software from the PPI; prior to 1998 the PPI
data do not hold quality constant.

BEA's prices for own-account software are
based on programmer wage rates. This implicitly
assumes no change in the productivity of
computer programmers, even with growing investment
in hardware and software to support
the creation of new software. Custom software


### ---Economics-2001-0-10.txt---
prices are a weighted average of prepackaged
and own-account software prices with arbitrary
weights of 75 percent for own-account and 25
percent for prepackaged software. These price
indexes do not hold the software performance
constant and present a distorted picture of software
prices, as well as software output and
investment.

F. Research Opportunities

The official price indexes for computers and
semiconductors provide the paradigm for economic
measurement. These indexes capture the
steady decline in IT prices and the recent acceleration
in this decline. The official price indexes
for central office switching equipment and prepackaged
software also hold quality constant.
BEA and BLS, the leading statistical agencies
in price research, have carried out much of the
best work in this area. However, a critical role
has been played by price research at IBM, long
the dominant firm in information technology.'8
It is important to emphasize that information
technology is not limited to applications of
semiconductors. Switching and terminal equipment
for voice, data, and video communications
has come to rely on semiconductor technology
and the empirical evidence on prices of this
equipment reflects this fact. Transmission gear
employs technologies with rates of progress that
far outstrip those of semiconductors. This important
gap in our official price statistics can
only be filled by constant quality price indexes
for all types of communications equipment.
Investment in software is more important
than investment in hardware. This was essentially
invisible until BEA introduced new measures
of prepackaged, custom, and own-account
software investment into the national accounts
in 1999. This is a crucial step in understanding
the role of information technology in the American
economy. Unfortunately, software prices
are another statistical blind spot, with only
prices of prepackaged software adequately represented
in the official system of price statistics.
The daunting challenge that lies ahead is to
construct constant quality price indexes for custom
and own-account software.

II. The Role of Information Technology
At the aggregate level IT is identified with the
outputs of computers, communications equipment,
and software. These products appear in
the GDP as investments by businesses, households,
and governments along with net exports
to the rest of the world. The GDP also includes
the services of IT products consumed by households
and governments. A methodology for analyzing
economic growth must capture the

substitution of IT outputs for other outputs of
goods and services.

While semiconductor technology is the driving
force behind the spread of IT, the impact of
the relentless decline in semiconductor prices is
transmitted through falling IT prices. Only net
exports of semiconductors, defined as the difference
between U.S. exports to the rest of the
world and U.S. imports, appear in the GDP.
Sales of semiconductors to domestic manufacturers
of IT products are precisely offset by
purchases of semiconductors and are excluded
from the GDP.

Constant quality price indexes, like those reviewed
in the previous section, are a key component
of the methodology for analyzing the
American growth resurgence. Computer prices
were incorporated into the NIPA in 1985 and
are now part of the PPI as well. Much more
recently, semiconductor prices have been included
in the NIPA and the PPI. Unfortunately,
evidence on the prices of communications
equipment and software is seriously incomplete,
so that the official price indexes are seriously
misleading.

A. Output

The output data in Table 1 are based on the
most recent benchmark revision of the national
accounts, updated through 1999.19 The output
concept is similar, but not identical, to the concept
of gross domestic product used by the
BEA. Both measures include final outputs purchased
by businesses, governments, households,
and the rest of the world. Unlike the BEA
concept, the output measure in Table 1 also
18 See Alfred D. Chandler, Jr. (2000 Table 1.1 p. 26).
19 See Jorgenson and Kevin J. Stiroh (2000b Appendix
A) for details on the estimates of output.


### ---Economics-2001-0-11.txt---



### ---Economics-2001-0-12.txt---
TABLE 2-GROwTH RATES OF OUTPUTS AND INPUTS
1990-1995 1995-1999

Prices Quantities Prices Quantities
Outputs

Gross domestic product 1.99 2.36 1.62 4.08
Information technology -4.42 12.15 -9.74 20.75
Computers -15.77 21.71 -32.09 38.87
Software -1.62 11.86 -2.43 20.80

Communications equipment -1.77 7.01 -2.90 11.42
Information technology services -2.95 12.19 -11.76 18.24
Noninformation technology investment 2.15 1.22 2.20 4.21
Noninformation technology consumption 2.35 2.06 2.31 2.79
Inputs

Gross domestic income 2.23 2.13 2.36 3.33
Information technology capital services -2.70 11.51 -10.46 19.41
Computer capital services -11.71 20.27 -24.81 36.36
Software capital services -1.83 12.67 -2.04 16.30
Communications equipment capital services 2.18 5.45 -5.90 8.07
Noninformation technology capital services 1.53 1.72 2.48 2.94
Labor services 3.02 1.70 3.39 2.18

Note: Average annual percentage rates of growth.
includes imputations for the service flows from
durable goods, including IT products, employed
in the household and government sectors.
The imputations for services of IT equipment
are based on the cost of capital for IT described
in more detail below. The cost of capital is
multiplied by the nominal value of IT capital
stock to obtain the imputed service flow from IT
products. In the business sector this accrues as
capital income to the firms that employ these
products as inputs. In the household and government
sectors the flow of capital income must
be imputed. This same type of imputation is
used for housing in the NIPA. The rental value
of renter-occupied housing accrues to real estate
firms as capital income, while the rental
value of owner-occupied housing is imputed to
households.

Current dollar GDP in Table 1 is $9.8 trillions
in 1999, including imputations, and real
output growth averaged 3.46 percent for the
period 1948-1999. These magnitudes can be
compared to the current dollar value of $9.3
trillions in 1999 and the average real growth
rate of 3.40 percent for the period 1948-1999
for the official GDP. Table 1 presents the current
dollar value and price indexes of the GDP
and IT output. This includes outputs of investment
goods in the form of computers, software,
communications equipment, and non-IT investment
goods. It also includes outputs of non-IT
consumption goods and services as well as imputed
IT capital service flows from households
and governments.

The most striking feature of the data in Table
1 is the rapid price decline for computer investment,
17.1 percent per year from 1959 to 1995.
Since 1995 this decline has almost doubled to
32.1 percent per year. By contrast the relative
price of software has been flat for much of
the period and began to fall only in the late
1980's. The price of communications equipment
behaves similarly to the software price,
while the consumption of capital services from
computers and software by households and governments
shows price declines similar to computer
investment.

The top panel of Table 2 summarizes the
growth rates of prices and quantities for major
output categories for 1990-1995 and 1995-
1999. Business investments in computers,
software, and communications equipment are
the largest categories of IT spending. Households
and governments have also spent sizable
amounts on computers, software,

communications equipment and the services
of information technology. Figure 4 shows
that the output of software is the largest IT


### ---Economics-2001-0-13.txt---
category as a share of GDP, followed by the
outputs of computers and communications
equipment.

B. Capital Services

This subsection presents capital estimates for
the U.S. economy for the period 1948 to 1999.20
These begin with BEA investment data; the
perpetual inventory method generates estimates
of capital stocks and these are aggregated, using
service prices as weights. This approach, originated
by Jorgenson and Zvi Griliches (1996), is
based on the identification of service prices with
marginal products of different types of capital.
The service price estimates incorporate the cost
of capital.21

The cost of capital is an annualization factor
that transforms the price of an asset into the
price of the corresponding capital input.22 This
includes the nominal rate of return, the rate of
depreciation, and the rate of capital loss due to
declining prices. The cost of capital is an essential
concept for the economics of information
technology,23 due to the astonishing decline of
IT prices given in Table 1.

The cost of capital is important in many areas
of economics, especially in modeling producer
behavior, productivity measurement, and the
economics of taxation.24 Many of the important
issues in measuring the cost of capital have been
debated for decades. The first of these is incorporation
of the rate of decline of asset prices
into the cost of capital. The assumption of perfect
foresight or rational expectations quickly
emerged as the most appropriate formulation


### ---Economics-2001-0-14.txt---
and has been used in almost all applications of
the cost of capital.25

The second empirical issue is the measurement
of economic depreciation. The stability of
patterns of depreciation in the face of changes in
tax policy and price shocks has been carefully
documented. The depreciation rates presented
by Jorgenson and Stiroh (2000b) summarize a
large body of empirical research on the behavior
of asset prices.2 A third empirical issue is the
description of the tax structure for capital income.
This depends on the tax laws prevailing
at each point of time. The resolution of these
issues has cleared the way for detailed measurements
of the cost of capital for all assets that
appear in the national accounts, including information
technology.27

The definition of capital includes all tangible
assets in the U.S. economy, equipment and
structures, as well as consumers' and government
durables, land, and inventories. The capital
service flows from durable goods employed
by households and governments enter measures
of both output and input. A steadily rising proportion
of these service flows are associated
with investments in IT. Investments in IT by
business, household, and government sectors
must be included in the GDP, along with household
and government IT capital services, in
order to capture the full impact of IT on the U.S.
economy.

Table 3 gives capital stocks from 1948 to
1999, as well as price indexes for total domestic
tangible assets and IT assets-computers, software,
and communications equipment. The estimate
of domestic tangible capital stock in
Table 3 is $35.4 trillions in 1999, considerably
greater than the $27.9 trillions in fixed capital
estimated by Shelby W. Herman (2000) of
BEA. The most important differences reflect the
inclusion of inventories and land in Table 3.
Business IT investments, as well as purchases
of computers, software, and communications
equipment by households and governments,
have grown spectacularly in recent years, but
remain relatively small. The stocks of all IT
assets combined account for only 4.35 percent
of domestic tangible capital stock in 1999. Table
4 presents estimates of the flow of capital
services and corresponding price indexes for
1948-1999.

The difference between growth in capital services
and capital stock is the improvement in
capital quality. This represents the substitution
towards assets with higher marginal products.
The shift toward IT increases the quality of
capital, since computers, software, and communications
equipment have relatively high marginal
products. Capital stock estimates fail to
account for this increase in quality and substantially
underestimate the impact of IT investment
on growth.

The growth of capital quality is slightly less
than 20 percent of capital input growth for the
period 1948-1995. However, improvements in
capital quality have increased steadily in relative
importance. These improvements jumped
to 44.9 percent of total growth in capital input
during the period 1995-1999, reflecting very
rapid restructuring of capital to take advantage
of the sharp acceleration in the IT price decline.
Capital stock has become progressively less accurate
as a measure of capital input and is now
seriously deficient.

Figure 5 gives the IT capital service flows as
a share of gross domestic income. The second
panel of Table 2 summarizes the growth rates of
prices and quantities of capital inputs for 1990-
1995 and 1995-1999. Growth of IT capital services
jumps from 11.51 percent per year in
1990-1995 to 19.41 percent in 1995-1999,
while growth of non-IT capital services increases
from 1.72 percent to 2.94 percent. This
reverses the trend toward slower capital growth
through 1995.

C. Labor Services

This subsection presents estimates of labor
input for the U.S. economy from 1948 to 1999.
These incorporate individual data from the Censuses
of Population for 1970, 1980, and 1990,
as well as the annual Current Population Surveys.
Constant quality indexes for the price and
25 See, for example, Jorgenson et al. (1987 pp. 40-49)
and Jorgenson and Griliches (1996).
26 Jorgenson and Stiroh (2000b Table B4 pp. 196-97)
give the depreciation rates employed in this study. Barbara
M. Fraumeni (1997) describes depreciation rates used in the
NIPA. Jorgenson (2000) surveys empirical studies of depre-
ciation.

27 See Jorgenson and Yun (2001). Diewert and Denis A.
Lawrence (2000) survey measures of the price and quantity
of capital input.


### ---Economics-2001-0-15.txt---



### ---Economics-2001-0-16.txt---
TABLE 4- INFORMATION TECHNOLOGY CAPITAL SERVICES AND GROSS DOMESTIC INCOME
Gross domestic

Computer Software Communications Total IT income
Year Value Price Value Price Value Price Value Price Value Price
1948 1.7 1.20 1.7 4.31 307.7 0.14

1949 1.3 0.79 1.3 2.83 297.0 0.14

1950 1.8 0.91 1.8 3.27 339.0 0.15

1951 2.1 0.90 2.1 3.21 370.6 0.15

1952 2.6 0.94 2.6 3.36 387.4 0.15

1953 3.2 0.96 3.2 3.46 418.2 0.15

1954 2.7 0.70 2.7 2.49 418.3 0.15

1955 3.6 0.85 3.6 3.05 461.3 0.16

1956 4.2 0.87 4.2 3.12 484.7 0.17

1957 3.7 0.68 3.7 2.44 503.6 0.17

1958 4.1 0.68 4.1 2.45 507.2 0.17

1959 0.2 444.36 0.1 0.63 5.2 0.80 5.5 2.87 551.9 0.18
1960 0.2 433.59 0.1 0.62 5.4 0.75 5.6 2.68 564.9 0.18
1961 0.3 637.21 0.1 0.58 5.6 0.71 6.0 2.59 581.8 0.18
1962 0.4 508.68 0.2 0.62 6.6 0.76 7.2 2.71 623.3 0.19
1963 0.6 311.81 0.3 0.58 6.5 0.67 7.3 2.34 666.9 0.20
1964 0.8 211.28 0.4 0.60 7.1 0.67 8.3 2.26 726.5 0.21
1965 1.3 182.17 0.6 0.59 9.1 0.78 11.0 2.52 795.1 0.22
1966 2.2 173.57 1.0 0.64 9.6 0.73 12.8 2.40 871.3 0.23
1967 2.3 110.97 1.1 0.50 9.8 0.66 13.2 2.01 918.2 0.23
1968 2.6 87.05 1.6 0.60 10.2 0.61 14.5 1.86 973.0 0.24
1969 2.8 68.23 1.7 0.52 11.3 0.61 15.8 1.76 1,045.8 0.25
1970 3.6 65.38 2.3 0.56 13.3 0.65 19.1 1.83 1,105.2 0.26
1971 5.2 72.48 3.7 0.77 14.9 0.67 23.9 1.99 1,178.8 0.27
1972 4.9 48.57 4.0 0.71 16.6 0.69 25.4 1.85 1,336.2 0.30
1973 4.4 33.06 4.5 0.71 22.8 0.88 31.7 2.04 1,502.5 0.32
1974 6.6 38.82 5.1 0.70 20.3 0.72 32.0 1.84 1,605.9 0.34
1975 5.9 28.43 6.7 0.80 23.2 0.77 35.7 1.85 1,785.8 0.37
1976 6.6 26.07 7.7 0.81 25.0 0.78 39.2 1.84 2,017.5 0.41
1977 7.0 20.69 8.4 0.82 41.8 1.20 57.2 2.40 2,235.7 0.44
1978 11.8 22.49 9.7 0.86 35.5 0.93 57.0 2.07 2,517.7 0.47
1979 11.6 13.33 11.6 0.90 47.9 1.14 71.1 2.15 2,834.9 0.51
1980 16.6 11.81 13.6 0.91 42.0 0.90 72.2 1.82 2,964.5 0.53
1981 17.7 7.89 15.5 0.90 40.5 0.79 73.6 1.53 3,285.2 0.58
1982 19.6 5.93 17.6 0.89 43.1 0.77 80.3 1.41 3,445.4 0.60
1983 26.4 5.46 20.6 0.91 49.4 0.82 96.4 1.43 3,798.8 0.66
1984 36.1 4.87 25.4 0.96 54.3 0.83 115.7 1.41 4,288.1 0.71
1985 39.6 3.70 30.6 0.99 63.1 0.89 133.3 1.35 4,542.6 0.73
1986 43.1 3.04 35.3 0.99 69.3 0.89 147.6 1.27 4,657.4 0.73
1987 53.4 2.93 42.1 1.04 86.5 1.02 181.9 1.36 5,078.1 0.77
1988 52.7 2.31 50.5 1.10 104.1 1.14 207.3 1.36 5,652.0 0.81
1989 57.6 2.08 60.4 1.13 105.8 1.07 223.8 1.29 5,988.8 0.84
1990 64.7 2.01 67.2 1.08 109.8 1.04 241.7 1.25 6,284.9 0.86
1991 64.2 1.76 70.8 1.00 104.2 0.93 239.2 1.12 6,403.3 0.88
1992 71.7 1.66 89.9 1.11 112.2 0.96 273.7 1.16 6,709.9 0.91
1993 77.8 1.45 90.4 0.98 126.9 1.03 295.1 1.11 6,988.8 0.92
1994 80.1 1.19 109.5 1.05 142.4 1.10 331.9 1.10 7,503.9 0.96
1995 99.3 1.12 115.5 0.99 160.7 1.16 375.6 1.09 7,815.3 0.96
1996 123.6 1.00 131.9 1.00 149.0 1.00 404.5 1.00 8,339.0 1.00
1997 134.7 0.76 156.2 1.02 157.1 0.98 448.1 0.92 9,009.4 1.04
1998 152.5 0.59 178.2 0.97 162.0 0.93 492.6 0.82 9,331.1 1.04
1999 157.7 0.42 204.4 0.91 175.3 0.91 537.4 0.72 9,817.4 1.06
Notes: Values are in billions of current dollars. Prices are normalized to one in 1996.


### ---Economics-2001-0-17.txt---
quantity of labor input account for the heterogeneity
of the workforce across sex, employment
class, age, and education levels. This
follows the approach of Jorgenson et al. (1987).
The estimates have been revised and updated by
Mun S. Ho and Jorgenson (2000).28

The distinction between labor input and labor
hours is analogous to the distinction between
capital services and capital stock. The growth in
labor quality is the difference between the
growth in labor input and hours worked. Labor
quality reflects the substitution of workers with
high marginal products for those with low marginal
products. Table 5 presents estimates of
labor input, hours worked, and labor quality.
The value of labor expenditures in Table 5 is
$5.8 trillions in 1999, 59.3 percent of the value
of output. This share accurately reflects the concept
of gross domestic income, including imputations
for the value of capital services in
household and government sectors. As shown in
Table 2, the growth rate of labor input accelerated
to 2.18 percent for 1995-1999 from 1.70
percent for 1990-1995. This is primarily due to
the growth of hours worked, which rose from
1.17 percent for 1990-1995 to 1.98 percent for
1995-1999, as labor-force participation increased
and unemployment rates plummeted.

The growth of labor quality has declined considerably
in the late 1990's, dropping from 0.53
percent for 1990-1995 to 0.20 percent for
1995-1999. This slowdown captures wellknown
demographic trends in the composition
of the workforce, as well as exhaustion of the
pool of available workers. Growth in hours
worked does not capture these changes in laborquality
growth and is a seriously misleading
measure of labor input.

III. The American Growth Resurgence
The American economy has undergone a remarkable
resurgence since the mid-1990's with
accelerating growth in output, labor productivity,
and total factor productivity. The purpose of
this section is to quantify the sources of growth
for 1948-1999 and various subperiods. An


### ---Economics-2001-0-18.txt---
TABLE 5-LABOR SERVICES

Labor services Weekly Hourly Hours

Year Price Quantity Value Quality Employment hours compensation worked
1948 0.08 1,924.6 156.1 0.75 61,536 39.1 1.2 125,127
1949 0.09 1,860.0 171.5 0.75 60,437 38.5 1.4 121,088
1950 0.09 1,961.0 179.2 0.76 62,424 38.5 1.4 125,144
1951 0.10 2,133.0 214.4 0.78 66,169 38.7 1.6 133,145
1952 0.10 2,197.2 227.2 0.79 67,407 38.5 1.7 135,067
1953 0.11 2,254.3 241.8 0.80 68,471 38.3 1.8 136,331
1954 0.11 2,190.3 243.9 0.81 66,843 37.8 1.9 131,477
1955 0.11 2,254.9 256.7 0.81 68,367 37.8 1.9 134,523
1956 0.12 2,305.0 275.0 0.82 69,968 37.5 2.0 136,502
1957 0.13 2,305.1 295.5 0.83 70,262 37.0 2.2 135,189
1958 0.14 2,245.3 309.1 0.83 68,578 36.7 2.4 130,886
1959 0.14 2,322.1 320.1 0.84 70,149 36.8 2.4 134,396
1960 0.15 2,352.2 344.1 0.84 71,128 36.5 2.5 135,171
1961 0.15 2,378.5 355.0 0.86 71,183 36.3 2.6 134,451
1962 0.15 2,474.1 376.7 0.87 72,673 36.4 2.7 137,612
1963 0.15 2,511.4 386.2 0.88 73,413 36.4 2.8 139,050
1964 0.16 2,578.1 417.6 0.88 74,990 36.3 3.0 141,447
1965 0.17 2,670.6 451.9 0.89 77,239 36.3 3.1 145,865
1966 0.18 2,788.5 500.3 0.89 80,802 36.0 3.3 151,448
1967 0.19 2,842.4 525.5 0.90 82,645 35.7 3.4 153,345
1968 0.20 2,917.0 588.3 0.91 84,733 35.5 3.8 156,329
1969 0.22 2,992.1 646.6 0.91 87,071 35.4 4.0 160,174
1970 0.23 2,938.6 687.3 0.91 86,867 34.9 4.4 157,488
1971 0.26 2,924.9 744.5 0.90 86,715 34.8 4.7 156,924
1972 0.27 3,011.7 817.6 0.91 88,838 34.8 5.1 160,873
1973 0.29 3,135.0 909.4 0.91 92,542 34.8 5.4 167,271
1974 0.31 3,148.2 988.5 0.91 94,121 34.2 5.9 167,425
1975 0.35 3,082.9 1,063.9 0.92 92,575 33.8 6.5 162,879
1976 0.38 3,174.4 1,194.0 0.92 94,922 33.9 7.1 167,169
1977 0.41 3,277.4 1,334.5 0.92 98,202 33.8 7.7 172,780
1978 0.44 3,430.3 1,504.2 0.92 102,931 33.8 8.3 180,842
1979 0.47 3,554.7 1,673.2 0.92 106,463 33.7 9.0 186,791
1980 0.52 3,535.7 1,827.9 0.92 107,061 33.3 9.9 185,591
1981 0.55 3,563.8 1,968.8 0.93 108,050 33.2 10.6 186,257
1982 0.60 3,519.7 2,096.3 0.93 106,749 32.9 11.5 182,772
1983 0.63 3,586.7 2,269.8 0.94 107,810 33.1 12.2 185,457
1984 0.66 3,786.7 2,499.1 0.94 112,604 33.2 12.9 194,555
1985 0.69 3,882.9 2,679.0 0.95 115,205 33.1 13.5 198,445
1986 0.75 3,926.3 2,931.1 0.95 117,171 32.9 14.6 200,242
1987 0.74 4,075.1 3,019.7 0.96 120,474 32.9 14.6 206,312
1988 0.75 4,207.7 3,172.2 0.96 123,927 32.9 15.0 211,918
1989 0.80 4,348.4 3,457.8 0.97 126,755 33.0 15.9 217,651
1990 0.84 4,381.5 3,680.8 0.97 128,341 32.9 16.8 219,306
1991 0.88 4,322.0 3,800.2 0.98 127,080 32.5 17.7 214,994
1992 0.94 4,353.9 4,086.9 0.98 127,238 32.6 19.0 215,477
1993 0.96 4,497.4 4,297.7 0.99 129,770 32.8 19.5 221,003
1994 0.96 4,628.3 4,453.1 0.99 132,799 32.9 19.6 226,975
1995 0.98 4,770.7 4,660.5 1.00 135,672 33.0 20.0 232,545
1996 1.00 4,861.7 4,861.7 1.00 138,018 32.8 20.6 235,798
1997 1.03 4,987.9 5,122.0 1.00 141,184 33.0 21.1 242,160
1998 1.08 5,108.8 5,491.5 1.00 144,305 33.0 22.2 247,783
1999 1.12 5,204.8 5,823.4 1.00 147,036 32.9 23.1 251,683
Notes: Value is in billions of current dollars. Quantity is in billions of 1996 dollars. Price and quality are normalized to one
in 1996. Employment is in thousands of workers. Weekly hours is hours per worker, divided by 52. Hourly compensation is
in current dollars. Hours worked are in millions of hours.


### ---Economics-2001-0-19.txt---
important objective is to account for the sharp
acceleration in the level of economic activity
since 1995 and, in particular, to document the
role of information technology.

The appropriate framework for analyzing
the impact of information technology is the
production possibility frontier, giving outputs
of IT investment goods as well as inputs of IT
capital services. An important advantage of
this framework is that prices of IT outputs and
inputs are linked through the price of IT capital
services. This framework successfully
captures the substitutions among outputs and
inputs in response to the rapid deployment of
IT. It also encompasses costs of adjustment,
while allowing financial markets to be modeled
independently.

As a consequence of the swift advance of
information technology, a number of the most
familiar concepts in growth economics have
been superseded. The aggregate production
function heads this list. Capital stock as a measure
of capital input is no longer adequate to
capture the rising importance of IT. This completely
obscures the restructuring of capital input
that is such an important wellspring of the
growth resurgence. Finally, hours worked must
be replaced as a measure of labor input.
A. Production Possibility Frontier

The production possibility frontier describes
efficient combinations of outputs and inputs for
the economy as a whole.29 Aggregate output Y
consists of outputs of investment goods and
consumption goods. These outputs are produced
from aggregate input X, consisting of
capital services and labor services. Productivity
is a "Hicks-neutral" augmentation of aggregate
input.

The production possibility frontier takes the
form:

(1) Y(In 9 IC , Is, It,9 Cn 9 Cc)

where the outputs include non-IT investment
goods I,, and investments in computers Ic, software
IS' and communications equipment It, as
well as non-IT consumption goods and services
Cn and IT capital services to households and
governments Cc. Inputs include non-IT capital
services Kn and the services of computers Kc,
software Ks, and telecommunications equipment
K, as well as labor input L.30 Totalfactor
productivity (TFP) is denoted by A.
The most important advantage of the production
possibility frontier is the explicit role that it
provides for constant quality prices of IT products.
These are used as deflators for nominal
expenditures on IT investments to obtain the
quantities of IT outputs. Investments in IT are
cumulated into stocks of IT capital. The flow of
IT capital services is an aggregate of these
stocks with service prices as weights. Similarly,
constant quality prices of IT capital services are
used in deflating the nominal values of consumption
of these services.

Another important advantage of the production
possibility frontier is the incorporation of costs of
adjustment. For example, an increase in the output
of IT investment goods requires forgoing part of
the output of consumption goods and non-IT investment
goods, so that adjusting the rate of investment
in IT is costly. However, costs of

adjustment are external to the producing unit and
are fully reflected in If prices. These prices incorporate
forward-looking expectations of the future
prices of If capital services.

B. Aggregate Production Function

The aggregate production function employed
by Robert M. Solow (1957, 1960) and, more recently,
by Jeremy Greenwood et al. (1997, 2000),
Arnold C. Harberger (1998), and Hercowitz
(1998) is a competing methodology. The production
function gives a single output as a function of
capital and labor inputs. There is no role for separate
prices of investment and consumption goods
and, hence, no place for constant quality IF price
indexes for outputs of IF investment goods.
Greenwood et al. employ a price index for
consumption to deflate the output of all


### ---Economics-2001-0-20.txt---
investment goods, including information technology.
Confronted by the fact that constant
quality prices of investment goods differ from
consumption goods prices, they borrow the concept
of embodiment from Solow (1960) in order
to convert investment goods output into an appropriate
form for measuring capital stock.31
Investment has two prices, one used in the measuring
output and the other used in measuring
capital stock. This inconsistency can be removed
by simply distinguishing between outputs
of consumption and investment goods, as
in the national accounts and equation (1). The
concept of embodiment can then be dropped.
Perhaps inadvertently, Greenwood et al. have
revisited the controversy accompanying the introduction
of a constant quality price index for
computers into the national accounts. They have
revived Denison's (1993) proposal to use a consumption
price index to deflate investment in
the NIPA. Denison found this appealing as a
means of avoiding the introduction of constant
quality price indexes for computers. Denison's
approach leads to a serious underestimate of
GDP growth and an overestimate of inflation.
Another limitation of the aggregate production
function is that it fails to incorporate costs
of adjustment. Robert E. Lucas, Jr. (1967) presented
a production model with internal costs of
adjustment. Fumio Hayashi (2000) shows how
to identify these adjustment costs from James
Tobin's (1969) Q-ratio, the ratio of the stock
market value of the producing unit to the market
value of the unit's assets. Implementation of
this approach requires simultaneous modeling
of production and asset valuation. If costs of
adjustment are external, as in the production
possibility frontier (1), asset valuation can be
modeled separately from production.32
C. Sources of Growth

Under the assumption that product and factor
markets are competitive, producer equilibrium
implies that the share-weighted growth
of outputs is the sum of the share-weighted
growth of inputs and growth in total factor
productivity:

(2) wi,,,,A In I,, + wi',cA In IC + WiP,sA In Is
+ wP,t/A In It + wc,A In C,,

+Wc,cAIn Cc

- VK,nA In K,, + VK,CA In KC

+ VK,sA In Ks + VK,tA In Kt

+ VLA In L + A In A

where wP and -v denote average value shares. The
shares of outputs and inputs add to one under
the additional assumption of constant returns,
WI,n + WI,c + WI's + WI,t + WC,1n + WC,c
VK,n + VK,c + VK,s + VK,t + VL 1.

Equation (2) makes it possible to identify the
contributions of outputs as well as inputs to U.S.
economic growth. The growth rate of output is
a weighted average of growth rates of investment
and consumption goods outputs. The contribution
of each output is its weighted growth
rate. Similarly, the growth rate of input is a
weighted average of growth rates of capital and
labor services and the contribution of each input
is its weighted growth rate. The contribution of
TFP, the growth rate of the augmentation factor
A in equation (2), is the difference between
growth rates of output and input.

Table 6 presents results of a growth accounting
decomposition, based on equation (2), for
the period 1948-1999 and various subperiods,
following Jorgenson and Stiroh (1999, 2000b).
Economic growth is broken down by output and
input categories, quantifying the contribution of
information technology to investment and consumption
outputs, as well as capital inputs.
These estimates identify computers, software,
and communications equipment as distinct
types of information technology.

Rearranging equation (2), the results can be
presented in terms of average labor productivity
(ALP), defined as y = YIH, the ratio of
output Y to hours worked H, and k = KIH is
the ratio of capital services K to hours worked:
(3) A Iny== VKA Ink

+ VL(A InL - A In H) + A In A.

3' Karl Whelan (1999) also employs Solow's concept of
embodiment.

32 See, for example, John Y. Campbell and Robert J.
Shiller (1998).


### ---Economics-2001-0-21.txt---
Equation (3) allocates ALP growth among three
sources. The first is capital deepening, the
growth in capital input per hour worked, and
reflects the capital-labor substitution. The second
is improvement in labor quality and captures
the rising proportion of hours by workers
with higher marginal products. The third is TFP
growth, which contributes point-for-point to
ALP growth.

D. Contributions of IT Investment

Figure 5 depicts the rapid increase in the
importance of IT services, reflecting the accelerating
pace of IT price declines. In 1995-1999
the capital service price for computers fell 24.81
percent per year, compared to an increase of
36.36 percent in capital input from computers.
As a consequence, the value of computer services
grew substantially. However, the current
dollar value of computers was only 1.6 percent
of gross domestic income in 1999.

The rapid accumulation of software appears
to have different sources. The price of software
services has declined only 2.04 percent per year
for 1995-1999. Nonetheless, firms have been
accumulating software very rapidly, with real
capital services growing 16.30 percent per year.
A possible explanation is that firms respond to
computer price declines by investing in complementary
inputs like software. However, a more
plausible explanation is that the price indexes
used to deflate software investment fail to hold
quality constant. This leads to an overstatement
of inflation and an understatement of growth.
Although the price decline for communications
equipment during the period 1995-1999 is
comparable to that of software, investment in
this equipment is more in line with prices. However,
prices of communications equipment also
fail to hold quality constant. The technology of
switching equipment, for example, is similar to
that of computers; investment in this category is
deflated by a constant-quality price index developed
by BEA. Conventional price deflators are
employed for transmission gear, such as fiberoptic
cables. This leads to an underestimate of
the growth rates of investment, capital stock,
capital services, and the GDP, as well as an
overestimate of the rate of inflation.
Figures 6 and 7 highlight the rising contributions
IT outputs to U.S. economic growth. Figure
6 shows the breakdown between IT and
non-IT outputs for subperiods from 1948 to
1999, while Figure 7 decomposes the contribution
of IT into its components. Although the


### ---Economics-2001-0-22.txt---
4.5 .

..

...:,%,.'....

.,". .

... .. - .... .. 1,

......... ...'.

. .......i

.-.. . i, . .. - "'. .... Rm.

... .:

...-, ..' . :.::. ''

%.-....:. - - -

... ...',%.. ''... i.- , '':, ;-

- -

''"

'' ".''.:.. -

.:.' ... ....................................,- ........: -'- :- ...,:.:,..."...........:..,....
... .1

'

...-.-......,..,..,...-.........,....,..::.,........, ...'........ .........:...'.:
.

---'---.--'-:'-'

''" ""' ,,,,,,

... .....: - --::-: .. - :

%. . ..

...

.. ' '' . ... .:-'-.-

... ...-I.. :.:.

'%. . ..

."..",,'..',..".',.,".,.

..... ... ..:.!'.:-.-

...:,., -'.'-.''-"":.-- ...........-..........--........
........::'......

.. -- ::. ...".. ......... ...........'..
.... .:. -'.--.-..'.'.-'-..'-'.-'.-'. -..-'.'--.".-.-..'-'..-.'.-.
...-...-......-.---...-.-..-.--.-..'.-.-.--............................-..
.

':

.:.:.

.....-..... .,.:,.""'""',-"""'...... .::
-:------'-.--' .. " ". - .,.,.:, ... .:..:..'
'

.11. ..

....

'.". ' '.' :..

4 .0 - '' %. I"'.

...

.. ... -'

....... %.

"....

......' ....

- '

I

%...

.

.

..

. . . .

.

""""'"

.

. . .. :

""' '"' I.......: '- ."--, ..' -' .,-.,: ..... - - .
,

..."..". "........." -1-:'-1 .:..:.
- .:.-... -- -. - - .. ... .

..'..-'..-.".'..-.-. .

. .

. . .

.

'.. - - , -', ,,

: ..

3.5 - ..... .

,-." -" - , -, ..' -'-

.

-..

.........

- -............

. . .

- -

... '

- -

:%.'. .

,,

..'.:

.... . ... ....

1

. .... .

. .

' ""

-

- - - ....

....-,.- :' -'? .... . . - . ... .. ......... ... ......:
' '

: .... ......

'"

". . ....1

......,.. ....%

.. .:- . -.:. - - - '-.'.....-..'...-..-..'.'.....,..,.,,-.....,."-....,.,.,,.."-,..:.'-.......
-.- : '.'. -, ....% ..

'----.--------1- - ::.,...,.......,..,.....'..'...-."...,...,...'.,'..'.....'...'..".."..'....'.."......-...'.....'...--.-:--.-.--.-.., "...... .:. ...... ...... .:--.-: :: ...' ':. , -'.
3.0 -

.. . ..:% ...... ......

.... .. - -. .".'.

. .. ...,..,...,.,..,,.

.,...,,,..,.,..,,..,,.,,.,,.,,.,...,..,,.,.,..,.,,.,..,,.,..,.,"..,,.,..- ". ' "'':.,
... .....................................-....-.......-...........................-.-
, ,............................................
.........:,,....,..,.....,,....,.::....,..,..........,...-........,............,.
.... ..... .. ... - .. .. .... ..

...... ... ....... - ..... ....

--:-:%%:'--.------'.'....... .. ...........
......

...............

.....

......

.....

...... :'

....:-' ' .'.'.'.-"'.'..."".-.'..'..""..'.".'."-'.'...'-.'..'.-'..'L- .......:... :- ..,...,.....:.-.".,.".."
:.... ...........

..''...:..--

.,. .:....%

...- . . .. ..... ... .. . .. ..

. .- ':: -.'.

.

2 .5 - '--' %. . .

.....--.- ..-...-... -.'

, ,

..%-

'I -"

-'---- - .:----..:..:. . . .%

. - - .

.. . . .... .. . . . .

' '

.. . . % -

.

"---'-.'---':'-'.- .. .

....

'

.. . . . . . ..... ..:

-:% .. ........,

-

'...-.,",..i..,..,...-...'.'..'...'..'..,."......'
%%...... ... .. . . ....... .- ... ...
' . .

'

.

' '

' ' "

-'.. 1. .. " ..'. . : . .

% ' "

... ...

,.-:' .... .'..."....% ...

-.......'.' ::. % '% ..-.. : -,-

'

2 .0 f '......'.."..'...".."

1-.... ----,-

..'.... . ..,. ...

.... ..'i.'..'.,.'....,:..,.,.l......'..'......'.'.
,:".-%:-'-,-::- -, "...... ....

--.':.'-'. ..... ....... .... .... .. :%. .%......
' '

.

"

. %

%

-

..: ..

.. i:

.

. .

- ''

.-1-'':::--'.-'---:-'-.-:-'.- ..il -.%
.: .. .,.- -;------.--% .-:---.-: .:-.. ' :.::......
.. ............................ ... .. --;;'.---'--- .. . --: ..' %. ..'. ;.'-'.--.-;,-- ..
1.5 -

i.,...".."".,.".",.,"."'."".,'.

...-?. ..................................
.. . . .....

.. %': ..,.,..,,.,.,,.,,.

.... .......................

:'.... %::..,...,..::"."..,...".,.,,..
-- %.." ' ",.'',.",..."....

.- ..

.

:

... ... ...- .....

%

.

.0 -:, .

- .

.. '-.' ..

.':...:.. :...:.,.%....:: .-..... ..
"-'--'--'-'..%... .:...%. -..... ': :...'. :
..

.

.

'

.

.-.-I

,...,.::.... "'%.

,

.

..-. ..

,

. .

.- -' "....---:....-%.'-..-

' ,

%... , :

'-' : -.".'-L-- ...: .:

.

.'

.. ......:.

0.5 . ..

.%

-

.

. .. %..

.-'

.. .

. ..

-

.. %

"-

.".'

...

..

": ....

'' '

.%:..:..

,

...

- -

'

..

-

..

0.0 .

1948-73 1973-W

SNon-ITConsumpbon MITConsumpd

FiGuRE 6. OUTPUT CONTRIBUTIO

Note: Output contributions are the average annual (pe
2.00 ,

.

.

.

.

.

.

.

.

..''....

.

,,

.... -::"

- . .

. . .

-

-' -'.' -, :' . .

. .

. . . . .

. ' .

,

. . .

. .

. : .. .

.

. . .. .

. .

.: -...::. .

.6 0 - . ... %. .. %. % : .

.

.

,...-.

...,. . ....-: .

'.. - : -:-:-:- ...'... .':.. - !..'... ..': "
,'

'

,,......

'''


### ---Economics-2001-0-23.txt---



### ---Economics-2001-0-24.txt---
importance of IT has steadily increased, Figure
6 shows that the recent investment and consumption
surge nearly doubled the output contribution
of IT. Figure 7 shows that computer
investment is the largest single IT contributor in
the late 1990's, but that investments in software
and communications equipment are becoming
increasingly important.

Figures 8 and 9 present a similar decomposition
of IT inputs into production. The contribution
of these inputs is rising even more
dramatically. Figure 8 shows that the contribution
of IT now accounts for more than 48.1
percent of the total contribution of capital input.
Figure 9 shows that computer hardware is the
largest IT contributor on the input side, reflecting
the growing share and accelerating growth
rate of computer investment in the late 1990's.
Private business investment predominates in
the output of IT, as shown by Jorgenson and
Stiroh (1999, 2000b).33 Household purchases of
IT equipment and services are next in importance.
Government purchases of IT equipment
and services, as well as net exports of IT products,
must be included in order to provide a
complete picture. Firms, consumers, governments,
and purchasers of U.S. exports are responding
to relative price changes, increasing
the contributions of computers, software, and
communications equipment.

Table 2 shows that the price of computer
investment fell by more than 32 percent per
year, the price of software 2.4 percent, the price
of communications equipment 2.9 percent, and
the price of IT services 11.8 percent during the
period 1995-1999, while non-IT prices rose 2.2
percent. In response to these price changes,
firms, households, and governments have accumulated
computers, software, and communications
equipment much more rapidly than other
forms of capital.

E. Total Factor Productivity

The price or "dual" approach to productivity
measurement makes it possible to identify the
role of IT production as a source of productivity
growth at the industry level.34 The rate of productivity
growth is measured as the decline in
the price of output, plus a weighted average of
the growth rates of input prices with value
shares of the inputs as weights. For the computer
industry this expression is dominated by
two terms: the decline in the price of computers
and the contribution of the price of semiconductors.
For the semiconductor industry the expression
is dominated by the decline in the price of
semiconductors.35

Jorgenson et al. (1987) have employed
Domar's (1961) model to trace aggregate productivity
growth to its sources at the level of
individual industries.36 More recently, Harberger
(1998), William Gullickson and Michael
J. Harper (1999), and Jorgenson and Stiroh
(2000a, 2000b) have used the model for similar
purposes. Productivity growth for each industry
is weighted by the ratio of the gross output of
the industry to GDP to estimate the industry
contribution to aggregate TFP growth.
If semiconductor output were only used to
produce computers, then its contribution to
computer-industry productivity growth, weighted
by computer-industry output, would precisely
cancel its independent contribution to aggregate
TFP growth. This is the ratio of the value of
semiconductor output to GDP, multiplied by the
rate of semiconductor price decline. In fact, semiconductors
are used to produce telecommunications
equipment and many other products.

However, the value of semiconductor output is
dominated by inputs into IT production.
The Domar aggregation folmula can be approximated
by expressing the declines in prices of
computers, communications equipment, and software
relative to the price of gross domestic income,
an aggregate of the prices of capital and
labor services. The rates of relative IT price decline
are weighted by ratios of the outputs of IT
products to the GDP. Table 7 reports details of this
TFP decomposition for 1948-1999; the IT and
non-IT contributions are presented in Figure
33 Bosworth and Triplett (2000) compare the results of
Jorgenson and Stiroh (2000b) with those of Oliner and
Sichel (2000).

34 The dual approach is presented by Jorgenson et al.
(1987 pp. 53-63).

35 Dulberger (1993), Triplett (1996), and Oliner and
Sichel (2000) present models of the relationships between
computer and semiconductor industries. These are special
cases of the Evsey Domar (1961) aggregation scheme.
36 See Jorgenson et al. (1987 pp. 63-66, 301-22).


### ---Economics-2001-0-25.txt---
10. The IT products contribute 0.50 percentage
points to TFP growth for 1995-1999, compared to
0.25 percentage points for 1990-1995. This reflects
the accelerating decline in relative price
changes resulting from shortening the product cycle
for semiconductors.

F. Output Growth

This subsection presents the sources of GDP
growth for the entire period 1948 to 1999. Capital
services contribute 1.70 percentage points,
labor services 1.14 percentage points, and TFP
growth only 0.61 percentage points. Input
growth is the source of nearly 82.3 percent of
U.S. growth over the past half century, while
TFP has accounted for 17.7 percent. Figure
11 shows the relatively modest contributions of
TFP in all subperiods.

More than three-quarters of the contribution
of capital reflects the accumulation of capital
stock, while improvement in the quality of capital
accounts for about one-quarter. Similarly,
increased labor hours account for 80 percent of
labor's contribution; the remainder is due to
improvements in labor quality. Substitutions
among capital and labor inputs in response to
price changes are essential components of the
sources of economic growth.

A look at the U.S. economy before and after
1973 reveals familiar features of the historical
record. After strong output and TFP growth in
the 1950's, 1960's, and early 1970's, the U.S.
economy slowed markedly through 1990, with
output growth falling from 3.99 percent to 2.86
percent and TFP growth declining from 0.92
percent to 0.25 percent. Growth in capital inputs
also slowed from 4.64 percent for 1948-1973 to
3.57 percent for 1973-1990. This contributed to
sluggish ALP growth-2.82 percent for 1948-
1973 and 1.26 percent for 1973-1990.
Relative to the early 1990's, output growth increased
by 1.72 percent in 1995-1999. The contribution
of IT production almost doubled, relative
to 1990-1995, but still accounted for only 28.9
percent of the increased growth of output. Although
the contribution of IT has increased
steadily throughout the period 1948-1999, there
has been a sharp response to the acceleration in the
IT price decline in 1995. Nonetheless, more than
70 percent of the increased output growth can be
attributed to non-IT products.

Between 1990-1995 and 1995-1999 the contribution
of capital input jumped by 0.95


### ---Economics-2001-0-26.txt---
1.00

0.80.II IE

0.60

0.40

0.20

0.00

-0.20 --

1948-i3 197340 1990405 1995-99

ONon-IT P cMl arT Produ

FIGURE 10. CONTRIBUTIONS OF INFORMATION TEcHNoLoGY To TOTAL FACTOR PRODucTIviTy GRowTH
Note: Contributions are average annual (percentage) relative price changes, weighted by average nominal output shares from
Table 7.

percentage points, the contribution of labor input
rose by only 0.24 percent, and TFP accelerated by
0.51 percent. Growth in ALP rose 0.92 as more
rapid capital deepening and growth in TFP offset
slower improvement in labor quality. Growth in
hours worked accelerated as unemployment fell to
a 30-year low. Labor markets have tightened considerably,
even as labor-force participation rates
increased.37

The contribution of capital input reflects the
investment boom of the late 1990's as businesses,
households, and governments poured

resources into plant and equipment, especially
computers, software, and communications
equipment. The contribution of capital, predominantly
IT, is considerably more important than
the contribution of labor. The contribution of IT
capital services has grown steadily throughout
the period 1948-1999, but Figure 9 reflects the
impact of the accelerating decline in IT prices.
After maintaining an average rate of 0.25 percent
for the period 1973-1990, TFP growth fell to
0.24 percent for 1990-1995 and then vaulted to
0.75 percent per year for 1995-1999. This is a
major source of growth in output and ALP for the
U.S. economy (Figures 11 and 12). While TFP
growth for 1995-1999 is lower than the rate of
1948-1973, the U.S. economy is recuperating
from the anemic productivity growth of the past
two decades. Although only half of the acceleration
in TFP from 1990-1995 to 1995-1999 can be
attributed to IT production, this is far greater than
the 4.26 percent share of IT in the GDP.
G. Average Labor Productivity

Output growth is the sum of growth in hours
and average labor productivity. Table 8 shows
the breakdown between growth in hours and
ALP for the same periods as in Table 6. For the
period 1948-1999, ALP growth predominated
in output growth, increasing just over 2 percent
per year for 1948-1999, while hours increased
about 1.4 percent per year. As shown in equation
(3), ALP growth depends on capital deepening,
a labor-quality effect, and TFP growth.
37 Katz and Krueger (1999) analyze the recent performance
of the U.S. labor market.


### ---Economics-2001-0-27.txt---
Figure 12 reveals the well-known productivity
slowdown of the 1970's and 1980's, emphasizing
the acceleration in labor productivity growth in the
late 1990's. The slowdown through 1990 reflects
reduced capital deepening, declining labor-quality
growth, and decelerating growth in TFP. The
growth of ALP slipped further during the early
1990's with a slump in capital deepening only
partly offset by a revival in labor quality growth
and an up-tick in TFP growth. A slowdown in


### ---Economics-2001-0-28.txt---
3.50

300

2.50

200

1.50

0.00

1948-73 1973-90 1990-95 1995-09

*Labor Quality 8 Non-IT Capital Deepening EIT Captal Deepening El Non-IT Productivlty OIT Productivity
FIGURE 12. SOURCES OF AVERAGE LABOR PRODUCTIVITY GROWTH
Note: Contributions are from Table 8.
hours combined with slowing ALP growth during
1990-1995 to produce a further slide in the
growth of output. In previous cyclical recoveries
during the postwar period, output growth accelerated
during the recovery, powered by more rapid
growth of hours and ALP.

Accelerating output growth during 1995-
1999 reflects growth in labor hours and ALP
almost equally.38 Comparing 1990-1995 to
1995-1999, the rate of output growth jumped
by 1.72 percent-due to an increase in hours
worked of 0.81 percent and another increase in
ALP growth of 0.92 percent. Figure 12 shows
the acceleration in ALP growth is due to capital
deepening as well as faster TFP growth. Capital
deepening contributed 0.60 percentage points,
offsetting a negative contribution of labor quality
of 0.20 percent. The acceleration in TFP
added 0.51 percentage points.

H. Research Opportunities

The use of computers, software, and communications
equipment must be carefully distinguished
from the production of IT.39 Massive
increases in computing power, like those experienced
by the U.S. economy, have two effects
on growth. First, as IT producers become more
efficient, more IT equipment and software is
produced from the same inputs. This raises productivity
in IT-producing industries and contributes
to TFP growth for the economy as a

whole. Labor productivity also grows at both
industry and aggregate levels.

Second, investment in information technology
leads to growth of productive capacity in
IT-using industries. Since labor is working with
more and better equipment, this increases ALP
through capital deepening. If the contributions
to aggregate output are captured by capital
deepening, aggregate TFP growth is unaffected.
40 Increasing deployment of IT affects
TFP growth only if there are spillovers from
IT-producing industries to IT-using industries.
Top priority must be given to identifying the
impact of investment in IT at the industry level.
Stiroh (1998) has shown that this is concen-
38 Stiroh (2000) shows that ALP growth is concentrated
in IT-producing and IT-using industries.
39 Economics and Statistics Administration (2000 Table
3.1 p. 23) lists IT-producing industries.
40 Martin N. Baily and Robert J. Gordon (1988).


### ---Economics-2001-0-29.txt---
trated in a small number of IT-using industries,
while Stiroh (2000) shows that aggregate ALP
growth can be attributed to productivity growth
in IT-producing and IT-using industries. The
next priority is to trace the increase in aggregate
TFP growth to its sources in individual industries.
Jorgenson and Stiroh (2000a, 2000b)
present the appropriate methodology and preliminary
results.

IV. Economics on Internet Time

The steadily rising importance of information
technology has created new research opportunities
in all areas of economics. Economic historians,
led by Chandler (2000) and Paul A. David
(2000),41 have placed the information age in
historical context. The Solow (1987) Paradox,
that we see computers everywhere but in the
productivity statistics,42 has provided a point of
departure. Since computers have now left an
indelible imprint on the productivity statistics,
the remaining issue is: Does the breathtaking
speed of technological change in semiconductors
differentiate this resurgence from previous
periods of rapid growth?

Capital and labor markets have been severely
impacted by information technology. Enormous
uncertainty surrounds the relationship between
equity valuations and future growth prospects
of the American economy.43 One theory attributes
rising valuations of equities since the
growth acceleration began in 1995 to the accumulation
of intangible assets, such as intellectual
property and organizational capital. An
alternative theory treats the high valuations of
technology stocks as a bubble that burst during
the year 2000.

The behavior of labor markets also poses
important puzzles. Widening wage differentials
between workers with more and less education
has been attributed to computerization of the
workplace. A possible explanation could be that
high-skilled workers are complementary to IT,
while low-skilled workers are substitutable. An
alternative explanation is that technical change
associated with IT is skill biased and increases
the wages of high-skilled workers relative to
low-skilled workers.44

Finally, information technology is altering
product markets and business organizations, as
attested by the large and growing business literature,
45 but a fully satisfactory model of the
semiconductor industry remains to be developed.
46 Such a model would derive the demand
for semiconductors from investment in information
technology in response to rapidly falling IT
prices. An important objective is to determine
the product cycle for successive generations of
new semiconductors endogenously.

The semiconductor industry and the information
technology industries are global in their
scope with an elaborate international division of
labor.47 This poses important questions about
the American growth resurgence. Where is the
evidence of a new economy in other leading
industrialized countries? An important explanation
is the absence of constant quality price
indexes for semiconductors and information
technology in national accounting systems outside
the U.S.48 Another conundrum is that
several important participants-Korea, Malaysia,
Singapore, and Taiwan-are "newly industrializing"
economies. What does this portend

for developing countries like China and India?
As policy makers attempt to fill the widening
gaps between the information required for
sound policy and the available data, the


### ---Economics-2001-0-30.txt---
traditional division of labor between statistical
agencies and policy-making bodies is breaking
down. In the mean time, monetary policy makers
must set policies without accurate measures
of price change. Similarly, fiscal policy makers
confront ongoing revisions of growth projections
that drastically affect the outlook for future
tax revenues and government spending.
The stagflation of the 1970's greatly undermined
the Keynesian Revolution, leading to a
New Classical Counterrevolution led by Lucas
(1981) that has transformed macroeconomics.
The unanticipated American growth revival of
the 1990's has similar potential for altering economic
perspectives. In fact, this is already foreshadowed
in a steady stream of excellent books
on the economics of information technology.49
We are the fortunate beneficiaries of a new
agenda for economic research that could refresh
our thinking and revitalize our discipline.
 ## Economics-2002-0


### ---Economics-2002-0-02.txt---
Diversity is the staff of economic life. Interpersonal
differences in tastes and talents,

whether naturally endowed or environmentally
produced, give us the unique "propensity to
truck, barter, and trade" that improves our standards
of living. Every elementary economics
student knows how different endowments in an
exchange economy create potential gains from
trade, and how competitive markets efficiently
intermediate and exhaust those gains. In production
activities, work is organized in highly
specialized ways to use our human resources to
the fullest.

All this is so elementary that economists
largely take it for granted, yet much of the
complexity of modem economic life is built
upon these foundations. The variety of choices
that confront us is astonishing. No consumer
buys more than a tiny fraction of goods that are
available to be purchased. The average person is
unable to identify by name more than a handful
of goods because most are irrelevant to anyone'
s personal economic behavior. And in work
activities, each of us masters hardly any of the
immense varieties of skill required in a modem
economy. Out of the totality of what is known
by society at large, a single person knows practically
nothing, no matter how well educated or
how brilliant! Work and production knowledge
are even more specialized than consumption
choices and activities.

How do decentralized markets accommodate
the diversity of choices, tastes, and productivities
that are so important in economic affairs?
The choice of quantities (the intensive margin)
by a typical buyer or seller has dominated neoclassical
economics. Most of price theory focuses
on the determination of price and

quantities of already-defined goods, but does
little to examine the extensive margin by which
the nature of the goods is chosen. Price theory
does not provide a rich enough structure to
analyze these issues, which require a framework
that takes heterogeneity and diversity as a fundamental,
primitive construct. Location or spatial
theory is specifically designed for that task.
It is a theory of choice based on interpersonal
differences in willingness to pay for differentiated
objects perhaps best known from Adam
Smith's theory of equalizing differences.' Differentiated
products are valued according to

their various qualities and product characteristics.
Comparing reservation prices with market
prices determines specific choices of buyers and
sellers out of the vast varieties that are available.
Many successful examples have clarified
how spatial models can be applied to the economics
of variety and diversity.

Much of my research reflects my attempts
over the years to work out some of the economic
issues associated with diversity and the
implications of heterogeneity for markets and
prices.2 There are three main themes: the determination
of value in the presence of diversity,
the sorting or allocation of diverse buyers to
diverse sellers, and the effects of heterogeneity
and sorting on inequality.

I. Value, Assignment, and Inequality
How do markets accommodate inherent differences
in goods, jobs, and productive talents
of people? How are these things valued? Just as
the value of land depends on its location, it is
often possible to think of goods, jobs, and people
in terms of their addresses in a map of
productive attributes or characteristics. Some
addresses are more desirable than others and
market prices equate the supply and demand for


### ---Economics-2002-0-03.txt---
the latent characteristics of goods at each location
on the map. Thinking spatially proves especially
useful when there are many more

varieties of goods than attributes that each contains.
Because there are fewer attributes than
goods, the dimensionality of the problem is
greatly reduced and analysis can proceed on
conventional cost-benefit terms.

Examples of applications include hedonic indexes
of quality change needed to correct price
indexes for changing product characteristics;
among such products as automobiles and computers;
regression analysis of housing prices on
house characteristics, used for real estate assessments
in urban economics; the capital asset
pricing model, where asset characteristics are
the means and covariances of their rate of return
distributions; studying how labor markets evaluate
jobs of varying quality, useful for estimating
the social value of safety and environmental
goods that are not directly traded; and how and
why labor markets sustain enormous differences
in rewards and rents among people of
different talents. All are manifestations of almost
exactly the same basic spatial problem:
valuing diversity.

Market values are an important part of the
story. The allocation of diversity in the economy
is another. How are buyers and sellers
matched or assigned to each other in market
equilibrium? These marriage-type questions
bear importantly on the economic consequences
of diversity because stratification of agents is
inherent in spatial equilibrium. Certain kinds of
buyers come to be associated with certain kinds
of sellers, even when there are no externalities
and social influences in preferences. Rich people
tend to ride in a better class of automobiles
than poor people. They are more likely to live
on the lakeshore and in other high-rent districts,
to eat in fancy restaurants, wear designer
clothes, send their children to better schools,
and work in more pleasant environments. But
there are many other manifestations of the same
basic sorting or assignment issues in the presence
of diversity. Widows and orphans tend to
hold their wealth in relatively safe assets. People
from similar ethnic groups tend to live together
in city enclaves, more talented students
are apt to be found in colleges and graduate
schools that have more talented teachers,
higher-quality lawyers work on the largest legal
claims, and people exposed to higher unemployment
risks tend to live in the same

neighborhoods.

The address analogy in spatial equilibrium
often extends to these kinds of matches or assignments:
goods with special attributes appeal
to buyers with specific kinds of tastes. In product
markets, sellers design their goods to cater
to specific types of customers. And in labor
markets, each of us in our career choices and
work activities seeks a niche in the incredibly
complex machinery of modern production and
the division of labor. The number of people
seeking these slots and the nature of the technologies
that affect the personal scale of operations
affects the distribution of rewards in
society.

Markets accommodate diversity by establishing
prices that tend to make different things
relatively close substitutes at the margin. Adam
Smith's insight that market prices tend to equalize
their net advantages is fundamental to these
problems. If one good has more desirable characteristics
than another, the less preferred variety
must compensate for its disadvantages by
selling at a lower price. Supply is inelastic and
exogenous in geographic spatial theory, but in
many applications both sides of the market must
be considered jointly. Sellers choose their varieties
by comparing prices with costs. Higherquality
goods are more costly to produce and
must be offered at a higher price. In equilibrium
these extra costs can only be supported if their
incremental value to some customers is at least
as large as their incremental costs. Thus diversity
creates inequality in prices and values. The
reverse statement is also true. Certain kinds of
inequality are necessary to sustain diverse outcomes.
For example, if higher-quality goods
were not more expensive to produce than lowerquality
ones and if all consumers had the same
relative ranking on the quality of two different
goods, then only the higher-quality good would
survive in the market. The lower-quality good is
driven out by the existence of a superior one
that can be produced at equal cost.
What is less obvious is that there are social
incentives to create inequality, even when
agents are initially identical in every conceivable
way. This is a third theme of the essay. The
basic idea also derives from Smith, who argued
that personal investments in skill acquisition,


### ---Economics-2002-0-04.txt---
not inherited differences in natural abilities, are
the principal causes of wage inequality in society.
Since labor-market skills are costly to learn,
in market equilibrium their costs must be reimbursed
by offering larger expected earnings to
potential entrants, otherwise students would not
have the proper economic incentives to study
them. Since much of the cost of education and
learning are in time and opportunities forgone,
the force of interest weighs heavily in these
decisions and can cause remarkably large differences
in observed earnings, as equilibrium
phenomena.

But once a skill has been acquired, its economic
return is greatest if it is used as intensively
as possible. That the costs of acquiring
most skills are to some extent independent of
how intensively they are utilized makes it efficient
for people to specialize their skills and
trade with each other. There are huge economies
of scale in skills. Once acquired, a skill can
be used over and over again without diminishing
its stock. Indeed, the reverse may be true,
which provides incentives for students to acquire
skills early and to increase their work
hours after having become skilled.

Furthermore, individuals have different talents
and are better suited to some productive
activities than others. The principle of comparative
advantage holds true for human-capital
production as well as for international trade. It
accounts for why work is so specialized and
why each person knows such a small amount of
what is known in total. It even holds if people
are identical ex ante. Similar ideas have received
lots of attention lately in the fields of
industrial organization and international trade,
but are just as important, if not more so, for the
organization of work.

The cost basis that supports induced or "voluntary"
inequality has other interesting consequences.
Unequal rewards motivate people to

strive for superior performance and influence
their decisions to acquire skills. The two interact
because new generations of workers replace
older generations: the assignment of people to
jobs changes over the life cycle. A large share
of the growth in personal earnings over managerial
and other careers occurs at discrete promotion
points to higher-ranking positions.
Competition for promotions, to acquire greater
skill, show one's stuff, and get more powerful
and higher-paying positions, plays an important
role both in the internal dynamics of organizations
and in the overall economy. Uncertainty of
outcomes and the statistical aspects of promotion
and job assignments guarantee that competition
for superior positions occurs in every
form of economic organization. The need to use
the record of past performance to assess prospects
for other positions automatically gives
people incentives to try to influence the measures
that will put them in a superior category.
The strength of these incentives depends on
how much of a difference-in money, prestige
and perquisites-it makes to achieve a better
grade and a higher classification.

II. Valuing Diversity

Much of my research consists of applications
of the problem of analyzing markets for differentiated
products when the measure of differentiation
is naturally ordered from best to worst.
Market prices reflect both the costs and values
of the underlying attributes of goods. Agents
implicitly use cost-benefit analysis to choose
locations in the product spectrum, with buyers
comparing the market prices of alternative varieties
with their relative values in use and with
sellers comparing market prices with their relative
costs. Equality between demand and supply
for each variety sustains the market
equilibrium price-quality structure.
Consider a commodity that comes in two
different varieties. For example, there are highand
low-quality cars, better and worse houses
(or schools or neighborhoods), good jobs and
bad ones, fast and slow computers, and so on.
Let c represent all other goods consumed and let
Zh and z1 measure the high- and low-quality
characteristics of the goods in question. The
relative prices of the two varieties in terms of
other goods are Ph and Pl In the situation considered,
which is typical of many markets, individual
buyers and sellers are small compared
to the overall market and individually have no
market power. Suppose that customers purchase
either one unit of the differentiated product or
none. This is a leading case. Most people live in
exactly one neighborhood, hold one job, and
drive one car. Preferences are given by a utility
function u(c,z) of the usual kind. The choice set
consists of three distinct points in the (p,z)


### ---Economics-2002-0-05.txt---
Plp'(z) 92(z)

P, Af0'(z(

00(Z)~~~~~~Z

A I

0,0 Zi Zh z

FIGURE 1. SPATIAL EQUILIBRIUM

plane, as in Figure 1. A consumer lives at point
A, (0,0), if neither variety is purchased, at point
B, (pl,zl), if the low-quality variety is most
preferred, and at point C, (Ph,Zh), if high quality
is chosen. High-quality goods must sell for
higher prices than low-quality goods, or else
low quality is dominated and disappears from
the market. Thus, higher z is always associated
with larger p if both Zh and z, are actually
traded. Given that some variety is purchased, a
consumer chooses Zh if the benefits of its additional
quality exceed its additional cost. Once
having decided on the choice of variety, the
consumer decides whether to purchase it or do
without.

The first part of the problem is equivalent to
choosing the maximum between u(y - Ph, Zh)
and u(y - pl, zl), where y is income. The
added cost of high quality is l\p = Ph - Pl- Its
added benefit is how much more the buyer is
willing to pay for it, A\O, and is defined as a
compensating variation:

(1) U(y - pI - AO, Zh) = U(y - pi, Z1)-
AXO is the money premium a person would pay
for Zh when the lower-quality item z, is available
at price p,. The optimal choice is Zh if
AXO > Ph - p, and z, otherwise. The second part of
the problem, whether or not to purchase the
good at all, is another cost-benefit comparison.
Define 0 as the compensating variation that
equates the utility of not purchasing anything to
that obtained from the best possible variety:
(2)

u(y,O)

= max{u(y - 0 - AO, Zh), U(y - 0, Z1)}.
The consumer does not purchase any z if both
0 < p, and 0 + AO < PhThis

can be neatly described diagrammatically
with a spatial bidfunction 0 (z), defined as
the amount a person with income y will pay for
various varieties at some constant utility index:
(3) u(y - 0(z), z) = constant.

0(z) is an indifference curve between money
and the measure of quality z. From (3), its
derivative a 0/az = uzJuC, is the marginal rate
of substitution between z and c. This is positive
and 0 (z) is upward sloping. Diminishing
marginal rate of substitution implies that
0"(z) < 0: the marginal willingness to pay
for additional quality is decreasing. The
curves labeled 0' in Figure 1 depict indifference
curves for three different kinds of
buyers. Consumers whose tastes look like 00 do
not purchase the differentiated product at all
because the indifference curve through (0,0) lies
above the available price-quality combinations
for z. Analogously, those whose tastes look like
01 purchase z, and those whose tastes are more
like 02 buy Zh- Choices of differentiated varieties
are nicely ordered by the intensity of preferences
for quality in this example.

Supply decisions of sellers are similar. The
benefit of selling a variety is its market price.
Production is profitable if price exceeds production
costs of at least some sellers. Parallel with
bid functions, these choices are depicted by a
spatial offer function p( z)-the locus of pricequality
pairs that result in the same profit. 'p( c)
is the supply price of quality z for that seller.
Since higher-quality goods are more costly to
produce, 'p'(z) > 0. Offer curves are increasing
and convex functions of z. Producers either
specialize their production in distinct varieties
or produce several of them in a product line.
Costs and production conditions, indivisibilities,
the nature of competition, and competitors'
costs factor into these outcomes. Figure 1 depicts
a case of specialization. The offer function
labeled Sp1 refers to a seller with comparative


### ---Economics-2002-0-06.txt---
advantage at producing the lower-quality variety
and the curve labeled 92 refers to a seller
whose production conditions are better suited to
high quality. Since (pl lies everywhere above
(Zh, Ph), seller 1 cannot offer the high-quality
variety at a profit level higher than that obtained
by offering the low-quality variety. Conversely,
since (P2 lies everywhere above (zl, Pl), seller 2
cannot offer the low-quality variety at a profit
level higher than that obtained by offering the
high-quality variety. Sellers whose minimum
acceptable profit-offer curve cover both prices
produce both objects. An example of such a
seller is one with offer function Sp*, who is
indifferent between selling the two varieties.
Some auto manufacturers produce a full
product line and others specialize in niche markets.
Research universities cater to students
with superior high-school records and achievements,
and would not be very cost effective as
junior colleges. Climatic and geographic endowments
give some California vineyards advantages
in producing high-quality wine that is
much harder to produce in New York and Michigan,
yet some California vintners produce both
higher- and lower-quality brands. The prestigious
law firms handling large legal claims are
careful about which cases they take on and
whom they admit as partners. Production activities
in which workers directly interact require
personnel who complement each other's personal
productivity and efficiency characteristics.
Because of direct complementarity in most
production settings, more interpersonal variation
in efficiency within production units can be
tolerated by transacting through the market
rather than directly in teams. Impersonal
transactions are the equivalent of intermediate
product markets and reduce the adverse consequences
that occur when less efficient workers
pull down the productivity of more efficient
ones.

Figure 1 shows how the market sustains diversity
as an equilibrium phenomenon. Different
kinds of buyers purchase different kinds of
goods. Consumers with tastes that correspond to
O'(z) buy z1 at price Pi and are supplied by
sellers with bid functions that correspond to
pl(z). Consumers with tastes that correspond
to 02( z) buy z, at price Ph and are supplied by
sellers with bid functions that correspond to
(p2( z). Neither seller has any incentive to try to
sell to consumers of the other type, nor does any
buyer have any incentive to purchase from the
other type of seller. All four types of agents can
do no better, given the opportunities available.
A. Interpreting the Implicit Value of
Characteristics

Empirical investigations of product and labor
differentiation use cross-section data to relate
prices p with attributes z. The hedonic regression
method regresses product prices on product
characteristics. It was initially developed to
study real cost reductions in auto and other
durable goods manufacturing that were concealed
by product design changes and quality
improvements. In labor economics, wages of
workers are regressed on their personal productivity
and job characteristics. In land and housing
markets, site and structure prices are
regressed on house attributes (size, architectural
style, and age) and onsite characteristics (neighborhood,
location, and public services). Labor
and land market studies are useful for imputing
the social value of certain intangible goods, like
safety and clean environments. An important
use in goods markets is to construct price and
quantity indexes that control for changes in the
quality of goods over time.

B. Cross-Section Values

Environmental and safety concerns are at the
forefront of public policy today. The rhetoric
and passions they arouse make it easy to forget
that these goods are costly to produce and that
rational decisions require comparing their benefits
with their costs. Assessing the costs of
these kinds of public policies is like finding the
costs of any other investment. Assessing benefits
requires estimating the willingness of consumers
to pay for more safety and better

environments. Practice is tricky because there
exist no explicit markets where safety and clean
air are directly traded, and from which demand
values can be directly inferred. Instead, safety
and environmental quality often are by-products
of other transactions and their valuations must
be imputed from the observed packages in
which they play a part.

Exposure to risk and pollution are affected by
work and residential choices. Some jobs are


### ---Economics-2002-0-07.txt---
more hazardous than others and the social and
physical environment varies greatly among
neighborhoods. Private cost-benefit considerations
underlying such choices are the basis for
imputing the required values. Housing in crimefree
neighborhoods is expensive because people
are willing to pay more for greater personal
safety and protection of their property and because
crime-free neighborhoods are scarce.
Wages on hazardous jobs must be higher in
order to induce workers to expose themselves to
greater risk of life and limb. Observed price
differences across jobs and locations are the
implicit prices of characteristics, as in Figure
1, with z interpreted as job safety or neighborhood
safety.

Valuations generally vary among agents.
People with different tastes and incomes have
different bid and offer functions, but more is
involved. In most types of economic exchange,
the Law of One Price implies that marginal
valuations across buyers are identical and that
differences in tastes manifest themselves only in
differences in quantities consumed. For example,
the "last" loaf of bread is worth about the
same-its market price-to a person who consumes
one loaf per week as to a person who
buys one loaf per day.

The Law of One Price always applies to the
specific houses, jobs, or goods markets that
embody intangible characteristics, but not necessarily
to the intangible characteristics themselves.
Whether there is a unique market price
of a characteristic depends on whether or not the
characteristics embodied in existing varieties
can be recombined or remanufactured by buyers
into different varieties. The leading example of
such "combinability" is asset and portfolio
management. Risk and return of any single asset
are relevant only insofar as they affect the risk
and return on one's total portfolio. A portfolio is
a linear combination of various assets, so covariance
of risk on one asset compared to others
is the key risk component. The implied linear
restrictions (or no-arbitrage conditions) imply a
unique market price for risk.

But the fact is that combinability of characteristics
across varieties is not possible for most
other goods. If it is expensive to untie bundles
after they have been manufactured, sellers must
design their goods for specific tastes and assemble
packages of characteristics that appeal to
specific market segments. This generally results
in differing attribute prices across packages.
One cannot buy another unit of comfort for a
sports car in an independent "comfort" market.
Instead, a larger car must be purchased. A
worker on a risky job cannot subcontract little
bits of the risk to others in a secondary market.
It is all-or-nothing. The only way less risk can
be chosen is by finding a safer job. Workers
come prepackaged with various combinations
of skills and traits, some productive and others
counterproductive. Employers cannot detach
the less desirable ones from any single worker.
Rather, an entirely different person has to be
hired. In these cases the market equilibrium
price function p( z) usually is nonlinear, and the
gradient p'(z) generally depends on z itself.
Since there is no single market price for z,
different people have different valuations of it at
the margin. Type 0' and 02 buyers in Figure
1 serve as an example. In principle an average
of the two slopes at z1 and Zh is appropriate for
assessing the (marginal) benefits of a small independent
public project affecting z in some

application, with the average weighted by benefit
incidence of the project among different
types of people.

So many factors determine market prices of
goods in practice that the best chance empirically
for isolating the implicit value of safety
and environment is to examine specific goods or
jobs where these aspects dominate other considerations
of choice. For instance, my labormarket
study with Richard Thaler (1 976) on the
value of life was one of the first to systematically
examine wage premiums on very risky
jobs, where risk was measured as excess insurance
premiums charged by private companies
on worker compensation policies.

The revealed preference argument in Figure
1 applies directly to that problem. Here z represents
job safety and p( z) is estimated by
statistically comparing the smaller market wage
that workers are paid on safer jobs compared to
higher wages paid on riskier ones, other things
equal. Examining risky jobs empirically confines
the study to jobs with relatively low values
of safety, e.g., to those in the neighborhood of z,
in the figure. Since most workers hold relatively
safe jobs (located closer to a point like z,, in the
figure), people holding risky jobs surely place
smaller values on safety than the median person,


### ---Economics-2002-0-08.txt---
much like the difference between people of
types 01 and &2 in the figure. Workers choosing
safe jobs are willing to pay at least as much for
safety as people choosing risky jobs, so the
wage premium on risky jobs is likely to be a
lower bound on the average person's willingness
to pay for safety.

Thaler and I estimated a "value of life" of
about $800,000 in dollars of Year 2000 purchasing
power. Other labor-market studies using
data on a wider range of risks have found
much larger values. The broader range of risks
in these studies is the most probable reason
for the larger estimates: they include more
between-variation (e.g., the differences between
z1 and Zh), whereas our study was largely confined
to within-variation (around zl). Similar
considerations apply to housing and land market
studies that impute values for crime, climate,
and pollution. Here, too, revealed

preference implies that estimated pollution and
crime gradients are likely to be lower bounds
for the average citizen because they ignore important
sorting considerations.

C. Time-Series Imputations

The main practical difficulty in assessing
changing standards of living is that goods
change their character over time. The prices of
tractors and automobiles today differ from 30
years ago, not only because manufacturing productivity
and input prices have changed, but

also because the products themselves have improved.
The nature of the problem is starkest
when entirely new goods appear on the market.
If their introduction is successful and they supplant
older varieties, how should they be linked
into a price index?

Conceptually, the only possibility is to think
in terms of the costs of providing ultimate services.
Technical changes reduce the cost of services.
Autos were successful because they were
a superior way of producing transport services
compared to animals. Electricity produced heat
and light more efficiently than steam and lantems,
and radio, television, and movies reduced
the cost of entertainment services relative to live
performances. In these examples, technical
changes should be factored into price indexes
for transport services, power, and entertainment
services. In practice it is extremely difficult to
link entirely new goods to old goods in this
way. Adjusting for quality improvements of existing
goods is more manageable.

Rising incomes naturally cause product quality
to improve over time, because the income
elasticity of demand for quality is positive.
Even when the prices per unit quality of goods
do not change, rising incomes increase the demand
for quality and raise average transactions
prices over time. Offsetting the rise in demand
for quality is that some aspects of quality (like
ornate detail on early twentieth-century houses)
are labor intensive, which causes their prices to
rise as incomes rise. As long as the cost factor is
not dominant, there is upward bias in measured
prices and downward bias in measured real living
standards from a change in the composition
of goods toward more expensive varieties.
Transactions prices rise not because cost has
increased, but because the quality of what is
being referred to as a particular good has increased.
Income effects are represented by

movements along a given p( z) locus in Figure
1. The same envelope of offer functions define
p(z), but bid functions shift up and to the right
as income rises because the willingness to pay
for increments of z increases with income. 01
and 0& in Figure 1 could represent two people
with the same underlying utility functions but
different incomes. With constant returns to
scale in production, prices of each variety do
not change as income rises, but average quality
purchased and average transactions prices both
increase. We should not confuse movements
along the p( z) locus with changes in the cost of
living. Most consumption decisions change as
income rises, and these are part and parcel of
the same general phenomenon. Standardized
comparisons that control for the changing composition
of goods eliminate this kind of bias.
To assess changes in the real costs of living,
we should account for shifts in the price-quality
locus or technological changes that extend the
real range of choice. Such shifts can cause
quality-specific prices either to rise or fall. For
example, p( z) and the cost of living go up in the
income-increasing experiment above if goods
are supplied inelastically. For example, rising
real incomes increase the demand for amenities
associated with geographic location, raising
site values and the costs of housing services
in the preferred locations. On the other hand,


### ---Economics-2002-0-09.txt---
PG _ 2(z)

Ph2t ./

phI B I/0

A F 0'(z) H _(

Ph2 D .............O E

P12 7 O"(Z)

4 4 ~ ~I I I I I

ZI Z1 2 Zb I ZLI Zh2' z

FIGURE 2. TIME-SERIES EFFECTS

innovations that extend the quality range of
goods tend to reduce the real costs of services
and reduce living costs for those who buy them.
Productivity-improving quality changes generally
reduce the real prices of goods. Prices and
available qualities are always changing. Some
sellers are innovating and attempting to increase
profits by extending the desirable characteristics
in their goods and reducing prices below those
of competitors. The prototypical example appears
in Figure 2. The first-period equilibrium is
the same as before, where goods are labeled zll
and Zh I and equilibria are found at points A and
B. In the second period the attributes of goods
have changed to Z12 and Zh2 so the price-quality
locus has moved down, and the average price
per unit quality has declined. (A ray from the
origin through A is steeper than one through D
and similarly for B and E.) Quality-adjusted
price indexes require measuring the distance
between the two price-quality loci, shown in
Figure 2 as the curve that connects A and B and
the one that connects D and E. The distance
between F and D is a measure of the qualityadjusted
change in price at quality level z,12
When technical change is so large that z could
not be produced in period 1 (a 32-bit computer
chip did not exist in 1980) shown, for example,
by Zh2', direct price comparisons of the improved
product are impossible. In principle this
can be overcome with sufficient structural
knowledge of bid functions. In this case, the
requirement amounts to knowing the exact
shape of 02( z) and of 02'( z), which represents
different indifference curves for the same consumer.
The conceptually appropriate measure of
distance is the difference between actual price
in period 2 and what type 2 buyers would have
been willing to pay for Zh2' in period 1 had it
been available, shown in Figure 2 as the distance
between G and H. Generally such detailed
structural knowledge is unavailable, so distance
must be measured by using overlapping varieties
whose quality is more or less comparable
across adjacent periods. For example, top-ofthe-
line goods in period 1 might be compared
with bottom-of-the-line goods in period 2.
Longer period comparisons are made by linking
or chaining indexes with common components
over the years.

Figure 2 contains a graphic example of why
these adjustments are important. Consider a
new good Zh2' not previously available, whose
current price, Ph2', exceeds that of the best prior
models. Unadjusted indexes would show rising
prices simply because Ph2' exceeds Ph 1. A more
accurate distance-based quality-adjusted index
would correctly show prices declining because
consumers would have been willing to pay more
than its period 2 cost in period 1, had it been
available. Note that there is a consumer whose
bid function, in this case 03( z), reflects higher
satisfaction at (Zh2', Ph2') than at any other
available (z, p) combination.

This example is transparent, but very relevant.
Until a few years ago, the National Income
and Product Accounts priced computers by the
box-by the package in which producers delivered
them. In the early years of the computer
revolution boxes were getting bigger and more
powerful, and their prices were increasing. It
was as if soap were suddenly produced in bigger
bars and the index used the price per bar, not the
price per unit volume or weight. Computer productivity
was grossly distorted in the official
indexes, though the aggregate consequences of
the error were limited because investment in
computers was not such a large share of total
investment as is true today. But even when box
prices were falling, as has been true for many
years, unadjusted price indexes distort productivity.
Prices are really falling much faster than
appears because products are improving so
much. The mainframe of yesterday is the laptop
of today. Considering that computers currently
make up almost half of gross business investment,
eliminating these biases is important for


### ---Economics-2002-0-10.txt---
getting an accurate assessment of national income.
The recent report of The Presidential
Commission on Price Indexes shows that more
widespread failures to make quality adjustments
of goods in price indexes have serious negative
biases for assessing real wage and productivity
growth, and overstate inflation and the social
security and other entitlements that are indexed
to them.

III. Sorting and Stratification

In any market equilibrium, interpersonal differences
in tastes and technologies affect the
locations chosen by buyers and sellers on the
attributes map. Some of these differences are
inexplicable: sometimes there really is no accounting
for different tastes. Others have more
proximate causes, but might just as well be
summarized stochastically. For instance, the
preferences for material goods of a person from
humble origins that subsequently makes lots of
money often seem to differ from those of her
children. Families that are contemplating having
children are likely to prefer larger cars to
smaller ones and suburban to downtown dwelling
units. In the labor market, a person's supply
price for physically demanding or dangerous
work depends on age, physical condition, and
number of dependents, among other things. Different
endowments affect productivities in different
pursuits. Musicians cannot be tone-deaf;
football players tend to be large; while lawyers,
and many economists, have a propensity to talk.
What matters for economic allocations in all
of these cases are the direct manifestations of
tastes. Here, the reservation prices themselves,
not the specific causes that make preferences,
differ case by case. The distributions of bid and
offer functions are sufficient (in the statistical
sense) for ascertaining the demand, supply, and
equilibrium price of each variety. But when
reservation prices are systematically associated
with observable traits of buyers and sellers,
these markets become stratified in many ways.
Stratification and sorting are ubiquitous in spatial
equilibrium and have many interesting consequences.
Neighborhood, bandwagon, status,

and other social externalities are often invoked
to account for stratification, but most stratification
occurs without them.

Sorting by income is one of the most important
forms of stratification in goods, land, and
labor markets. As noted above, people who buy
high-quality goods, and live in better residences
and neighborhoods tend to be richer than other
people. Similarly, high-wage people use their
higher earning capacity to purchase more job
amenities and better working conditions. Stratification
of job quality by the skill characteristics
of workers can make it difficult to estimate
equalizing wage differences for the implicit attributes
of jobs. Sometimes the data on job and
worker characteristics are so colinear that the
ceteris paribus conditions required for identifying
taste parameters alone are overwhelmed by
income stratification and cannot be observed in
the data. Yet it is incorrect to take this as evidence
of failure of the equalizing or compensation
principle in the labor market.

Stratification and colinearity is itself an implication
of the economic theory of preferences
when income effects are important. High-wage
jobs generally are the good jobs, not the bad
ones. They are staffed by skillful workers
whose higher incomes make job amenities more
valuable and who buy more of them. There is no
logical contradiction here. The identifying restrictions
for backing out implicit valuations in
the data just are not satisfied. After all, sometimes
there is not the right kind of variation in
the data to estimate a conventional demand
curve, but that does not imply that a demand
curve does not exist. These difficulties are less
often encountered in imputing implicit prices of
characteristics from goods and land market
prices. Prices and attributes of goods and land
are almost always measured independently of
the characteristics of buyers and sellers. Of
course many aspects of product attributes may
be highly correlated, making it difficult to extract
the value of any single one of them. The
stratification problem more often arises in labor
markets, because observed wages always reflect
the total bundle of both job and workerproductivity
attributes. The correlation between
them can be too large to separate the components.
In sum, potential for stratification places
limits on the empirical applicability of the hedonic
method. Sometimes it cannot be used, but
the test is always empirical.

Colinearity affects the precision with which
effects can be estimated, but sometimes there is
bias in estimating the importance of equalizing


### ---Economics-2002-0-11.txt---
Wage

\ -0~~~~~2

N . \

(p1 (p

yp2

Pension

FIGuRE 3. ACTUAL VERSUS OBSERVED FUNCTIONS
differences. It is not that coefficients are noisy;
it is rather that they have the wrong sign. This
frequently occurs in the labor-market setting
when one estimates compensating differences
in wage functions. A good example involves the
trade-off between pensions and wages. Other
things equal, jobs with higher pension benefits
should have lower wages because the total
amount that employers are willing to pay for a
given worker should not vary with the composition
of the compensation package and because
workers are willing to trade wages for pensions.
In Figure 3, the observed trade-off is shown by
the locus AB, which is the estimated market
relation of wages to pensions. Note that it is
positively sloped, but this is not because workers
do not view both wages and pension benefits
as goods. Instead, it reflects the fact that more
productive workers, who are richer, take some of
their income in the form of wages and some in the
form of pensions. The (pl offer function reflects
the bid by firms to low-productivity workers and
the (p2 function reflects the bid by firms to highproductivity
workers. Even if all workers had the
same preferences, as shown by the dotted offer
functions (the one to the northeast reflecting
higher utility), the market equilibrium would select
points A and B. For any given worker, the
trade-off would be negative because then the (p or
0 curves would be relevant. But the market observes
the AB line, because key productivity factors
cannot be held constant. The AB line might
well be estimated with precision, depending on
the amount and nature of the data, but it identifies
neither a firm's nor worker's willingness to trade
off pensions for wages.

IV. Diversity and Specialization

Specialization, division of labor, and the organization
of work socially create much of the
diversity observed in economics. In his compelling
discussion, Smith identified scale economies
as the principal cause of specialization.
Instead of working alone and doing everything
by oneself, it is productive for a worker to join
teams and assign individual members to a few
mutually exclusive tasks. The division of labor
is one of the most important bits of economic
analysis at the extensive margin. How are productive
activities packaged and bundled together
into jobs, and who works on them?

For the economy as a whole, the most important
reason by far for specialization and division
of labor are scale economies in utilizing acquired
skills. The returns to investing in a particular
skill are proportional to the frequency of
its subsequent use. This makes it efficient to use
th'e skills one has already acquired as intensively
as possible and not to spread oneself too
thinly over a highly diversified portfolio. These
same connections between capacity and utilization
apply to all capital goods, not only to
human-capital varieties. The costs of constructing
an office building depend on its size; but
once it is built, it is efficient to keep the offices
as fully occupied as possible because the marginal
cost of an unoccupied office is much
smaller than its average cost.

Specialization is optimal if learning new
skills involves significant fixed costs that are
only loosely linked to the intensity of subsequent
utilization. Then it is best to learn a few
skills very well and use them all the time. These
basic forces produce enormous social gains
from trade. We could all build our own houses
and educate our own children. Instead, we use
markets to buy new houses built by expert
house builders and purchase educational services
for our children from expert teachers. The
houses are better and the children learn more.
Specialization and trade are important causes of
economic diversity among people in society.
Many aspects of economic diversity and its
manifestations in economic inequality serve
valuable social purposes. The fact is that substantial
inequality is necessary for decentralized
societies to function. Many components of variance
of earnings among persons are sustained


### ---Economics-2002-0-12.txt---
by personal activities that would characterize
the most free and open societies. They are necessary
to sustain both human-capital investment
incentives and work incentives.

For example, in almost every society doctors
earn more than other people. These wage differentials
persist in equilibrium in order to compensate
prospective medical students for their
arduous and costly training. The number and
quality of doctors would fall if earnings were
artificially compressed and the rate of return to
medical education was reduced. A society with
few doctors would score more egalitarian points
on a Gini coefficient, but it would not be a better
society. This example is trivial, but the point is
far more general and often not so obvious.
Much inequality in the overall distribution of
earnings is attributable to substantial differences
in mean earnings among workers of different
ages and educational attainments that
are associated with occupational choices and
human-capital investments over the life cycle.
They reflect rational personal choices that
change a person's economic status and current
incomes over a lifetime.

Rising earnings over working life as well as
earnings differences between occupational
and educational categories largely reflect returns
to human-capital investments. A clear
distinction between human-capital (lifetime)
wealth and current earnings is needed because
learning always involves choosing prospects
with smaller earnings now and higher earnings
in the future. Observing that a person has
low current earnings is no signal of lifetime
poverty if future earnings will be high. Similarly,
high current earnings are no signal of
excess wealth if a person paid the price in low
previous earnings. The point is that the distribution
of current income is far more unequal
than the distribution of human-capital
wealth. Inequality indexes based on current
data alone give a very misleading impression
of true inequality.

The reasoning is most easily understood in
terms of an example. Consider a completely
equal society in which all workers have exactly
the same age-earnings profiles over their careers.
Then the distribution of annual earnings is
a deterministic function of the age distribution
of workers. Societies with more variation in
worker age show more inequality, but that is not
a very interesting aspect of inequality. In fact,
the age distribution makes an enormous difference
to measured inequality indexes: earnings
data used to assess inequality that are not age
standardized are practically worthless for assessing
inequality. Remarkably, such adjustments
are seldom made. Standardizing current
earnings data for education presents more difficult
conceptual problems because family backgrounds
and personal financial barriers on

educational choices distort investment margins
and make wage differentials not fully equalizing
on costs. Nonetheless, some portion of educational
wage differences-and, judging from

the remarkable uniformity of estimated rates of
return to education in different countries, perhaps
a major portion-are equalizing on their
costs. They represent productive, socially manufactured
diversity and inequality that we cannot
live very well without.

V. Personal Productivity and the Distribution of
Earnings

Of course, not all differences are willfully
created. Many are caused by inherited interpersonal
differences in talents and tastes. The hedonic
or characteristics approach to labor
markets bears similarities to statistical factoranalytic
ideas in accounting for earnings differences
across individuals. Factor analysis
partitions observed variance into a small number
of unobserved, latent "causes" or factors. A
leading example is intelligence testing, where
test scores are thought to reflect the quantitative,
analytical, verbal, and mechanical abilities of
subjects. Similarly, personal productivity and
earning power are ultimately determined by
such things as strength, intelligence, dexterity,
and attention to detail. Think of a model in
which a person's earning power is "scored" as
the sum of the amounts of each attribute possessed,
times their market prices. If the number
of factors is small, important proximate causes
of the distribution of earnings are reduced to
small dimensions. The economic rationale for
compacting the determinants of earnings into a
small set of universal factors and prices turns on
the existence of unique economywide prices
("loadings") on the factors. Since these price
weights are parameters in any factor representation
of earnings across persons, dimensionality


### ---Economics-2002-0-13.txt---
is reduced only if the same prices apply to all
persons.

The implausibility of invariance in any market
equilibrium is transparent in the goods market.
Is it possible that the marginal value of a
unit of comfort in an automobile should be the
same as a unit of comfort in an easy chair? Not
at all. These commodities represent different,
imperfectly substitutable services that cannot be
unbundled into such components. We do not see
easy chairs in autos and bucket seats in living
rooms. No "arbitrage" opportunities are available
for trading comfort in one kind of good
with comfort in an entirely different class of
goods, because those characteristics cannot be
untied from the larger bundle of attributes for
which the good was designed. Again, the Law
of One Price does not apply to characteristics,
and their implicit value usually differs among
categories of goods. It would be the same only
by accident.

Similarly, why should the value of another
unit of strength to, say, an accountant be the
same as its value to an athlete? It would be the
same if there were aggregate markets for such
things as strength and intelligence and if a unit
of "accounting strength" was a perfect substitute
for units of "athletic strength" in all productive
activities. The images of accountants on
the 50-yard line and linemen running interference
against the IRS is not, however, reassuring
in this regard. Once again the bundling of personal
productivity characteristics and the impossibility
of untying bundles and repackaging

them into something else is crucial. The marginal
products of underlying factors vary across
activities-strength is more important to professional
athletes than to accountants-and the
shadow prices of these factors vary across activities
as well. Thus, athletes are stronger than
accountants and accountants have more quantitative
skills than athletes. This is another important
manifestation of sorting and stratification in
spatial equilibria.

Activity-specific prices generate comparative
advantage. Just as differences in the relative
abundance of sunshine to rainfall in Portugal
and England give each country its comparative
advantage in wine or cloth, so too does strength
give people who have it a comparative advantage
in some forms of athletics, while arithmetic
skills and attention to detail give other people
comparative advantage in accounting. Comparative
advantage has interesting consequences.
For one, people observed in various job and
educational categories tend to be selected and
stratified by the personal attributes that give
them a competitive edge in a specific field. A
person's financial self-interest is served by selecting
the occupation that maximizes expected
earnings. If this is a major consideration in
occupational choice (though certainly not the
only one), observed earnings of individuals who
voluntarily chose an occupation might be a poor
estimate of what the earnings of people who
avoided that occupation would have been.
Obviously, the earnings of successful athletes
or actresses are not representative of the average
person's prospects in those fields. But the point
is more subtle in other important applications,
for instance in interpreting income differences
between people with more and less education.
As seen above, if all people were identical, the
education-earnings premium would be sustained
by the supply conditions that pay must
compensate for incurring the costs of investment
to equalize net advantages across trades.
When people differ, things are more complicated.
Those who expect the largest returns on
their educational investments are more likely to
make them. Ability rents persist in equilibrium.
There are two main reasons why rates of
return to human-capital investments and human
wealth differ among people. First, natural talents
complement occupation-specific humancapital
investments in different ways. Verbal
ability is indispensable for lawyers and quantitative
ability for engineers and scientists, for
example. People with greater endowments of
such traits have better prospects for success in
those activities. Another way of saying it is that
there are "ability" rents in occupational choices.
Wage differences are not fully equalizing on the
costs of acquiring skills because natural endowments
and premarket investments cause these
costs to differ among people. Second, there are
substantial financial barriers to educational investments
because human capital is not legal

collateral for investment loans. The main manifestation
of this is traditionally seen in high
stratification of educational attainments of children
according to parental wealth. Financing
difficulties cause inefficiencies and inequities in
human-capital investment decisions because


### ---Economics-2002-0-14.txt---
some socially desirable investment opportunities
are not available to poor people. Here, too,
earnings differences are not fully equalizing on
educational costs. They manifest the effects of a
form of financial "noncompeting groups," as
well as the effects of true differences in costs
and talents.

The effects of equal educational opportunities
on the distribution of earnings depends on
interpersonal differences in talents, abilities,
and motivation on the one hand, and on the
importance of noncompeting groups and financial
barriers on the other. Econometricians have
assessed the "ability-bias" in estimated rates of
return to higher education. This work is best
interpreted in terms of a one-factor representation
of skill where individuals are essentially
rank ordered from most able to least able, or
according to absolute advantage. Then, if financial
barriers are not too negatively correlated
with ability, people with more education tend to
be more able than those with less, and the wage
difference between college and high-school
graduates is an upwardly biased estimate of
what noncollege graduates would have earned
had they gone to college. As a practical matter,
the estimates of bias typically are rather small.
The characteristics model enriches analysis by
allowing selection by comparative advantage
rather than absolute advantage. Here abilities
and talents have different values in different
labor-market pursuits.

For example, Robert Willis and I (1979)
modeled educational choice by combining traditional
human-capital ideas with the theory of
comparative advantage, and developed a
method to estimate the behavior determinants of
actual choices observed in the data. We found
that high-school and college graduates do indeed
have different comparative advantages
across skills that dominate the selection process.
Detailed analysis of the earnings patterns of
World War II veterans showed the usual result
that high-school graduates would not have
earned as much as those who actually chose to
get their college degrees. But we also discovered
that persons who subsequently gained college
degrees probably would have earned

relatively less as high-school graduates than
those who voluntarily discontinued their education
after high-school graduation. Positive selection
at both levels of school is inconsistent
with a simple rank-order interpretation or
single-factor model of ability. It is only consistent
with two or more dimensions of ability that
are negatively correlated across people. Comparative
advantage also accounts for why most
estimates of ability-bias interpreted as a single
factor (absolute advantage) are so small. Those
who leave school early do relatively well in
their pursuits so that simple cross-sectional estimates
are not much different from abilitycorrected
ones.

VI. Manufactured Inequality

The production of income is not deterministic.
Sometimes, random forces play an important
role in assigning earnings to individuals.
For example, some occupations are risky. The
arts come to mind, where only a few individuals
can support themselves on the earnings from
their trade. Musicians have very skewed earnings
distributions, but most young music students
who choose to enter the field understand
that there is only a small chance that they will
end up at the desired end of the distribution.
Somewhat counterintuitively, perhaps, the
variance in outcomes that is introduced by this
randomness can improve welfare. The idea is
rooted in early work by Milton Friedman (1953)
and examined again in a different context by
Theodore Bergstrom (1986). Indivisibilities
play a key role here. Most people live in one
and only one house. Area amenities enjoyed
depend on the location of the house and on
individuals' choices on location that are correlated
with income. For example, rich people
may choose to live in New York City rather
than in Kankakee, Illinois because there are
more ways to spend income in New York than
there are in Kankakee. Conversely, the life of a
poor person in Manhattan is difficult because
the amenities are expensive and tend to be targeted
toward high-income people. There is
complementarity in the utility function between
the quality of housing, in this case proxied by
urban amenities, and the level of consumption
of other goods. The situation is illustrated in
Figure 4.

A consumer may choose to live in a highquality
house, Zh, or a low-quality house, zl,
with corresponding house prices Ph and Pl- Because
of the complementarity in consumption


### ---Economics-2002-0-15.txt---
Utility

U(Y-Ph,Zh)

As>_ ~~~~U(Y-pi,ZI)

Y1 YO Yk Y2 Income

FIGURE 4. MANUFACTURED INEQUALITY

between other goods and housing, those whose
incomes are below Yk derive higher utility from
living in the low-quality house in a low-amenity
location and those whose incomes are above Yk
derive higher utility from living in the highquality
house in a high-amenity location. But
consumers with incomes between YI and Y2 can
improve their expected utility by entering into
gambles, which would offer them expected utility
that lies along the line that connects A and B.
For example, an individual with income yo who
chooses to live in a low-quality house offering
utility U(yo - Pl, zl) would prefer a fair gamble
that paid income Yi half the time and income
Y2 half the time. Were he to lose the gamble, he
would live in the low-quality house and derive
utility U(y, - pl, Z,). Were he to win, he would
live in the high-quality house and derive utility
U(Y2 - Ph, Zh). His expected utility would be at
point C, which yields more utility than the certain
utility obtained at point D.

Occupational lotteries of this sort manufacture
inequality but make the individuals who
enter the occupation better off. Because the
variance in outcomes gives individuals a shot at
a much better standard of living, they willingly
bear the risk that results in observed ex post
inequality. Going to a high-stakes law firm in
New York may turn out to be a good option,
offering a partnership, high income, and an entertaining
city in which to spend income. Those
who lose the law-firm lottery accept jobs as
corporate counsel in Kankakee, buy a less expensive
house, and enjoy fewer amenities. Although
it is better to win the lottery than to lose
it, the existence of the risky occupation makes
all individuals who enter better off in an expected
utility sense, primarily because the inequality
manufactured by this occupation

allows different combinations of income and
amenities that are complementary.

VI. Conclusion

Despite the importance of diversity in economic
life, only a small part of economic theory
is devoted to analyzing differences. Competitive
markets value diversity and sort out complex
patterns of tastes and technologies that
translate into supply of and demand for an enormous
variety of products and factors of production.
Markets accommodate diversity by

establishing values that make differentiated
items relatively close substitutes at the margin.
The markets match buyers and sellers in
marriage-type equilibria where agents sort according
to their talents in response to market
prices. To bring about the appropriate sorting,
markets must create inequality in values. Thus, in
labor markets, large differences in earnings can
result even when individuals are ex ante identical.
The theory of diversity applies universally
and is manifest in many economic problems. In
addition to issues involving earnings inequality,
occupational choice, and the differentiation of
products, risk analysis of environmental safety
concerns and price index problems are analyzable
using standard economic approaches to
diversity. In such problems, sorting is key, so
market valuations understate the average individual'
s distaste for disagreeable attributes and
overstate the average individual's preference for
agreeable ones. Sorting plays a role in price index
problems, where we seek to ascertain shifts in
prices as a result of technology shifts, not movements
along a price line that results, say, from
increases in real income. Thus, price might appear
to rise because richer people buy new varieties of
a commodity that has superior attributes that are
not captured in standard measurement.
Although tastes may differ, it is the reservation
prices themselves and not the causes of the
differences that have consequence for economic
allocation. Differences in talents are behind
much of occupational choice where the theory
of comparative advantage is central. Individuals
specialize in skills because the fixed costs of
skill acquisition is only loosely linked to the
levels of utilization. It pays for a worker to learn


### ---Economics-2002-0-16.txt---
one thing well and exploit it over and over
again. Talents are multidimensional in general,
so those who go on to college are better at
college jobs than those individuals who do not
go on to college. But the converse is also true:
Those who opt against college are better at their
jobs than those who complete college would be
at noncollege jobs. Rather than strict hierarchy,
comparative and even some absolute advantage
play important roles.

This essay has explored three themes: Markets
value diversity, markets sort buyers and
sellers appropriately to take advantage of heterogeneous
talents and tastes, and sorting and
choice create income inequality.

Value is determined in diverse markets in the
standard way, equating supply with demand.
The difference is that there are more margins on
which to operate. Not only is quantity a choice
variable, but consumers and producers can substitute
along varying dimensions of quality.
Equilibrium is established when no seller can be
made better off by altering the quality of his
product and offering it to different buyers and
when no buyer can be made better off by substituting
a different quality good for the one that
he currently consumes.

Just as value is determined by market equilibrium,
so too is the allocation of buyers to
sellers set by the market. Sellers who have a
comparative advantage in producing highquality
products sell to consumers who are willing
to pay a sufficient premium for additional
quality. Conversely, sellers who have a comparative
advantage in producing low-quality goods
cheaply cater to consumers who prefer to substitute
away from high quality so that they can
spend the saved dollars on other goods.
Finally, income inequality results from heterogeneity.
Some of this is determined by nature
as individuals are born with different abilities,
but inequality is also manufactured by actions
that individuals take. The most obvious actions
involve investments in human capital, either
through formal schooling or work experience.
Such investments create inequality, but are beneficial
to individuals and society as a whole
because they improve the overall standard of
living. A more subtle variant involves gambles
that individuals take as they choose to enter
risky occupations or make risky investments.
Because of indivisibilities, risky payoffs allow
individuals to couple the consumption of large
amounts of some goods with high-quality versions
of others, such as living in expensive
cities when income turns out to be high. Losers
of occupational lotteries combine their lower
consumption with lower-quality indivisible
goods, consuming less and living in less expensive
cities. Expected utility is higher than it
would be, absent this type of inequality.
Markets value diversity. Individuals, using
their respective talents and different preferences,
respond to these valuations and create
important induced differentiation in consumption
patterns, earnings, and occupational choices.
 ## Economics-2003-0


### ---Economics-2003-0-02.txt---
Macroeconomics was born as a distinct field
in the 1940's, as a part of the intellectual response
to the Great Depression. The term then
referred to the body of knowledge and expertise
that we hoped would prevent the recurrence of
that economic disaster. My thesis in this lecture
is that macroeconomics in this original sense
has succeeded: Its central problem of depression
prevention has been solved, for all practical
purposes, and has in fact been solved for many
decades. There remain important gains in welfare
from better fiscal policies, but I argue that
these are gains from providing people with better
incentives to work and to save, not from
better fine-tuning of spending flows. Taking
U.S. performance over the past 50 years as a
benchmark, the potential for welfare gains from
better long-run, supply-side policies exceeds by
far the potential from further improvements in
short-run demand management.

My plan is to review the theory and evidence
leading to this conclusion. Section I outlines the
general logic of quantitative welfare analysis, in
which policy comparisons are reduced to differences
perceived and valued by individuals. It
also provides a brief review of some examples-
examples that will be familiar to

many-of changes in long-run monetary and
fiscal policies that consumers would view as
equivalent to increases of 5-15 percent in their
overall consumption levels.

Section II describes a thought-experiment in
which a single consumer is magically relieved
of all consumption variability about trend. How
much average consumption would he be willing
t

o give up in return? About one-half of onetenth
of a percent, I calculate. I will defend this
estimate as giving the right order of magnitude
of the potential gain to society from improved
stabilization policies, but to do this, many questions
need to be addressed.

How much of aggregate consumption variability
should be viewed as pathological? How
much can or should be removed by monetary
and fiscal means? Section III reviews evidence
bearing on these questions. Section IV considers
attitudes toward risk: How much do people
dislike consumption uncertainty? How much
would they pay to have it reduced? We also
know that business-cycle risk is not evenly distributed
or easily diversified, so welfare cost
estimates that ignore this fact may badly understate
the costs of fluctuations. Section V reviews
recently developed models that let us explore
this possibility systematically. These are hard
questions, and definitive answers are too much
to ask for. But I argue in the end that, based on
what we know now, it is unrealistic to hope for
gains larger than a tenth of a percent from better
countercyclical policies.

I. Welfare Analysis of Public Policies:
Logic and Results

Suppose we want to compare the effects of
two policies, A and B say, on a single consumer.
Under policy A the consumer's welfare is
U(CA), where CA is the consumption level he
enjoys under that policy, and under policy B it
is U(CB). Suppose that he prefers cB: U(CA) <
U(csB). Let A > 0 solve

U((l + A)CA) = U(CB).

We call this number A-in units of a percentage
of all consumption goods-the welfare gain of
a change in policy from A to B. To evaluate the
effects of policy change on many different consumers,
we can calculate welfare gains (perhaps
losses, for some) for all of them, one at a time,
and add the needed compensations to obtain the


### ---Economics-2003-0-03.txt---
THE AMERICAN ECONOMIC REVIEW

welfare gain for the group. We can also specify
the compensation in terms of one or a subset
of goods, rather than all of them: There is no
single, right way to carry these comparisons out.
However it is done, we obtain a method for
evaluating policies that has comprehensible units
and is built up from individual preferences.
There is a great tradition of quantitative public
finance that applies this general framework
using well-chosen Taylor expansions to calculate
estimates of the compensation parameter A,
"welfare triangles" as Arnold C. Harberger
called them. Today we use numerical simulation
of general-equilibrium models, often dynamic
and subject to unpredictable shocks, to
carry out welfare analysis with the general logic
that I have just sketched. Some examples will, I
hope, convey the applicability of this approach
and some of the estimates that have emerged.
Martin J. Bailey's (1956) thought-experiment
of a perfectly predictable inflation at a constant
rate, induced by sustained growth in the money
supply, was a pioneering example of the quantitative
evaluation of policy. In a replication of
the Bailey study, I estimated the welfare gain
from reducing the annual inflation rate from 10
to 0 percent to be a perpetual consumption flow
of 1 percent of income.1 Some economists take
estimates like this to imply that inflation is a
relatively modest problem, but 1 percent of income
is a serious amount of money, and in any
case, the gain depends on how much inflation
there is. The gain from eliminating a 200-
percent annual inflation-well within the range
of recent experience in several South American
economies-is about 7 percent of income.
The development of growth theory, in which
the evolution of an economy over time is traced
to its sources in consumer preferences, technology,
and government policies, opened the way
for extending general-equilibrium policy analysis
to a much wider class of dynamic settings. In
the 1980's, a number of economists used versions
of neoclassical growth theory to examine
the effects of taxation on the total stock of
capital, not just the composition of that stock.2
The models used in these studies differ in their
' Lucas (2000). My estimates are based on the money
demand estimates in Allan H. Meltzer (1963).
2 For example, William A. Brock and Stephen J.
Tumovsky (1981), Christophe P. Chamley (1981), Lawdetails,
but all were variations on a one-good
growth model in which consumers (either an
infinitely lived dynasty or a succession of generations)
maximize the utility of consumption
and leisure over time, firms maximize profit,
and markets are continuously cleared.
In general, these studies found that reducing
capital income taxation from its current U.S.
level to zero (using other taxes to support an
unchanged rate of government spending) would
increase the balanced-growth capital stock by
30 to 60 percent. With a capital share of around
0.3, these numbers imply an increase of consumption
along a balanced growth path of 7.5 to
15 percent. Of course, reaching such a balanced
path involves a period of high investment rates
and low consumption. Taking these transition
costs into account, overall welfare gains amount
to perhaps 2 to 4 percent of annual consumption,
in perpetuity.

Production per adult in France is about 70
percent of production per adult in the United
States. Edward C. Prescott (2002) observes that
hours worked per adult in France, measured as
a fraction of available hours, are also about 70
percent of the comparable U.S. figure. Using
estimates for France and the United States of the
ratio (1 + Tr)/( - Th) that equals the marginal
rate of substitution between consumption
and leisure in the neoclassical growth model, he
shows that tax differences can account for the
entire difference in hours worked and, amplified
by the indirect effect on capital accumulation,
for the entire difference in production. The
steady-state welfare gain to French households
of adopting American tax rates on labor and
consumption would be the equivalent of a consumption
increase of about 20 percent. The conclusion
is not simply that if the French were to
work American hours, they could produce as
much as Americans do. It is that the utility
consequences of doing so would be equivalent
to a 20-percent increase in consumption with no
increase in work effort!

The gain from reducing French taxes to U.S.
levels can in part be viewed as the gain from
adopting a flat tax on incomes,3 but it is doubtrence
H. Summers (1981), Alan J. Auerbach and Laurence
J. Kotlikoff (1987), and Kenneth L. Judd (1987).
3 See also Robert E. Hall and Alvin Rabushka (1995).
2

MARCH 2003


### ---Economics-2003-0-04.txt---
ful that all of it can be obtained simply by
rearranging the tax structure. It entails a reduction
in government spending as well, which
Prescott interprets as a reduction in the level of
transfer payments, or in the government provision
of goods that most people would buy anyway,
financed by distorting taxes. Think of
elementary schooling or day care. The gains
from eliminating such fiscal "cross-hauling" (as
Sherwin Rosen [1996] called the Swedish daycare
system) involve more than eliminating "excess
burden," but they may well be large.
The stakes in choosing the right monetary
and fiscal policies are high. Sustained inflation,
tax structures that penalize capital accumulation
and work effort, and tax-financed government
provision of private goods all have uncompensated
costs amounting to sizeable fractions of
income. We can see these costs in differences in
economic performance across different countries
and time periods. Even in the United States,
which visibly benefits from the lowest excess
burdens in the modem world, economic analysis
has identified large potential gains from further
improvements in long-run fiscal policy.
II. Gains from Stabilization:

A Baseline Calculation

In the rest of the lecture, I want to apply the
public finance framework just outlined to the
assessment of gains from improved stabilization
policy. Such an exercise presupposes a view of
the workings of the economy in which short-run
monetary and fiscal policies affect resource allocation
in ways that are different from the
supply side effects I have just been discussing.
One possibility is that instability in the quantity
of money or its rate of growth, arising from
government or private sources, induces inefficient
real variability. If that were all there was
to it, the ideal stabilization policy would be to
fix the money growth rate. (Of course, such a
policy would require the Federal Reserve to
take an active role in preventing or offsetting
instabilities in the private banking system.) But
this cannot be all there is to it, because an
economy in which monetary fluctuations induce
real inefficiencies-indeed, any economy in
which money has value-must be one that operates
under missing markets and nominal rigidities
that make changes in money into

something other than mere units changes. Then
it must also be the case that these same rigidities
prevent the economy from responding efficiently
to real shocks, raising the possibility that
a monetary policy that reacts to real shocks in
some way can improve efficiency.

If we had a theory that could let us sort these
issues out, we could use it to work out the
details of an ideal stabilization policy and to
evaluate the effects on welfare of adopting it.
This seems to me an entirely reasonable research
goal-I have been thinking success is
just around the comer for 30 years-but it has
not yet been attained. In lieu of such a theory, I
will try to get quantitative sense of the answer to
the thought-experiment I have posed by studying
a series of simpler thought-experiments.
In the rest of this section, I ask what the effect
on welfare would be if all consumption variability
could be eliminated.4 To this end, consider
a single consumer, endowed with the
stochastic consumption stream

(1) c, = Aete- (l/2)r2e,

where log(se) is a normally distributed random
variable with mean 0 and variance oC2. Under
these assumptions

E(e-(1/2)t02tt) = 1

and mean consumption at t is Ae t. Preferences
over such consumption paths are assumed to be
(2) E{ ;-y},

1 + p 1-y '

where p is a subjective discount rate, y is the
coefficient of risk aversion, and the expectation
is taken with respect to the common distribution
of the shocks so, el ... .

Such a risk-averse consumer would obviously
prefer a deterministic consumption path
to a risky path with the same mean. We quantify
this utility difference by multiplying the risky
path by the constant factor 1 + A in all dates
and states, choosing A so that the household is


### ---Economics-2003-0-05.txt---
THE AMERICAN ECONOMIC REVIEW

indifferent between the deterministic stream
and the compensated, risky stream. That is, A is
chosen to solve

(3) E: E t ((1 + A)ct) lMany

questions have been raised about this
estimate, and subsequent research on this issue
has pursued many of them, taking the discussion
deep into new scientific territory. In the
next four sections, I will review some of the
main findings.

III. Removeable Variance: Two Estimates
(Aegt)l -Y

t=0

where ct is given by (1). Canceling, taking logs,
and collecting terms gives

(4) A ^ YI '2.

A 2 y r

This compensation parameter A-the welfare
gain from eliminating consumption riskdepends,
naturally enough, on the amount of

risk that is present, cr2, and the aversion people
have for this risk, y.

We can get an initial idea of the value to the
economy as a whole of removing aggregate risk
by viewing this agent as representative of U.S.
consumers in general. In this case, to estimate A
we need estimates of the variance o02 of the log
of consumption about its trend, and of the coefficient
y of risk aversion. Using annual U.S.
data for the period 1947-2001, the standard
deviation of the log of real, per capita consumption
about a linear trend is 0.032.5 Estimates of the
parameter y in use in macroeconomics and public
finance applications today range from 1 (log
utility) to 4. Using log utility, for example, the
formula (4) yields the welfare cost estimate
(5) A = (0.032)2 = 0.0005,

about one-twentieth of 1 percent of consumption.
Compared to the examples of welfare gains
from fiscal and monetary policy changes that I
cited above, this estimate seems trivially small:
more than an order of magnitude smaller than
the gain from ending a 10-percent inflation!
5 The comparable figure using a Hodrick-Prescott trend
with the smoothing parameter 400 is 0.022.
Even if we do not know exactly how much
consumption risk would be removed by an optimal
monetary and fiscal policy, it is clear that
it would fall far short of the removal of all
variability. The major empirical finding in macroeconomics
over the past 25 years was the

demonstration by Finn E. Kydland and Prescott
(1982), replicated and refined by Gary D.
Hansen (1985) and by many others since then,
that technology shocks measured by the method
of Robert M. Solow (1957) can induce a reasonably
parameterized stochastic growth model
to exhibit nearly the same variability in production
and consumption as we see in postwar U.S.
time series. In the basic growth model, equilibrium
and optimal growth are equivalent, so that
if technology shocks are all there is to postwar
business cycles, resources are already being allocated
efficiently and a variance-reducing
monetary-fiscal policy would be welfare reducing.
Even if the equilibrium is inefficient, due to
distorting taxes, missing markets or the like, in
the face of unavoidable technology and preference
shocks an optimal monetary and fiscal
policy will surely be associated with a positive
level of consumption variance. We need to estimate
the size of that part and remove it from
the estimate of ao used in (4).

Matthew D. Shapiro and Mark W. Watson's
(1988) study is one of several relatively atheoretical
attempts to break down the variance of
production and other variables into a fraction
due to what these authors call "demand" shocks
(and which I will call "nominal" shocks) and
fractions due to technology and other sources.
Their study represents quarterly U.S. time series
over the period 1951-1985 as distributed lags of
serially independent shocks. The observables
include first differences of a measure of hours
worked, a log real GDP measure, and the corresponding
implicit price deflator. To these

three rates of change are added an ex post real
interest rate (the three-month Treasury bill rate
4

MARCH 2003


### ---Economics-2003-0-06.txt---
minus the inflation rate) and the change in the
relative price of oil. The coefficients of an invertible
vector autoregression are estimated,
subject to several restrictions. This procedure
yields time series of estimated shocks g t and
decompositions of the variance of each of the
five variables into the fractions "explained" by
the most recent k values of each of the five
shocks.

Shapiro and Watson apply a variety of theoretical
principles to the interpretation of
their estimates. They do not consistently follow
the general-equilibrium practice of interpreting
all shocks as shifts in preferences,
technologies, or the behavior of policy variables,
but they have in mind some kind of

monetary growth model that does not have a
long-run Phillips curve.6 Real variables, in
the long run, are determined by real factors
only. Nominal shocks can affect real variables
and relative prices in the short run but not in
the long run. This idea is not tested: Long-run
neutrality is imposed on the statistical model.
In return it becomes possible to estimate separately
the importance of nominal shocks to
the short- and medium-run variability of output,
hours, and real interest rates.7

In the five-variable scheme that Shapiro and
Watson use, there are two nominal variablesthe
inflation rate and the nominal interest rateand
three real ones-output, hours, and the
relative price of oil. They assume as well five
shocks, two of which are nominal in the sense
of having no effect on real variables in the long
run. They are not able to measure the effects of
the two dimensions of nominal instability separately.
The other three shocks are taken to be
real. The assumed exogeneity of oil price
shocks plus a long-run neutrality hypothesis on
hours are used to estimate the importance of
three distinct real shocks. This aspect of their
identification seems to me questionable, and in
any case it is of an entirely different nature from
the neutrality of nominal shocks. I will just
lump the effects of the real shocks together, as


Shapiro and Watson do with the two nominal
shocks, and interpret their paper as partitioning
the variance of output and hours into nominal
and real sources. The resulting Table 1 is a
condensation of their Table 2.

The two zeroes for output and hours in the
last, long-run, row of Table 1 are there by the
definition of a nominal shock. But the two 94-
percent entries in this row for inflation and the
nominal interest rate could have come out any
way. I take the fact that these values are so close
to 1 as a confirmation of Shapiro and Watson's
procedure for identifying nominal shocks. According
to Table 1, these nominal shocks have
accounted for something less than 30 percent of
short-run production variability in the postwar
United States. This effect decays slowly, with
no change after one year, a reduction to 20
percent after two years, and so on.
One can ask whether a better estimate of the
importance of nominal shocks could have obtained
by using Ml or some other observable
measure of monetary shocks. Many studies
have proceeded in this more direct way,8 and
much has been learned, but in the end one does
not know whether the importance of monetary
shocks has been estimated or just the importance
of a particular, possibly very defective,
measure of them. Information on future prices is
conveyed to people by changes in monetary
aggregates, of course, but it is also conveyed by
interest-rate and exchange-rate movements, by
changes in the fiscal situation that may lead to
tighter or easier money later on, by changes in
financial regulations, by statements of influential
people, and by many other factors. Shapiro
and Watson's method bypasses these hard


### ---Economics-2003-0-07.txt---
THE AMERICAN ECONOMIC REVIEW

measurement questions and goes directly to an
estimation of the importance of nominal shocks
in general, those we know how to measure and
those we do not, whatever they may be.
A second reason for preferring the procedure
Shapiro and Watson used is that the effects of
nominal shocks as they estimate them include
the effects of real shocks that could have been
offset by monetary policy but were not. Whatever
it is that keeps prices from rising in proportion
to a given increase in money must also
keep relative prices from adjusting as neoclassical
theory would predict they should to, say,
an increase in the OPEC-set price of oil. Effects
of either kind-those initiated by monetary
changes and those initiated by real shocks-will
last only as long as the rigidity or glitch that
gives rise to them lasts, vanishing in the long
run, and will be identified as arising from the
"nominal," or "demand," shock under the Shapiro
and Watson identification procedure. Thus
I want to interpret the estimates in columns 2
and 3 of Table 1 as upper bounds on the variance
that could have been removed from output
and hours at different horizons under some
monetary policy other than the one actually
pursued. The table gives no information on
what this variance-minimizing monetary policy
might have been, and there is no presumption
that it would have been a policy that does not
respond to real shocks.

Shapiro and Watson applied the theoretical
idea that nominal shocks should be neutral in
the long run to obtain an estimate of the fraction
of short-run output variability that can be attributed
to such shocks. Prescott (1986a) proceeded
in a quite different way to arrive at an estimate
of the fraction of output variability that can be
attributed to technology shocks. He used actual
Solow residuals to estimate the variance and
serial correlation of the underlying technology
shocks. Feeding shocks with these properties
into a fully calibrated real-business-cycle model
resulted in output variability that was about 84
percent of actual variability.9 In a complementary
study, S. Rao Aiyagari (1994) arrived at an
estimate of 79 percent for the contribution of
9 Questions of measurement errors are discussed in the
paper and by Summers (1986) in the same volume. In
Prescott (1986b), estimates of 0.5 to 0.75 for the contribu-
tion of technology shocks to output variance are proposed.
technology shocks, based on comovements of
production and labor input over the cycle.
Shapiro and Watson find that at most 30
percent of cyclical output variability can be
attributed to nominal shocks. Working from the
opposite direction, Prescott and Aiyagari conclude
that at least 75 percent of cyclical output
variability must be due to technology shocks.
These findings are not as consistent as they may
appear, because there are important real factors
besides technological shocks-shocks to the tax
system, to the terms of trade, to household technology,
or to preferences-that are cyclically
important but not captured in either of the categories
I have considered so far.l? Even so, on
the basis of this evidence I find it hard to imagine
that more than 30 percent of the cyclical
variability observed in the postwar United
States could or should be removed by changes
in the way monetary and fiscal policy is
conducted.

IV. Risk Aversion

The estimate of the potential gains from stabilization
reviewed in Section II rests on assumed
consumer preferences of the constant
relative risk aversion (CRRA) family, using but
two parameters-the subjective discount rate p
and the risk-aversion coefficient y-to characterize
all households. This preference family is
almost universally used in macroeconomic and
public finance applications. The familiar formula
for an economy's average return on capital
under CRRA preferences,

(6) r = p + yg,

where g is the growth rate of consumption,
makes it clear why fairly low 3y values must be
used. Per capita consumption growth in the
United States is about 0.02 and the after-tax
return on capital is around 0.05, so the fact that
p must be positive requires that y in (6) be at
most 2.5. Moreover, a value as high as 2.5
would imply much larger interest rate differenlO
For example, Shapiro and Watson attribute a large
share of output variance to a shock which they call "labor
supply" [and which I would call "household technology,"
following Jess Benhabib et al. (1991) and Jeremy
Greenwood and Zvi Hercowitz (1991)].
6

MARCH 2003


### ---Economics-2003-0-08.txt---
tials than those we see between fast-growing
economies like Taiwan and mature economies
like the United States. This is the kind of evidence
that leads to the use of y values at or near
1 in applications.

But the CRRA model has problems. Rajnish
Mehra and Prescott (1985) showed that if one
wants to use a stochastic growth model with
CRRA preferences to account for the entire
return differential between stocks and bondshistorically
about 6 percent-as a premium for

risk, the parameter y must be enormous, perhaps
50 or 100.11 Such values obviously cannot
be squared with (6). This "equity premium puzzle"
remains unsolved, and has given rise to a
vast literature that is clearly closely related to
the question of assessing the costs of
instability.12

One response to the puzzle is to adopt a
three- rather than two-parameter description
of preferences. Larry G. Epstein and Stanley
E. Zin (1989, 1991) and Philippe Weil (1990)
proposed different forms of recursive utility,
preference families in which there is one parameter
to determine intertemporal substitutability
and a second one to describe risk

aversion. The first corresponds to the parameter
y in (6), and can be assigned a small
value to fit estimated average returns to capital.
Then the risk-aversion parameter can be
chosen as large as necessary to account for
the equity premium.

Thomas D. Tallarini, Jr. (2000) uses preferences
of the Epstein-Zin type, with an intertemporal
substitution elasticity of 1, to construct a
real-business-cycle model of the U.S. economy.
He finds an astonishing separation of quantity
and asset price determination: The behavior of
aggregate quantities depends hardly at all on
attitudes toward risk, so the coefficient of risk
aversion is left free to account for the equity
premium perfectly.13 Tallarini estimates a welfare
cost of aggregate consumption risk of 10
percent of consumption, comparable to some


of the supply-side gains cited in Section I, and
two orders of magnitude larger than the estimate
I proposed in Section II.14 As Maurice Obstfeld
(1994) shows, this result is basically the formula
(4) with a coefficient of risk aversion two
orders of magnitude larger than the one I used.
Fernando Alvarez and Urban J. Jermann
(2000) take a nonparametric approach to the
evaluation of the potential gains from stabilization
policy, relating the marginal cost of business-
cycle risk to observed market prices
without ever committing to a utility function.
Their estimation procedure is based on the observation
that consumption streams with a wide
variety of different risk characteristics-or
something very nearly equivalent to them-are
available for sale in securities markets. They
use a mix of asset-pricing theory and statistical
methods to infer the prices of a claim to the
actual, average consumption path and alternative
consumption paths with some of the uncertainty
removed. They call the price differentials
so estimated marginal welfare costs, and show
that they will be upper bounds to the corresponding
total cost: my compensation parameter
A. The basic underlying hypotheses are that
asset markets are complete and that asset-price
differences reflect risk and timing differences
and nothing else.

The gain from the removal of all consumption
variability about trend, estimated in this
way, is large-around 30 percent of consumption.
15 This is a reflection of the high risk aversion
needed to match the 6-percent equity
premium, and can be compared to Tallarini's
estimate of 10 percent. But the gain from removing
risk at what Alvarez and Jermann call
business-cycle frequencies-cycles of eight


### ---Economics-2003-0-09.txt---
THE AMERICAN ECONOMIC REVIEW

years or less-is two orders of magnitude
smaller, around 0.3 percent. Most of the high
return on equity is estimated to be compensation
for long-term risk only, risk that could not be
much reduced by short-run policies that are
neutral in the long run.

Accepting Shapiro and Watson's finding that
less that 30 percent of output variance at
business-cycle frequencies can be attributed to
nominal shocks, the lower Alvarez and Jermann
estimate of 0.3 should be reduced to 0.1 if it is
to serve my purpose as an estimate of the value
of potential improvements in stabilization policy.
But it is important to keep in mind that this
estimate is not smaller than Tallarini's because
of a different estimate of risk aversion. Tallarini's
estimate of y - 100 is the parametric analogue
of Alvarez and Jermann's "market price of
risk," based on exactly the same resolution of
the equity premium puzzle. The different cost
estimate is entirely due to differences in the
consumption paths being compared.

Resolving empirical difficulties by adding
new parameters always works, but often only by
raising more problems. The risk-aversion levels
needed to match the equity premium, under the
assumption that asset markets are complete,
ought to show up somewhere besides securities
prices, but they do not seem to do so. No one
has found risk-aversion parameters of 50 or 100
in the diversification of individual portfolios, in
the level of insurance deductibles, in the wage
premiums associated with occupations with
high earnings risk, or in the revenues raised by
state-operated lotteries. It would be good to
have the equity premium resolved, but I think
we need to look beyond high estimates of risk
aversion to do it. The great contribution of
Alvarez and Jermann is to show that even using
the highest available estimate of risk aversion,
the gain from further reductions in businesscycle
risk is below one-tenth of 1 percent of
consumption. The evidence also leaves one free
to believe-as I do-that the gain is in fact one
or two orders of magnitude smaller.
V. Incomplete Markets and Distribution Effects
The calculations I have described so far treat
households as identical and individual risks as
diversifiable. But as Per Krusell and Anthony A.
Smith, Jr. (1999) observe, "it is quite plausible
that the welfare costs of cycles are not so high
on average, but may be very high for, say, the
very poor or currently unemployed members of
society." Several recent studies have pursued
this possibility.16 Doing so evidently requires
models with incomplete risk sharing and differently
situated agents.

Krusell and Smith (1999, 2002) study a
model economy in which individual families
are subject to three kinds of stochastic shocks.
There is an aggregate productivity shock that
affects everyone, and employment shocks that
differ from person to person. Families are infinitely
lived dynasties, but every 40 years or so
a family draws a new head, whose subjective
discount rate is drawn from a fixed distribution.
Dynasties with patient heads will accumulate
wealth while others will run their wealth
down.17 The sizes of these shocks are chosen so
that the model economy experiences realistic
GDP fluctuations, unemployment spells have
realistic properties, and the overall wealth distribution
matches the U.S. distribution: In the
model, the wealthiest 5 percent of households
own 54 percent of total wealth; in reality, they
hold 51 percent.

It is essential to the substantive question that
motivates this study that neither the employment
shocks nor the uncertainty about the character
of the household head can be diversified
away. Otherwise, the individual effects of the
aggregate productivity shocks would be the
same as in the representative agent models I
have already discussed. One may argue over
why it is that markets do not permit such diversification,
but it seems clear enough that they do
not: Where is the market where people can be
insured against the risk of having irresponsible
or incompetent parents or children?
These exogenous forces acting differentially
across households induce different individual
choices, which in turn lead to differences in
individual capital holdings. The state space in
this economy is very large, much larger than
i6 For example, Ayse Imrohoroglu (1989), Andrew
Atkeson and Christopher Phelan (1994), Krusell and Smith
(1999, 2002), Kjetil Storesletten et al. (2001), and Tom
Krebs (2002).

17 This way of modeling wealth changes within a fixed
distribution across families was introduced in John Laitner
(1992).

8

MARCH 2003


### ---Economics-2003-0-10.txt---
anything people were working with numerically
15 years ago, and without the method developed
in Krusell and Smith (1998) it would not have
been possible to work out the predictions of this
model. A key simplification comes from the fact
that the impact on any one family of the shocks
that hit others has to work through two prices,
the real wage and the rental price of capital.
These prices in turn depend only on the total
stock of capital, regardless of the way it is
distributed, and total employment, regardless of
who has a job and who does not. By exploiting
these features, solutions can be calculated using
an iterative procedure that works like a dream:
For determining the behavior of aggregates,
they discovered, realistically modeled household
heterogeneity just does not matter very
much.

For individual behavior and welfare, of
course, heterogeneity is everything. In the
thought-experiments that Krusell and Smith run
with their model, removal of the business cycle
is defined to be equivalent to setting the aggregate
productivity shock equal to a constant. It is
important to be clear on what the effect of such
a change would be on the behavior of the employment
shocks to which individuals are subject,
but the magical character of the experiment
makes it hard to know how this question is best
resolved. I will describe what Krusell and Smith
did, and deal with some other possibilities later on.
Suppose that a shock y = az + - affects an
individual's behavior, where z is the aggregate
shock and e is idiosyncratic. We project the
individual shock on the aggregate, e = cz + 7J,
where the residual qr is uncorrelated with z, and
then think of an ideal stabilization policy as one
that replaces

y = az + e = (a + c)z + r1

with

y = (a + c)E(z) + *1.

Not only is the direct effect of the productivity
shock z removed but also the indirect effects of
z on the individual employment shocks e.18 In


this particular application, removing the variance
of the aggregate shock is estimated to
reduce the standard deviation of the individual
employment shocks by 16 percent.19

The first such thought-experiment Krusell
and Smith describe involves a comparison between
the expected utility drawn from the
steady state of the economy with aggregate
shocks and the expected utility from the steady
state of the economy with aggregate shocks and
their indirect effects removed in the way I have
just described. The welfare gain from eliminating
cycles in this sense turns out to be negative!
In a model, like this one, in which markets for
risk pooling are incomplete, people will engage
in precautionary savings, overaccumulating
capital in the effort to self-insure. This implies
larger average consumption in the more risky
economy. Of course, there are costs to accumulating
the higher capital stock, but these costs are
not fully counted in a steady-state comparison.
In any case, as Krusell and Smith emphasize,
there is nothing really distributional about a
steady-state comparison: Every infinitely lived
dynasty is assigned a place in the wealth distribution
at random, and no one of them can be
identified as permanently rich or poor. The
whole motivation of the paper is to focus on the
situation of people described as "hand-to-mouth
consumers," but a steady-state comparison
misses them. This observation motivates a second
thought-experiment-one with much more
complicated dynamics than the first-in which
an economy is permitted to reach its steadystate
wealth distribution with realistic aggregate
shocks, and then is relieved of aggregate risk.
The full transition to a new steady state is then
worked out and taken into account in the utility
comparisons. In this experiment, we can identify
individuals as "rich" or "poor" by their position
in the initial wealth distribution, and discuss
the effects of risk removal category by category.
The average welfare gain in this second experiment
is about 0.1 of 1 percent of consumption,
about twice the estimate in Section II of
this paper. (Krusell and Smith also assume log
utility.) But this figure masks a lot of diversity.
Low wealth, unemployed people-people who


### ---Economics-2003-0-11.txt---
THE AMERICAN ECONOMIC REVIEW

would borrow against future labor income if
they could-enjoy a utility gain equivalent to a
4-percent perpetual increase in consumption.
Oddly, the very wealthy can also gain, as much
as 2 percent. Krusell and Smith conjecture that
this is due to the higher interest rates implied by
the overall decrease in precautionary savings
and capital. Finally, there is a large group of
middle wealth households that are made worse
off by eliminating aggregate risk.

These calculations are sensitive-especially at
the poor end of the distribution-to what is assumed
about the incomes of unemployed people.
Krusell and Smith calibrate this, roughly, to current
U.S. unemployment insurance replacement
rates. If one were estimating the costs of the depression
of the 1930's, before the current welfare
system was in place, lower rates would be used
and the cost estimates would increase sharply.20 It
would also be interesting to use a model like this
to examine the trade-offs between reductions in
aggregate risk and an improved welfare system.
Storesletten et al. (2001) study distributional
influences on welfare cost estimates with methods
that are closely related to Krusell and
Smith's, but they obtain larger estimates of the
gains from removing all aggregate shocks. They
use an overlapping generations setup with 43
working age generations, in which the youngest
cohort is always credit constrained. In such a
setting, the young are helpless in the face of
shocks of all kinds and reductions in variance
can yield large welfare gains. But if the age
effects are averaged out to reflect the importance
of intrafamily lending (as I think they
should be) the gains estimated by Storesletten et
al. under log utility are no larger than Krusell
and Smith's.21 In contrast to earlier studies,
however, the Storesletten et al. model implies
that estimated welfare gains rise faster than
proportionately as risk aversion is increased:
From Exhibit 2, for example, the average gain
increases from 0.6 of a percent to 2.5 as y is
increased from 2 to 4.

Two features of the theory interact to bring
this about.22 First, and most crucial, is a differ-
20 See Satyajit Chatterjee and Dean Corbae (2000).
21 Based on Exhibits 2 and A.3.1.

22 Storesletten et al. do a good job of breaking the
differences into intelligible pieces. I also found the example
explicitly solved in Krebs (2002) very helpful in this regard.
ence in the way reductions in the variance of
aggregate shocks affect risks faced at the individual
level. In the Storesletten et al. simulations,
a bad realization of the aggregate

productivity shock increases the conditional
variance of the idiosyncratic risk that people
face, so aggregate and individual risks are compounded
in a way that Krusell and Smith rule
out. A second difference is that idiosyncratic
shocks are assumed to have a random walk
component, so their effects are long lasting. A
bad aggregate shock increases the chances that
a young worker will draw a bad individual
shock, and if he does he will suffer its effects
throughout his prime working years.
The effects of these two assumptions are
clear: They convert small, transient shocks at
the aggregate level into large, persistent shocks
to the earnings of a small fraction of households.
Whether they are realistic is question of
fact. That individual earnings differences are
highly persistent has been clear since Lee
Lillard and Robert Willis's pioneering (1978)
study. The fanning out over time of the earnings
and consumption distributions within a cohort
that Angus Deaton and Christina Paxson (1994)
document is striking evidence of a sizeable,
uninsurable random walk component in earnings.
The relation of the variance of earnings
shocks to the aggregate state of the economy,
also emphasized by N. Gregory Mankiw (1986)
in connection with the equity premium puzzle,
has only recently been studied empirically.
Storesletten et al. find a negative relation over
time between cross-section earnings means and
standard deviations in Panel Studies of Income
Dynamics data. Costas Meghir and Luigi
Pistaferri (2001) obtain smaller estimates, but
also conclude that "the unemployment rate and
the variance of permanent [earnings] shocks
appear to be quite synchronized" in the 1970's
and 1980's.

These issues are central to an accurate description
of the risk situation that individual
agents face, and hence to the assessment of
welfare gains from policies that alter this situation.
The development of tractable equilibrium
models capable of bringing cross-section and
panel evidence to bear on this and other macroeconomic
questions is an enormous step forward.
But Krusell and Smith find only modest
effects of heterogeneity on the estimates of wel-
10

MARCH 2003


### ---Economics-2003-0-12.txt---
fare gains from the elimination of aggregate
risk, and even accepting the Storesletten et al.
view entails an upward revision of a factor of
only about 5.

The real promise of the Krusell-Smith model
and related formulations, I think, will be in the
study of the relation of policies that reduce the
impact of risk by reducing the variance of
shocks (like aggregate stabilization policies) to
those that act by reallocating risks (like social
insurance policies). Traditionally, these two
kinds of policies have been studied by different
economists, using unrelated models and different
data sets. But both appear explicitly in the
models I have reviewed here, and it is clear that
it will soon be possible to provide a unified
analysis of their costs and benefits.
VI. Other Directions

My plan was to go down a list of all the
things that could have gone wrong with my
1987 calculations but, as I should have anticipated,
possibilities were added to the list faster
than I could eliminate them. I will just note
some of the more interesting of these possibilities,
and then conclude. The level of consumption
risk in a society is, in part, subject to
choice. When in an economy that is subject to
larger shocks, people will live with more consumption
variability and the associated loss in
welfare, but they may also substitute into riskavoiding
technologies, accepting reduced average
levels of production. This possibility shows
up in the precautionary savings-overaccumulation
of capital-that Krusell and Smith (1999,
2002) found. As Garey Ramey and Valerie A.
Ramey (1991) suggested, this kind of substitution
surely shows up in other forms as well.
In an endogenous growth framework, substitution
against risky technologies can affect rates
of growth as well as output levels. Larry E.
Jones et al. (1999) and Epaulard and Pommeret
(2001) explore some of these possibilities,
though neither study attributes large welfare
gains to volatility-induced reductions in growth
rates. Gadi Barlevy (2001) proposes a convex
adjustment cost that makes an erratic path of
investment in knowledge less effective than a
smooth path at the same average level. In such
a setting, reducing shock variability can lead to
higher growth even without an effect on the
average level of investment. He obtains welfare
gains as large as 7 percent of consumption in
models based on this idea, but everything
hinges on a curvature parameter on which there
is little evidence. This is a promising frontier on
which there is much to be done. Surely there are
others.

VII. Conclusions

If business cycles were simply efficient responses
of quantities and prices to unpredictable
shifts in technology and preferences, there
would be no need for distinct stabilization or
demand management policies and certainly no
point to such legislation as the Employment Act
of 1946. If, on the other hand, rigidities of some
kind prevent the economy from reacting efficiently
to nominal or real shocks, or both, there
is a need to design suitable policies and to
assess their performance. In my opinion, this is
the case: I think the stability of monetary aggregates
and nominal spending in the postwar
United States is a major reason for the stability
of aggregate production and consumption during
these years, relative to the experience of the
interwar period and the contemporary experience
of other economies. If so, this stability
must be seen in part as an achievement of the
economists, Keynesian and monetarist, who
guided economic policy over these years.
The question I have addressed in this lecture
is whether stabilization policies that go beyond
the general stabilization of spending that characterizes
the last 50 years, whatever form they
might take, promise important increases in welfare.
The answer to this question is "No": The
potential gains from improved stabilization policies
are on the order of hundredths of a percent
of consumption, perhaps two orders of magnitude
smaller than the potential benefits of available
"supply-side" fiscal reforms. This answer
does depend, certainly, on the degree of risk
aversion. It does not appear to be very sensitive
to the way distribution effects are dealt with,
though it does presuppose a system of unemployment
insurance at postwar U.S. levels. I
have been as explicit as I can be on the way
theory and evidence bear on these conclusions.
When Don Patinkin gave his Money, Interest,
and Prices the subtitle "An Integration of Monetary
and Value Theory," value theory meant, to


### ---Economics-2003-0-13.txt---
THE AMERICAN ECONOMIC REVIEW

him, a purely static theory of general equilibrium.
Fluctuations in production and employment,
due to monetary disturbances or to shocks
of any other kind, were viewed as inducing
disequilibrium adjustments, unrelated to anyone'
s purposeful behavior, modeled with vast
numbers of free parameters. For us, today, value
theory refers to models of dynamic economies
subject to unpredictable shocks, populated by
agents who are good at processing information
and making choices over time. The macroeconomic
research I have discussed today makes
essential use of value theory in this modem
sense: formulating explicit models, computing
solutions, comparing their behavior quantitatively
to observed time series and other data
sets. As a result, we are able to form a much
sharper quantitative view of the potential of
changes in policy to improve peoples' lives than
was possible a generation ago.
 ## Economics-2004-0


### ---Economics-2004-0-02.txt---
Social Securityt

By PETER DIAMOND*

I frequently find economists who express a
view of the system that is very far from mine.
For example, many young economists and economics
students say that they expect to get no
benefits at all from Social Security. This expectation
does not seem sensible to me. If there is
no legislation changing Social Security, trust
fund assets and payroll tax revenue (and revenue
from the taxation of benefits) are projected
to be sufficient to pay all the benefits scheduled
under current law until 2042 (Board of Trustees
of Social Security and Medicare, 2003). After
the trust fund assets are exhausted the payroll
tax revenue would continue to be available to
pay benefits, with the flow of revenues at that
time sufficient to pay roughly three-quarters of
the benefits scheduled in current law. The estimate
for the end of the 75-year projection period
shows enough revenue to pay roughly
two-thirds of scheduled benefits. With initial
benefits indexed to earnings, average real benefits
would be higher than today, although replacement
rates would only be roughly 60

percent of current levels for the medium
worker. This projection is a far cry from no
benefits.

Moreover, I anticipate that Congress will act
before the trust fund is exhausted, both lowering


benefits relative to those scheduled under current
law and providing additional revenues to
finance higher benefits than are payable after
2042. After all, the financial problem of Social
Security is not so very large (unlike the larger
and more complex set of financial problems of
Medicare and Medicaid).1 An increase in tax
revenue of just over 15 percent of currently
projected payroll tax revenues would handle the
projected cash flow problem for 75 years on a
present value basis. On an annual cash flow
basis, the share of GDP needed to provide all of
the benefits scheduled in current law would
increase from 4.4 percent of GDP today to 7.0
percent in 2077.2 Like almost everyone else, I
do not favor addressing the projected deficit by
simply adding more revenues with no other
changes. Nor do I picture that solution as having
any political prospects. But solving the problem
with a mix of benefit reductions and revenue
increases does not require large changes, nor
does it require a fundamental restructuring of
the program.

It is not just in the perception of the projections
and the forecast of politics that I find
myself in disagreement with opinions that I
often hear. More generally, I think the system
works better than many economists think. I
hope to convince you that the approach inherent
in the current U.S. system broadly makes good
sense. In particular, I will argue that it makes
sense to mandate taxes to finance a reasonable
replacement of earnings after retirement; that it
makes sense to mandate that retirement benefits
be paid as an annuity; that it makes sense to
mandate protection for family members, both
young children and surviving spouses; that it


### ---Economics-2004-0-03.txt---
THE AMERICAN ECONOMIC REVIEW

makes sense to have a progressive benefit formula;
that it makes sense to limit benefits to
those who are old enough and stop working or
are even older (whether they stop or not); and
that having been generous to early cohorts, it
makes sense now to continue with a system that
is only partially funded.

This is not to say that I agree with all of the
details of the current structure by any means. Of
course we should change benefit and tax rules
so that we restore actuarial balance-so that
projected revenues are sufficient to pay for projected
benefits over at least 75 years. And other
changes would be desirable as well. I am just
arguing that the overall design of Social Security
makes good sense. In addition to presenting
the basis of my support for the broad structure
of Social Security, I will identify some rules
needing change and I will speculate on why
some economists seem to have a different view
from mine.

But I will not get into the debate of whether
there should be fully funded individual accounts
financed from existing payroll tax revenues
(carve-out accounts). Nor shall I discuss the
political and economic issues associated with
the potential role of stocks in Social Security.
Those controversial subjects would take up
most of the address and I prefer to write about
more fundamental issues. Those interested in
my view as to why carve-out accounts would
not be good policy in the United States today
can turn to Chapter 8 in my book with Peter R.
Orszag (2004), which also contains a package
of changes to restore actuarial balance and
strengthen protection of some vulnerable
groups. We discuss the potential role of stocks
as well.

I will not say much about the advantage of a
mandate to save for retirement-there is little
call for eliminating such a mandate. After a
discussion of a framework for thinking about
Social Security (Section I), I will consider annuitization
(Section II), treatment of the family
(Section III), the interaction among income distribution,
insurance, and labor supply (Section
IV), the degree of funding (Section V), and
adjustments over time to benefits and taxes
(Section VI).3 Not discussed but worth keeping
3My approach has some similarity to the three-
dimensional analysis of different pension systems in Assar
in mind are the supporting antipoverty programs
as they affect the elderly [Supplemental
Security Income (SSI) and Medicaid], and the
provision of medical insurance for the elderly
(Medicare). Nor will I discuss the Disability
Insurance program, which is a critical part of
Social Security.

I. Providing Retirement Income

One-third of the elderly received at least 90
percent of their income from Social Security in
2001, with nearly two-thirds receiving at least
half (Social Security Administration, 2003). Yet
Social Security was always meant to be a foundation
for retirement income and not a level to
be relied on exclusively. The average new
award for a retired worker in 2002 was just over
$900 per month. For a worker retiring in 2002 at
age 62 (the modal retirement age), a worker in
the middle of the earnings distribution received
a benefit of roughly one-third of (wageindexed)
lifetime average earnings in 2000 dollars.
If the worker had a nonworking spouse of
the same age, the benefit would be largerabout
one-half of the worker's lifetime average
earnings.4 These are low replacement ratesyou
would not want to retire on one-third to
one-half of what you had earned on average in
your lifetime. Benefits would look even lower
compared to earnings over the last decade of
work for a worker with the typical age-earnings
profile. As a foundation for retirement income,
Social Security is something substantial to build
on. As a level to live on, it is clearly inadequate.
Excessive reliance on Social Security, despite
its relatively low replacement rates, together
with a more general picture of many workers
with inadequate wealth at retirement age, seem
Lindbeck and Mats Persson (2003). They refer to the three
dimensions as the distinction between defined benefit and
defined contribution, funded and unfunded, and actuarial
and nonactuarial. All three are matters of degree, not zero-
one choices. A primary difference is that my presentation is
focused on issues particularly salient in the U.S. context,
while I think that theirs was influenced by the Swedish
reform.

4 Of women receiving benefits in 2002, roughly one-
third received benefits solely as beneficiaries. The rest had
at least ten years of earnings history. For the latter group,
the replacement rate for the couple would be lower than the
one reported for the case of a nonearning spouse.
MARCH 2004

2


### ---Economics-2004-0-04.txt---
the best evidence for evaluating whether workers
make adequate preparation for retirement.
I prefer the term "inadequate preparation" to
"insufficient savings." Preparation involves
multiple decisions. Indeed, one decision is to
save, to have less consumption than after-tax
earned income. Another is investing well. A
third is getting adequate insurance for earnings
risk to have a satisfactory outcome in retirement
despite a possibly adverse earnings experience.
And a fourth is using insurance to arrange income
flows after retirement. There are lots of
ways that workers could end up with inadequate
consumption after retirement relative to what
might sensibly and efficiently be done with earlier
earnings.

In addition to having low savings, many
workers have problems converting savings in
different years into retirement incomes in later
years in different states of nature. We know
from 401(k) studies that many workers do not
diversify sensibly and many do not choose a
sensible portfolio for long-term investments.5
The tendency of many workers to accept the
default allocation set by their employers is suggestive
that they do not have a clear view of
how to choose a portfolio. Outside employerorganized
retirement savings, others pay advisors
as much as 1 percent of assets each year to
help them select mutual funds (in what are
called wrap accounts). Paying 1 percent extra
per year reduces the accumulation at the end of
a 40-year career by roughly 20 percent.6 Mutual
funds, even very similar ones, come with quite
different annual charges. While the average of
charges of mutual funds containing equities is
currently 1 1/4 percent of assets per year (including
a prorating of front loads), some workers
pay much more. A fee of the average size takes
away roughly 25 percent from what would be
there at retirement without any fee. Thus many
workers find it harder to accumulate enough for
retirement than they might, than an idealized
theory says they should. To be clear, I am not
proposing that these market opportunities be
banned-although improvement in regulation
would be welcome. Rather, I am saying that
analysts of Social Security should be realistic


about the actual functioning of the market
alternatives.

Investing is only part of the story. We lack
market institutions to provide good insurance
for the risk in earnings trajectories, thereby affecting
the realized pattern of assets at retirement
relative to earnings potential. In the
Arrow-Debreu framework, workers have deterministic
budget constraints from selling their
labor supplies conditional on all the states of
nature in which they have labor that they choose
to sell. That is, they transfer resources across
states of nature to those where the purchasing
power is needed more. Making the same point
in a finance vocabulary, markets do not currently
exist for directly hedging the risks in
earnings opportunities, and if they did exist I do
not think we would see many workers using
them.7

In addition to problems in converting earnings
opportunities into wealth devoted to retirement
consumption, the wealth that is privately
allocated to retirement consumption does not
make adequate use of annuities. This problem
would be more severe without the annuities
provided by Social Security, since the utility
value of the marginal annuity decreases with the
extent of existing annuitization.

These shortcomings in providing for retirement
income fall on surviving spouses even
more heavily than on workers. While 5 percent
of elderly married couples have incomes below
the poverty line, with another 3 percent near
poverty, these figures more than triple when we
consider widows. Indeed, widowhood is associated
with a roughly 30 percent drop in income
relative to needs (Karen Holden and Cathleen
Zick, 1998).8 This is strongly suggestive of
inadequate protection of family members.
To my mind, the heart of the context for
thinking about Social Security is that it substitutes
for poor decision making and for missing
insurance opportunities (missing perhaps because
poor decision making implies low demand)
. The various shortcomings that are
apparent even in the presence of Social Security


### ---Economics-2004-0-05.txt---
THE AMERICAN ECONOMIC REVIEW

would be more severe in the absence of a program.
These different shortcomings in preparation
for retirement relate to different issuesinadequate
overall provision for retirement

relates to having a mandatory program, inadequate
annuitization relates to providing benefits
in annuitized form, inadequate protection of
family members relates to providing benefits for
surviving spouses and young children. I start
with the annuitization issue, since there is little
overt move to end the mandatory nature of
Social Security as a whole. But there are calls
for decreasing the role of annuities in Social
Security.

But first a word on the Arrow-Debreu framework.
I have referred to it above as a way to
describe the properties of a Pareto-optimal allocation.
I think that when economists quickly
consider economic issues outside their own subdisciplines,
they frequently think implicitly in
terms of the Arrow-Debreu model with its connection
to first-best outcomes (also incorporating
overlapping generations for some issues). In
contrast, economists thinking about issues in
their subdisciplines often share a framework
that is more complex and are more inclined to
do second-best analyses, which are more directly
policy-relevant. As in other subdisciplines,
analysts of Social Security are well
aware of the issues I have identified, although
the issues are not present in all analyses by any
means. The Arrow-Debreu model tends to start
our thinking in terms of the standard, fully
rational model of individual decision making
and in terms of a complete set of markets. That
is a reasonable place to start as long as it is not
also the end of modeling and thinking. For
example, simulations of Social Security reforms
that assume an overlapping generations model
with fully rational lifetime utility maximization
should not be taken as the whole story for Social
Security policy-making. It is inadequate and
potentially misleading to study the effects of
Social Security in models in which there is no
particular reason for Social Security to exist in
the first place. This would be akin to treating
Pigouvian taxation to correct externalities as
distortive by ignoring the externalities.
Interest in the description of behavior that
deviates from that in the Arrow-Debreu model
has grown enormously lately. Long before behavioral
economics became a hot topic, public
policies reflected recognition that the model of
homo economicus, while very useful, is not a
fully adequate basis for the design of all policies.
For example, federal legislation introduced
a "cooling-off' period during which contracts
with door-to-door salespeople could be cancelled
without penalty precisely because of deviations
from homo economicus. And social

security discussions have long recognized inadequate
savings for retirement by many workers
and inadequate annuitization by most. In addition,
social security systems have been concerned
about protection of the family and not
just the worker.9 Also possibly relevant, but not
much studied, is whether significant numbers of
workers retire too soon for their own good.
These issues of poor choices in the presence of
available opportunities are in addition to insurance
market limitations that come from

market incompleteness and from asymmetric
information.

Inadequate attention to the future in general
and its stochastic structure in particular implies
some form of time inconsistency. Normative
criteria for evaluating institutions become more
complicated without time consistency throughout
the population. Insofar as individuals are not
time-consistent, it seems essential to do normative
evaluations in terms of shorter periods (e.g.,
years) as well as in terms of lifetimes. We care
about actual consumption levels as well as the
levels of lifetime resources.

This requires more than just a positive theory
of how people determine consumption but also
normative criteria for evaluating consumption
at different times. The vocabulary of someone
being different selves at different times is suggestive,
although I am concerned that taking it
too literally, failing to recognize the tight links
between the different selves who are the same
person at somewhat different ages, is failing to
address adequately the underlying issues.'? And
9 The growth of two-career families has altered the na-
ture of this concern and presumably the most sensible
design for the system, but has not made the problem go
away. There is a tension in social security systems, just as
there is in income taxation, between treatment of individu-
als and treatment of the family. Diversity in the way resources
are allocated within different families affects the
evaluation of different benefit designs.
10 More generally, there is a difference between taking a
model literally and taking it seriously-which involves
learning from models in order to think about a reality that is
more complex than is captured in any model-indeed that is
4

MARCH 2004


### ---Economics-2004-0-06.txt---
since the political process is not equivalent to a
consistent approach to policy over time (which,
it seems to me, is an essential property of democracy
given divergent preferences and

views), we must consider issues from multiple
perspectives.11

An education built around the Arrow-Debreu
model may lead to overvaluing the fundamental
welfare theorem. The wonderful properties of
competitive equilibrium in certain unrealistic
circumstances lead the profession to be very
aware of distortions that prevent first-best outcomes.
But some distortions are associated with
redistribution and with easing other deviations
from first-best rules. Stressing the distortions
caused by government policies and not giving
equal weight to the redistribution and insurance
and revenue generation accomplished by these
policies, effectively doing partial first-best
thinking rather than complete second-best
thinking, can lead to unbalanced inferences
about policies.12

II. Annuitization

Some mandate for retirement saving is not
particularly controversial among policy-oriented
economists, so I begin with mandatory annuitization.
First, let us consider the point of payments
that are conditional on being alive. With
some saving for retirement (over and above
precautionary balances) a worker can learn of
rates of return (and risks) available in the market
for investing for different lengths of time
(that is, including an illiquidity premium). Anyone
investing for some period of time (for example,
bank certificates of deposit, insurance
contracts, mutual funds with an early withdrawal
penalty, direct loans) could wonder how
much more might be paid if the investor were
still alive provided there was no payment at all
if the investor were no longer alive. With a


noticeable probability of the investor's dying
before reaching the end of the contract period
and little cost for checking whether the investor
is still alive, it would be worthwhile for a borrower
(bank, insurance company, mutual fund,
or direct borrower) to offer some additional
payment in return for being freed from payment
in the event of the death of the investor.
This is the essence of an annuity and the
essence of why for an investor with no interest
in bequests and a tolerance for some illiquidity,
an annuitized asset dominates the same asset
without an annuity feature. This is how the
Arrow-Debreu model works when markets are
complete-the gain from annuitization can be
thought of as a lowering of the price of future
consumption by forgoing deliveries after one's
death. The formal argument for the dominance
of annuitization was made by Menahem Yaari
(1965) in the context of a conventional annuity
that guarantees payments over the rest of one's
life. But the argument is much broader than that
(Thomas Davidoff et al., 2003). Moreover, simulations
show a sizable quantitative importance
of annuity opportunities.

People do care about their children. But, a
bequest motive does not eliminate the advantage
of some annuitization. With a bequest motive
and complete Arrow-Debreu markets, one
would determine how much of one's lifetime
budget constraint to give away and when to give
it (e.g., when children reach some age). It would
seem very odd to prefer to have one's children
receive an amount in present value that was
conditional on how long one lived (even if one
did not want to make a transfer before dying).
So, one would still use annuities, the purchase
of commodities conditional on being alive, for
all of one's own planned consumption. That is,
having a bequest motive is not a basis for doing
no annuitization in a complete market settingunless
one was roughly risk neutral about both
the amount of bequest and its timing. Without
complete markets, a willingness to invest in
illiquid assets for future consumption leads to
the same advantage for some annuitization.


### ---Economics-2004-0-07.txt---
THE AMERICAN ECONOMIC REVIEW

Despite the advantages of annuities, we see
only a small fraction of people doing voluntary
annuitization.14 Furthermore, those who do annuitize
make very odd choices. They buy nominal
annuities. There is wide popularity of what
are called guarantees-continued payments after
death up to some limit.15 Such guarantees
undo some of the underlying annuitization, and
are a relatively expensive form of holding nonannuitized
wealth (given the relative administrative
costs on annuities and other accounts).
They represent an increase in the riskiness of
one's bequest, not a decrease. That is, an annuity
without a guarantee costs less, allowing one
to leave one's heirs a determinate amount in
present value, rather than a random amount
depending on the date of death.'6 More generally,
many features of insurance markets are
hard to reconcile with sensible decisions by
households and the equilibrium industry response
we would expect in the presence of

sensible demands. The extremely limited options
available for annuitization seem to reflect
the natural response of the supply of insurance
to the nature of demand.17

Some have tried to explain this limited use of
annuities by the degree of annuitization that
already exists in government programs. But voluntary
annuitization, while present for centuries
before the creation of these programs, was not
extensive in the population and is unlikely to
become extensive if the programs were removed.
Asymmetric information is another candidate
for explaining this situation, and it does
cause an adverse selection effect on pricing that
14 There is a thriving market in what are called variable
annuities, but these are tax-favored investment vehicles
with a bit of insurance included (to get the favorable tax
treatment) and an option to purchase a genuine annuity, an
option that appears to be rarely taken. With the recent
addition of a lump-sum option in many defined benefit
plans, many workers may be forgoing an annuity, although
it is difficult to tell since some may simply be waiting to
annuitize later.

15 Among TIAA-CREF annuitants roughly three-
quarters choose some guarantee period (John Ameriks,
2002).

16 Guarantees may play a role in addressing adverse
selection, but that is, itself, a reflection of poor functioning
in this market relative to ideals.

17 Although markets provide both term life insurance
and whole-life contracts, the only annuities in the market are
for payments over the rest of one's life, from the date of the
first payment.

would discourage some individuals from annuitizing.
While large systematic differences in
life expectancy do exist, much of the difference
is readily attributed to easily measured factors,
so insurance companies could do more to overcome
this problem, given the potential for large
gains to the insured.18 In the United Kingdom,
there is a sizable market for individual purchase
of annuities because of a large tax incentive for
their purchase from assets in tax-favored individual
retirement accounts. In the presence of
this demand, suppliers are offering annuities
with better prices for those with "impaired
lives." We do not see this risk classification in
the United States, presumably because there is
not a ready market in which firms could take
advantage of selection by risk classification and
better pricing since so few households purchase
annuities on an individual (nongroup) basis. So,
adverse selection alone can not explain the low
level of annuitization that is present.'9
I believe the major issue behind this pattern
of insurance demand is the failure of many to
understand the advantages of annuitization.
This plausibly relates to the failure of much of
the population to understand the properties of
stochastic variables, as has been documented by
cognitive psychologists. It is to be expected that
the set of insurance products that are marketed
will reflect the shortcomings of consumer
understanding-it is very expensive to try to
sell a product the virtues of which potential
customers do not understand. I think that without
Social Security, inadequate annuitization
would be even more widespread than inadequate
savings.

In any event, social security systems in advanced
countries typically provide benefits as
annuities, annuities that are generally indexed to
prices or wages (or a combination). This is a
simple application of the view that in a mandatory
program, individuals should be given what
we think they would want if they were well-
18 Antidiscrimination rules do limit the variables that
insurance companies can recognize in pricing. But more
could readily be done with allowable categories, such as
smoking, type of job, earnings level. The adaptations of the
life insurance market to the presence of adverse selection
are suggestive that adaptations would occur if the market for
annuities were of a comparable size to that for life insurance.
19 Note that there is considerable risk classification for
life insurance.

6

MARCH 2004


### ---Economics-2004-0-08.txt---
informed and well-educated. The presence of
mandatory annuitization does not prevent bequests,
although it raises the cost and requires
action to do so. Those surviving to the start of
their benefits and with sufficient life expectancy
can use part of their monthly Social Security
benefits to finance a long-term life insurance
contract, thereby providing a bequest with an
explicit choice of the relationship between the
real size of the bequest and the date of death.
This action contrasts with simply leaving unspent
funds to one's heirs, a strategy that leaves
an amount dependent on the history of consumption
relative to the income earned on

assets.20

In other words, the government's choice between
providing retirement benefits as annuities
or as lump sums can be considered as a choice
of a default, one which most individuals could
reverse-by purchasing life insurance if provided
an annuity or purchasing an annuity if
offered a lump sum (B. Douglas Berheim,
1991). Reversing the government choice,
though, takes time, thought, and effort and it has
a cost. That is, the government provides annuitization
at a far lower cost than does the private
market. The absence of selling costs (other than
equivalent information provision) and economies
of scale contribute to this advantage. Administrative
costs of Social Security are less

than 1 percent of annual expenditures, and a
great deal of that is due to the disability program,
which is naturally more expensive to run.
In contrast, privately provided insurance has
higher costs-life insurance company accounting
generally recognizes over 10 percent of
premia used for administrative costs and profit.
The private market is more expensive and does
not do a better job of delivering annuity products
that people need.21 This is one reason to


have government provision rather than a mandate
to purchase an annuity in the private market.
22 As with many other settings, we expect
individuals to undo little of what is provided.23
So it makes sense to offer what we think people
might sensibly want. Moreover, the wider functioning
of the life insurance market than the
annuities market suggests a further advantage to
using substantial annuitization as the default.
A mandatory retirement income program requires
a choice of the form of benefit and it is
hard to think of a basis for choosing the form
which is other than what makes sense for the
bulk of the population. It seems to me that this
is an annuity in some form.24

A. Lifetime Income Distribution

Mandated annuitization affects lifetime
income distribution.25 Suppose one were


### ---Economics-2004-0-09.txt---
THE AMERICAN ECONOMIC REVIEW

comparing two mandatory programs, one with
lump-sum payments and one with annuities.
This comparison would be easy if individual
choices between annuitization and nonannuitization
were unaffected by the government

choice. Then one would simply compare the
implicit price of the trade-off between annuities
and lump sums in the alternative mandatory
programs with the explicit price at which
individuals could make transactions. For example,
if everyone annuitized and the market
had a single price for all annuities, then we
would compare the price implicit in the comparison
of the programs with the actual uniform
price. In this case, we would find

mandatory annuitization attractive because
the government would be likely to have a
better price than the market.26 Conversely, if
everyone would purchase life insurance to
undo a mandatory annuity (and rates were
uniform), then we would find mandated annuities
unattractive since the private market
price for life insurance would likely be larger
than the implicit price if the government
switched from annuities to lump sums.
The story becomes a little more complicated
if we assume that everyone annuitizes
and the market would offer different prices to
different people. This might happen with a
mandate to purchase annuities in the private
market if the market had some degree of price
diversity by risk class.27 Then, in addition to
the difference from the average price with and
without the government annuity, we would
note the differences in prices for different
people. Relative to annuities priced differently
for different groups, the uniform

annuitization implicit in the mandated annuitization
would favor those with longer expected
lives-women relative to men, male

high earners relative to male low earners,
female high earners relative to female low
earners. A progressive benefit formula can be
26W. H. Beveridge (1942) argued that in the United
Kingdom the government systematically did better than
private insurance markets.

27 Annuity pricing that varies with stochastic health out-
comes implies a risk of the classification to which one will
belong. With annuitization done at a single time in life, the
degree of risk classification involves a tension between
providing more insurance and providing more accurate labor
market incentives.

used to offset the systematic variation in life
expectancy with earnings within genders.
For this or any income distribution comparison,
we must have a counterfactual, preferably
a plausible one. Without a mandate, the
relevant counterfactual is that approximately
no one would annuitize. Pretty much everyone
would lose the insurance gains from annuitization.
28 We can compare the mandate

with this counterfactual in two steps-first the
value of annuitization assuming actuarially
fair pricing and then the difference, described
above, between fair and uniform pricing.
Since those groups with shorter life expectancies
have more to gain from fair annuitization
[assuming CRRA preferences in the

usual range and realistic mortality rates (Jeffrey
Brown, 2003)] this counterfactual shows
much less diversity in the utility value of
annuitization than the previous comparison.29
Indeed, Brown finds that the utility value of
annuitization (relative to wealth) is similar for
groups divided by gender, race, and education.
Thus the differences in expected payments
from different life expectancies have
less distributional impact in utility terms than
in expected payment calculations.30
281 ignore the role of access to minimum income guar-
antees (SSI).

29 Someone with a higher probability of dying would
find a larger decrease in the price of consumption when
going from unconditional purchase to purchase conditional
on being alive with fair pricing. Without annuitization,
someone with a higher probability of dying would generally
consume less in later years, ceteris paribus, and so have less
consumption on which to receive a price decrease. Given
the preference structure in Brown, the net result of these two
effects is that those with shorter lives gain more from fair
annuitization, tending to offset the redistribution from a
change from fair to uniform annuity pricing. Interpretation
of the Brown analysis of the total impact of annuitization is
aided by the analysis in Bernheim (1987) of the valuation of
marginal annuitization relative to life expectancy and li-
quidity constraints.

30 Implicitly this income distribution discussion has as-
sumed rational lifetime consumption allocation-the only
behavioral element being an unexplained, and unexplain-
able (on rational grounds), failure to purchase annuities.
Similarly, the simulations showing the value of annuiti-
zation assume optimal consumption paths both with and
without annuitization. Any full normative evaluation of
annuitization should reflect the fact that those living
longer after retirement will have a larger marginal utility
of consumption for any given wealth for retirement
consumption.

MARCH 2004

8


### ---Economics-2004-0-10.txt---
B. Labor Incentives

The implicitly uniform-price annuitization in
Social Security also affects labor market incentives.
The use of uniform annuity pricing (overall
or within still heterogeneous risk classes)
violates the conditions for first-best optimization.
Compared to first-best pricing, the decision
that would be distorted is that of labor
supply. If annuity pricing is breakeven, then
some are being taxed on work while others are
being subsidized compared to a system where
annuities are priced for individual life expectancies.
An alternative counterfactual would be
a failure to annuitize at all. Without annuitization,
we would have more accurate labor market
incentives person-by-person, but earnings would
finance less satisfactory consumption trajectories.
We would fail to insure not only life expectancy
realizations but also changes in life
expectancy as information accrues. That is,
even unfair annuities can raise individual welfare
if the alternative is no annuities.
I conclude that having a mandated retirement
income program provide its benefits as annuities
is sensible.

III. Workers and Families: Young Child,
Spouse, and Survivor Benefits

Social Security provides more than just retirement
benefits for workers. It provides benefits
for disabled workers and their families, for
young children of a deceased worker, and for
elderly spouses and surviving spouses. In addition,
a divorced spouse may be eligible for the
same benefits as a spouse if the marriage lasted
at least ten years.31 Benefits other than worker
benefits are referred to as auxiliary benefits.
These benefits are subject to a maximum
rule-a beneficiary receives the largest benefit
he or she is eligible for-with no increment for
also being eligible for a smaller benefit. That is,
if someone has worked at least ten years, on
retirement he or she is eligible for a retired
worker benefit. He or she is also eligible for a
spouse benefit if married to a retired worker
beneficiary. But the total amount of benefit is
equal to the maximum of the two benefits. Sim-


ilarly, someone eligible for a worker benefit and
also eligible for a survivor benefit receives only
the larger benefit.32 A central design feature is
that these auxiliary benefits are not paid for on
an individual basis-workers with the same
earnings history receive the same retired worker
benefits whether or not they have family members
or ex-spouses collecting auxiliary benefits.
Auxiliary benefits raise four questions. Does
it make sense to mandate benefits for family
members and ex-spouses? Does it make sense to
base benefits on a maximum rule? Does it make
sense to finance all of the auxiliary benefits
from the program as a whole rather than in part
or in full from the benefits of the retired worker?
Are the details of benefit determination rules as
well-designed as might be?

Let me start with the first, most basic question.
It makes sense to provide auxiliary benefits
since studies suggest that significant numbers of
workers do not insure their lives adequately and
would not make good choices between singleand
joint-life annuities. More generally we are
learning more about the ways in which the
allocation of resources within the family does
not conform to a single maximization with a
single budget constraint. Since the government
cares about the different family members (and
not just the worker), direct allocations to family
members matter since they will change the allocation
of resources within the family. Protecting
family members is a role governments have
recognized for centuries.

The other questions are more complex and
need more detailed analyses. Two issues are
central here. These are the positive and normative
issues of how consumption is actually allocated
within families and how to combine

evaluations and rules that affect both individuals
and families. Research on the determination
of allocations within the family is still in an
early stage of development. And normative
analysis has not progressed much beyond identification
of the dilemma in recognizing both


### ---Economics-2004-0-11.txt---
THE AMERICAN ECONOMIC REVIEW

individuals and families.33 So my answers here
are speculative and primarily meant to identify
where research might lead us to policy
improvements.

Offhand, the maximum rule does not provide
labor incentives well (incentives are stronger for
workers who have spouses likely to collect auxiliary
benefits since two benefits are increased
by more earnings, and weaker for workers who
are likely to receive a larger spouse benefit since
further earnings by someone receiving a spouse
benefit do not increase that benefit). A similar
(but less extreme) issue arises with income taxation
of a lower-earning spouse.

Offhand, the cost of auxiliary benefits should
be shared between a worker and the program as
a whole. The benefit formula is progressive,
with a higher replacement rate for lower earners,
reflecting differences in retirement needs.
As part of responding to needs, it seems right to
recognize dependents in determining need and
so benefits. But the current rule is not the only
way to do that. Some provision of auxiliary
benefits for children from the general program
makes sense, in keeping with our generalized
support of children in education; some provision
for spouses is relevant in the role of
progressivity-two can not live as cheaply as
one. But the current system has gone too far and
I share in the criticism that too much is given to
the nonworking spouses of high earners.34 Using
system resources to finance large transfers
to those in the upper tier of the earnings distribution
offsets too much of the progressivity in
other portions of the system. Designing a different
system would be politically sensitive and
complex and would need detailed analysis.
The determination of survivor benefits has
also received considerable criticism. Recogniz-
33 If one wants to do a normative analysis solely on the
basis of individual experience, one needs to consider how
resource allocation within the family is affected by the rules
determining benefits and the impact of benefits on marriage
rates. Such an analysis could consider the effects of extend-
ing auxiliary benefits to all long-term relationships, includ-
ing same-sex marriages.

34 Scaling back the spousal benefit for spouses of high
earners (for example by a cap), or a more general overhaul
of auxiliary benefits is likely to meet considerable political
resistance. This suggests not tackling this issue in reform
plans hoping for an early restoration of actuarial balance. In
addition, a general overhaul should be preceded by consid-
erable further analysis.

ing the role of couples in sharing resources at
least partially, it makes sense to relate the benefits
of an elderly survivor to the benefits that
had been received by the couple-a survivor
replacement rate. Currently, survivor replacement
rates vary with the past earnings of husband
and wife, usually, but not always, between
one-half and two-thirds.35 A uniform survivor
replacement rate seems more likely to approximate
relative needs than the current system. The
much higher poverty rate of widows than of
couples, noted above, suggests a higher survivor
replacement rate is needed, with threequarters
having been suggested by some

analysts.36 The change to a uniform and higher
survivor replacement rate could be financed out of
a suitable mix of the total resources of the program
and the benefits of the couple while both are alive.
The current recognition of divorce is to allow
benefits for unremarried divorced spouses and divorced
surviving spouses after at least ten years of
marriage. Since there is a family maximum, some
of these benefits are paid by the system as a whole
and some out of the other auxiliary benefits. The
adaptation of the system for the growth in divorce
seems to me a major issue for research. I do not
know if we can design a system that would be
better, recognizing both labor market incentives
and income distribution issues, or if such a design
could survive political hurdles.37 But it is worth
thinking hard about.

35 It is common to cite the range of one-half to two-thirds
for the survivor's replacement rate, ignoring actuarial ad-
justments. But there are cases above this range once we
include adjustments for the ages at which the benefits are
claimed.

36 Among the TIAA-CREF annuitants who choose a
joint-life annuity from the three available options, roughly
70 percent choose a full benefit to the survivor, nearly 20
percent choose two-thirds, and the rest choose one-half.
Roughly 70 percent of men and 30 percent of women
choose joint-life annuities (Ameriks, 2002).
37 Research hurdles come from combining concerns
about individuals and families. Incentives for retirement
depend on benefits relative to individual earnings, while
need reflects family incomes. Political hurdles come from
the diversity of views about the structure of benefits. Some
like the discouragement of labor force participation by those
with children, others prefer not to subsidize that activity.
Divorced women are among the most vulnerable beneficia-
ries. With the benefits for divorcees with limited earnings
tied to the benefits for spouses, reducing spouse benefits for
high earners affects both well-off and vulnerable groups.
Finding a way to satisfy diverse constituencies will not be
easy, as is shown by repeated groups looking at this issue.
10

MARCH 2004


### ---Economics-2004-0-12.txt---
In sum, mandating benefits for the families of
workers is important, along with mandating
savings and mandating annuitization-the inclusion
of family benefits in Social Security
makes sense. There is good reason to think that
the current rules can be improved, but research
difficulties and political hurdles will need to be
overcome if we are to make improvements.
IV. Income Distribution, Insurance, and Labor
Supply38

In determining retirement benefits, Social Security
first averages the best 35 wage-indexed
annual earnings,39 then it uses a progressive
benefit formula to determine what real benefits
would be if first claimed at the age for full
benefits (commonly, if somewhat misleadingly,
called the normal retirement age),40 and then it
adjusts benefits for the age at which they start.
Moreover, between age 62 (the earliest age at
which retirement benefits can be claimed) and
the age for full benefits (which is in transition
from 65 to 67), benefits are only paid if earnings
are low enough, referred to as an earnings or
retirement test. Each of these steps in determining
benefits affects income distribution, insurance,
and labor supply. I will skip over

implications of using 35 years (as opposed to
more or fewer years or all of lifetime earnings
subject to tax)41 and of using a wage index to
weight the earnings in different years in determining
benefits (as opposed to using an

interest rate)42 and concentrate on the effects


of a progressive benefit formula and a retirement
test.43

Consider the stochastic process of earnings
opportunities. Individual workers face considerable
risks that are only partially correlated with
the economywide average earnings used in indexing.
Wages move differently by industry and
firm and region and some individuals have career
opportunities strongly affected by industry
and firm and region developments. We do not
have trading in the type of indexes Robert J.
Shiller (1993) has proposed in order to give
workers the ability to hedge these aspects of
their risks.44 Even if we managed to have trading
in such indices, it is beyond credibility that
most workers would take appropriate advantage
of these opportunities. When many workers can
not sort out the basics of portfolio diversification
in their 401(k)s, there is no reason to anticipate
successful execution of far more

complex financial strategies. By having replacement
rates that are higher for lower levels of
lifetime earnings, a social security system that


### ---Economics-2004-0-13.txt---
THE AMERICAN ECONOMIC REVIEW

makes benefits a progressive function of lifetime
earnings offers insurance about lifetime
earnings that is not available in the market.
If the taxes and benefits for a cohort broke
even in present value terms, the use of a progressive
benefit formula would imply that the
labor supplies of lower earners were being subsidized
and those of higher earners were being
taxed.45 This is the familiar pattern with insurance
with asymmetric information-a combination
of insurance and incentives neither of
which satisfy the conditions for first-best optimization.
This effect of progressivity is in

addition to the effects from annuity pricing discussed
above. Some of the effects of annuitization
and progressivity would be offsettingthose
with higher earnings of each gender tend
to live longer-and some would be compounding-
women on average have lower earnings
and longer lives. That taxes and benefits do not
break even on a cohort basis is discussed in the
next section.

The progressivity in the benefit formula uses
taxes that distort labor supply in order to redistribute
income and provide insurance. The progressive
annual income tax also redistributes
income, provides insurance against earnings uncertainty,
and distorts labor supply. Since these
two institutions work on different tax bases and
provide payments at different times, there is
room for each of them to contribute despite the
presence of the other. Annual income taxation
recognizes short-term needs, coming from borrowing
constraints and from behavior that is not
time-consistent. It also recognizes capital income
as part of determining tax rates. Ex post,
all of one's Social Security taxable earnings (in
the best 35 years) contributed to benefits in a
way that varies with age but not with the level
of annual earnings, given lifetime earnings.
This avoids the distortions coming from having
different marginal tax rates in different years as
a function of annual earnings, or annual capital
income. The use of a lifetime measure also
separates out issues of lifetime earnings from
the age-earnings profile in doing redistribu-
45 This resembles an EITC being financed by a positive
income tax. Unlike the EITC, which has a region of high
marginal taxes as the subsidies are phased out, Social Se-
curity has a monotonic transition from marginal subsidies to
marginal taxes.

tion.46 While both annual income taxation and
lifetime social security have received analyses
of the trade-off among redistribution, insurance,
and distortions, there has not been much work
considering the simultaneous use of both
institutions.

A. Retirement Test

For a mandate to save for later consumption
to have bite, workers can not be allowed to
claim benefits whenever they want, including
immediately. To claim Social Security retirement
benefits, a worker must be at least 62. The
system could simply start paying benefits at age
62. Instead, between age 62 and the age for full
benefits, workers can only start receiving benefits
if their current earnings are low enough,
corresponding to full or partial retirement for
many workers.47 Any delay in the start of benefits
increases their monthly amount, tending to
counterbalance the delay in the start of benefits.
The impact of this retirement test on labor market
incentives is in addition to effects discussed
above that apply to each year of labor supply.
That is, the effect of Social Security on incentives
for continued work past age 62 has two
parts. One is the effect of a delay in the start of
benefits together with their later increase as a
consequence of the delay in their start.48 The
46 For example, if everyone had the same age-earnings
profile, Social Security would do no redistribution within a
cohort, while annual income taxes would subject each per-
son to earnings subsidies when younger and taxes when
older.

47 Benefits are paid to workers younger than the age for
full benefits if earnings are below the exempt amount, which
equals $11,640 in 2004. Earnings above this amount result
in a 50-percent reduction in benefits, until benefits reach
zero. Rules are different for the year in which the age for
full benefits is reached. After reaching the age for full
benefits, benefits may be claimed whatever the ongoing
level of earnings. A worker can receive a larger benefit by
delaying the start of benefits up to age 70.
48 The start of benefits can be delayed even if the worker
retires. For a worker without liquidity constraints, labor
supply is not encouraged by a net subsidy from delay (in the
case of a long expected life) since delay is available any-
way. But work is discouraged for those with shorter life
expectancies. While some eligible workers do not claim
benefits right away, overwhelmingly, retired workers do
claim fairly quickly. For those who would claim as soon as
they stopped working, work is encouraged by a larger
increase in benefits from delay as a result of a longer
expected life.

12

MARCH 2004


### ---Economics-2004-0-14.txt---
second is the extent to which additional work,
and so additional payroll taxes, increase the
measure of lifetime earnings and so add to
benefits.

For an average worker at ages 62 and 63,
Social Security had a roughly zero marginal tax
for the average worker when the age for full
benefits was 65.49 With the increase in the age
for full benefits there will be a small tax at these
ages. While implicit taxes used to be much
larger above the age for full benefits, the retirement
test has been eliminated for those ages.
With differences in life expectancy, a zero tax
on an average worker implies that some workers
are taxed and some are subsidized by the pres-
ence of the retirement test.50

The retirement test has two effects. One is to
raise (delayed) monthly benefits for those continuing
to work. To the extent that a worker
would have consumed out of benefits received
while still working, the delay in the start of
benefits raises later consumption (for both the
worker and possibly a surviving spouse) since
more is saved. This is advantageous to the extent
that consumption falls too much after retirement.
51 The combination of a delay and

increase in benefits is also redistribution across
workers based on life expectancy along the lines
discussed above. On the other hand, benefit
ineligibility while continuing to work discourages
work for those not fully valuing their increased
later benefits and those with shorter life
expectancy. Empirical estimates find that the
overall labor supply effect is modest, suggesting
that the increase in monthly benefits effect is
more important. The retirement test also helps


with the risk in earnings trajectories that comes
from how opportunities to earn (and disutilities)
develop toward the end of a career. The retirement
test addresses that risk to the extent that
there is taxation on continued work and those
continuing to work are less needy on average
than those who stop working earlier.52 Thus I
conclude that the retirement test does distort
labor supply, but that distortion is more than
offset by the gains from improved lifetime consumption
allocations and increased insurance.
Limiting the range of ages at which the retirement
test applies makes sense. Otherwise
some of those working to very advanced ages
would have replacement rates above 100 percent
and, if liquidity-constrained, would prefer
to have part of benefits while still working.
Currently the age for the end of the retirement
test is the age for full benefits. I am not aware of
any analysis of the optimal choice of an age for
the end of the retirement test.

B. Labor Supply at Younger Ages

I have focused on the retirement decision
since elasticities here are larger than those with
earlier labor supply decisions. But younger
workers pay payroll taxes and anticipate an
increase in benefits once they retire as a consequence
of the earnings that were subject to tax.
The effect on labor supply is relevant for choosing
the size of a mandatory retirement income
system. This incentive depends on the perceived
link between taxed earnings and retirement (and
disability) benefits. While those nearing retirement
age often gather information on the workings
of the system and seek advice on the
advantages of different timing of retirement,
younger workers are not well informed.53 Some


### ---Economics-2004-0-15.txt---
THE AMERICAN ECONOMIC REVIEW

simulations have assumed that younger workers
perceive no increase in future benefits as a consequence
of additional earnings. This leads to a
big boost in apparent efficiency from a switch to
individual accounts if it is also assumed that
money going into individual accounts has no
implicit tax. Both of these assumptions seem
wrong to me.

I believe that there is wide awareness of the
existence of some link between earnings and
later benefits, although understanding of how
the link works is not so wide. Misperception of
the link sometimes takes the form of imagining
that Social Security is like a corporate defined
benefit pension that heavily weights later years.
This perception would correspond to an implicit
tax at some ages and an implicit subsidy at
others, not a full tax at all ages. The extent to
which labor supply is affected by concern that
there will be no benefits would be greatly modified
by any reform that restored actuarial balance,
not just one with individual accounts.
Insofar as workers have high subjective discount
rates, mandating savings in any form affects
labor incentives and the exact link between
taxes and benefits is of reduced consequence.
My sense of a small difference between pension
systems in incentives for younger workers is
supported by the evidence of quite modest labor
market responses in Latin American countries
that have introduced individual accounts.
I have now argued for the use of a mandatory
retirement income system paying annuitized
benefits to workers and their families based on
a progressive benefit formula and using a retirement
test at some ages but not at others. I turn
next to two issues that bear more on reform
options, as well as reflecting the history of the
system. First I will discuss the redistribution
across cohorts and then the use of automatic
indexing as well as periodic legislation.
V. Benefits by Cohort

Social Security is often criticized for distorting
labor supply and savings. Despite the linking
of these two decisions, the issues are very
different. I have already noted that mandatory
annuitization with uniform pricing distorts labor
supplies relative to an idealized alternative, but
seems to be a welfare improvement relative to a
world with no annuities. And I discussed other
labor market issues where Social Security combines
incentives with redistribution and insurance.
In contrast, the rules of Social Security do
not distort savings. That is, Social Security certainly
affects savings and so national capital.
But the term distortion is usually reserved for an
intervention that would prevent Pareto optimality
in an economy that would otherwise satisfy
the conditions of the Fundamental Welfare Theorem.
To examine this meaning of distort (as
opposed to merely change) we need to consider
the impact of Social Security on the marginal
return to private savings (the size of a tax
wedge). By itself, Social Security has no effect
on the return to marginal savings since benefit
levels do not depend on capital income. Social
Security does interact with the income tax, but
the effect of the existence of Social Security on
the income taxation of the return on marginal
savings can have either sign for differently situated
workers, although it probably includes a
wedge on average.54

A mandate to pay taxes and receive benefits
would affect private savings even if there were
no marginal distortion at all. Effects come from
the requirement that people pay taxes at levels
and times when they might not have saved the
same amount. Effects also come from redistribution,
both within and across generations, that
is, from income effects as opposed to substitution
effects. I am not aware of any study of the
impact on savings from the progressive benefit
formula-the presence of higher benefits relative
to taxes for low earners who have a lower
54 Perhaps the largest effect comes from the cutoffs
below which benefits are not taxed or are taxed at a lower
rate. Since the cutoffs are compared with income including
capital income, there is an increase in the tax wedge on
savings for those who are affected in this way. Another
effect comes from the possibility that taxable benefits might
increase the marginal tax bracket. But Social Security dis-
places some private savings. Whatever savings are dis-
placed by Social Security might themselves have affected
the marginal tax rates (depending on the tax treatment of
displaced savings). Moreover, one needs to consider the
effect of Social Security on the income tax rate when the
savings are done as well as when the proceeds are
received-the employer share of the payroll tax is not part
of taxable income for the income tax. Thus, the effect could
be positive or negative depending on the level of displaced
savings and their income tax treatment. This indirect link is
present in many other programs that are not talked about in
this way. For example, government support of education
raises earnings and so marginal tax rates. The implied
increase in savings distortions does not seem to be of
consequence.

MARCH 2004

14


### ---Economics-2004-0-16.txt---
propensity to save than high earners.55 The redistribution
across generations has received particular
attention and has led to consideration of
the impact on national capital.

A. Transfers by Cohort

Everyone is aware of the decision to pay
earlier cohorts of retirees benefits far larger than
could have been financed by the taxes they paid
and the interest that could have been earned on
them. Figure 1, an updating to 2002 dollars of
analysis done by Dean R. Leimer (1994), shows
the lifetime transfers by cohort (left scale) and
the cumulative net payments by cohort (right
scale) for cohorts born through 1949, and so


turning 55 this year.56 The aggregate net transfers
to these cohorts is roughly $11.5 trillion.
How much did this early generosity reduce
national capital? We have some estimates but
they are surely not reliable. A believable timeseries
econometric study is probably not doable
and there is no consensus that one has been
done satisfactorily. Another approach would be
by simulation. But a credible simulation requires
modeling the appropriate underlying
behavior-the extent to which different workers
would save on their own without such a program.
Surely, a simulation with all workers
being fully rational lifetime utility maximizers
has no credibility. And we would also need to
track the effects on national savings from Social


### ---Economics-2004-0-17.txt---
THE AMERICAN ECONOMIC REVIEW

Security displacing transfers to the elderly from
the government (through the program for the
poor elderly-Old Age Assistance, which became
SSI) and from individuals (through cash
gifts and shared housing).

While the impact on national capital would
be an interesting positive question if we could
answer it well, it is important to recognize the
additional issues needed for a normative analysis.
The goal of Social Security's early generosity
was to raise the consumption of early
cohorts of elderly. Apart from business-cycle
effects, higher consumption implies lower
savings-implying that lower national capital
was required by the goal, not an unintended side
effect. A normative evaluation of the impact of
the redistribution to early cohorts would consider
how much their wages were lower than
those of later cohorts and how little they had
saved, as well as the return on capital. It
would also consider the pattern of transfers
within benefited and paying cohorts. However
such an analysis would come out-balancing
very worthwhile transfers with some less
worthwhile ones-most of the transfers are
now history.

Given the infinite horizon present value budget
constraint of Social Security (in the absence
of transfers from general revenues) this early
generosity is the cause of lower benefits in the
future than could otherwise be afforded. That is,
the legacy of the early generosity of Social
Security shows up in assets that are not there. If
they were present, they would be earning interest
that could contribute to paying for benefits.
The cumulative curve in Figure 1 gives a sense
of the magnitude of the trust fund that is not
there because of Social Security's history. But,
Figure 1 is by cohort, and so does not show how
much larger the trust fund would be today if
every cohort had been on a breakeven basis.
Although such a calculation is doable, it would
not be the best basis for insight into reform
options. Rather, that comes from considering
the elements likely to constrain reform. Past
payments are history and political considerations
suggest that it is unlikely that benefits
will be directly reduced for those already retired
or those nearing retirement, although these benefits
might be affected by changes in tax treatment
or in the inflation indexing of benefits,
changes that would apply to everyone. A partial
picture of that constraint would be that cohorts
over 55 would not be affected by reform.57 The
measure is not exact since cohorts over 55
would be affected by any payroll tax change and
slightly younger cohorts are likely to have limited
changes in benefits as we phase in any
benefit reductions that are part of a reform. An
ideal definition of this constraint would conform
to a theory of political constraints on reform
coming from past generosity. We do not
have a full theory, but this gives a reasonable
sense of the size of the legacy that needs to be
financed from future cohorts.

Peter Orszag and I have referred to the missing
assets on this cohort basis as a legacy debt.58
Thus the legacy debt is not a debt in the traditional
sense of that word, but that term crystallizes
the need to allocate the cost of the assets
that are not there across cohorts. Spreading the
cost of that early generosity across cohorts is
inherent in any plan that restores actuarial balance.
While only an approximation to the real
constraint, the number is roughly $11.5 trillion
(a bit more than one year's GDP). If we were to
go to full funding, then this is roughly the cost
that would fall on the generations during the
buildup to full funding. Alternatively, instead of
ever achieving full funding, we can consider a
wider allocation of the legacy cost by aiming to
preserve a ratio of the legacy cost to taxable
payroll. This would parallel the idea of preserving
the ratio of the public debt (or the interest on
the public debt) to GDP. Spreading the legacy
cost over all future cohorts implies less than full
funding of Social Security. Without extensive
evaluation of its consumption transfers, the effect
of Social Security on national capital is not,
by itself, a basis for concluding that the system
should have been fully funded or should become
fully funded.

The baby boomers are much larger than earlier
cohorts. The 1983 legislation included payroll
tax revenues in excess of current outlays in
order to build a trust fund which would then be
used to finance the retirement of this very large
cohort. That is, taxes were higher early to allow
57 In his charge to the Commission to Strengthen Social
Security, President Bush included the principle that Social
Security reform not affect the benefits of anyone 55 or older
(Commission to Strengthen Social Security, 2002).
58 This is the same as the "closed group" measure of
balance, with the group including everyone 55 and over.
16

MARCH 2004


### ---Economics-2004-0-18.txt---
them to be lower later.59 Politically, the trust
fund is very likely to be used for Social Security
purposes in the sense that the constraint on
future Social Security expenditures includes the
value of the assets in the trust fund. A separate
issue is the extent to which the higher payroll
taxes since 1983 increased national capital.60
This is a source of controversy, with a wide
range of presumptions and no ability to settle
the question econometrically.61 I believe that a
large part was saved-despite the large federal
deficit outside Social Security for the 1980's
and early 1990's. In my view, a larger unified
deficit, if Social Security had not been in surplus,
would not have had a strong effect on tax
and spending legislation. Congress had great
difficulty in legislating tax and spending
changes to lower the deficit. Without the Social
Security surplus, a somewhat larger unified deficit
would not have changed the basic character
of the situation-a deficit widely perceived as
being too large and a difficulty in raising taxes
and lowering spending. Looking beyond the
baby boomers we do not currently perceive a
need to single out a cohort that will differ
greatly from others and perhaps call for something
other than a smooth adjustment of taxes
and benefits.

Redistribution across cohorts has not been
done in a lump-sum fashion, but through the
choice of tax rates and benefit formulas. Thus
the redistribution has affected labor supplies as
well as savings decisions. In the early days, the
generous benefit formulas (in effect or antici-


pated in the future) subsidized labor, just as the
lower benefits relative to taxes for younger
workers today taxes labor.62 This is similar to
the role of the progressive benefit formula discussed
above. Given the pattern of redistribution
by cohort shown in Figure 1, much of
redistribution served as an incentive for much of
the working life of recipients. This is in contrast
with analysis in two-period models where the
initial elderly recipients of transfers receive a
lump-sum transfer and all later cohorts have
implicit taxes on earnings to pay for it. Both the
transfers and the taxes have influenced labor
supply.

VI. Automatic and Legislated Adjustments to
Aggregate Realizations and Risk Allocation
The actuarial projection for the 1983 reform
envisioned a buildup of the trust fund, followed
by its decline back to the precautionary level of
one year's expenditures at the end of the 75-
year projection period. It has not worked out
that way. Instead of having just enough money
for 75 years of expenditures, plus a small trust
fund at the end, it is now projected that the trust
fund buildup will be sufficient to pay currently
scheduled benefits only until 2042. That is, the
policy that was designed for a 75-year horizon
will, if the projection is correct, cover all of
expenditures for only 60 years. By the scale of
preparing for long-term outcomes, that does not
seem to me to be too bad. Of course one could
argue, with hindsight, that Congress should
have looked further into the future than 75
years, although it was hard enough to reach
agreement on legislation even with that target.
Current discussions have extended the notion
of actuarial balance to include "sustainability"-
that the ratio of the trust fund to annual expenditures
not decline at the end of the horizon.
This criterion of sustainable solvency is meant
to avoid a repeat of the post-1983 experience
where the projected actuarial deficit returned
quickly (although the trust fund exhaustion date
was distant). Projected deficits returned quickly
because of what is called the terminal year
problem, or the cliff problem. That is, each year,


### ---Economics-2004-0-19.txt---
THE AMERICAN ECONOMIC REVIEW

the realized net cash balance of Social Security
is added to the trust fund and another year is
added at the end of the 75-year horizon. With a
constant tax rate and the current benefit formula,
the added year is in worse fiscal shape
than the average of years before. Indeed of the
current 75-year actuarial deficit of 1.9 percent
of taxable payroll, a full 1.1 percent is due to the
fact that the projection now goes 20 years fur-
ther into the future than it did in 1983.
The 1983 legislation included future decreases
in benefits by increasing the age for full
benefits. At the time of the 1983 legislation,
there was still a tax rate increase on the books.
That was kept and took effect in 1990. Indeed
from the initiation of the program in 1935 until
1990, there was always a future tax rate increase
on the books. Given the political ease of raising
benefits or cutting taxes, and the political difficulty
of raising taxes or cutting benefits, having
future tax rate increases and future benefit decreases
on the books lowers the political cost of
preserving balance, since it is easier to legislate
future pain than current pain. Avoiding a recurrence
of actuarial imbalance a short time after
reform requires a substantial trust fund at the
end of the projection period, so that it can fall
for awhile without triggering imbalance,
and/or a change in the time shape of taxes and
benefits. A changed time shape can be legislated
directly (as we legislated an increase in
the full benefit age in 1983 and have legislated
future tax increases) or could be expected
from the adoption of further automatic
adjustments (for example, by including an
adjustment for life expectancy).

Before considering the choice between legislated
changes and automatic adjustment, let us
consider the allocation in a complete-market
Arrow-Debreu equilibrium. In the model, outcomes
are fully specified. Given subjective beliefs
about the probability structure of the states
of nature, one can express the value of equilibrium
for an individual. Also fully specified is
standard modeling of incomplete markets,
which replaces complete market auctioneerannounced
future allocations by accurate predictions
of future market equilibria as repeated
trading unfolds. Time-inconsistent individual
behavior does not interfere with the ability to
describe outcomes in this way, although it will
generally interfere with the efficiency properties
of equilibrium.

Most social security systems lack the completeness
that is needed to specify outcomes

solely in terms of underlying economic variables
(and the stochastic structure of states of
nature). U.S. legislation determines the payroll
tax rate for each year into the indefinite future.
The level of earnings that are subject to tax each
year is automatically indexed-thereby relating
taxable earnings to economic outcomes.63 Legislation
also sets down the rules for benefit
payments in terms of individual earnings histories
and price and wage indices. While each part
is fully specified, no mechanism ensures that the
Social Security budget constraint is satisfied.
Thus, there is the expectation that sooner or
later something will have to be changed. That is,
in order to model future labor and consumption,
we need to model future legislative outcomes.
This is hard.64 In some exercises, the Congressional
Budget Office has been instructed by law
to ignore some possible future legislation (such
as extensions of sunsets of income tax changes).
But this is not a satisfactory solution for academic
analysts, nor for individuals who are
making lifetime plans.

We have a theory using incomplete contracts
as part of the theory of the firm. In that theory
agents have well-defined property rights and
well-specified behaviors that determine the outcomes
not covered by the contracts, With incomplete
legislation, the future legislative
process plays a key role in determining outcomes
that are incompletely specified.6 Analyzing
an equilibrium that includes a legislative
process is difficult-requiring modeling the interaction
of the personal preferences of elected
officials with their concerns about reelection, as
well as election outcomes (R. Douglas Arnold,
1990). It is not that this is unknowable in principle,
but that we are a long way from a genu-
63 Some of the income tax revenue from the taxation of
benefits goes to Social Security as well. This revenue is
dependent on future income tax rates.
64 In the list of reasons why members of the Panel on
Privatization of Social Security of the National Academy of
Social Insurance disagreed on the advantages of individual
accounts, a central element was the divergence of views on
the political implications of accounts-particularly the sus-
tainability both of rules for the accounts and of the structure
of traditional benefits (Diamond, 1999).
65 The legislative process can also change what is com-
pletely specified, but at least we can analyze what happens
if there are not any changes.

18

MARCH 2004


### ---Economics-2004-0-20.txt---
inely usable, empirically validated theory and
we are studying a process that generates very
limited data relating outcomes to underlying
factors.66

Incomplete specification is not a necessary
part of a mandatory social security system. For
example, in Chile workers are required to save
10 percent of covered earnings in mutual funds,
using the accumulation for an annuity purchase
or phased withdrawals after reaching benefit
eligibility. Thus the workings of the system are
fully specified in terms of economic outcomes.
7 This does not imply that the Chilean
government will never change the rules of the
system. Indeed, it has made frequent changes in
some details. But it does mean that we can
analyze the outcomes of the current system under
the assumption of no further legislation
without being internally inconsistent. We cannot
do that for the United States-there are
states of nature that require some legislative
change, indeed the probability of such a future
need is very high today.

The Chilean approach of a fully funded defined
contribution system is not the only way to
have a fully specified system. Sweden has one
too. In Sweden, the payroll tax rate is 18.5
percent. While 2.5 percent of payroll goes into
fully funded individual accounts, 16 percent is
used for a partially funded system, called a
notional defined contribution system (NDC).
An NDC system mimics a fully funded defined
contribution system in that it accumulates
a notional balance for each worker that
increases each year by taxes paid and a notional
interest rate.68 At retirement, this balance
is converted into an annuity based on the


life expectancy of that cohort and the same
notional interest rate.69 The notional interest
rate is set administratively (with automatic
adjustment), not by returns realized on assets
held. In this way an NDC system mimics a
defined benefit system. Thus it is very much a
hybrid. Whether this system is referred to as a
defined benefit system or an unfunded defined
contribution system matters since the vocabulary
with which a system is described can
influence the politics of both creation and
adaptation.

By itself a well-structured NDC system, with
a decent size buffer stock of assets, will have
little probability of needing legislative intervention
as long as economic growth is large
enough. Even so, the Swedes have gone further
by introducing an automatic balancing system. I
will not digress to describe the Swedish automatic
balance rules. It suffices to say that if
economic growth is sufficiently slow, the notional
interest rate is automatically loweredreducing
both benefits in payment and future
benefits in response to this slower rate of
growth. Thus the Swedish system can be analyzed
for an indefinite future without an assumption
about the structure of future

legislation, so one does not need a fully funded
system to have that property. Sweden, like
Chile, puts all of the risk of future outcomes on
the side of benefits and none on the side of
taxes.70

Some simple ways for pretty much ensuring
automatic balance can illustrate alternative approaches.
In the French and German pension

systems, workers accumulate "points" based on
earnings that have been subject to tax. Think of
this as a sum of wage-indexed wages over a
worker's career. The accumulations of points
determine relative pensions for retirees. Unlike
what is actually done in France and Germany,
points could be converted into cash
benefits by automatically adjusting the value
of a point to exhaust available revenues. Conversely,
we could think of adjusting the tax


### ---Economics-2004-0-21.txt---
THE AMERICAN ECONOMIC REVIEW

rate each year to produce enough revenues to
cover expenditures for given values of
points.71 Both types of adjustment need a
small buffer stock of assets (or borrowing
ability) because of lags between setting benefit
or tax rules and the determination of actual
expenditures and revenues.72

U.S. Social Security uses price and wage
indexing in the determination of both benefits
and the payroll tax base.73 This reliance on
automatic adjustments decreases the frequency
of the need for legislation.74 One popular proposal
is to extend automatic adjustments to include
an adjustment for life expectancy. Such a
change would play two roles-one is to have an
automatic adjustment rather than legislating in
anticipation of or in response to life expectancy
changes. The other is to decrease the actuarial
imbalance in a way that may be easier politically
than comparable direct changes.

But what mix of benefit and revenue changes
is the best response to increased life expectancy?75
71 To some, this is the heart of a defined benefit system,
including possibly placing the risk outside the labor market,
as can be done if the risk is shifted to corporations or
general revenues.

72 One difference is that if we attempt to increase tax
revenues (as opposed to lowering benefit payments), we
face the risk of exceeding the maximum that could be
collected (i.e., moving to the wrong side of the Laffer
curve). Presumably, with a sensible execution of this ap-
proach, this risk would be so low as not to be a problem.
Adjustment possibilities are more complex than just some
combination of tax increases and benefit decreases (or the
converses). In the presence of a projected deficit, benefit
reduction can be large enough to lower tax rates and tax
increases can be large enough to raise benefits. Recognizing
more complexity, some tax rates could go up while others
go down and benefits for some groups could go up while
benefits for other groups go down. Indeed, several proposed
reform plans include increased benefits for some vulnerable
groups along with general benefit cuts.
73 The current indexing is not complete-there is no
adjustment of benefits for inflation between the years a
worker is 60 and 62. This gap should be closed.
74 Indeed, the 1972 automatic indexation for inflation
(which was done incorrectly) was an attempt to codify how
Congress had been behaving, thereby reducing the fre-
quency of the need to legislate. The automatic indexing was
done incorrectly because congressional actions had been
unsatisfactory in structure, without this being as apparent as
when the changes became automatic and inflation increased.
75 We could also consider an automatic adjustment of the
earliest eligibility age and of the actuarial adjustments for
the age at which benefits start. The latter, but not the
former, is included in Sweden. Indexing the earliest
eligibility age is complex since the sizes of the groups
Part of an approach to this question is to ask
how individual lifetime plans should vary with
life expectancy. This depends on how the potential
earnings trajectory and the difficulty
(disutility) of work change along with life expectancy.
If both opportunities and difficulties
in a year depended on the proportional position
of that year relative to life expectancy (and
mortality rates also depended on relative age),
then all of an optimal individual adjustment
would come in working longer. That is, optimal
work would be a fixed fraction of life expectancy.
The change in Social Security with the
same pattern has all of the adjustment in lower
benefits for each age of retirement.
However, I suspect that the proportional case
assumes too large a change in both earnings
opportunities and difficulty in work relative to
life expectancy. If the optimal outcome for an
individual were to work a smaller percentage of
life expectancy, then a sensible approach would
spread the implied drop in lifetime consumption
over both pre- and postretirement years. Decreased
preretirement consumption corresponds
to an increase in the Social Security tax rate. In
historic data, where the steady growth in life
expectancy has been accompanied by a steady
growth in real earnings, we have a steady decline
in the percentage of life expectancy
worked. This suggests a mix of tax and benefit
changes since we do expect a continued correlation
between life expectancies and earnings
levels.76 I also believe that, at least among academic
economists, the life cycle of productivity
relates to more than just health and it is
unclear how such other factors are correlated
with life expectancy. I think an automatic adjustment
for life expectancy that included adjustment
in both benefits and tax rates would be
a good idea.

Should we have more automatic adjustments
helped and hurt by an increase are not likely to be simply
related to life expectancy.

76 Automatic adjustment of benefits for life expectancy
is naturally done on a cohort basis, while any adjustment in
taxes is naturally done on a yearly basis. Thus more rapid
increases in life expectancy would fall differently on differ-
ent cohorts when taxes are included in the adjustment than
when they are not. My plan with Orszag (2004) takes the
approach of a mix of automatic tax and benefit changes for
life expectancy, while Model 3 of the Commission to
Strengthen Social Security does all its automatic adjustment
for life expectancy on the side of benefits.
20

MARCH 2004


### ---Economics-2004-0-22.txt---
and so even less pressure for legislation? For
example, we could use additional adjustments
depending on real wage growth. Or we could go
directly to automatic adjustments based on
overall financial balance so Congress never
again needed to legislate.77 Such indexing
would need to choose the mix of revenue and
benefit changes in the response to imbalance. It
strikes me as implausible that a system with a
sensible tax rate would want to do all of the
adjustment on the side of benefits.78 That is, it
seems likely that the optimal size of a social
security system relative to the economy varies
with the same factors that affect actuarial
balance.

Relying on fully automatic adjustment
rather than assuming there will be periodic
new legislation bears some similarity to a
familiar distinction from macroeconomicsrules
vs. discretion for monetary policy. Parallel
issues include the concern about setting rules
without fully knowing how the economy adjusts
to the policy actions and recognition that the
economy may evolve so that currently good
rules may become less so in the future. But
there are also different issues. Social Security
set up for the indefinite future involves a level
of detail complexity that seems higher than setting
rules for the Fed. Moreover, Congress
could invite the Fed to set out a rule it will then
follow. Thus we need to ask whether Congress
would do a better job in setting out rules once
and for all rather than adjusting them from time
to time. While legislating from time to time is
an inherently easier intellectual problem, we
need to be concerned about the asymmetries in
the political ease of legislation addressing surpluses
and deficits, an asymmetry that is reduced
by legislating automatic adjustments.
Also there may be more similarity across the


political spectrum in normative evaluations of
the impacts of monetary policy than of the evaluations
of the sizes of taxes and benefits for
different workers and family structures. As with
monetary policy, I think that some discretion
can improve outcomes.

In considering possible automatic adjustments,
one can look at how adjustment is currently
debated and how it was done before. In
our last major reform in 1983, there was an
explicit sense of balancing benefit and revenue
changes (Paul C. Light, 1985). Currently, we
view both benefit reductions and tax revenue
increases as candidates to contribute to restoring
a projected position of financial balance. The
Commission appointed by President Bush put
forth two plans which restored actuarial balance
(Commission to Strengthen Social Security,
2002). One of them included new dedicated
revenues, and both of them included large transfers
of general revenues, which one cannot help
but think of as in large part coming from new
revenues and not just spending decreases and
certainly not benefit decreases. The plan that
Orszag and I have put forth explicitly divides
some of the proposed changes for restoring actuarial
balance (both one-time changes and new
automatic changes) between revenue changes
and benefit changes.

A. Fully Funded Defined Contribution and
Partially Funded Defined Benefit

The parallel to the workings of the ArrowDebreu
model and the completeness of the specification
makes economists more comfortable

thinking in terms of mandated fully funded defined
contribution systems than the type of partially
funded defined benefit system we have.
So, I want to draw out some comparisons. One
is that portfolio risk in a mandatory fully funded
defined contribution system is highly correlated
with the portfolio risk of the rest of individual
retirement savings. Thus the increased use of
defined contribution private pensions raises the
value of a defined benefit Social Security system
relative to individual accounts.79 In contrast, a


### ---Economics-2004-0-23.txt---
THE AMERICAN ECONOMIC REVIEW

system that is less than fully funded will have less
correlation with the returns on private retirement
savings, as it adjusts benefits in response
to the growth of tax revenues as well as the
returns on whatever assets are held.80 This comparison
holds even with initial benefits fully
automatically adjusted for the actuarial positionreturns
on assets and the growth of taxable earnings
are only partially correlated. Thus portfolio
diversification considerations suggest an advantage
to having (at least) some underfunding in
Social Security to complement private savings.
This diversification advantage comes with
the redistribution to earlier cohorts that is inherent
in a less than fully funded system.81 Thus
one can readily argue for a Pareto gain (ex ante)
from moving from a fully funded system to a
partially funded system with the risk associated
with the incomplete funding falling on benefits.
82 Note that the reverse argument does not
work-just adding funding to an unfunded system
that provides all of retirement benefits will
not generate a Pareto improvement. The gain
from diversification plays out over time, while
the redistribution required to build up funding
hurts the oldest cohorts who provide the funding
and are affected by the diversification argument
very little or not at all.83 Thus the diversification
income issues in that some reform proposals have been
packaged with increased tax incentives for individual retire-
ment savings. The debate has not included the possibility of
a mandatory widening of employer-provided retirement in-
come (with a government-provided default) along the lines
of the recent reform in Australia and earlier discussion in the
United States (President's Commission on Pension Policy,
1981). The diversity in private savings is also relevant for
considering the right size for mandated benefit provision.
80 For example, with an NDC system, the notional assets
in an account earn a notional rate of return that might be set
to equal the growth rate of the wage bill (if other factors are
not too strong). Then, one can consider the correlation
between the return on assets and the growth rate of the wage
bill over different time horizons. Since the internal rate of
return in a pay-as-you-go system is related to the timing of
tax payments and benefits as well as the growth rate of the
wage bill, a more complicated correlation could be exam-
ined.

81 In principle, diversification could also be accom-
plished by swaps of different tax revenues.
82 Throughout this address, I have assumed that the
interest rate is above the growth of wages, so that the
economy is not on the wrong side of the golden-rule level of
capital.

83 This logic is clear in a two-period model. With more
periods and externalities, one might find a complex way to
argument by itself does not lead to the possibility
of a Pareto gain from adding funding to an
unfunded system, just from reducing funding of
a fully funded defined contribution system.
The comparison above assumed all of the
response in the unfunded system occurred in
benefits. By having some of the response to
aggregate shocks fall on taxes, a less than fully
funded system is capable of doing additional
risk sharing across generations that does not
occur with a fully funded defined contribution
system (Douglas Gale, 1990). Of course how
good a job Congress does in adapting such a
system (whether done automatically or by repeated
legislation) is a further issue that must be
recognized. Thus, the current system provides
insurance for individual earnings risk through
the progressive benefit formula and the retirement
test and provides insurance for aggregate
earnings risk through the defined benefit structure
with less than full funding.

My broad conclusion here is that the absence
of a complete specification of Social Security is
not by itself an argument that there is anything
wrong with our current approach.

VII. Concluding Remarks

Occasionally, I run into people who believe
that no one in his right mind would design a
retirement income system like the one we have.
Some of the details do seem far from satisfactory
to me. However, looking at the big picture,
this structure makes sense. Mandated savings
makes sense if you think that many workers
would not provide themselves a reasonable replacement
rate. This is not just an issue of

avoiding poverty, but one that extends quite far
up the income distribution. Mandating annuitization
makes sense if you think that workers do
not adequately understand the value of annuities.
Protection of spouses and children makes
sense if you think that many workers would not
do that adequately. Relating benefits to a measure
of lifetime earnings surely makes sense. A
progressive benefit formula makes sense to provide
higher replacement rates for lower earners,
in order to supplement annual income taxation
benefit everyone, taking advantage of the externalities to
offset the payment for funding. But I have not seen anyone
show such a possibility.

MARCH 2004

22


### ---Economics-2004-0-24.txt---

 ## Economics-2005-0


### ---Economics-2005-0-01.txt---
Social insurance is a subject I have been
studying for nearly 40 years. The intellectual
and policy revolution in social insurance that is
occurring around the world is among the most
significant and positive developments of current
economics.1

Social insurance programs have become the
most important, the most expensive, and often
the most controversial aspect of government
domestic policy, not only in the United States
but also in many other countries, including developing
and industrialized nations. In the

United States, these programs include Social
Security retirement, disability, and survivor insurance,
unemployment insurance, and Medicare
insurance for those age 65 and older.
Together these programs accounted for 37
percent of federal government spending and
more than 7 percent of GDP in 2003. These
ratios have increased rapidly in the past and are
projected to increase even faster in the future
because of the more rapid aging of the
population.

I will discuss how the major forms of social
insurance could be improved by shifting to a
system that combines government insurance with
individual investment-based accounts: unemployment
insurance savings accounts (UISAs)

backed up by a government line of credit, per-
sonal retirement accounts (PRAs) that supplement
ordinary pay-as-you-go Social Security
benefits, and personal retirement health accounts
(PRHAs) that finance a range of Medicare
choices. I think that such reforms would
raise economic well-being and are also appealing
on broader philosophical grounds.

Several nations are now doing this for their
retirement programs, including such diverse
countries as Australia and Mexico, England and
China, Chile and Sweden (Feldstein, 1998a;
Feldstein and Horst Siebert, 2002). The focus
by governments around the world on social
insurance pension reform is driven in part by the
realization that the aging of their populations
implies that the tax rates required to fund social
insurance pension benefits will rise rapidly if
the programs are not changed.

The impetus for broader social insurance reform
comes from the recognition that existing
programs have substantial undesirable effects on
incentives and therefore on economic performance.
Unemployment insurance (UI) programs
raise unemployment. Retirement pensions induce
earlier retirement and depress saving. And health
insurance programs increase medical costs. Governments
are driven by a desire to reduce the
economic waste and poor macroeconomic performance
that these disincentives create and to avoid
the resulting tax consequences, as well as the
increased tax cost, of the aging population.
Economic research has helped policy officials
to recognize these undesirable effects and
to redesign social insurance programs. The pace
of reform and the nature of the program changes
differ from country to country, reflecting initial
conditions and local political realities. Reforms
are inevitably only partial and part of an ongoing
process. But the reforms generally make the
programs more economically efficient, providing
more protection relative to the financial
costs and the economic distortions. I will examine
some of the favorable changes that have
already occurred in U.S. unemployment, retirement,
and health care programs.

Before looking at these specific types of social
insurance, I want to discuss three general


### ---Economics-2005-0-03.txt---
questions. Just what is social insurance? Why
do, or should, we have such programs? And
what are the principles by which such programs
should be evaluated and redesigned?
I. Social Insurance and Welfare Programs
The word "insurance" is used to describe
these transfer programs because they deal with
risks: the risk of job loss, of health care expenses,
and of inadequate assets during retirement.
But social insurance is very different
from private insurance. The key distinction is
that participation in social insurance programs
is mandatory or is induced by substantial fiscal
subsidies.

Social insurance programs are also very different
from welfare programs. Welfare benefits
are means tested, i.e., they are paid only to those
with incomes (and assets) below some level. In
the United States, these means-tested programs
include Medicaid, food stamps, subsidized
housing, school lunches, and others.2 In contrast,
social insurance programs are "event conditioned.
" Benefits are paid when some event
occurs in an individual's life regardless of the
individual's income or assets. Unemployment
benefits are paid to those who lose their jobs and
Medicare benefits to those who are ill and over
65. Social Security benefits are available to
those over age 62, disability benefits to those
unable to work, and survivor benefits to the
widows and children of deceased workers.
Unlike welfare programs, social insurance
programs are not designed to be vehicles for
income redistribution. Although some fraction
of social insurance outlays is paid to those with
low incomes, most of the benefits go to middleand
higher-income households. This is particularly
true in the United States, where cash benefits
to retirees and the unemployed are positively
related to previous earnings and where health care
is provided by private hospitals and physicians,
even when financed by social insurance.
Social insurance may appear to be redistributing
income to the poor because benefits are
paid to those who are temporarily poor due to
the event that triggered the payment of benefits.
This ignores the permanent or lifetime income
of these recipients. It also ignores the effect of
the social insurance on the incentive to accumulate
funds for these rainy days. Social Security
benefits that replace 50 percent or more of
after-tax pre-retirement income reduce significantly
the incentive to save for old age and
therefore depress income in retirement. Unemployment
benefits with high replacement rates
have a similar effect on saving to finance spells
of unemployment.

The lack of redistribution is well illustrated
by the Social Security retirement program. Last
year, a new retiree who had annual earnings at
or above the Social Security program maximum
taxable amount ($87,900 in 2004) for at least 35
years received a benefit of $21,900. In contrast,
someone with lifetime earnings in the middle of
the earnings distribution received only about
two-thirds as much in retirement benefits. And
someone with low earnings (i.e., 45 percent of
the average wage) received benefits of less than
$9,000 a year. This lack of redistribution is
compounded by the rules governing benefits to
spouses and widows. A retiree who previously
had maximum taxable income and who retired
with a dependent spouse received more than
$32,000 a year from Social Security, while the
widow of a low-income earner would receive
less than $9,000 a year.

The Social Security program appears to be
redistributive because everyone pays the same
tax rate, while the ratio of benefits to lifetime
earnings is designed to fall as those earnings
rise. In practice, however, this apparent redistribution
is offset by the longer expected life of
higher-income individuals, their increased use
of spousal benefits, and the later age at which
they begin to work and to pay taxes. Research
by Jeffrey Liebman (2002), based on a large
sample of actual individual earnings histories,
showed that less than 10 percent of Social Security
benefits represented net redistribution
across income groups within the same birth
cohort. Julia Lynn Coronado et al. (2000)
showed that the combination of taxes and benefits
for the Social Security program leaves the
lifetime Gini coefficient of the population's income
essentially unchanged. In addition, the
general equilibrium effects of Social Security
tilt the pretax distribution of income toward
higher income individuals by reducing capital
accumulation, which in turn lowers real wages
and raises the return to capital.

2 See Robert A. Moffitt (2003) for detailed studies of a
variety of welfare programs.


### ---Economics-2005-0-04.txt---
Unemployment insurance also does not redistribute
to the poor. In Massachusetts, a state considered
to have a very generous UI program, the
UI benefits were financed in 2003 by a payroll tax
on only the first $10,800 of earnings (with a zero
marginal tax rate above that level), while basic
benefits were 50 percent of previous wages up to
more than $50,000 of wages per year.
An individual who earns $50,000 a year pays
the same tax as someone who earns $11,000 a
year but would receive benefits that are nearly
five times as high.3 Taken by itself this would
mean a substantial redistribution from lowwage
earners to higher-wage earners. Moreover,
since benefits are paid only to individuals who
have earned some minimum amount during the
past year, those with long spells of unemployment
may not be eligible for any benefits at all.
Although the same Medicare rules apply to
everyone over age 65, higher-income seniors
often get substantially more benefits than those
with lower incomes. Mark McClellan and
Jonathan Skinner (1997) concluded that Medicare
produced net transfers from the poor to the
wealthy as a result of the higher annual expenditures
and longer survival times of wealthier
Medicare beneficiaries. In the same spirit, Skinner
and Weiping Zhou (2004) found greater use
of mammography screening, diabetic eye exams,
and other indicators of good care among
high-income Medicare groups than among
those with lower incomes.

The very high level of spending on the middle
class social insurance programs hurts the
low-income population in another way: by putting
a drain on the government budget in a way
that reduces the funds available for helping the
poor. Social insurance programs cost $800 billion
in 2003, while federal spending on all
means-tested programs, except Medicaid, was
less than $150 billion.4 Over the past four decades,
the spending on means-tested programs
(except Medicaid) has remained relatively constant
(rising from 1.0 percent of GDP to 1.3
percent of GDP) while the social insurance pro-
grams that are not means tested rose from 2.7
percent of GDP to 7.4 percent of GDP.
The negative effect of social insurance spending
on means-tested programs is not only an
observed fact but is also what optimal tax theory
implies. The deadweight burden of an extra
dollar of taxes increases with the share of income
taken in taxes. The high level of taxes that is
needed to finance middle-class social insurance
programs therefore increases the deadweight burden
of any incremental taxes that would be used to
finance means-tested poverty programs. The large
social insurance programs thus reduce the optimal
size of means-tested poverty programs.
II. Why Social Insurance?

Some writers see social insurance in broad
philosophical terms, reflecting their specific
views of the appropriate role of government in
society. One such view, more common in Europe
than in the United States, is that social
insurance should be judged by its contribution
to social solidarity, i.e., to the sense that all of
the individuals in the nation are, in effect,
viewed as a single family and treated equally.
This leads to the principle of uniform health care
for all, although this is more often an asserted
political goal than a practical reality. Similarly,
they may reject any role for company-based
private pensions in order that all workers participate
in a common pay-as-you-go state plan.
The opposite philosophical view is that the
provision of health care or retirement income is
not a legitimate role for government because it
forces individuals to participate in a common
program rather than taking personal responsibility
and making decisions that reflect their
own preferences. Milton Friedman's Capitalism
and Freedom (1962) is the classic statement of
this view that social insurance programs are
inappropriate because they infringe upon individual
liberty.

The social solidarity view is often combined
with the statement that individuals are incapable
of making the complex decisions required to
plan for retirement income or to choose health
insurance or health care. The opposite view
emphasizes that individuals differ in their tastes
and are better able than governments to judge
what is in their own best interest.
I believe in the diversity of individual preferences
and the ability of most individuals to act


### ---Economics-2005-0-05.txt---
in their own self interest. But I also believe that
there is a role for government that justifies the
provision of social insurance benefits. I come to
this conclusion on utilitarian grounds rather
than from any philosophical commitment to social
solidarity.

There are two distinct reasons for providing
social insurance. Both reflect the asymmetry of
information. The first is that asymmetric information
weakens the functioning of private insurance
markets. The second is the inability of
the government to distinguish between those
who are poor in old age or when unemployed
because of bad luck or an irrational lack of
foresight from those who are intentionally
"gaming" the system by not saving in order to
receive transfer payments. Both problems show
that the case for social insurance cannot be rejected
simply by arguing that such programs force
people to act against their own best interests.
But these problems of asymmetric information
or any other market failures do not necessarily
justify government action. While a

perfect and benevolent government would be
better than a private market burdened by market
imperfections, actual governments are neither
perfect nor necessarily benevolent. Political actors
do not maximize a social welfare function,
but reflect political pressures and bureaucratic
preferences. Moreover, social insurance programs
impose costs that must be weighed

against the benefits of overcoming market imperfections.
Both require empirical evaluation.

Consider first the asymmetry of information
in insurance markets. To be specific, consider
the case of private annuities. If individuals can
buy annuities on actuarially fair terms they may
increase their expected utility by annuitizing
their assets at retirement. But if individuals differ
in their life expectancy and know more
about their mortality prospects than the insurance
company can learn, those with shorter life
expectancy will want to annuitize a smaller
portion of their wealth. Insurance companies
will recognize the resulting self-selection and
offer annuities with premiums that reflect the
mortality rates of the long-lived individuals
who are their most likely customers. This produces
a downward spiral in the demand for
annuities that is limited only when, at some
point, the risk-reducing value of annuitizing
outweighs the less than actuarially fair pricing
of individual annuities.

A mandatory social insurance program like traditional
Social Security circumvents this asymmetry
of information by providing everyone with a
retirement annuity rather than a lump sum at retirement
age. But whether this is better than an
imperfect private annuity market, in which some
annuitize little and others not at all, depends on the
implicit rates of return available on the social
insurance annuity, on the private annuity, and on
non-annuitized saving. It also depends on the degree
of diversity in preferred spending patterns in
retirement and in attitudes about bequests, since
complete annuitization at retirement would not
permit the purchase of retirement homes or other
major consumer outlays or the making of bequests
or inter vivos gifts.

The problem of information asymmetry in
private annuities could be reduced if individuals
purchased annuities at relatively young ages,
before they could accumulate much information
about their own likely mortality risks in old age.
Alternatively, a mandatory annuity could be
more attractive if it were based on the higher
return available in an investment-based program
rather than in a mature pure tax-financed
pay-as-you-go program.

Two conclusions follow from this. First, the
existence of asymmetric information may justify
a social insurance program (a government
annuity in this case) but does not necessarily do
so. The case for a mandatory annuity program
depends on calculations that could be done but
that have not yet been done. Second, the appropriateness
of a social insurance program and its
optimal size can be increased if the cost of the
social insurance option is reduced, something
that depends on how it is financed.
Consider now the second form of asymmetric
information that might provide a rationale for a
social insurance program: the government's inability
to distinguish those who are poor

through bad luck or inadvertence from those
who deliberately choose to act in a way that
leads to eligibility for free benefits. A primary
reason for social insurance programs is that
some individuals would not act in their own
interest, saving far too little for their retirement,
for health care after they are no longer working,
or to finance consumption when they are unemployed.
Although some economists may reject
the likelihood of such irrational behavior as a
basis for policy analysis, as individuals we all
recognize that such irrationality exists in practice.


### ---Economics-2005-0-06.txt---
Recent work on behavioral economics has
helped to make the possibility of such irrationality
or myopic behavior a part of mainstream
economics.

But such departures from rational saving by a
fraction of the population need not justify the
general provision of social insurance benefits.
Why not simply provide means-tested benefits
instead of the universal provision of social insurance
benefits? The primary reason for not
doing so is that some rational and farsighted
individuals would be induced by a means-tested
system to act in a way that allows them to
qualify for benefits. Doing so would impose tax
costs on the rest of the population that could
make overall well-being lower than in a universal
(i.e., not means-tested) program.

Consider a simple example of a means-tested
retirement program (Feldstein, 1987). Assume
that some fraction of the population is myopic
and would not save anything for retirement. A
means-tested program would provide a benefit
for all such myopic retirees. How would rational
working-age individuals respond to such a
system? They would have the choice of either
saving for their own retirement or consuming all
of their income before they retire and receiving
the means-tested benefit. The potential meanstested
benefit acts as a kind of tax on saving,
reducing the incremental retirement consumption
that saving would produce. A rational individual
would decide whether to act as if he or
she is myopic by comparing the lifetime utilities
with optimal positive saving and with no saving.
Those with relatively high incomes would
not be tempted by the means-tested benefit. But
others with lower incomes would have higher
lifetime utility by increasing their consumption
during working years even if the means-tested
benefits would only provide lower consumption
during retirement than optimal saving would
allow. Although they would achieve higher lifetime
utility through their action, their benefits
would be financed by tax-financed transfers,
which would make others worse off.

There is no way for the government to distinguish
between the genuinely myopic and

those who are rational utility maximizers gaming
the system. The government could, in principle,
set the means-tested benefit so low that
very few rational individuals would be tempted.
My judgment is that our relatively affluent society
would not accept that policy. The means-
tested benefits would be set at a higher level that
would tempt many rational individuals to save
nothing.

A policy of forcing everyone to save for his
or her own retirement would eliminate the problem
of those who game the system. The only
adverse effect of such a policy is that some
individuals might be required to shift more of
their lifetime consumption to their retirement
years than they would prefer. For them, part of
the mandatory saving would be a tax to the
extent that they valued the saving less than
current consumption.

The choice between such a mandatory saving
plan-essentially a kind of investment-based
Social Security pension-and a means-tested
benefit should depend on numbers. How many
people would receive means-tested benefits?
How much deadweight loss would be involved
in financing those benefits? How many people
in a mandatory saving plan would have to provide
more for their retirement than they want?
In the absence of such a mandatory investment-
based option, the policy choice is between
a means-tested program and a universal pay-asyou-
go retirement benefit. Such a pay-asyou-
go plan forces all individuals (after the
initial generation) to receive a lower rate of
return than they could obtain on private saving.
As such, it also imposes a tax on labor income
since each extra dollar of earnings would induce
an additional pay-as-you-go tax liability. The
reduction in saving that is induced by the payas-
you-go system also causes a fall in capital
income and therefore in corporate and personal
tax revenue that requires higher marginal tax
rates to recover the lost revenue.

Both of these examples of asymmetric information
show that a social insurance program may be
an appropriate response to a market failure but that
it need not be. Even when there is a market failure,
it may be better to do nothing or to have a meanstested
welfare program. The choice depends in
part on the relative costs of the different options,
and those in turn depend on the design of the
potential social insurance program. Whether such
a program is investment-based or purely tax financed
on a pay-as-you-go basis is an important
feature of that cost.

Economists can help to evaluate these choices
by estimating the relevant costs and benefits of the
different options. My own conclusion is that
investment-based social insurance programs for


### ---Economics-2005-0-07.txt---
retirement, unemployment, and health care of the
retired population are more appropriate than payas-
you-go programs, means-tested programs, or a
policy of doing nothing.

There is an important political economy
reason for economists to work on improving
the design of social insurance programs rather
than advocating means-tested programs for
unemployment and old age. Elected governments
will inevitably seek to create universal
benefits to capture political support from the
largest possible majority of voters. Otto von
Bismark introduced social insurance in Prussia
in 1881 in an attempt to win support for
his conservative government and to fend off
the appeal of the nascent social democrats.
Even if it were economically desirable to do
so, economists could not prevent the spread of
social insurance by arguing that means-tested
programs would be more efficient. If economists
don't analyze the effects of social insurance
programs and recommend rules that

reflect good economics, the political process
will inevitably produce inferior programs.
III. Principles of Social Insurance
Accepting that there is a reason for mandatory
social insurance programs does not imply the appropriateness
of the programs that we have inherited
from the past. Today's Social Security and
unemployment insurance were enacted nearly 70
years ago. Economic conditions, administrative
technologies, and assumptions about economic
behavior have all changed dramatically since then.
And yet during these past 70 years, the key social
insurance programs have expanded without fundamental
change.

Before I consider some of the specific ways
in which our basic social insurance programs
can be reformed and strengthened, I want to
discuss broader principles that can help us to
think about each of the specific programs. I'll
begin with three fundamental political principles
and then turn to four economic principles.
A. Three Political Principles

Political principles involve value judgments
to a greater extent than the economic principles
to which I will turn later. I can explain the
political principles that shape my view about
appropriate reforms but I cannot prove that they
should determine policy. You may or may not
agree with them. Of course, you might agree
with me about specific reforms even if you
reject some or all of these principles.
Permitting Individual Choice.-Individuals
differ in their preferences. We do not all have
the same risk aversion, the same time preference,
the same relative taste for goods and leisure.
Letting individuals choose among options
in a way that reflects their individual preferences
should be an important aspect of social
insurance design. For Milton Friedman, such
freedom to choose is paramount. For me, it is
important but not decisive. In cases where
asymmetric information creates serious efficiency
problems, I might restrict that choice.
But I prefer to allow as much choice as possible.
I think that allowing individuals to make their
own choices is morally correct and generally
improves individual, and therefore social,
well-being.

But allowing choice means that programs
should be designed so that choice enhances
economic efficiency rather than creating deadweight
losses. A good example of such a program
redesign was the Social Security reform
that introduced the actuarial adjustment for
early and delayed retirement in a way that, in
principle, will allow individuals to decide when
they will start collecting benefits without changing
the actuarial present value of their benefits.5
Creating Program Transparency.--Social
insurance programs involve complex rules
about the benefits to be received, the taxes to be
paid, and the link, if any, between them. Who
among you is confident about even the most
basic Social Security rules that determine benefits
at retirement? If you are a man, what
benefit would your wife receive if she collects
on her own rather than as your spouse? How
would that change if she earned more or worked
another year? If she decides to retire at age 62
rather than 65? I'm told that there are more than
2500 separate rules in the Social Security
handbook.

The complexity of the rules weakens the perceived
link between the payroll taxes paid and
SThis adjustment will provide actuarially equivalent
benefits with a 3 percent real rate of return.


### ---Economics-2005-0-08.txt---
subsequent benefits. Many employees may simply
regard their Social Security payroll tax as
similar to the income tax, thereby increasing the
perceived marginal tax rate and raising the
deadweight loss of the tax.

Lack of transparency also permits programs
to have effects that might not be politically
acceptable if they were more explicit. For example,
some defenders of the current Social
Security system believe it permits a substantial
amount of redistribution that Congress would
not be willing to build into an investment-based
system of individual accounts. Although the
Social Security rules do not actually achieve
that redistribution, the important political principle
is that it is inappropriate in a democracy to
use a deliberately opaque system to achieve a
redistribution of income that would be rejected
if proposed in a more transparent way.
The Social Security program lacks transparency
because it is a defined benefit system
rather than a defined contribution plan of the
sort that now characterizes most private pensions.
Converting Social Security to a defined
contribution plan-even an unfunded "notional"
system such as Sweden and Italy now
have-would allow individuals to see the link
between their taxes and the resulting benefits.
An explicit decision by Congress to supplement
the contributions of low-income earners
in such a defined contribution plan would
achieve income redistribution without a loss
of transparency.

Recognizing Political Dynamics.-When we
economists talk about policy design, we generally
think about enacting permanent reforms.
But experience shows that legislated rules do
change and that the initial conditions influence
the path of that change. When designing a particular
program or advocating a particular design,
economists should recognize that some
designs are more stable than others and should
anticipate how a program might evolve.
The Medicare drug legislation enacted in
2003 is a good example. Medicare beneficiaries
will pay the first $250 a year in drug expenses,
followed by a 25-percent coinsurance rate to a
maximum benefit limit. Patients must then pay
100 percent of the drug cost up to $3600 in
out-of-pocket payments (in 2006). Above that,
Medicare will pay 95 percent of any additional
drug costs.

This rather strange design was accepted to
limit the total cost of the plan while delivering
benefits to a very large number of senior citizen
voters. An economically more rational plan
with the same budget cost would have insured at
least part of the range that is currently uninsured
and kept total costs down by a larger deductible.
But that would have had the political disadvantage
of giving benefits to fewer individuals. It
seems likely that future legislation will address
the residual insurance gap in a way that will
raise the total cost of the program.
There is another and potentially more significant
aspect of the future evolution of this Medicare
drug program. If all of the drugs consumed
by seniors come to be covered by government
insurance, there will be strong pressure to regulate
the price of those drugs. Such price regulation
is, in turn, likely to discourage the
development of drugs for those diseases that
particularly affect the elderly. It would be sadly
ironic if an insurance plan initiated to improve
drug access for seniors led ultimately to a reduced
availability of new drugs for this group.
B. Four Economic Principles

Let me turn now from these three political
principles-permitting individual choice, creating
program transparency, and recognizing political
dynamics-to four economic principles.
Recognizing the Economic Effects of Social
Insurance Programs and Their Taxes.-Noneconomists
who write about social insurance

programs often implicitly assume that social
insurance programs do not affect the behavior
of beneficiaries or the overall performance of
the economy. Evidence shows that the opposite
is true. Social insurance programs have important
and sometimes harmful effects on the
economy that are not fully recognized by the
public, Congress, or the politically responsible
officials.

A substantial volume of work during the past
quarter century has shown the various ways in
which social insurance programs do affect individual
behavior and the overall economy. These
effects include reducing national saving, inducing
early retirement, raising the unemployment
rate, pushing up the cost of health care, and
crowding out private health insurance. Any serious
evaluation of social insurance programs,


### ---Economics-2005-0-09.txt---
and any attempt to improve their design, should
take these effects into account.

There is, of course, controversy about the
magnitude of these effects, just as there is about
most other economic parameters. Decisions
about program design have to use the evidence
that is available, even if parameter estimates
come with substantial uncertainty. But there is
clearly room for economists to use new data,
new statistical methods, and new natural policy
experiments to improve our knowledge and,
therefore, to improve policy design.
Adverse effects result from specific program
designs and are not inherent in the goals of the
programs. For example, John Gruber and David
Wise (1999) showed that the rules governing
retirement benefits induced early retirement in
several European countries but that different
rules at different times and in other countries
did not induce early retirement. The U.S. benefit
rules that now specify an almost actuarially fair
relationship between benefits and retirement
age reduces substantially the perceived bias in
favor of early retirement.

More generally, social insurance programs
not only distort economic behavior directly,
thereby creating deadweight losses, but also
create further deadweight losses because of the
taxes that are levied to finance those programs.
I believe that the deadweight losses of those taxes
are much larger than is generally recognized.
I will illustrate this with the effect of the
50-percent increase in the payroll tax rate that
could occur if there is no change in benefit
rules. Deadweight losses depend on marginal
tax rates. Consider an individual who now faces
a combined federal and state marginal rate of
income tax of 30 percent without social insurance.
The current 15.3 percent employeremployee
payroll tax rate,6 when adjusted for
the interaction with the income tax7 and for the
present actuarial value of the additional retiree
and survivor benefits that result from increased
taxable earnings, now increases the overall marginal
tax rate from 30 percent to about 37.7
percent.8 A 50-percent rise in the 15.3 percent
marginal tax rate, adjusted for the income tax
interaction, would increase this effective marginal
tax rate from 37.7 percent to 44.2

percent.9

The increase in the deadweight loss that
would result from this tax increase reflects both
the reduction in labor supply-broadly defined
to include not just working hours but also the
accumulation of human capital, the choice of
occupation, effort, etc., and the change in the
form of compensation-away from taxable cash
and to less valuable fringe benefits. Although
neither behavioral change can be measured explicitly,
the resulting deadweight loss can be
calculated empirically by estimating the extent
to which the higher payroll tax would reduce
taxable labor income. It is appropriate to focus
on the decline in taxable labor income without
evaluating the two separate effects because the
relative price of the two components-the marginal
tax rate on the reward for increased labor
supply and the marginal tax rate that determines
the net cost to the taxpayer of fringe benefitsremains
the same when the tax rate changes.
Taxable labor income is, therefore, a Hicksian
composite good that can be used to assess the
deadweight loss (Feldstein, 1999a).10
6 The 15.3 percent rate includes Medicare as well as the
Social Security pension and disability taxes. Currently the
pension and survivor insurance portion is 10.6 percent of
taxable payroll. The disability tax is 1.8 percent and the
Medicare portion is 2.9 percent. The costs of these three
components will rise at different speeds but the combined
cost will eventually increase the required tax to 150 percent
of the 15.3 percent rate.

7 Since the employer half of the 15.3 percent payroll tax
is excluded from the personal income tax base, the effective
marginal rate of tax in this example is 30 percent plus the
7.65 percent paid by the employee plus 70 percent of the
7.65 percent payroll tax paid by the employer: 0.30 +
0.0765 + 0.70(0.0765) = 0.43. The extent to which a labor
supply response causes the tax to be shifted does not alter
the appropriate calculation of the marginal tax rate.
8 Feldstein and Samwick (1992) show that the present
actuarial value of the incremental benefits varies substantially
among different age and demographic groups, from no
value for young workers and some married women to more
than a 100 percent offset of the incremental tax for older
men with dependent wives. If this offset is approximated by
50 percent of the 10.6 percent of the old age and survivors
portion of the tax, the 43 percent marginal tax rate calcu-
lated in footnote 8 is reduced to 37.7 percent.
9 The increase in the payroll tax rate is subject to the
income tax offset (reducing the additional 7.65 percent to
6.5 percent). There is no incremental benefit associated with
the higher tax rate.

10 The situation is more complex when we deal with
taxes on capital income. These distort choices among finan-
cial instruments by both issuers and purchasers, choices
about the form of business (corporate vs. non-corporate,
domestic vs. foreign) and choices about saving vs. spend-
ing. The key elasticity that matters in the saving vs. spendThe


### ---Economics-2005-0-10.txt---
elasticity of taxable labor income with
respect to the net-of-tax share, i.e., to one minus
the marginal tax rate on labor income, is much
greater than the traditional elasticity of labor
supply as measured by labor force participation
and average hours worked. Estimating this elasticity
is now a subject of very active research
among public finance economists. Although a
wide range of estimates has been produced,
some studies are more reliable than others. I
believe that a conservative estimate is that the
compensated elasticity of taxable income with
respect to the net-of-tax rate is one-half.
Using this elasticity and the 2004 taxable
payroll implies that a rise in the effective marginal
tax rate from 37.7 percent to 44.2 percent
increases the annual deadweight loss by $96
billion, or nearly one percent of GDP.11 Since
the 6.5-percent increase in the marginal tax rate
applies only to taxable labor income (about 40
percent of GDP), the deadweight loss is equal to
about one-third of the incremental tax revenue.
Even this understates the relative size of the
deadweight loss because it ignores the reduction
in the tax base and therefore in the tax revenue
that results from the higher marginal tax. When
that reduction in taxable income is taken into
account, the incremental deadweight loss is
nearly 50 percent of the incremental revenue.12
The true cost per additional dollar of payroll tax
revenue is therefore $1.50.

Note that this is just the deadweight loss or
excess burden-i.e., the pure waste-associated
with the incremental tax. It does not include the
deadweight loss of the existing tax or the direct
burden of the taxes themselves. And it does not
include the deadweight loss caused by the program
distortions.

Although scaling back the rise in future benefits
would reduce the increase in the deadweight
loss, it would also reduce the protection
that Social Security provides to future retirees.
An alternative approach is therefore to redesign
the program so that the increased financing required
for the aging population has less of the
character of a tax.

One way to do that is to strengthen the taxpayers
perception of the link between taxes paid
and future benefits. That is one of the advantages
of shifting from the existing complicated
defined benefit rules to a defined contribution
system, even to a notional defined contribution
system. Although a notional defined contribution
plan would remain a pay-as-you-go system,
it would clearly link each worker's social insurance
tax payment to his or her resulting future
benefits.

A defined contribution system would provide
a tax-benefit link for those groups in the population
that now receive no extra benefits at all in
exchange for their additional taxes. For them,
the Social Security payroll tax is a pure tax just
like the income tax. These include both young
and older workers who are not in the top 35
wage-indexed earning years of their life, the
basis on which Social Security benefits are calculated,
as well as working women who will

eventually claim benefits based on their husbands'
earnings.

Although an unfunded notional defined contribution
system would provide some remedy,

the very low implicit rate of return in an unfunded
system implies that the payroll tax
would retain much of its distorting character. A
pay-as-you-go plan that substitutes a 2-percent
real return for private saving that would otherwise
earn a 5-percent real return is equivalent
over a lifetime of saving and dissaving to a tax
rate of about 75 percent.13


### ---Economics-2005-0-11.txt---
A much more substantial reduction in the
effective tax rate would be achieved by financing
the increased cost of Social Security and
Medicare by a funded system that would permit
future benefits to be financed without a large
increase in the tax rate. Moreover, to the extent
that the additional saving that individuals do
earns a favorable rate of return, they might not
consider it a tax at all. I will return to this issue
later when I discuss Social Security reform
more fully.

Designing Programs by Balancing Protection
and Distortion, while Seeking Reforms that
Improve the Available Tradeoff.-Social insur-
ance programs generally involve a tradeoff of
protection and distortion. Social insurance programs
protect individuals against undesirably
low levels of consumption during old age, spells
of unemployment, or when hit by large medical
bills. They also protect individuals from the
need to work longer than health warrants, to
accept a job when additional searching would
be adequately productive, or to forego appropriate
medical care because of an inability to pay.
But the same social insurance programs also
distort incentives in ways that cause inefficient
use of resources: early retirement, low saving,
unproductively long job searches, and excessive
consumption of medical care.

Social insurance program parameters should
be chosen to balance protection and distortion.
The level of Social Security benefits should
reflect the fact that high benefits relative to
previous income improve the protection against
reduced consumption in old age but also depress
saving and may induce early retirement. A high
level of unemployment insurance benefits helps
the unemployed to maintain consumption but
also encourages longer spells of unemployment
and the choice of jobs that have a greater likelihood
of leading to a layoff. Low co-payments
in health insurance reduce the risk of foregoing
needed care or suffering a major drop in other
consumption, but they also lead to an increased
demand for care that is worth less than its cost
of production. More complete protection in
each program also raises the program cost and,
therefore, creates greater distortions through the
tax system.

As protection becomes more complete, the
marginal value of protection declines and the
incremental distortion rises. The primary goal
of social insurance should, therefore, generally
be to prevent catastrophic losses: poverty in old
age, long-term loss of income when unemployed,
very expensive out-of-pocket medical
costs, and the consequences of permanent
disability. More generally, at the optimum,
the marginal value of additional protection
should just equal the marginal cost of the
distortion. Economists can help the policy
process by evaluating the protection and distortion
created by different changes in program
design.

Useful economic analysis can go beyond selecting
an optimal point on a protection-distortion
frontier. It is important to seek ways to shift the
frontier, permitting less distortion at each level of
protection. Reforms based on individual accounts
that I describe later in the paper would achieve
that improvement.

Redesigning Programs to Keep Pace with
Changing Conditions.--Three important changes
that should influence the design of our social insurance
programs have occurred since those

programs began: a changed economy, new technology,
and a different understanding of the effect
of government programs on individual behavior.
The Social Security and unemployment insurance
programs were created during the depression
of the 1930s when individual savings
had been destroyed by widespread bank failures
and when many individuals had been unemployed
for a year or more because of a lack of
aggregate demand. Keynesian economists in the
1940s like Harvard's Seymour Harris praised
the unfunded character of the new Social Security
program for its ability to depress national
saving and stimulate aggregate demand (Harris,
1941). In contrast to those depression years,
conditions in the past half-century have been
very different, with relatively low unemployment
rates and a system of government deposit
insurance that protects individual savings. The
unemployment insurance and Social Security
that may have been appropriate in the 1930s is
no longer appropriate for the economy of the
twenty-first century.

dollar paid into such a plan at age 45 only grows to $1.81.
The mandatory saving in the form of the low-return Social
Security is therefore equivalent to a 75.6 percent tax (that
reduces the gain of $3.32 to $0.81).


### ---Economics-2005-0-12.txt---
A second relevant change has been in the
technology of financial administration made
possible by the introduction of computers.
When Social Security was created, President
Roosevelt wanted it to be a funded system
rather than a pay-as-you-go system.14 There
was of course no way to have individually controlled
personal retirement accounts and the Republicans
in Congress did not want to trust the
government to manage a large pool of funds.
And since the Congressional Democrats were
eager to start paying benefits, the result was a de
facto shift to a pay-as-you-go structure. The
creation of individual investment accounts for
every adult, which would have been technically
impossible in the 1930s, is no longer even difficult.
Today more than 90 million Americans
own mutual funds, including IRAs and 401(k)
plan accounts. In contrast to the formidable task
in the 1930s of keeping track of everyone's
Social Security account without the help of
computers, the creation of a system of individual
investment-based accounts would now be
relatively easy.

The third important change has been in the
economic profession's understanding of how
fiscal incentives affect individual behavior. In
the 1930s, economists assumed that individuals
were so unresponsive to taxes and benefits that
any behavioral response could simply be ignored.
Most economists continued to ignore the
adverse incentive effects even when the top
marginal tax rate was over 90 percent, as it was
from 1944 to 1963. The adverse effects of high
unemployment insurance benefits on job search
and on the choice of jobs were also ignored.
Today economists recognize that high marginal
rates of income tax and the marginal tax
rates implicit in various benefit rules reduce
taxable income and create substantial deadweight
losses.

These three changes imply that if Social Security
and unemployment insurance were being
created now, the programs would likely be significantly
different from those in current law.
Economists today would regard the adverse effect
of Social Security on saving and capital
accumulation as a deterrent to growth rather
than as a favorable source of Keynesian demand.
The widespread ownership of mutual

funds, IRAs, and 401(k)s would be a natural
starting point for any new social insurance program.
And every aspect of behavior would be
assumed to be more responsive to tax rates and
program design.

More generally, the reforms that will be enacted
in the future will inevitably evolve as
economists learn more and as the set of feasible
options changes. An interesting example of this
changing perception of what is feasible is the
possible transition to personal retirement accounts
in an investment-based Social Security
program. About 20 years ago, when I served as
chairman of the Council of Economic Advisers
in the Reagan administration, the Social Security
retirement program was on the verge of a
crisis. The trust fund was about to reach zero
and the projected taxes to be collected over the
next few years were not large enough to pay the
benefits specified in law. President Reagan appointed
a bipartisan commission to find a solution.
The resulting plan called for an acceleration of
scheduled future tax increases, the taxation of
Social Security benefits, and a variety of other
smaller measures.

President Reagan was unhappy with these
proposals and asked a small group of us in the
Administration whether there wasn't something
better to be done, perhaps along the
lines of the Chilean reform that used investment-
based personal retirement accounts

instead of a pay-as-you-go system. None of us
could design a feasible transition to such a
plan. It looked to me and to the others as if
accumulating funds to finance such personal
retirement account annuities would involve
a double burden on the transition generation
that was both unfair and politically
infeasible.

I now know that that was wrong. Research
that Andrew Samwick and I have done in recent
years (Feldstein and Samwick, 1998a, 1998b,
2002) shows that it would be possible to transition
gradually to a completely investment-based
plan without ever increasing the combination of
pay-as-you-go taxes and personal retirement account
(PRA) saving by more than 2 percent of
payroll earnings, or about 1 percent of GDP, and
without reducing the benefit that retirees receive
from the combination of the traditional taxfinanced
program and the new investment-based


### ---Economics-2005-0-13.txt---
annuities.15 The key, we learned, is to have a
transition in which the personal retirement account
annuities gradually substitute for pay-asyou-
go benefits, allowing the pay-as-you-go tax
rate to decline and the PRA contribution rate to
increase until the transition is complete.
Of course, the demonstration that such a transition
is feasible doesn't mean that it is desirable.
A pure investment-based PRA plan would
involve more risk than many individuals might
want, a subject to which I will return later. But
a shift to a mixed system that avoids an increase
in the payroll tax rate or in private saving might
be an economic improvement. I'm sorry that I
couldn't offer that solution when President Reagan
asked for it.

My point in recalling this is that economic
research has changed what we regard as feasible.
Similarly, future research can and will develop
new ways to provide social insurance
protection with greater economic efficiency and
more responsiveness to individual tastes. A basic
principle of designing social insurance policies
should be a willingness to accept such
ideas when they become available.

Separating Social Insurance from Income
Redistribution.-As I indicated earlier, social
insurance programs are not means tested. Eligibility
for benefits does not depend on the income
or wealth of the recipient but on an event
like reaching age 65, beginning a spell of unemployment,
or incurring a medical problem.

Not surprisingly, the evidence that I cited makes
it clear that today's social insurance programs
do not redistribute income to the poor. Indeed,
the positive correlation of income and longevity
tilts the net benefit of Social Security and
Medicare to households with higher lifetime
incomes. The structure of unemployment
insurance rules causes a similar shift in that
program.

There is, of course, a role for means-tested
programs that are more narrowly focused on
individuals who demonstrate that they have low
income or assets. Although I doubt the desirability
of the myriad of existing in-kind programs
like food stamps and housing subsidies
(Moffitt, 2003), I have no doubt about the appropriateness
of transferring income to the very

poor.

There is, moreover, a clear case for being
more generous to some demographic groups
than to others. The existing Supplemental Security
Income program provides means-tested
benefits to those over age 65 whose Social Security
benefits plus private resources do not
together reach some minimal level. A more
generous means-tested program, targeted at individuals
over age 75, would not distort labor
supply to the same extent that it would for
younger ones. It is possible, therefore, to have
more protection with less distortion in such a
means-tested program. It is a shameful feature
of our Social Security system that, even with the
Supplemental Security Income program, 10 percent
of those over age 65 are in poverty while
Social Security provides nearly $500 billion a
year in benefits to individuals who are financially
more comfortable.

To the extent that distributional concerns motivate
the design of social insurance, the emphasis
should be on eliminating poverty and not on
the overall distribution of income or the general
extent of inequality. Like most economists, I
accept the Pareto principle that an economy is
better off if someone gains and no one loses.
This is true even if the gainer has above-average
income, causing a Gini coefficient measure of
income distribution to shift to greater inequality.
Although there may be spiteful egalitarians
who reject this Pareto principle, I believe that
most economists agree with me. To see if you
do, ask yourself whether you think it would be
a good thing if everyone reading this article
received $50 by some magical process that did
not decrease the income or wealth of anyone
else. Since we are an above-income group, national
inequality would rise. Nevertheless, I
think there are few who would reject bestowing
this extra wealth on us all.

This brings me to the end of my four economic
principles of social insurance. I turn now to discuss
how the three major forms of social insurance
could be improved by shifting to a system that
combines government insurance with individual
investment-based accounts: unemployment insurance
savings accounts (UISAs) backed up by a
government line of credit, personal retirement accounts
(PRAs) that supplement ordinary pay-asyou-
go Social Security benefits, and personal
15 Alternative plans could achieve the transition without
any rise in taxes by allowing the Social Security Trust Fund
to borrow for a temporary period.


### ---Economics-2005-0-14.txt---
retirement health accounts (PRHAs) that finance a
range of Medicare choices.

IV. Unemployment Insurance

Although unemployment insurance is a relatively
small program with total federal and state
outlays in 2003 of $39 billion, it is particularly
important because of its impact on macroeconomic
performance. It is also significant as an
illustration of how reforms have been able to
reduce distortion while retaining protection for
those who need it. Moreover, it is a form of
social insurance where further reforms through
investment-based accounts could achieve substantial
economic gains.

The unemployment insurance program in the
United States was created in 1935 in the depth
of the depression. The program is administered
by the individual states but under federal rules
that substantially restrict the scope of state governments'
actions. Benefits of a typical recipient
are 50 percent of previous earnings and can be
collected for up to six months. The European
unemployment benefit programs are substantially
more generous in both the relative level
and the duration of benefits, with clearly adverse
effects on European unemployment rates.
Thirty years ago, when I began doing research
on unemployment insurance (Feldstein,
1973a, 1973b), there was a general perception
that unemployment benefits were relatively low
and that they had little or no effect on economic
behavior. People were assumed to be unemployed
solely because there was inadequate aggregate
demand. Reformers focused on seeking
increases in the level and duration of benefits to
help those who were unemployed for what were
assumed to be reasons beyond their own control.
We now know that perception was wrong.
Unemployment insurance benefits raise the unemployment
rate in a variety of ways that economists
have now analyzed and measured. But
back in the 1960s and 1970s, the higher unemployment
rates that were actually induced by
unemployment insurance were instead incorrectly
perceived as due to inadequate demand.
When the government tried to reduce this high
structural unemployment with expansionary
monetary and fiscal policies, the result was rising
inflation. Fortunately, this is now better
understood. Monetary policy no longer tries to
reduce structural unemployment. But although
unemployment insurance is therefore no longer
a source of increased inflation, it continues to
raise the rate of unemployment. This is a particularly
serious problem in Europe where unemployment
rates remain close to 10 percent.

The old notion that unemployment benefits
were too low to affect the economy was the
result of a misleading comparison of the average
weekly unemployment benefit and the average
weekly wage. Although the average

benefit was only about 30 percent of the average
wage of all workers, the unemployed had substantially
lower pre-unemployment wages than

the labor force as a whole. Unemployment insurance
benefits actually averaged about 50 percent
of the pre-unemployment income of those
who received benefits, with even higher replacement
rates in states that supplemented the
basic benefit with payments for spouses and
children. But even this substantially understated
the relevant replacement rate because benefits
were not subject to the income and payroll taxes
that were levied on wages. Since the combined
marginal rate of income and payroll tax for the
spouse of a high-earning individual could then
easily exceed 50 percent, the ratio of untaxed UI
benefits to the individual's net-of-tax potential
earnings could exceed 100 percent. For such a
person, it was possible to have a higher net
income by remaining unemployed than by returning
to work.

Even significantly lower benefit replacement
rates could have substantial adverse incentive
effects, as a number of studies eventually
showed. Although macroeconomists came to
recognize that much unemployment was not of
an involuntary Keynesian type but was a productive
search for good job matches, the accumulating
evidence showed that UI benefits were
inducing excessively long periods of searching
in which the gain from the marginal search was
less than the value of the foregone output. For
example, Larry Katz and Bruce Meyer (1990)
showed that the probability that an unemployed
person takes a job rises dramatically in the few
weeks just before their benefits would expire.
Jim Poterba and I (1984) found that the median
value of the reported reservation wage of new
UI recipients was actually higher than the wage
on their previous job, that it was an increasing
function of the UI replacement rate, and that it
came down only very slowly during their spell
of unemployment.


### ---Economics-2005-0-15.txt---
Longer durations of unemployment are not
the only adverse effect of UI benefits. The practice
of temporary layoffs in which unemployed
individuals have a spell of unemployment but
return to their original employer is substantially
encouraged by high UI replacement rates (Feldstein,
1976, 1978a). High benefits also encourage
individuals to accept work in firms with
high seasonal or cyclical layoffs. That reduces
the wage that such firms have to pay and thus
subsidizes the expansion of those high unemployment
industries.

As all of this became clear, the most obvious
first reform was to include unemployment benefits
in taxable income. Although there was
initially strong opposition to this idea, it was
hard to argue with the position that cash income
is cash income and should be taxed. The notion
that taxing unemployment insurance would inappropriately
burden the poor was clearly contrary
to the fact that the income tax allows a
substantial exclusion of income before any tax
is levied. A poor UI recipient would pay no tax.
The initial legislative compromise was to include
only half of UI benefits in taxable income
and to do so only for relatively high-income taxpayers.
This provided a natural experiment that
Gary Solon (1985) used to show that the relative
duration of unemployment fell for those whose
benefits were taxed. Later, in the Tax Reform Act
of 1986, the UI benefits were fully subject to the
income tax like all other forms of labor income.
Taxing UI benefits eliminated the possibility
that an individual could have a higher net income
from UI benefits than by working. It is
hard to know what the aggregate effect on unemployment
has been, but my personal estimate

is that the unemployment rate probably fell by
about one-half percentage point after benefits
were taxed, an effect equal to more than
500,000 jobs at any time.

The evidence that UI benefits cause substantial
distortion led to analytic studies of the level
of benefits that optimally balances distortion
and protection. Martin Bailey (1978) presented
an analytic model in which the optimal level of
benefits depends on the individual's coefficient
of relative risk aversion and on the elasticity of
the duration of unemployment with respect to
the UI benefit replacement ratio. John Gruber
(1997) used this framework to derive an explicit
optimal UI benefit based on data on the effect of
unemployment on household food consump-
tion, concluding that the optimal replacement
rate should be much less than the 50 percent in
current law. More recently, however, Raj
Chetty (2003) showed that the measure of risk
aversion that is relevant to designing the optimal
UI benefit may be substantially greater than
the risk aversion that is relevant to financial
investments, because many types of household
spending cannot be adjusted in the short-run,
which is relevant to unemployment spells. Chetty'
s analysis points to optimal UI replacement
rates that are close to the levels that we observe
in the United States.

These calculations of optimal UI benefits assume
that individuals have no financial assets.
In contrast, if individuals save optimally, the
optimal value of UI benefits--especially for
short and moderate spells would be very much
less. Although there is evidence that individuals
who face greater income uncertainty have
somewhat higher saving rates, it would be
wrong to assume that in the absence of unemployment
insurance everyone would save

enough to finance consumption optimally during
spells of unemployment. Some individuals
would be too short-sighted to save for potential
unemployment.

What is the optimal response to this problem?
One possibility would be to continue the current
system of paying UI benefits but with the level
and time path of benefits selected to balance the
gain from protection and the loss from distortion.
Another possibility would be to shift to a meanstested
program, although that would induce some
individuals to game the system, saving nothing so
that they could qualify for means-tested benefits
when they became unemployed. The same problem
of asymmetric information would prevail, as
in the case of Social Security retirement benefits
that I discussed before: the government could not
distinguish individuals who were too short-sighted
to save from those who were gaming the system.
On efficiency grounds, the choice between the
current system and government means-tested benefits
would depend on the response of unemployment
to the benefit level and on the relative
number of those who would save optimally, those
too shortsighted to save, and those who would
choose not to save in order to qualify for the
means-tested benefits.

A third possibility is to require everyone to
have an unemployment insurance savings account
earmarked to pay benefits if unemployment


### ---Economics-2005-0-16.txt---
occurs. Dan Altman and I (Feldstein and
Altman, 1998) explored a variety of such possible
plans. In a typical plan, each individual
would be required to accumulate funds in an
unemployment insurance savings account until
the balance was enough to pay benefits for two
spells of six months at 50 percent of the individual'
s current wage. These funds would be
invested and would earn a market rate of return.
After a transition period to accumulate account
balances, anyone who would be eligible for
unemployment benefits under today's UI rules
would instead be able to withdraw the same
amount from his unemployment insurance savings
account. If a balance remains in the account
when the individual reaches retirement
age, the funds would be available for the individual
to take and spend. An individual who
dies before retirement bequeaths the account
balance. In short, individuals would regard the
funds in the UISA as their own money. For
someone who expects to have a positive balance
in his account until retirement, the UISA plan
would provide the same income protection as
the current UI system, but without any
distortion.

What about individuals who experience so
much unemployment that they use up the funds
in their UISA? Such individuals would be able
to borrow from a government UI fund to receive
the same benefits that they would withdraw if
they had a positive account balance. After they
return to work, they would again save to repay
the loan with interest and to rebuild their UISA
balance. If they expect their account to accumulate
a positive balance in the future, the dollars
that they borrow would be a very real obligation
and the incentives to return to work would not
be distorted by the government loan. They
would have full protection and no distortion
while unemployed and would accumulate personal
wealth after they returned to work.
It is only those who expect that they will have
a negative balance in their account when they
retire for whom this plan would represent no
improvement over current law. For them, the
protection and distortion would be the same as
it is with the current UI rules.

The extent of the gain from introducing unemployment
insurance savings accounts therefore depends
on the proportion of the unemployed who
expect to retire with negative balances and on the
sensitivity of unemployment to the change in in-
centives. Dan Altman and I did some preliminary
empirical analysis of this approach using a sample
of men in the National Longitudinal Survey. We
found that, even with no favorable behavioral response
of unemployment to the improved incentives,
less than 10 percent of benefits would be
paid to those who eventually retire with negative
balances (or who had negative balances when our
data sample ended).

Our analysis thus implied that the UI program
could be redesigned around individual unemployment
insurance savings accounts in a way
that substantially reduces the current distorting
effect while not reducing either the availability
of funds when unemployment occurs or the
protection against relatively large cumulative
amounts of lifetime unemployment. More research
on this potential form of unemployment
insurance would certainly be valuable.
V. Social Security16

Social Security is the largest social insurance
program in the United States, with expenditures
in 2003 of $470 billion, or 22 percent of total
federal government spending. It includes not
only annuities for retirees and survivors but also
a separate program of disability insurance that
accounts for some 15 percent of the total Social
Security outlays. Since the disability insurance
program involves a range of very different issues,
I will not be considering it in this article.
Social Security is now a defined benefit, payas-
you-go program in contrast to the defined
contribution, investment-based structure of most
private pensions. In a defined benefit program,
an individual's benefit at retirement depends on
his earnings during his working years and not
on the performance of asset prices during that
time. The program is a pay-as-you-go one because
most of the payroll taxes collected in each
year are used to pay concurrent benefits. There
is not the kind of asset accumulation and financial
investment that there would be in a private


### ---Economics-2005-0-17.txt---
pension. The benefits are financed by a payroll
tax on earnings currently up to about the eightyfifth
percentile of the distribution of wages
($87,900 in 2004). The payroll tax rate devoted
to the Social Security program other than disability
is now 10.6 percent, divided equally
between employers and employees.

Benefits take the form of an annuity that is
indexed to the CPI to retain its real value during
the individual's retirement years. The level of
benefits at retirement depends on the average
wage-indexed earnings of the individual during
his or her highest 35 earning years. The benefit
formula provides higher annual benefits per dollar
of previous earnings at lower earnings levels
than at higher levels. Individuals who retire
before their normal retirement age (now rising
gradually from 65 to 67) receive an actuarially
reduced benefit, while those who retire after
their normal retirement age receive an actuarially
increased benefit.

Couples may collect benefits either on the basis
of their separate earnings records or as 150 percent
of the benefits of the individual with the higher
benefit level. A surviving spouse can receive 100
percent of the benefit of the higher earning spouse.
These rules for spouse benefits have the effect of
causing many women who pay Social Security
taxes to get little or nothing back for their taxes
since their benefits are based on their husbands'
incomes. This is true not only for married women
but also for younger women who expect to marry
and for divorced women and widows who will
also expect to claim benefits based on their former
(or future) husbands' earnings.

I became interested in Social Security as a
graduate student in the 1960s when I realized
that the tests of consumption theory-Friedman'
s work on the permanent income hypothesis
and Modigliani's work on life cycle
saving- completely ignored the role of Social
Security even though it had become the major
source of retiree income. I realized also that the
theory of Social Security's effect on saving was
more complex than a simple displacement of
financial wealth by Social Security. To the extent
that Social Security induces earlier retirement,
it raises the desired level of financial
wealth. The net effect of Social Security on
saving therefore depends on the balance between
the positive induced retirement effect and
the negative wealth displacement effect, an issue
that could be settled only empirically.
My initial time series analysis (Feldstein, 1974)
implied that Social Security "wealth," the present
actuarial value of future Social Security benefits,
significantly reduced personal saving. Reestimating
this equation 22 years later with a corresponding
amount of additional data produced reassuringly
similar results (Feldstein,1996b) The
conclusion that Social Security depresses private
saving was also supported by household data and
cross-country analysis. Other researchers who
looked at this question generally supported the
primary conclusion, although with estimates of
varying magnitudes (Congressional Budget Office,
1998).

This adverse effect of Social Security on saving
is relevant to understanding the significance
of Paul Samuelson's very important 1958 overlapping
generations paper (Samuelson, 1958).
Samuelson showed that a pure pay-as-you-go
Social Security system in an economy without
capital and without technical progress would
generate an implicit rate of return on each generation'
s taxes equal to the rate of growth of the
population. This occurs because the number of
taxpayers is larger by the rate of growth of
population than the number of retirees. If technical
progress is added to this economy, the
implicit return in a pure pay-as-you-go system
is still the rate of growth of the tax base, now
the sum of the growth rate of population and the
growth rate of productivity.

Samuelson noted that this positive rate of return
meant that an unfunded Social Security program
could raise welfare in an economy that has no
capital assets. But what happens when we recognize
that all economies do have capital stocks and
that Social Security transfers act as a substitute for
real capital accumulation? With that more realistic
description, the reduction in the rate of saving
caused by the provision of pay-as-you-go annuities
can cause a reduction in the present value of
all current and future consumption.
To understand why, consider first an oversimplified
textbook economy in which there are no
capital income taxes. Each generation of workers
receives an implicit pay-as-you-go return equal to
the sum of population and productivity growth but
foregoes the larger return equal to the marginal
product of capital. For each such taxpayer generation,
there is, therefore, a cost of having a payas-
you-go Social Security program rather than
providing for retirement consumption by saving
and investing in real assets (or the financial assets


### ---Economics-2005-0-18.txt---
that represent a claim on those real assets.) However,
the initial generation of retirees had the good
fortune to receive benefits without ever having
paid taxes.

It can be shown that aggregating the consumption
losses of all generations of taxpayers back to
the initial date using a discount rate equal to the
marginal product of capital produces a present
value loss just equal to the windfall gain of the
initial retirees. (Feldstein and Liebman, 2002a;
Feldstein, 2005b) In short, with these simplified
"textbook" assumptions, the introduction of a payas-
you-go Social Security program does not reduce
the present value of national consumption
but rather redistributes it from later generations to
the first one. More generally, whenever the program
is expanded, those who are about to retire or
who will soon retire receive a windfall gain at the
expense of all future generations but with no
change in the present value of consumption over
all generations.

This neutrality result depends, however, on the
implicit assumption that there are no distorting
capital income taxes.17 With capital taxes, the
appropriate intra-generational rate of discount of
consumption is less than the marginal product of
capital. Moreover, the appropriate rate for aggregating
the consumption of different future generations
may not be a market rate at all but a
discount rate equal to the rate of decline in the
marginal utility of consumption. With a realistic
growth rate and any plausible value of the elasticity
of marginal utility with respect to consumption,
the appropriate rate of discount would be far
less than the marginal product of capital.'8 With
that discount rate, the net present value of the loss
of consumption due to introducing and then repeatedly
expanding a pay-as-you-go Social Security
would be very large. Similarly, shifting from
a pure pay-as-you-go program to a mixed program
with a substantial investment-based portion would
cause a large rise in the present value of
consumption.

The reduction in saving and in the present
value of consumption is not the only adverse
effect of a pay-as-you-go program. A second
important effect is the distortion of labor supply
and of the form in which compensation is paid
because of the increase in the marginal tax rate.
The relevant marginal tax rate is the statutory
rate net of the anticipated increase in the actuarial
value of benefits. The increase in benefits
is zero for many married women. It is also zero
for individuals who are not in one of their 35
highest earning years, typically when they are
either young or old, so that the higher effective
marginal tax rate comes when the individual's
attachment to work is relatively weak. Many
individuals may also underestimate the effect of
additional earnings on future benefits. In all of
these cases, the payroll tax may create a substantial
deadweight burden.

This incremental deadweight loss from distorting
the labor supply would be essentially
eliminated if individuals earned a market return
on their Social Security savings, as they would
in an investment-based system. That would
make the actuarial present value of their benefits
equal to their Social Security savings. The only
labor supply distortion would result if some
individuals were forced to do more saving for
retirement than they preferred or thought that
the implicit actuarial terms of the annuity did
not reflect their own mortality risk. A mixed
system that combines pay-as-you-go and investment-
based components would reduce but not
eliminate the labor supply distortion.
A third distortion caused by a traditional payas-
you-go system is the incentive to retire early
when an implicit tax results from the loss of
benefits caused by delayed retirement. Although
the United States has now largely eliminated
this by an appropriate actuarial adjustment, this
distortion remains a major problem in Europe
and elsewhere (Gruber and Wise, 1999). Early
retirement increases the annual cost of Social
Security benefits and reduces the available labor
income tax base. This leads to a higher marginal
rate of Social Security tax, further increasing
that source of deadweight loss. When combined
with formal or informal restrictions that prevent
reducing wages to offset the high payroll tax
rates, these taxes contribute to the high unemployment
rates that we see in Europe. The U.S.
experience shows that this problem can be eliminated
within the pay-as-you-go system. It
would, of course, also be eliminated in an
investment-based system in which retirement


### ---Economics-2005-0-19.txt---
income is withdrawn from personal accounts,
which can be bequeathed if the individual dies
before exhausting the account or before annuitizing
the accumulated balance.

The analysis of these three types of distortion
should make it clear that the shift to a "notional"
defined contribution system would help only a
little to reduce the adverse effects of the current
pay-as-you-go system. A notional defined contribution
system is one in which each individual
has an account that is credited with his tax
payments and with a notional rate of return on
his accumulated balance, but in which there is
no actual investment in financial assets. The
notional rate of return that is feasible in the long
term is the modified Samuelson return, i.e., the
rate of growth of the tax base. Since there is no
real capital accumulation, the reduction in the
present value of consumption is not changed.
The distortion in labor supply and in the form of
compensation is reduced (but not eliminated)
because individuals can more clearly see the
link between their taxes and their future benefits.
A notional defined contribution system also
reduces the distortion in retirement decisions
because individuals reduce their future benefits
if they retire early and increase them by delayed
retirement. But even with this improved transparency,
the low implicit pay-as-you-go rate of
return leaves a substantial distortion in work
and compensation incentives.

Although the scope for reducing the substantial
deadweight losses of the pure pay-as-you-go
system and for increasing the present value of
all future consumption should provide a strong
incentive for a change in policy, they are not the
reason that has driven the political process in
many countries to move from a pure pay-asyou-
go to a mixed system or to consider such a
change. The primary driving force is the recognition
that the increasing age of the population
will require a very large tax increase or benefit
cut if nothing is done to change the existing
system. This is not a temporary effect of the
baby boom generation reaching retirement age
but a permanent result of the trend to increased
longevity. This demographic change is significant
not only because it drives the political
process but also because it increases the potential
gain of making such a change.

The desirability of shifting to an investmentbased
or mixed system depends on four issues: (a)
the transition process and its cost; (b) the ongoing
administrative costs; (c) the riskiness of financial
investments; and (d) the effect on the income
distribution and especially on the poorest group. I
have done work on these issues during the past
decade, both alone and with colleagues Jeffrey
Liebman, Elena Ranguelova, and Andrew Samwick.
I will now summarize what I have learned.
I commented earlier on the transition problem
when I discussed my experience with President
Reagan. The common view that the

transition from a pure pay-as-you-go system to
a mixed system requires the transition generation
to "pay double"-once to save for their
own retirement and once to meet obligations to
existing retirees-is wrong. As examples of
what could be done, Andrew Samwick and I
(1998a, 1998b, 2002) showed how the projected
rise in the pay-as-you-go tax rate to more than
19 percent19 could be avoided if individuals
contribute just 1.5 percent of wages out-ofpocket
to personal retirement accounts. The key
to the transition is using personal retirement
account annuities to supplement the pay-asyou-
go benefits. The growth of the PRA annuities
offsets the slowdown of the pay-as-gobenefits
that results from not increasing the tax
rate as the population ages.

There is no free lunch in this process. The
key is that additional saving during the transition
years can reduce long-run costs. This extra
saving could be voluntary, induced by a matching
PRA contribution out of existing payroll tax
receipts.20 Although the matching would reduce
the "trust fund" balances, the transition need not
involve borrowing by the Social Security system
from general revenue (Feldstein and Samwick,
2002).21

19 The 19 percent is based on Social Security Administra-
tion's "intermediate" demographic and economic assumptions
but ignores the effect of the higher tax rate of the size of the tax
base for the payroll and personal income taxes.
20 The matching rate could be higher for low income
individuals to assure very high levels of participation. In the
extreme, the entire contribution for low-income individuals
could come from payroll tax revenue, requiring no contri-
bution of their own. High income individuals would gener-
ally welcome a chance for more tax favored saving and
could therefore be induced to participate with little or no
matching.

21 Stated differently, the Social Security Trust Fund
could remain positive and be rising at the end of the 75 year
projection period, showing that the system has long-run
stability.


### ---Economics-2005-0-20.txt---
Even a transition in which the initial personal
retirement account deposits are financed wholly
by government borrowing could eventually
raise national saving and the present value of
future consumption. Financing the initial deposit
to personal retirement accounts by government
borrowing would have no immediate

direct effect on national saving because the increased
budget deficit would be offset by the
deposit of those funds into the personal retirement
accounts. Over time, the availability of the
PRA annuities could permit reducing the payas-
you-go benefits (relative to those projected in
current law) without lowering total retirement
income. This reduction in the pay-as-you-go
benefits would mean that the annual rise in the
budget deficit would be less than the amount
transferred to the PRAs. That difference would
be an increase in national saving.22
I turn next to the issue of the administrative
cost of PRAs. Some critics of PRAs argue that
the administrative costs of a PRA program
could offset all of their higher return relative to
the pay-as-you-go system. Although there have
been bad cost experiences in some countries,
this certainly need not be so. Sweden's recent
PRA program involves administrative costs
equal to between 30 and 100 basis points of the
assets, an amount that will come down further
as the total assets grow, since the administrative
costs depend on the number of transactions and
not on the value of the assets. TIAA-CREF
operates an individual account system with a
variable annuity for a charge of only 37 basis
points.23

The issue of risk is an important consideration
in both the pay-as-you-go and investment-based
systems. Although pay-as-you-go programs do
not have asset price risk, they have the political
risk that future taxpayers may not be willing to
raise taxes when demographic or economic
changes would make it necessary to do so in order
to finance promised benefits. The United States
enacted benefit cuts in 1983 by increasing the age
for full benefits. Many Latin American countries
cut cash benefits in the 1980s and 1990s. More
recently, Germany, Italy, and Japan have announced
or enacted reductions in state pension
benefits.24 Perhaps the most reliable way to avoid
future legislation that causes an unexpected reduction
in retirement income is to develop a mixed
system that does not require a future rise in the
payroll tax rate.

The issue of asset price risk is more complex.
On the basis of a substantial amount of research,
I believe that a suitable mixed system that combines
tax-financed pay-as-you-go benefits with
investment-based PRA annuities can satisfy
three conditions: a substantially lower longterm
cost of financing retirement income than
the tax projected for the pay-as-you-go system;
a higher expected level of benefits from the
combination of pay-as-you-go and the PRA annuities;
and a very low probability that the actual
level of combined benefits will be less than
the pay-as-you-go benefits projected in current
law.25 The low risk could be achieved by a
combination of three things: the floor on retirement
income provided by the pay-as-you-go
benefits in the mixed system, restrictions on the
investments that can be made in the PRAs, and
explicit guarantees provided by either the government
or the private market. Because individuals
differ in their risk preferences, the solution
that can best reflect those different preferences
may be the availability of a variety of alternative
guarantees from the private organizations
that manage the PRAs and PRA annuities (Feldstein,
2005a).

I turn, finally, to the issue of the distributional
effect of the shift from our pay-as-you-go system
to a mixed system. I have already discussed
the evidence that the current Social Security
system does very little redistribution and leaves
some 10 percent of seniors with incomes below
the poverty line. Women who were never married,
widows, and particularly those who were
widowed or divorced at a relatively early age


### ---Economics-2005-0-21.txt---
generally have quite low benefits under current
law and could do substantially better under a
mixed system (Feldstein and Liebman, 2002b).
Divorced women would benefit if the total of
the husbands and wives' PRAs are pooled and
divided at the time of divorce.

While a PRA itself does not cause any income
redistribution, the redistributive structure
of the pay-as-you-go benefits could, in principle,
be changed to make the combined benefit
achieve any degree of redistribution.
In summary, it seems clear from the research
that has been done that the current pay-as-you-go
system could be gradually replaced with a mixed
system that includes investment-based personal
retirement accounts in a way that maintains or
exceeds the benefits that are projected in current
law, while sharply reducing the long-run cost of
achieving those benefits. This transition could be
financed with relatively small additional PRA saving
by individuals or by using existing payroll tax
revenue. Even if the tax revenue is used, the initial
fiscal deficit would not decrease national saving
because of the concurrent increase in private saving.
National saving would rise in the long run
because the PRA savings would exceed the increased
fiscal deficits.

A mixed system would eliminate the need for
a future increase in the Social Security payroll
tax and would, therefore, avoid the political risk
that future taxpayers would be unwilling to raise
taxes to finance promised benefits. It could be
designed so that, despite the asset price uncertainty,
there would be little risk that the combined
benefits would be less than the currently
projected pay-as-you-go benefits. The remaining
asset-price risk could be substantially reduced
by guarantees that could be produced by
the private financial market.

VI. Medicare

Medicare, the federal health care program for
those over age 65, is more difficult to reform
than either unemployment insurance or Social
Security. The program is more complex and the
reaction to proposed changes is often more
emotional. And yet without reform, Medicare
costs will rise even more dramatically than the
cost of Social Security retirement benefits, reflecting
the increasing numbers of the very old
(who consume relatively more medical care)
and the changing medical technology that pro-
vides new opportunities to spend money to prolong
life and increase the quality of life.
Before looking at Medicare, it is useful to
consider the basic theory of health insurance
and the current way that the government provides
a kind of quasi-social insurance for the
population under age 65 by its favorable tax
treatment of employer payments for health insurance.
This is important in itself and suggests
an approach that may be useful for reforming
Medicare.

The basic theory of insurance implies that a
risk-averse individual will prefer an actuarially
fair insurance policy to an uncertain and exogenous
distribution of potential losses. But the
distribution of potential health spending is not
exogenous and the gain from risk reduction
must be balanced against the distorting effect of
insurance on the demand for care. As insurance
becomes more complete, the marginal gain
from additional risk reduction declines and the
marginal deadweight loss from distorting the
demand for health care rises. At the optimum
level of health insurance (e.g., at the optimum
coinsurance rate) there is a deadweight loss
caused by the distortion in the demand for care
because individuals, advised by their doctors,
make decisions about diagnosis and treatment
based on a net price of care that is very much
less than the cost of producing that care. That
level of insurance is, however, efficient because
the deadweight loss from distorted demand is
less than the gain from risk reduction.
In actual practice, the demand for health insurance
is greatly increased by the tax treatment
of excluding employer payments for health insurance
from the taxable income of employees.
Allowing employees to buy health insurance
with pretax dollars in this way changes the
nature of health insurance. For someone with a
marginal tax rate of 40 percent, the ability to
buy health insurance at a cost of only 60 cents
per dollar of premium substantially increases
the demand for health insurance with low deductibles
and low coinsurance rates. This increases
the deadweight loss caused by the

distortion in demand for care.

Direct attempts to eliminate or even reduce
the tax subsidy or to constrain it by requiring
minimum deductibles or coinsurance rates have
not been politically successful. When the Reagan
Administration proposed to limit the employers'
deduction for health insurance premiums,


### ---Economics-2005-0-22.txt---
it was unable to get any member of the
President's own party to introduce the legislation
in Congress.

Recently, however, Congress enacted legislation
to shift the incentives away from excessive
health insurance. The new health savings account
(HSA) rules, enacted as part of the 2003
Medicare legislation, allow individuals or their
employers to deposit up to $5,000 of pretax
income into a health savings account if they
have a health insurance policy with an equally
large deductible and with protection against catastrophic
expenses. The individual foregoes the
advantage of the tax-free income in the form of
the employer-paid premium, but gets an even
larger tax-free income in the form of the health
savings account contribution. Assets in these
IRA-like accounts earn tax-free investment income.
Funds not spent in one year automatically
carry forward to the future and can be used to
finance any kind of health care without paying
tax. Funds can also be withdrawn for any other
kind of spending by paying tax at that time.
The health savings accounts create a strong
incentive to choose policies with high deductibles
instead of the current comprehensive lowdeductible
and low coinsurance policies. This,
in turn, should change the nature of the demand
for care. Until individual health spending
reaches the deductible limit, money spent on
health care is the individual's own money and
not that of an insurance company. Spending
below the high deductible limit, therefore,
would not have any of the distortion caused by
current policies with low deductibles and low
coinsurance rates. And the requirement that the
policies provide protection for catastrophic levels
of spending means that the most important
form of protection is retained or increased at the
same time that the distortions are reduced.26
Of course, anyone who spends several days
in a hospital will exceed the deductible limit. At
that point, the insurance company is paying for
care as it would today and the patient and his
doctor no longer have the incentive to be cost
conscious. The favorable incentive effects of
the HSA could be increased without reducing
the individual's insurance protection by replacing
the deductible with a 50-percent coinsur-
ance rate on spending up to twice the level of
the HSA saving deposit. For example, the limit
associated with the $5,000 HSA deposit would
shift from a $5,000 deductible to a 50 percent
co-payment on the first $10,000 of care, causing
significantly more individuals and health spending
to be in the cost-conscious range.

The shift from the current tax-induced comprehensive
insurance to large deductibles or coinsurance
is not only a way to limit excessive
health care spending, i.e., spending that individuals
and their doctors recognize as less valuable
to them than the cost of production. It is also a
way of making health spending reflect each
individual's preferences. While all of us want
good health, the lifestyles choices that individuals
make show that some of us value it more
than others. Although we all understand the
adverse health effects of obesity, smoking, and
the lack of exercise, not everyone acts on this
information. Many people knowingly make the
tradeoff to enjoy more eating, to smoke, and to
avoid the rigors of exercise. Just as people make
different lifestyle choices, some are more willing
than others to sacrifice more of other consumption
to increase spending on health care.
Health savings accounts will allow this expression
of taste in health care spending instead of
effectively inducing almost everyone to purchase
high-cost health care.

Health savings accounts may be a model for
Medicare reform. If nothing is done, the cost of
Medicare to the federal government will rise
from 2.4 percent of GDP to about 6 percent by
2030 and 8 percent by 2050. The rising cost of
Medicare is similar to, but even more dramatic
than, the rising cost of Social Security retirement
benefits. The remedy for this problem
should have two components: changing the
spending incentives to slow the growth of
Medicare outlays, and using a mixed financing
system to raise the needed funds without the
sharp tax increase that would otherwise be
needed (Feldstein and Samwick, 1997; Feldstein,
1999b).

If health savings accounts are successful, the
high deductibles and coinsurance that will
evolve because of the HSAs for those under age
65 may establish a precedent that will also affect
future Medicare benefits and therefore the
spending incentives of the Medicare population.
A mixed financing system for Medicare
could combine a tax-financed Medicare annuity


### ---Economics-2005-0-23.txt---
for retirees geared to the then-current cost of
health care plus an opportunity for individuals
during their working years to accumulate funds
in retirement health savings accounts. These
combined funds could be used at retirement to
pay for the type of health plan that the retiree
prefers: a comprehensive insurance plan of the
type that Medicare now provides; membership
in a health maintenance organization that provides
a managed care plan; or a lower cost plan
with substantial deductibles and coinsurance.
Alternatively, working age individuals could
use the funds contributed annually to their retirement
health savings account to purchase

health insurance for their retirement years,
thereby minimizing the problem of asymmetric
information in policy choice at retirement
that would occur in buying insurance after
retirement.

VII. Conclusion

The reform of social insurance is clearly a
work in progress in the United States and in
other countries as well. Policymakers can do
much to improve the major social insurance
programs that protect the unemployed, the aged,
and the ill. Economists can contribute to this
process by improving our understanding of the
effect of social insurance rules and by deriving
new program designs. In this paper I have emphasized
the use of personal investment-based
accounts created and regulated by the government
and earmarked for unemployment benefits,
for retirement income, and for health care
during retirement. Such accounts have the potential
to provide a better tradeoff of increased
protection and reduced distortion. They also
give individuals greater discretion in tailoring
benefits to their own tastes.

I am an optimist about economic policy. I
have examined what is wrong with our current
social insurance programs and what could be
done to improve them in the future. I believe
that the policy process does evolve and that
economists have contributed to that evolution.
We see that in the important reforms of the past
two decades that I have described. But there is
still much for economists to do in designing
better policies for the future and in educating
the public and the political decision makers
about the desirability of making such changes.
 ## Economics-2006-0


### ---Economics-2006-0-03.txt---
You cannot simply tell a person in dire
need, wait for the market to take care of
you. That is a most callous thing to say,
and only makes a person feel owned, and
with no control over his life.

Letter to the Editor,

New York Times, 2005

[I]t is not enough to simply liberate people
and assume that they will automatically
pursue economic prosperity. People

need to be instilled with certain beliefs,
like the belief that ... individuals have the
power to shape their own destiny. ... It's
important to understand the beliefs that
encourage people to work hard and grow
rich.

David Brooks,

New York Times, 2005

I. Consumers and Markets

Economic theories and ideologies are
founded on the principle that consumers have
well-defined preferences, and consistently behave
to advance their self-interest. Jeremy
Bentham (1789) said, "My notion of man is
that ... he aims at happiness ... in every thing he
does." Herbert Simon (1957) said, "The rational
man of economics is a maximizer, who will
settle for nothing less than the best." Some
economists have even taken self-interest to explain
choice tautologically:



An article can have no value unless it has
utility. No one will give anything for an
article unless it yield him satisfaction.
Doubtless people are sometimes foolish,
and buy things, as children do, to please a
moment's fancy; but at least they think at
the moment that there is a wish to be
gratified. Doubtless, too, people often buy
things which, though yielding pleasure for
the moment, or postponing pain, are in the
end harmful. But here ... we must accept
the consumer as the final judge. The fact
that he is willing to give up something in
order to procure an article proves once for
all that for him it has utility,-it fills a
want.

Frank Taussig, 1912

Consumers who know their own tastes, and
are relentlessly self-interested and self-reliant,
relish choice, and welcome market opportunities
that expand their options. Most economists
accept this concept of the consumer, and the
attendant economic theory that demonstrates
the efficiency and Pareto optimality of decentralized,
competitive markets. Over the past 30
years in the United States and elsewhere, these
market-oriented views have driven economic
policy, leading to deregulation of air and truck
transportation, telecommunications, and energy
markets; establishment of property rights and
markets to manage environmental externalities;
and globalization of international markets for
goods, capital, and services. Notable successes
were the deregulation of truck and air transportation,
and of telecommunications, where dysfunctional
regulation worked at cross-purposes
to competition. Another success was making
air pollution a property right, allowing Coasian
markets to internalize environmental externalities.
There have also been striking failures,
such as the breakdown of the incompletely deregulated
energy market in California a few

years ago, the rail transport deregulation in
Great Britain which got wrong the incentives
for track maintenance, and the British system
of private retirement accounts which allowed
5


### ---Economics-2006-0-04.txt---
excessive fees and overselling. However, the
sweep of decentralization and privatization is, I
believe, widely viewed by economists as an
almost universal success, with the failures
due to correctable flaws in market design. Romantics
of the economic right would carry the
concepts of self-interested consumers and
free markets even further, embracing a withering
of authority and a nirvana of Hayekian
self-reliance.

Most reasoned discussions of privatization
among economists concentrate on information
asymmetries, incentives, economies of scale
and scope, risk management, and the relative
efficiency and sustainability of alternative
forms of market organization. There are serious
economic questions as to whether, for example,
the technologies of network industries inevitably
lead to concentration, with an attendant loss
of choice and efficiency. There are serious questions
as to whether adverse selection will defeat
the efficiency gains from competition in multiple-
payer privatized insurance markets. It is a
worthy scientific enterprise to study these issues,
and look to the historical record of privatization
for answers, but not one that I will take
up in this paper. I will concentrate, instead, on
the decision-making of consumers, the market
outcomes they achieve as a result, and the influence
of these outcomes on their attitudes
toward markets.

In the general public we see widespread unease
about market solutions. Free trade and
globalization, privatization of social insurance,
and deregulation of energy markets all elicit
opposition from many consumers, sometimes
reasoned but often inchoate. It is no coincidence
that support for market solutions is concentrated
among the economically successful, and opposition
among the less successful. Free choice
has moral appeal, but moral fiber is strongest
when not cut by self-interest. Market mechanisms
have to compete for votes with alternative
resource allocation schemes more favorable to
the underdogs; and in this competition, fairness
to me is my primary concern, efficiency is
someone else's problem. In addition, there is
ideological opposition to market solutions. In
the liberal orthodoxy, markets are dominated by
the powerful and rapacious, and the motives of
government bureaucrats are purer than those of
private bureaucrats. In this ideology, the process
of privatization often serves the interests of the
politically connected. The Enrons and Haliburtons
of this world reinforce these views. However,
ideologies themselves are woven from
human sentiments, and antipathy to market solutions
is more than just doctrine.

My concern in this paper is that it is not
enough to find ways to handle information and
technology issues in privatization if consumers
are not up to the task of functioning satisfactorily
in such markets. The argument is not that
consumers should be coddled; they may need to
see the stick to get the incentives for selfreliance
right. However, the efficiency and stability
of an economy requires that all consumers
be part of the franchise, in reality and in perception,
so that good economic policies, including
privatization and free markets when they
make sense, receive broad support. I will discuss
these issues at two levels. First, I will give
a selective review of the behavioral evidence on
consumer decision-making, and how this influences
market outcomes and attitudes toward
markets. Second, I will summarize results that
my research group has obtained on a current,
concrete privatization issue, the new Medicare
Part D prescription drug program, which is offering
market choices within a social insurance
program. I will ask whether consumers are, in
fact, able to manage their choices adequately in
this new market, and whether they will, in fact,
gain from the added choice offered by privatization.
The following fundamental questions,
explored in pioneering papers by James J. Choi
et al. (2003) and Richard H. Thaler and Cass R.
Sunstein (2003), comprise an important scientific
agenda:

* Are consumers sufficiently consistent in advancing
their self-interest in specific markets
to achieve the levels of efficiency and wellbeing
that privatization promises?

* What can be done as part of the design of
privatization, such as information, instruction,
and support structures, to help consumers
satisfactorily pursue their self-interest?
* When privatization is in consumers' selfinterest,
how can they be enlightened and

convinced to support the change?


### ---Economics-2006-0-05.txt---
II. The Challenge of Choice

Agoraphobia (ayopa` + 6f3o(, literally
"fear of the marketplace") Fear of leaving
a safe place, fear of being in situations
from which escape might be difficult or
embarrassing; fear of losing control in a
public place such as a restaurant or shopping
mall.

Psychology Today

Studies of consumer perceptions, motivations,
and behavior give a complex picture of
self-interest and the determinants of well-being.
Consumers often find choice overwhelming,
and decision-making uncomfortable. In the
words of a Dutch proverb, "He who has choice
has trouble." We routinely use procrastination,
precommitments, habit, imitation, social norms,
defaults, and superstitions to avoid confronting
choice. We pass up trading opportunities, particularly
in unfamiliar situations. We are suspicious
of trading partners, and fearful of
deception, exploitation, or unfair treatment. In
short, we exhibit various degrees of agoraphobia,
a term that means literally "fear of the
marketplace," adapted by psychiatrists to mean
fear of leaving a safe place for a situation from
which it might be difficult or embarrassing to
escape. Reflect on the major decisions in your
own lives-choice of college, occupation, car,
house, and spouse-and in most cases you will
feel you made the right choice, but will recall
the choice process itself as an emotional, stressful
experience.

By rational calculation and accumulated experience,
we benefit from choice. Then, why do
consumers fear markets and find choice troubling?
First, there is market risk. Forget the
antiseptic, well-lighted budget sets and markets
of economics textbooks. Real-life markets are
rough, murky, tumultuous places where commodity
attributes shift, supply is uncertain,
prices are volatile, and information is imperfect.
Caveat emptor prevails, and caution and calculation
are vital. The sure-footed may thrive, but
their success may come in part from the failures
of the less experienced and nimble. Second,
there are personal risks, including the risk of
misperception and miscalculation, of misunderstanding
the available alternatives, of misread-
ing one's own tastes, of yielding to a moment's
whim and regretting it afterward. Finally, there
is social risk, the interactions between people
that trade requires; the stress of information
acquisition, search, and bargaining; the stress of
dealing with pushy or deceptive sales tactics;
and the risk of being embarrassed or defrauded.
How do consumers deal with these risks?
And what is it about these risks that leads to
broad biases against market-based resource allocation?
Perhaps such inference is rooted in
human psychology. Consumers often have the
perceptual illusion that other freeway lanes or
supermarket lines move faster than their own,
because the occasions on which this occurs are
particularly noticeable and irritating. Similarly,
they may have the perceptual illusion that they
are particularly unlucky, or subject to discrimination
and exploitation in markets, because
their bad experiences stand out. Markets that
work well for you are invisible, those that don't
are a source of frustration and grief.
III. The Consumer's Mind

What if everything is an illusion and nothing
exists? In that case, I definitely overpaid
for my carpet.

Woody Allen

To understand how consumers deal with market,
personal, and social risks, it is useful to
study how they think, and the social context of
thought and trade. While the mutual benefit of
trade is the aspect emphasized in economics,
trade is also a contest, with the issues, emotions,
and stresses that competitions entail: Is the playing
field level and the referee fair? Will my
opponent play by the rules? Can I match her
knowledge and skills? The competition itself,
not just the outcome, becomes a source of pleasure
or pain. Trade is part of the way that
humans as social animals define and defend
themselves, a process that is both cognitive and
visceral.

Mind and trade are linked in human prehistory.
I relate an evolutionary tale, adapted from
Matthew Ridley's book The Origins of Virtue.
A few million years ago, the great apes established
family groups that were successful in the
essentials-obtaining food, protecting themselves


### ---Economics-2006-0-06.txt---
from predators, and reproducing. In common
with other animals, they evolved a sense of
personal space sufficient to provide some defense
against attack, and a system of trust and
reputation that allowed them to suspend their
"fight or flee" defenses and live together with
family members. These spatial social interactions
had a physiological basis-reward pathways
in the brain and neurotransmitters that
facilitated social contact, reciprocity, and mutual
aid. Some of these apes discovered that
through division of labor, specialization, and
trade, they could be more productive and fertile,
and live better and longer. But trade, particularly
outside the family group, was risky business.
To get close enough to a stranger to trade
flints for furs, one had to risk being attacked.
The most successful apes dealt with this by
developing the ability to form bonds of trust
over larger social groups than the family. This
was accomplished by adapting the brain's visceral
reward pathways that already allowed
family units to function. Second, these apes
developed analytic, social, and communication
skills that allowed them to operate in larger
social and economic groups. These were cerebral
activities, and evolution selected species
with more cerebral capacity. Among these apes
were our ancestors. They gave us large brains,
with the capacity to explore the corners of our
universe, and to engage in sophisticated economic
activities. They also gave us an emotional
reward system that processes economic
actions in much the same visceral way that it
processes personal interactions: when to approach
and when to avoid, whom to trust, and
when to form personal or professional bonds.
The evolutionary tale I have just told is speculation,
based on observations of contemporary
apes and other animals, and fossil records.
However, the role of trust and reward pathways
in the brain, and how they affect economic
conduct, is something that we can investigate
experimentally, using the tools of brain science
and the new discipline of neuroeconomics to
study the processing of economic choice problems
at a physiological level. Brain measurements
include maps of energy consumption,
observed under experimental treatments that alter
electrochemistry and cognitive task. These
measurements fall short of Edgeworth's wistful
call in 1881 for a hedinometer to record pleasure,
but they provide some insight into the
sensations that economists call utility.
The early biologists observed that as the human
embryo developed, it seemed to go through
stages of evolution, from a simple one-celled
creature to its complex final form. That view
was superficial, but it does seem to be the case
that human physiology, and in particular the
structure of the brain, is consistent with a layering
of added functionality over a simpler and
more primitive core. The aspects of brain function
that we identify with being human-language,
the cognitive processes of deduction and
induction, the ability to empathize and interact
with others-are primarily sited in the frontal
lobe of the cerebrum, the outer layer of the brain
whose relative size and complexity in humans
differentiate us from most other species. The
more primitive limbic system, buried at the base
of the cerebrum, is heavily involved in emotion
and the reward pathways associated with sensations
of pain and pleasure. The limbic system
is active in animal behavior at a visceral level:
approach and avoidance, foraging, territory, and
reproduction. The electrochemistry of the limbic
system is similar in all animals, and on
the evolutionary scale clearly predates human
development.

Most people think of economic activity as
quite cerebral, learned through lengthy education
and shaped by culture. If the brain is the
hardware, then the utilitarian calculus might be
pictured as software, an operating system that is
stored and run at various, possibly relocatable,
hardware sites, and is modified, Linux-like, by
experience and selection. In this view, monitoring
the brain can tell you something about the
burden the software places on the hardware, but
relatively little about what the software is doing.
The picture that is now emerging, however, is
that economic behavior, like the brain itself, has
layers. Working a spreadsheet to balance a retirement
portfolio is indeed a high-level, learned
skill. Economic trading, however, also seems to
involve relatively primitive circuits in the limbic
system. Therefore, you should not be surprised
to learn that brain hardware is associated
with economic decisions in a substantial and
relatively direct way. Specifically, the limbic
system and its reward pathways qualify as the


### ---Economics-2006-0-07.txt---
brain's primary center for recording pleasure,
and are active when we are involved in matters
of threat, trust, sex, and economic trade.' If you
have ever dismayed over convincing students
that economics is a sexy subject, you can now
tell them that shopping and sex share the same
neurotransmitters and receptors.

The linkages from physiological sensation to
conscious interpretation and reasoning may be
complex, and physiology alone may give an
incomplete picture, just as computer hardware
monitoring gives an incomplete picture of what
software is doing. Nevertheless, it should be
clear than any ability to measure directly in the
brain the impact of economic choice tasks on
reward pathways is potentially an immensely
powerful tool for linking economic activities
and consumer well-being.

How do organisms process sensations of
pleasure and pain? The answer goes directly to
the question of whether there is a single, absolute
physiological scale of well-being or utility,
and whether the organism consciously or unconsciously
acts out of self-interest to maximize
this quantity. First, both behavioral observation
and brain studies indicate that organisms seem
to be on a hedonic treadmill, quickly habituating
to homeostasis, and experiencing pleasure
from gains and pain from losses relative to the
reference point that homeostasis defines (see
Sanfay et al., 2003). People quickly grow to
accept the city in which they are located, their
job, their mate, and their health status. They
may recognize and complain about unfavorable
absolute states, but their levels of satisfaction by
various measures are not nearly as differentiated
as they would have to be if their sensation of
well-being were experienced on an absolute scale.
Second, the picture that emerges from brain
studies is that the dopamine reward pathways in
the limbic region play a central role in experi-


encing pleasure, and also mitigate, with a lag,
the sensation of pain (see Becerra et al., 1999;
McClure et al., 2004). Adaptation to homeostasis
and differentiation between the pleasure and
pain circuits coincide with the powerful endowment
and loss aversion effects, and sensitivity to
framing and context, found in behavioral studies,
and suggest that these phenomena are tied
fundamentally to brain structure. This is good
news and bad news for utilitarians: the limbic
system reward pathways record pleasure and
pain on what seems to be close to a utilitarian
scale, but brain circuitry processes experience
in ways that are not necessarily consistent with
relentless maximization of hedonic sensation.
One of the interesting bits of contemporary
biology has been the establishment for a variety
of species of simple direct links from particular
genes to the production of, and receptors for,
specific neurotransmitters, and from this to specific
social behavior. One peptide, oxytocin, is
particularly involved in bonding and trust between
animals, most notably between parents
and their offspring. This is relevant to economics
because, in the words of Kenneth Arrow,
"every commercial transaction involves an element
of trust." In a study that strikes at the heart
of consumer sovereignty, Fehr et al. (2005) and
Michael Kosfield et al. (2005) administer oxytocin
or a placebo to subjects, and then ask them
to play the trust game. In this game, an investor
is given 100 MU. She has the option of placing
Y MU with an anonymous trustee, who through
the experimenter receives triple this amount.
The trustee then volunteers to send Z MU back
to the investor. The trustee's subgame is a dictator
game in which norms of fairness and reputation
matter, but the rational response in a
single-shot anonymous game is to return nothing.
By backward induction, the investor should
send nothing. In fact, both the investment and
the return are usually positive, with the level of
investment higher in subjects who are administered
the "trust" peptide oxytocin. Oxytocin has
no effect, however, on play of the dictator subgame,
where trust does not matter. The conclusion
is that economic perceptions and decisions
are sensitive to brain chemistry, and susceptible
to chemical manipulation.

Neuroeconomics is a new subject, and the
future will determine its potential and limits for


### ---Economics-2006-0-08.txt---
understanding economic choice behavior. It already
seems to confirm and explain, however,
that brain structure and chemistry are behind
some systematic anomalies in economic behavior,
particularly failures to form perceptions and
pursue self-interest consistently when confronted
with choices involving remote, uncertain,
or ambiguous outcomes, failures to recall
or anticipate in full color the sensations that
outcomes produce, and the quick adaptation to
circumstance, the hedonic treadmill.
IV. Personal Risk

What information consumes is rather obvious:
it consumes the attention of its

recipients. Hence a wealth of information
creates a poverty of attention, and a need
to allocate that attention efficiently among
the overabundance of information sources
that might consume it.

Herbert Simon, 1971

A large literature from behavioral economics
and psychology finds that people

often make inconsistent choices, fail to
learn from experience, exhibit reluctance
to trade, base their own satisfaction on
how their situation compares with others',
and in other ways depart from the standard
model of the rational economic

agent. If people display bounded rationality
when it comes to maximizing

utility, then their choices do not necessarily
reflect their "true" preferences,

and an exclusive reliance on choices to
infer what people desire loses some of
its appeal.

Daniel Kahneman and

Alan Krueger, forthcoming

The biological evidence that the human brain
is complex and layered, more an imperfect
meeting of minds than an optimizing computer,
follows and supports behavioral evidence from
cognitive psychology and experimental economics
showing that humans are, well, all too
human in the ways they retrieve and evaluate
information, and process decisions.2 In over-
view, these studies suggest that homo economicus-
sovereign in tastes, steely-eyed and
point-on in perception of risk, and relentless in
maximization of happiness-is a rare species.
While consumer behavior in familiar market
settings may have these characteristics, when
we approach the consumer from a different angle,
asking direct and unusual questions about
beliefs or values, or offering novel products and
services, we find alarming variations from the
story of consistent advancement of self-interest.
All these apparently normal consumers are revealed
to be shells filled with heuristics that
have been shaped by evolutionary selection and
experience. These heuristics often work. For
example, two of my rules which seem successful
are: "Never buy a Rolex from a street
vender" and "Never accept an e-mail offer to
transfer millions of dollars to my bank account.
" However, throw the consumer a curve
ball, in the form of a question that fails to fit a
standard heuristic for market response, and the
essential "irrationality" of the organism is revealed.
For most economists, this is the plot line
for "Stepford Consumers," a real horror movie.
Even if this bleak portrayal is true, however, it
does not mean that policy conclusions based on
consumer rationality are wrong, only that the
consumer may need to be coaxed and wheedled
into responding to market choices with sufficient
diligence to approximate rational promo-
tion of self-interest.

Most of the evidence on consumer decisionmaking
comes from laboratory experiments.

Economists reviewing the experimental evidence
sometimes comment that markets punish
inconsistencies, and consumers learn to avoid
them. They then conclude that while these flaws
may appear in experiments, they are not important
for economic behavior. This may be true in
repeated, familiar market settings where the
conduct and rewards of others provide good
2 Edited volumes that survey this subject include Kah-
neman et al. (1999), Kahneman and Amos Tversky (2000),
Thomas D. Gilovich (2002), John H. Kagen and Alvin E.
Roth (1995), George Loewenstein et al. (2003), and Charles
R. Plott (forthcoming). See also Charles Bellemare et al.
(2005), Ronald Bosman et al. (2005), Camerer (1999),
Camerer and Thaler (1995), Donald Green et al. (1998),
Teck H. Ho et al. (forthcoming), Michael D. Hurd et al.
(1998), and Olaf Johansson-Stenman and Hector Svedsater
(2003).


### ---Economics-2006-0-09.txt---
examples. Some consumers are slow learners,
however, and many markets are inconsistent
teachers, providing more irritation than illumination,
giving random awards and punishments
that consumers cannot always translate into accurate
road maps for successful behavior. Even
if consumers do learn from experience, remember
P. T. Barnum's comment that "there is a
fool born every minute," additional mugs for
the market game. Importantly, the sting of market
punishment breeds agoraphobia. Just as
children humiliated in the classroom may be
turned off rather than educated, consumers humiliated
in the marketplace may develop an

aversion to markets, where opportunities for
choice may be interpreted as opportunities for
mistakes, embarrassment, and regret.
A. Memory and Perceptions

There are now extensive experiments and
insights from cognitive psychology showing
that memory is imperfect and perceptions are
often biased and statistically flawed (for detailed
surveys see Matthew Rabin, 1998; McFadden,
1999). Consider, first, factual and
affective memory. Our memories guide our perceptions
of alternatives and our preferences, and
imperfections in remembering facts and sensations
can distort our perspective, leading to inconsistent
behavior and disappointment. Table

1 summarizes some of these effects; I will comment
on how they can lead to suboptimal market
outcomes.

What we store and retrieve from memory
is affected by mood and emotion. Laura
Carstensen (Susan M. Charles et al., 2003;
James J. Gross et al., 1997) finds that advertisements
are remembered better, and influence
choice more, when the affective content of the
ad matches the mood of the consumer. George
Loewenstein (1996) finds that emotional sensations
are more easily remembered than nonemotional
ones, but emotions themselves are

difficult to retrieve from memory-we remember
experiencing episodes of pleasure or pain,
and these memories can powerfully condition
our behavior-"once burned, twice shy"-but
we fortunately cannot relive the experiences in
their original intensity.

Finding and retrieving information from
memory is a complex cognitive task. The answer
may be on the tip of your tongue, but
sometimes the tip of your tongue is hard to find.
We use contemporary cues to guide memory
search, and to fill in and bluff when memory
fails. Consequently, what we remember is influenced
substantially by current context and
mood, and these are vulnerable to manipulation
in the presentation of choice alternatives.
Selective memory is the phenomenon in
which we remember what draws our attention.
Coincidences stick in our minds, noncoincidences
are forgotten. This influences probability
judgments. A good example is the belief in the
"hot hand" in athletics, the idea that players can
get in the groove for some period of time and
play consistently above their game. Objectively,
the hot-hand phenomenon does not exist-the
observed distribution of runs of success is consistent
with independent Bernoulli trials, not
with heterogeneous spurts and slumps. The explanation
is that long runs are coincidences that
are selectively remembered. One of the implications
of selective memory for market behavior
is that people build up elaborate and
complex beliefs about causal relationships between
events, taking natural events personally,


### ---Economics-2006-0-10.txt---
TABLE 2-JUDGMENT AND THE FORMATION OF PERCEPTIONS AND BELIEFS
Effect Description

Anchoring Judgments are influenced by quantitative cues contained in the decision task
Context/framing History and framing of the decision task influence perception and motivation
Endowment/reference point Status quo is a "safe" known alternative: "The devil you know is better than the devil
you don't"

Extension Representative rates are more available than integrated experience
Prominence/order The format or order of decision tasks influences the weight given to different aspects
Prospect Probability calculus is inconsistent; asymmetry in gains and losses
Regression Causal structure attributed to fluctuations; failure to anticipate regression to mean
Representativeness Frequency neglect in exemplars
and persuading themselves that they are systematically
lucky or unlucky in handling market
risk.

Another important memory effect is subjective
time. You all know the canard, "Time flies
when you are having fun." We have trouble
keeping time scales straight in our memories.
We telescope time, so past events seem more
recent than they actually were. We are unsuccessful
in integrating sensation over time. In a
phenomenon studied by Daniel Kahneman,
Alan Krueger, and others (Kahneman and
Krueger, forthcoming; Donald C. Redelmeier
and Kahneman, 1996), episodes of pleasure or
pain are remembered selectively in terms of
peak and most recent sensation. This can lead
consumers to choices that "remember" better
than they "experience." There is a relationship
between subjective time and brain structure--
current sensation is recorded in the limbic system
and its reward pathways, memory of past
and anticipation of future sensations are processed
in the cerebrum, more analytic and less
colorful. David Laibson and colleagues have
studied this as the physiological explanation for
hyperbolic discounting (Fehr, 2001; Laibson,
2005; Laibson et al., 2005). A final comment is
that subjective time is not a new element in
explaining consumers' sensations and behavior.
Francis Y. Edgeworth (1881) proposed, following
William S. Jevons (1871), that the same
objective time may correspond to different rates
of thought and feeling in different periods, so that
the utility of an experience will be the subjective
time integral of the sensations involved.
Perceptions and beliefs are influenced by the
way we process information (see Table 2).
Memory plays a role, e.g., selective memory is
implicated in regression and representativeness
effects. We overemphasize recent, available experience
in forming beliefs, and depend heavily
on readily available cues to construct our perceptions
when we need them to make choices.

In experiments, consumers are often influenced
by the context and framing of perceptual
tasks and choices, and anchor their perceptions
to cues contained in the choice task. Anchoring
affects statements of willingness to pay (WTP)
for public goods obtained by direct elicitation
when consumers have incompletely articulated
tastes for these goods (see Green et al., 1998).
In addition, anchoring distorts responses to factual
questions in surveys. Beyond this, why
should economists be interested? The answer is
that anchoring effects appear clearly in market
transactions involving complex commodities.
For example, houses and automobiles are typically
sold by bargaining, starting from an initial
listing price or manufacturer's suggested retail
price. Field experiments with real estate agents
show that manipulation of initial offers can influence
bargaining outcomes. A study by Itamar
Simonson and Amos Tversky (1992) finds that
when products are positioned so that one appears
to be a bargain, a form of anchoring, then
consumers will flock to the apparent bargain
alternative. When I told a friend who owns a
Boston seafood restaurant that he could use this
result to reposition his wine list and increase his
profits, his response was "tell me something I
didn't learn in hotel school."

Anchoring is one example of how consumers
may be influenced by context and framing that
should be irrelevant to choice. A second important
example is the endowment effect, also
called a reference point or status quo effect, in


### ---Economics-2006-0-11.txt---
which consumers show a reluctance to trade
away from any position in which they are established.
The endowment effect appears in

stated preference studies, where WTP for an
increased amount of a commodity is typically
far less than willingness to accept (WTA) a
reduced amount of the commodity. Some gap is
expected, due to diminishing marginal utility,
but experiments show gaps far too large to be
explained by classical income and substitution
effects. For example, a study by McFadden et
al. (1988) of stated WTP for changes in reliability
of electricity supply found that mean stated
WTP for a change between two levels, neither
of them the status quo, was valued consistently
by consumers independently of their status quo,
but in comparisons between the status quo and
any alternative, the status quo was given extra
value, independent of its level. It appears that
the hedonic treadmill is at work, with people
habituating to their current state, and viewing
changes with distaste.

A dramatic illustration of the endowment effect
is the now-classic cup experiment of Jack
L. Knetsch (1989), in which a random assignment
of coffee cups in a class, followed by an
opportunity to trade, produced a large gap between
WTP and WTA, with far less trading than
should be needed to move from a random allocation
to a Pareto optimal one (see also Kahneman
et al., 1990). I repeated this experiment in
an introductory microeconomics course at
Berkeley, using pencils embossed with the
course name. About half of the 345 students,
172, were randomly assigned a pencil. Then, a
Vickery sealed-bid uniform-price double auction
was held to reallocate the pencils (see Kiho
Yoon, 2005). In this auction, each bidder has an
incentive to report her true value, independently
of the strategies of others. The income effect of
being endowed with a pencil is negligible, so
that with random assignment the distributions
of money marginal utilities of a pencil should be
the same for buyers and sellers. Then if consumers
are neoclassically rational, there should
be no endowment effect.

Consider a market with N participants with
values v, > "> VN, and K randomly allocated


### ---Economics-2006-0-12.txt---
pencils. In the incentive-compatible Vickery
double auction, successful buyers pay vK+ 1,
and successful sellers receive vK, with the market
operator covering the difference. The number
of pencils J initially allocated to the K
highest value participants has a binomial distribution,
b(K, KIN). The volume in the efficient
auction is then K - J, which has mean K(N -
K)IN and variance K2(N - K)/N2.

In the experiment, the expected volume is
86.25, with a standard deviation of 6.56. The
actual market-clearing price was vK+ 1 = K =
35, and the number of market-clearing transactions
was 32. Under the hypothesis of no endowment
effect, the probability of 32 or fewer
transactions is on the order of 10-16. The median
offer to buy was 10 cents and the median
offer to sell was 100 cents. A runs test confirms
(T = 12.5) that buyers and sellers do not have
the same value distribution. Thus, there is a
strong, trade-suppressing endowment effect,
generated instantaneously by a random allocation
of pencils. Either tastes are changing endogenously,
with quick habituation to the status
quo, or agoraphobia is real-consumers find
trade an edgy experience, instinctively mistrust
the market, and resist trading for small gains.
Consumer preferences among risky prospects-
lotteries-show a number of behavioral
anomalies that appear to be related to the endowment
effect. In summary, consumers appear
to evaluate lotteries as changes from a reference
point that may be sensitive to framing, and to
exhibit asymmetric loss aversion in which
losses loom larger than gains, with consumers
displaying risk aversion for gains and risk
seeking for losses, a certainty effect in which
there is a pure preference for sure things over
lotteries, and a prospect effect in which the
probabilities of low-probability events are overestimated.
One of the consequences of these

effects is that consumers will often refuse to
take any share of either side of an offered lottery,
a result consistent with the observed paucity
of real-world wagers. An additional reason
that individuals are suspicious of lotteries, and
often avoid them, is the superstitious belief that
there are hidden causal forces at work, interventions
that place the lottery in ambagious relationship
to the rest of life.

There is experimental evidence that endowment
effects are attenuated when traders are
experienced (see Mikhail Myagkov and Plott,
1997; John A. List, 2004). Thus, the observed
paucity of trades in lotteries may occur primarily
for novel events and inexperienced traders.
These facts are consistent with a proposition
that learning by observing and by doing may be
effective in selecting rational market behavior
rules in arenas with sufficient repetitiveness to
allow these effects to operate.

B. Calculation and Processing

The ideal rational consumer has the computational
power to value complex commodities

and consistently handle risk, discounting, and
option calculations, and the logical clarity to
work through the consequences of decisions
and optimize choices. In practice, both computational
and logical skills are limited. This may
be inconsequential for repeated short-lived
choices, such as picking out your breakfast cereal
or deciding when to change lanes, but these
limitations become critical for unfamiliar, not
easily reversed choices, such as occupation, job
change, house, automobile, children. The deficiencies
are most severe when choice involves
small, ambagious risks in the distant future, as
in the case of smoking and other addictive activities,
a perfect storm in which distortion of
perceptions of time, risk, and affect combine
with difficult computations of options and contingencies.
Table 3 lists some of the effects that
impede accurate processing and maximization
of preferences.

A first limitation is that we miss many choice
opportunities, and are barely conscious of others
we make almost automatically. Driving an
automobile is an example. We may ignore opportunities
to change lanes or pass, or may

decide to do so without conscious thought. Such
decisions are usually sensible; we develop habits
that work well and save scarce attention
time. They may not, however, be optimal. In
particular, lack of attention may lead to procrastination
and default choices that are, after the
fact, clearly not optimal.

I think it is remarkable on balance how well
most people function in markets, even people
with little academic aptitude. This may be because


### ---Economics-2006-0-13.txt---
we are adapted to trade, and because we
are good at copying successful behavior. Nevertheless,
such processing deficiencies as disjunction
and innumeracy do confuse choice.

Ellen Peters at Decision Research studies the
ability of people to understand and logically
relate numbers, an essential skill in trading that
involves prices or barter terms, or more complex
valuations requiring risk assessment or discounting.
Even if individuals do not consciously
"run the numbers" to determine choices, they
still have to form perceptions and make judgments
based on numerical information. The behavioral
evidence is that innumeracy rates are
high and significantly distort decisions. Peters
and her coauthors (Peters et al., forthcoming)
find that half the population is unable to read
and make sense of numbers in the newspaper.
Among those who score badly on a battery that
measures basic numerical and logical skills, one
finds errors such as altering ratings of risk and
choices when probabilities are presented as
number of successes out of a hundred, number
of failures out of a hundred, or as percent successes.
In one telling experiment, subjects are
offered a prize if they draw a red jellybean from
their choice of bowls. Bowl A contains 9 red
and 91 white beans, while bowl B contains 1 red
and 9 white beans, so the odds of success are
objectively better with bowl B. Nevertheless,
subjects who score low in numeracy often
choose bowl A because it "gives more chances
to win."

One could be hard-nosed about such people
and say that if they have not educated themselves
sufficiently to look after their own interests
in markets, the consequences are on their
shoulders. The economically unsuccessful can
vote, however, and they demonstrably have
used the vote at various times and places to pick
bad governments and bad economic policies.
The argument against "sink or swim" is that
when designing market mechanisms, it is in
society's interest to take a protective interest in
this segment of the population, building in information
and decision-making aids, and protection
from market wolves, which give these
people a chance of success, thereby increasing
the fairness of these mechanisms and support
for them. This argument becomes stronger
when one considers the sociality of choice, and
observes that there is more than "self' in
self-interest.

V. Social Risk

In risk perception, humans act less as
individuals and more as social beings who
have internalized social pressures and delegated
their decision-making processes to

institutions. They manage as well as they
do, without knowing the risks they face,
by following social rules on what to ignore.


Mary Douglas and

Aaron Wildavsky, 1982

Man is a social animal, identified with family
and kin, and with troops, tribes, clubs, ethnicities,
and nationalities. This has several consequences
for economic choice behavior. First,
individuals may look to their social networks
for information. Second, they may look to social
networks for approval, and use social accountability
to limit choice. Social norms can be
comforting, limiting options and regrets, but
they can also lead to embarrassment, ostracism,


### ---Economics-2006-0-14.txt---
and agoraphobia. Third, consumers may, out of
pure self-interest, engage in mutually beneficial
reciprocity, simple when the acts are synchronous,
involving more complex elements of reputation
and trust when they are not. Pursuing
comparative advantage, with division of labor
and trade, is a form of reciprocity. Fourth, they
may engage in genetic altruism, making choices
that are in the interest of their progeny rather
than themselves as individuals. Fifth, they
may exhibit altruistic behavior that does not
obviously serve their personal or genetic selfinterest,
such as incurring costs to sanction
greedy behavior.

A. Information

One major way sociality works is through
transmission of information, learning by imitation
rather than learning by doing. People
constantly make interpersonal comparisons,
judging the desirability of options from the apparent
satisfaction and advice of others. While
personal experience is the proximate determinant
of the utility of familiar objects, and may
be extrapolated to similar objects, our primary
sources of information on new objects come
from others, through observation, advice, and
association. McFadden and Kenneth E. Train
(1996) show that in innovation games with uncertain
payoffs, it may pay to wait, and learn by
observing rather than learn by doing. Charles F.
Manski (1991) has explored the possibility that
individuals faced with dynamic stochastic decision
problems that pose immense computational
challenges may simply look to others to infer
valuation functions to be used to judge the future
payoff of current acts, or to infer satisfactory
policies. An objection to such copycat
behavior is that it fails to take account of the
individual's idiosyncratic tastes, and correcting
this quickly gets the individual back into
the computational difficulties that imitation
was intended to circumvent. But if tastes as well
as perceptions are modified socially, the relevance
and value of the lessons from others
increases.

Economic demographer Hans Peter Kohler
(2001) has investigated the effect of word-ofmouth
communication from friends on choice
of contraceptive. He studies Korean peasant
women, who have access to relatively little public
information on efficacy, costs, and side effects
of new contraceptives. Choices within
villages show little diversity, but there is substantial,
persistent diversity across villages.
This pattern is not explained by income, education,
or price differences. Word-of-mouth
communication from friends was found to be
the important explanation of most women's
choices. Lack of inter-village mobility explained
multiple equilibria, with persistent intervillage
differences. Thus, some apparent taste
heterogeneity is due to the boundedly rational
practice of imitation in balkanized social networks.
The implications of social information
networks for economic policymakers is something
that is part of the bible of marketingproduct
launch and penetration is critical to
tipping network opinion and ensuring success.
Serious education of network information leaders
through demonstration and experience is
important not only for promotion of a product,
but also for its design.

In addition to providing information, social
networks may discipline the behavior of members
through consensus on social norms, accountability
for choices, and sanctions for

behavior that violates norms.3 The individual
gains from affiliation with such networks if
imitation and conformity save energy, if the
"expectation that one will be called upon to
justify one's beliefs, feelings, or actions, to others"
improves decision-making, and if approval
is itself a source of pleasure. The classical idea
of herd mentality is that social animals find it
easier and more comfortable to adhere to a
group, accept group roles, and mimic group
behavior than to act independently. Accountability
reinforces herd mentality in fixed groups,
and promotes safety in numbers. Individual
membership may be voluntary, as in the pellaton
of tightly packed riders in a bicycle race,
with riders tightly clustered and constrained
in order to save energy in preparation for
"breakaways."

3 See Gary S. Becker (1976), Francis Bloch et al. (2005),
Alan P. Hamlin (1991), and Matthijs Poppe (2005).


### ---Economics-2006-0-15.txt---
B. Reciprocity and Altruism

Reciprocity is a simple form of social interaction,
present in economic trade and explained
by self-interest. Reciprocity is easy to
establish when it is synchronous, as in bilateral
barter. Asynchronous reciprocity, however,
requires reputation and trust. Norms for
fair practice, and sanctions for bad behavior,
may evolve in social networks to facilitate
asynchronous reciprocity, and individuals
may by habit or internalization conform to
these norms even in novel situations where
the normal cycle of approval and reputation is
suspended (see Fehr and Klaus M. Schmidt,
1999; Laetitia B. Mulder et al., 2005). Consider
the single-shot ultimatum game with
anonymous players. Player 1 proposes a division
of a prize of 100 units. If Player 2
accepts, the players get the proposed shares;
otherwise, they get nothing. It is rational for
Player 2 to accept any positive amount, and
thus rational for Player 1 to offer the minimum
positive amount. If, however, the probability
of acceptance a(s) by Player 2 is less
than one when the share s offered by Player 1
is low, then Player l's optimal strategy is to
maximize a(s) vg (1 - s). Students in a cross
section of developed countries play similarly.
Offers are usually 42 to 50 percent of the
prize, and offers less than 20 percent are
rejected about half the time. These results are
consistent with social norms for fairness in
which individuals altruistically incur costs to
punish greedy behavior.

Sam Bowles and a team of experimental
economists and ethnographers have conducted
anonymous ultimatum game experiments in 15
isolated societies whose ways of life provide
natural experiments on the influence of cultural
norms (see Joseph Henrich et al., 2001, 2004).
The findings overall are that cultures where
cooperative activity is important, and particularly
where people are exposed to markets, induce
offers in the ultimatum game that are more
equitable.

Genetic altruism is the phenomenon of
self-sacrifice for the good of your family or
kinship group. Genetic altruism appears to
explain cooperation in most species, and
seems to have a convincing evolutionary basis.
It has been a central theme of sociobiologists
in the past four decades, but the

concept itself is as old as the concept of
self-interest, as in a quote from Adam Smith
(1759):

Every man feels [after himself, the pleasures
and pains] of the members of his

own family. Those who usually live in the
same house with him, his parents, his
children, his brothers and sisters, are naturally
the objects of his warmest affections.
They are naturally and usually the

persons upon whose happiness or misery
his conduct must have the greatest

influence.

Despite its recognized importance, particularly
in economic models of the family and
of intergenerational transfers, genetic altruism
has not been systematically studied as a
determinant of economic behavior. The operation
of genetic selection could be very indirect.
Thus, the acquisition of language, the
exploitation of comparative advantage, the
formation of successful defenses against marauders
and disease, and a disposition to "fair
play" that reduces interpersonal conflict may
all arise from the selective advantage of group
traits that promote sociality. Then altruistic
behavior, including pure altruism with gifts
to unrelated individuals with no possibility
of personal gain, might be explained as an
indirect consequence of genetic self-interest,
as might the "warm glow" most humans experience
when placed in a supportive, cooperative
environment, the distaste people have
for aggressive, greedy traders, the potlatch
pride of being more generous than your
neighbors.

Summarizing, physiological, behavioral, and
sociological evidence indicate strongly that
consumers will often fail to promote their
self-interest reliably when choices involve risk,
ambiguity, integration of experience, and perceptions
of remote and/or unlikely events. Consumers'
failures will loom large, and this may
generate agoraphobia. Market-oriented economic
policy needs to take into account how
consumers' market experiences and outcomes
will influence well-being and acceptance of
market solutions.


### ---Economics-2006-0-16.txt---
VI. Consumers and Medicare Part D

Medicare's Part D drug plan is extraordinarily
complex. This government program

takes the cake, the candles, the

platter, and the crumbs.

Kathleen Pender,

San Francisco Chronicle

Medicare Part D is not that difficult to
understand. There has been a lot of confusing
information in the news about Part

D Medicare.

OregonHealthInsurance.com

The new Medicare Part D program that began
operation on January 1, 2006, provides prescription
drug coverage through Medicareapproved
plans offered by private insurance

companies and HMOs. Consumers in the Medicare
population can choose to opt out, or to
enroll in one of the private plans available in
their geographic area. This is a large and
complex government program that provides
substantial entitlements for the elderly and substantial
insurance against catastrophic drug
costs. If the entire eligible Medicare population
of 41 million were to enroll in this program,
then at current levels of prescription drug use,
the net subsidy from general government revenues
would be about $44.8 billion per year; this
includes some double counting of Medicaid,
veterans, and other programs that currently
cover prescription drug costs, and assumes that
all employer and union plans meet Medicare
requirements and qualify for the subsidy. There
is an adverse selection problem. If the approximately
27 percent of the elderly whose annual
pharmacy bills are currently below $842, the
breakeven point in 2006, were to delay enrollment
until health conditions warrant, the net
cost of the program would rise another $4.2
billion. However, moral hazard is the bigger
issue.4 In the Medicare population, people with
prescription drug coverage average 1.1 more
prescriptions than those without. If the 26 percent
of the population who currently pay all
4 See Peter Adams et al. (2003), Dana P. Goldman et al.
(2004), Anne E. Hall (2004), Haiden A. Huskamp et al.
(2004, 2005), John R. Moran and Kosali I. Simon (2005),
and Z. Yang et al. (2004).

their pharmacy bills enroll in Part D, experience
this increase in number of prescriptions, and
face the current average monthly cost of a new
prescription, $66, then this increases the cost of
the program by $6.8 billion. In these worst
cases, the effect of adverse selection and moral
hazard together is projected to increase the cost
of the program to $55.8 billion.

The creation of a market in which private
companies compete to offer coverage, and in
which consumers have choices of carriers and
plans, was an important element in the Part D
legislation. For economists, it is an interesting
economic policy experiment in whether the
benefits of competition can overcome the problems
of adverse selection and moral hazard that
always lurk in private insurance markets;
whether the Center for Medicare and Medicaid
Services (CMS) can efficiently manage its principle/
agent and underwriting relationship with
private insurers; and whether consumers can
understand and evaluate plan alternatives in
their own self-interest. In 2004, the National
Institutes of Health asked research groups working
on the economics of aging if they could
provide information on the impact of the Part D
program. My research group attempted to do
this by modifying a survey we were planning to
study health perceptions and choices of the elderly.
During the week of November 7-15,

2005, just before enrollment for Part D began,
we surveyed 4,739 persons age 50 and older and
gathered information on health conditions and
prescription drug use, knowledge and enrollment
intentions for Part D, and preferences
across different plans. Our initial findings are
given in Joachim Winter et al. (2005). I will
summarize a few findings here, with particular
attention to the question of whether consumers
are sufficiently self-reliant to take advantage of
the choices offered by the private market structure
of this program.

The Part D program is complex because of its
interactions with existing employer or unionprovided
drug coverage and with Medigap insurance,
and its provisions for means-tested cost
reductions for low-income consumers. There
are five main classes of eligible consumers:
* Standard Medicare, including those with
Medigap policies that do not cover drugs


### ---Economics-2006-0-17.txt---
Standard Medicare with Medigap policies
that cover drugs

Employee- or union-provided coverage, including
drugs

Medicare Advantage (HMO or PPO) policies
that cover drugs

Medicaid beneficiaries

Generally, those in the last three categories receive
Part D coverage by default. Those with
Standard Medicare will default out of Part D
if they do not take action, but have the choice
of enrolling in a privately offered plan, or of
converting to Medicare Advantage coverage.
In virtually all cases, there are Part D plans
that are more advantageous than Medigap
policy drug coverage. The analysis that follows
applies to the people currently on Standard
Medicare.

CMS has established a standard plan under
Part D that has an annual premium of $444, a
deductible of $250, pays 75 percent of prescription
drug pharmacy bills above $250 up to
$2,250, provides no additional benefits until
pharmacy bills reach $5,100, and pays 95 percent
of pharmacy bills above that level. CMS
requires approved private plans to offer comparable
coverage.

Table 4 summarizes consumer out-of-pocket
costs under the standard plan, not including the
annual premium, for various pharmacy bills.
The private insurers who provide drug coverage
within the Plan D framework may offer enhancements
to the standard plan, at higher

premiums, including coverage for the $250 deductible
and/or for the gap or "doughnut hole"
in the standard plan, which pays no added benefits
for pharmacy bills above $2,250 or below
$5,100. They may offer broader formularies
than Medicare requires, variations in the coinsurance
or copayment tier structure, and convenience
features such as broad pharmacy

participation and mail-order services. Approved
plans must have formularies that include at least
two drugs in each therapeutic category; the fraction
of the 100 most frequently prescribed drugs
included in currently approved formularies
ranges from 65 percent to 100 percent, with a
median of about 90 percent. Enrollees may
change plans annually. There are penalties for
late enrollment, currently a 1-percent increase
in premiums per month's delay past the initial
enrollment period, which ends in May 2006. In
evaluating alternatives, consumers need to take
into account not only their current pharmacy
bills, but also the probabilities of developing
new health conditions that will require treatment,
and the distribution of costs of these
treatments. As a result, consumers are being
asked to make relatively complex plan assessments,
generally with relatively incomplete information
on future prospects. Because of the
late enrollment penalties, there is not only a
current financial risk of making a poor decision,
but also an option pricing problem of determining
the value of enrolling to lock in current
premium rates. Not surprisingly, some seniors
are finding this a difficult choice, and the media
has had a field day publicizing Part D's complexity.
The economic policy question is this:
After the dust settles, will most consumers have
made good use of the choices offered by the
private market, so that a market-oriented design
contributes to consumer well-being? Is further
intervention on behalf of the vulnerable
needed?

Our survey, entitled the "Retirement Perspectives
Survey" (RPS-2005), was fielded as a
self-administered Internet questionnaire from
November 7-15, 2005, using a panel of subjects
enrolled by Knowledge Networks, a commercial
survey firm. This panel was recruited from
a random sample of the underlying population,
and all panel members were provided with identical
hardware (Web TVs) through which they


### ---Economics-2006-0-18.txt---
TABLE 5-NUMBER OF PRESCRIPTIONS

Age 50-64 2.8

Age 65 + All 4

Age 65+ Pay own pharmacy bills 3.3

Age 65+ Others pay pharmacy bills 4.4
TABLE 6--PERCENT WITH LITTLE OR No KNOWLEDGE OF
PART D

All 39.5

High SES 32.5

Bad health 49.8

Low cognition 46.9

Low SES, bad health, and low cognition 54.3
respond to periodic surveys. Members are compensated
for participation on the panel. For our
study, 5,879 members of the panel aged 50
and over were contacted. Of these, 4,738 individuals
completed the survey. Our present analysis
is restricted to those respondents who are in
the Medicare-eligible population, for the purposes
of our study defined as age 65 and older
(N = 1996).

The survey lasted about 22 minutes and covered,
in addition to questions about Part D,
questions about health status and conditions,
long-term care choices, prescription drug use
and cost, and attitudes toward risk. We also use
the 2001 Medicare Current Beneficiary Survey
(MCBS) distribution of annual pharmacy bills,
and an AARP survey giving median prices of
commonly prescribed drugs (as of April 2005)
for nine health conditions. Table 5 gives the
average numbers of prescriptions used by various
groups. Notable is the increase in the number
of prescriptions for those who have their
pharmacy bills paid by others, relative to those
who pay their own bills.

We find that despite the complexity of the
Part D program's competing plans, a majority
of the Medicare population has at least some
knowledge and intends to enroll. However, lowincome,
less educated elderly with poor health
or some cognitive impairment are significantly
less informed and may fail to take advantage of
the program. Table 6 gives the fractions of the
Medicare population who just before enrollment
started said they had little or no knowlTABLE
7-PERCENT NOT LIKELY TO ENROLL

All 17.0

Good health 19.0

Bad health 11.7

Well informed 14.7

Poorly informed 19.6

edge of Part D. Table 7 gives the percentages of
the Medicare population who said just before
enrollment started they were unlikely to enroll
in a Part D plan. This does not include people
who will not enroll directly in Part D because
they already have prescription drug coverage
that is at least as good as the Medicare standard
plan. Overall, 17 percent say they are unlikely
to enroll. The percentages are higher for those
in good health, and those poorly informed. The
percentage differences are small, but statistically
significant.

A revealing assessment of the consistency of
individual intentions is obtained by comparing
enrollment choices with the alternatives that
minimize the expected present value (EPV) of
out-of-pocket cost (OPC). Underlying the enrollment
decision is an option value problem: If
an eligible person enrolls immediately in Part
D, her EPV of OPC in each year from 2006 to
the end of her life will be the $444 annual
premium plus her expected pharmacy bill, less
the Part D benefit. If, on the other hand, she
delays one year, then the EPV of her OPC is her
expected pharmacy bill for 2006 plus the EPV
of her OPC from 2007 forward, assuming that
she makes the decision to enroll or delay in
2007 and subsequent years to minimize EPV of
OPC, and assuming that these future decisions
take into account the new information she will
obtain on health and prescription costs as she
goes along, and the Medicare premium penalty
for late enrollment, which is 7 percent in 2007,
and 12 percent per year thereafter. With information
on the probabilities of developing new
health conditions, and the distributions of drug
costs for required therapies, this can be formulated
as a dynamic stochastic programming
problem, and solved by backward recursion to
determine a threshold depending on age, such
that if the current pharmacy bill is below the
threshold, an individual who seeks to minimize


### ---Economics-2006-0-19.txt---
EPV of OPC cost will choose to delay. We
simplify this computation by approximating a
necessary condition for delay, ignoring the influence
on expected cost today of the additional
information and contingent decisions that will
be gained as future health conditions and pharmacy
bills are realized. This approximation was
found to be reasonably accurate in a study of
retirement decisions by Robin L. Lumsdaine et
al. (1994). We implement this calculation using
U. S. Life tables, estimates from the Health and
Retirement Survey of the annual probability of
developing a condition requiring a new prescription
drug therapy, and estimates from our
survey and the MCBS of the distribution of
annual drug costs for a new therapy.5
Figure 2 gives the thresholds we obtain using
this approximation; these apply to people who
do not receive means-tested premium reductions.
There are four factors that may modify


this calculation for an individual. First, additional
information on health that will be revealed
in the future, and decisions contingent on
this information, give delay some added option
value. Second, risk aversion gives immediate
enrollment added insurance value. Trial calculations
indicate that the full option pricing calculation,
and risk aversion for a person with
moderate coefficient of absolute risk aversion,
have effects on the threshold for delay that are
relatively small, on the order of $100 or less.
Third, individuals may have different personal
probabilities for new health conditions and prescription
drug requirements than the ones we

have used. Fourth, individuals may have different
discount rates than the 5-percent discount
rate we have employed. For people with 2005
pharmacy bills above $802, the option of delaying
enrollment is "out of the money"-these
people can expect to reduce their OPC for prescription
drugs in 2006 with Part D coverage, in
addition to being insured against risks of high
future bills. The difference between the $802
threshold and the $842 break-even level for a
consumer's current pharmacy bill is the expected
value of the consumer's new pharmacy


### ---Economics-2006-0-20.txt---
TABLE 8-ENROLLMENT INTENTIONS

Intended

choice

Action that minimizes EPV of OPC

Enroll Delay Total

Enroll 63.3% 19.4% 82.7%

Delay 10.0% 7.3% 17.3%

Total 73.4% 26.6% 100.0%

TABLE 9-PLAN CHOICE

Min EPV

Alternative Choice of OPC

Standard 46.9% 45.5%

Guaranteed Benefit 27.1% 3.3%

Major Cost Protection 6.0% 0.0%

No Copay 20.0% 51.2%

bills in 2006. About 72.5 percent of the Medicare
population meet this condition. For those
with lower bills, there is an annual pharmacy
bill threshold that rises with age from just below
$500 to close to $750. Individuals who are
prepared to self-insure and are currently below
this threshold will probably find delay desirable,
while those between this threshold and $802
will probably find immediate enrollment desirable.
Approximately 24.4 percent of the Medicare
population falls in the region where delay
is probably desirable, and 3.1 percent in the
region where immediate enrollment is probably,
but not definitely, desirable.

Table 8 classifies enrollment intentions
against the action that minimizes EPV of OPC.
The table shows that the choice of 70.6 percent
of the population minimizes EPV of OPC.
However, there are 10 percent who intend to
delay even though it is likely in their selfinterest
to enroll. On the other hand, 19.4 percent
of those intending to enroll would achieve
lower EPV of OPC by delaying. Of course,
some of that group may want the insurance
against catastrophic costs in the future, and
these could be rational decisions if there is very
strong aversion to the risk of large, low-probability
losses.

A final part of our survey asked subjects for
their preferences among the alternatives of no
prescription drug coverage, the Medicare Part D
standard plan, and three hypothetical alternative
plans:

* Guaranteed Benefit Plan: Medicare pays 52.3
percent of approved prescription drug costs,
no matter how high or low these costs are.
The annual premium of $444 is the same as
the standard plan.

* Major Cost Protection Plan: Pays all approved
prescription drug costs above $2,444
per year, but nothing until your cost at the
pharmacy reaches this level. The annual premium
of $444 is the same as the standard
plan.

* No Copay Plan: You pay an up-front annual
premium of $1,889 per year, and all approved
prescription drug costs are then fully covered,
with no copayments.

The alternative plans all have the same actuarial
value as the standard plan for the Medicare
population, but differ in the degree to which
they provide insurance against major pharmacy
costs. The Major Cost Protection Plan and No
Copay Plan provide almost complete insurance
against major costs, with the latter eliminating
the deductible and charging an up-front premium
for the actuarial value of this replacement.
The Guaranteed Benefit Plan is more
favorable than the Major Cost Protection Plan at
low pharmacy bills, but entails substantial risk
at high bills. These hypothetical alternatives
vary more from the standard plan than most
products currently being offered, but preferences
among them provides some indication of
preferences for features of actual plans.
Enrollee choice among the alternative plans
is not explained well by cost minimization; only
36.3 percent of enrollees choose the plan that
minimizes EPV of OPC. Further, consumers do
not seem to place much value on the insurance
component of the alternative plans-among enrollees,
the Guaranteed Benefit Plan that offers
relatively poor insurance against catastrophic
drug costs is the minimum cost alternative in
only 3.2 percent of cases, but is preferred by
27.1 percent, while the plans that offer almost
complete insurance are preferred by only 26
percent, even though they include the
minimum-cost alternative for 51.2 percent. We
conclude that consumers are likely to have


### ---Economics-2006-0-21.txt---
difficulty choosing among plans to fine-tune
their prescription drug coverage, and do not
seem to be informed about or attuned to the
insurance feature of Part D plans.

VII. Conclusions

We conclude from our survey that significant
fractions of the Medicare population, particularly
among those with low SES, bad health,
and low cognitive ability, are poorly informed
about the Part D prescription drug program, and
risk making poor plan choices. Most of the
Medicare population, 89.2 percent, intend to
enroll, although this drops to 80.4 percent among
the poorly informed. When one compares preferences
with alternatives that minimize the expected
present value of out-of-pocket costs, one
finds that 10 percent of the elderly intend to
delay enrollment even though it increases their
expected costs, and 19.4 percent intend to enroll
immediately even though it increases their expected
costs. Choice among plans is erratic, and
shows little attention to or concern about the
insurance features of Part D plans. Procrastination
is a predictable behavioral response to the
complexity and ambiguity surrounding Part D,
making it likely that many who intend to enroll
will miss the May 15, 2006, enrollment deadline.
Consequently, there is likely to be considerable
churning and grumbling in this market in
the future.

How could the Part D market be managed
to overcome consumers' lack of information,
behavioral aversion to market choices, and
procrastination when faced with ambiguous alternatives?
First, CMS should pursue an aggressive
marketing program to find the vulnerable
who are insufficiently informed to act in their
self-interest, sell the neglected and undervalued
benefits of the insurance that Part D offers, and
coax consumers into making sensible plan
choices. This could include giving insurers incentives
to scour for vulnerable seniors. Marketing
of Part D should benefit consumers as
long as it is not done deceptively. Policies that
have proven effective in encouraging early retirement
in downsizing firms may also work in
this market. The most effective is "default in"
rather than "default out"-all individuals are
assigned a plan unless they choose a plan them-
selves or explicitly opt out; see Choi et al.
(2003). This could be done by providing stepby-
step decision forms that require seniors to
choose a plan, opt out, or let Medicare or an
ombudsman make a choice for them; one suggestion
is that these be called Plan D-EZ to
match simplified IRS forms. Another marketing
method that works for retirement is the use of
windows with attractive incentives. This could
be adapted to encourage Part D enrollment by
combining stiff late enrollment penalties with a
program to convert nonenrollees, such as a series
of "last ever" penalty amnesty windows in
the future, particularly for the vulnerable. A
number of private plans are being offered with
quite low premiums and basic coverage, which
encourage enrollment of the healthy. If CMS
ensured that a basic plan, with zero premium, a
limited formulary, and copayments sufficient
for actuarial balance, was always a market option,
then all seniors should enroll in either the
basic or a more comprehensive plan, assuring
affordable medications and catastrophic coverage
for the entire Medicare population.
The new Medicare Part D prescription drug
insurance market illustrates that leaving a large
block of uninformed consumers to "sink or
swim," and relying on their self-interest to
achieve satisfactory outcomes, can be unrealistic.
To make the Part D market work, in the
sense that it provides choices that consumers
want, and achieves the efficiencies it seeks,
CMS will have to make a diligent effort to
manage the market, and to reach all consumers
and provide them with information and assistance
in making wise choices. What the Part D
market, and other market privatization initiatives,
need is a component of Thaler and Sunstein'
s (2003) libertarian paternalism, in which
understanding consumers' limitations, helping
consumers to help themselves, and convincing
them that the market will serve their interests
are intrinsic parts of mechanism design.
 ## Economics-2007-0


### ---Economics-2007-0-03.txt---
Macroeconomics changed between the early
1960s and the late 1970s. The macroeconomics
of the early 1960s was avowedly Keynesian.
This was manifested in the textbooks of the
time, which showed a remarkable unity from
the introductory through the graduate levels.'
John Maynard Keynes appeared, posthumously,
on the cover of Time.2 Even Milton Friedman
was famously-although perhaps misleadinglyquoted:
"We are all Keynesians now."3 A little
more than a decade later Robert Lucas and
Thomas Sargent (1979) had published "After
Keynesian Macroeconomics." The love-fest was
over.

The decline of the old-style Keynesian economics
was due in part to the simultaneous rise
in inflation and unemployment in the late 1960s
and early 1970s. That occurrence was impossible
to reconcile with the simple nonaccelerationist
Phillips curves of the time.

But Keynesian economics also declined because
of a change in economic methodology.
The Keynesians had emphasized the dependence
of consumption on disposable income
and, similarly, of investment on current profits
and current cash flow.4 They posited a
Phillips curve, where nominal-rather than
real-wage inflation depended upon the unemployment
rate, which was used as an indication
of the looseness of the labor market.
They based these functions on their own introspection
regarding how the various actors

in the economy would behave. They also
brought some discipline into their judgments
by estimating statistical relations.5
But a new school of thought, based on classical


### ---Economics-2007-0-04.txt---
economics, objected to the casual ways of
these folks. New Classical critics of Keynesian
economics insisted instead that these relations
be derived from fundamentals. They said that
macroeconomic relationships should be derived
from profit-maximizing by firms and from utilitymaximizing
by consumers with economic arguments
in their utility functions.

The new methodology had a profound effect
on macroeconomics. Five separate neutrality results
overturned aspects of macroeconomics
that Keynesians had previously considered incontestable.
These five neutralities are: the independence
of consumption and current income

(the life-cycle permanent income hypothesis);
the irrelevance of current profits to investment
spending (the Modigliani-Miller theorem); the
long-run independence of inflation and unemployment
(natural rate theory); the inability of
monetary policy to stabilize output (the rational
expectations hypothesis); and the irrelevance of
taxes and budget deficits to consumption (Ricardian
equivalence).6 These results fly in the face
of Keynesian economics. They undermine its
conclusions about the behavior of the economy
and the impact of stabilization policy.
The discovery of these five neutrality propositions
surprised macroeconomists. They had
not suspected that radically anti-Keynesian conclusions
were the logical outcome of such seemingly
innocuous maximizing assumptions.

I. Neutralities and Preferences

How did macroeconomists react to the discovery
of the five neutralities? On the one hand,
the New Classical economists viewed their neutrality
results as a telltale: that Keynesian economists
of the previous generation had been
thinking in the wrong way. In their view, scientific
reasoning was producing a new, leaner,
more precise economics.

On the other hand, Keynesian economists, for
the most part, reacted differently. In due course
they came to view the neutralities as logically
impeccable. These New Keynesians accepted
the methodological dictums of the New Classical
economics: that constrained maximization
of profit and utility functions is the appropriate
microfoundation for macroeconomics. They
also viewed the neutralities as having a certain
sort of generality. The neutralities do commonly
describe equilibria of competitive economies
with complete information, irrespective of people'
s preferences-as long as those preferences
correspond to economists' typical descriptions
of them. The Keynesians then resurrected somebut
not all-of the Keynesian conclusions by adding
a variety of frictions to the New Classical
model. Those frictions include credit constraints,
market imperfections, information failures, tax
distortions, staggered contracts, uncertainty,
menu costs, and bounded rationality. This formulation
preserves many (but not all) Keynesian
conclusions regarding cyclical fluctuations and
macroeconomic policy.

This lecture will suggest a new stance in
regard to each of the five neutralities. Like New
Classical and New Keynesian economics, it will
derive behavior from utility and profit maximization.
That captures the purposefulness of economic
decisions. But this lecture will also
question the generality of the preferences that
lead to the five neutralities. There is a sense in
which those preferences are very narrowly defined.
They have important missing motivation-
since they fail to incorporate the norms of
the decision makers. Those norms reflect how
the respective decision makers think they and
others should or should not behave, even in the
absence of frictions. Preferences reflecting such
norms yield a macroeconomics with important
remnants of the early Keynesian thinking. They
also yield a macroeconomics that, in important
details, cannot be obtained only with frictions.
We shall see that, with such preferences, even
in the absence of frictions, each of the five
neutralities will be systematically violated. Specifically:


* A realistic norm regarding consumption behavior
will make consumption directly dependent
on current income, in violation of the
neutrality of consumption given wealth;
* A realistic norm will make investment directly
dependent on cash flow, in violation of
Modigliani-Miller;

* A realistic norm will make wages and prices
dependent on nominal considerations and
thus violate natural rate theory;

6 Of course, it took some time for the implications of
these neutrality results to be fully appreciated. For example,
life-cycle consumption and Modigliani-Miller were initially
considered as nothing more than useful codicils to Keynesian
thinking.


### ---Economics-2007-0-05.txt---
* A realistic norm will make income and employment
dependent on systematic monetary

policy, and thus violate rational expectations
theory; and

* A realistic norm will make current consumption
dependent on the current generation's
social security receipts, in violation of Ricardian
equivalence.

Additionally, insofar as the behavior assumed
by the early Keynesians differed from the behavior
that produces the neutralities, there is
likely to be a bias in favor of the Keynesians.
The Keynesians based their models on their
observation of motivations, rather than on abstract
derivations. If there is a difference between
real behavior and behavior derived from
abstract preferences, New Classical economics
has no way to pick up those differences. In
contrast, models with norms based on observation
will systematically incorporate such behavior-
although, of course, as with any method,
there is the possibility for error.
Inclusion of the "missing motivations in macroeconomics"
then combines the observations

of the Keynesians with the intentionality of
economic decisions in New Classical economics.
Such a synthesis yields the best of the two
approaches.

Two Disclaimers.--Before beginning in earnest,
let me offer two brief disclaimers. First,
none of the behavior revealing of the norms that
are introduced in this lecture will be new. On
the contrary, I have purposefully chosen phenomena
that have been emphasized since The
General Theory by macroeconomists who have
followed Keynes in voicing their continuing
doubts about classical interpretations of macroeconomic
behavior.

Second, this lecture will discuss different
norms that respectively correspond to the five
neutralities. I shall assume that these norms are
exogenous. Such assumptions of exogeneity are
standard in economic analysis. In a given problem
in a given time frame, some terms are
assumed constant, while others are allowed to
vary. I ask you to withhold your doubts regarding
whether such exogeneity is a correct assumption
or not. The incorporation of such

endogeneity is the next step-not the first
step-in the study of the effect of norms on
macroeconomics, especially since such endoge-
neity may sometimes dampen, but will rarely
nullify, the conclusions of this lecture.
II. The Five Neutrality Results

For clarity, this section will now give an
overview of each of the five neutrality results.
A. Dependence of Consumption on Wealth,
Not Income

Standard theory tells us that, under only
somewhat special conditions, consumption depends
on wealth, which is the value of current
assets plus the discounted value of future earnings.
7 Thus there is no tendency for people to
make their expenditures conform to the pattern
of their income receipts (as long as their wealth
is given).

Changes in the pattern of current income that
leave overall wealth constant are neutral in their
effects on current consumption.

B. The Modigliani-Miller Theorem

One version of the Modigliani-Miller Theorem
says that a firm's investment strategy is
totally independent of its liquidity position.8
Thus, for example, a corporation with an unexpected
windfall will not spend any additional
investment dollars. Instead, it will pass the
windfall on to shareholders or seek other financial
investments, since it will make only those
investments whose risk-adjusted rate of return
exceeds the rate of return on capital.
Changes in the firm's finances will thus be
neutral in their effect on current investment.
C. Natural Rate Theory

According to Natural Rate Theory, there is
some single rate of unemployment that is the
only level that could be permanently maintained
without ever-increasing inflation or everincreasing
deflation.9 A fiscal/monetary policy
mix that sought to maintain employment that
was any higher would result in permanently
increasing inflation. A fiscal/monetary mix that


### ---Economics-2007-0-06.txt---
sought to maintain employment that was any
lower would result in permanently decreasing
inflation. Fiscal/monetary mixes that yield different
levels of long-term (steady) inflation will
thus be neutral in their effects on long-term
unemployment.

D. Rational Expectations

According to Rational Expectations Theory,
a systematic response of monetary policy to the
business cycle will have no effect on the stability
of the macroeconomy.10 Wage and price
setters will foresee the systematic component of
the money supply; they will raise or lower
prices and wages exactly proportionally, and
thereby neutralize its effect on demand.
The stability of the economy is thus neutral
with respect to the systematic reaction of monetary
policy to the business cycle.

E. Ricardian Equivalence

According to Ricardian Equivalence, under
somewhat special conditions, a representative
consumer who receives a lump-sum intergenerational
transfer (for example, in the form of a
social security payment) will not spend a single
dime extra." Instead, she will pass on the whole
extra income, dollar-for-dollar, to her heirs,
who will have to pay the higher tax bills necessary
to retire the increased debt incurred in
funding the transfer to the previous generation.
The transfer is neutral in its effect on current
consumption.

III. The Missing Motivation: Norms'2
Each of the neutralities is based on the assumption
that the respective decision makers
are utility maximizers. But in each case the
utility functions of the decision makers have
been very narrowly described. They depend
only on real outcomes. For example, in the
consumption-neutrality models, utility depends
on consumption and leisure; in ModiglianiMiller,
it depends only on the discounted real
return to shareholders.

But as early as the beginning of the twentieth
century, Vilfredo Pareto pointed out that such
characterizations of utility missed important aspects
of motivation.13 According to Pareto, people
typically have opinions as to how they
should, or how they should not, behave. They
also have views regarding how others should, or
should not, behave. Such views are called
norms, and they may be individual14 as well as
social. The role of norms can be easily represented
in people's preferences by modifying the
utility function to include losses in utility insofar
as they, or others, fail to live up to their
standards.

Sociology has a further concept that gives an
easy and natural way to add those norms to the
utility function. Sociologists say that people
have an ideal for how they should or should not
behave. Furthermore, that ideal is often conceptualized
in terms of the behavior of someone
they know, or some exemplar whom they do not
know. The standard utility function is then modified
by adding a loss in utility, dependent on
the distance of behavior from that ideal.
Religion and religious identity give us a good
example of such norms. Consider the Gospels.
They are the most sacred texts of Christianity.
What do they describe? The life of Christ. How
should a Christian behave? "His life and conversation
ought to be worthy of the Gospel of
Christ [emphasis added]."'" How is a good
Christian supposed to feel when she has not
lived up to her conception of that ideal?
Ashamed.16

10 See Lucas (1972), Thomas J. Sargent (1973), and
Lucas and Sargent (1979).

11 See Robert J. Barro (1974) for the modem reincarnation
of these ideas, first discovered by Ricardo.
12 This section, including much of its exact wording, has
been taken from a joint manuscript with Rachel Kranton
(Akerlof and Kranton 2006). I should emphasize that these
insights have been developed jointly. The initial instigation
of our project is wholly due to Kranton. It is impossible for
me to say which ideas or wordings are mine and which are
hers. 13 See Pareto (1920). George C. Homans and Charles P.
Curtis (1934) give an excellent summary of Pareto that is
fully consistent with the emphasis here. Jon Elster (1989)
also presents a similar conception of norms.
14 For example, the protagonist of the novel Rice Mother
(Rani Manicka 2002) did not believe she should wear red
with black.

15 See http://www.orthodoxytoday.org/articles/StBasilBehavior.
php.

16 Of course, there are many interpretations of the Gos-
pel, and some of them are even contradictory. But that does
not affect whether the person should be ashamed or not. She


### ---Economics-2007-0-07.txt---
A. Importance of Norms in Motivation:
Some Examples

But religion is only one of the many realms
where people have such an ideal. To appreciate
the ubiquity of norms in motivation, it is useful
to see some further examples. Those examples
will demonstrate that people tend to be happy
when they live up to how they think they should
be; and they are, correspondingly, unhappy
when they fail to live up to those norms.
For the audience for this lecture, most of
whom are professors, teaching provides an especially
familiar example. We have a view of
what it means to be a good teacher. On our
lucky days, when we live up to our standards
and our classes go well, we tend to be happy; on
our off days, when something goes awry in
class, we may even feel quite miserable.
Such motivation in the workplace is the rule,
rather than the exception. Most workers, like
teachers, care about the conduct of their jobs.
Randy Hodson (2001), who surveyed ethnographies
of the US workplace, found that most
employees care about their dignity at work.
They want to conceive of what they do as useful.
And they feel a lack of dignity if they are
thwarted, either by their own actions or by the
actions of others. Those who are unable to get
such satisfaction are likely to show their displeasure
by acting up in some way or other.

Studs Terkel's Working (1972) captures in a
single volume much of the ethnographic findings
summarized by Hodson. Terkel interviews
people from many different occupations about
their feelings about their jobs and concludes that
people "search for daily meaning as well as
daily bread" (1972, xi). Some of the interviewees
are successful in this search: like the stone
mason, who cruises his Indiana county and
basks in pride as he not infrequently passes his
past work. At the opposite extreme is an Illinois
steelworker, whose work denies him the dignity
he seeks. He takes out his frustration at work by
being disrespectful, and, after hours, by getting
into tavern brawls. Most workers are somewhere
between these extremes, but in all cases,
following Terkel, they have a feeling for how
they should behave at work. It is not just about
the money; it is also about living up to an ideal
about who they think they should be.
Such belief regarding how people should behave,
and their behavior in accordance with
such belief, goes beyond the workplace. It affects
disparate areas, from playing golf to life in
the family. Betty Friedan's Feminine Mystique
gives what may be as good a description of
norms and their impact on people's lives as can
be found anywhere-in this case regarding the
norms for middle-class women of the previous
generation. Here is a brief sample of her description:


"Millions of women lived their lives in
the image of those pretty pictures of the
American suburban housewife, kissing
their husbands goodbye in front of the
picture window, depositing their stationwagonsful
of children at school, and smiling

as they ran the new electric waxer

over the spotless kitchen floor .... Their
only dream was to be perfect wives and
mothers; their highest ambition was to
have five children and a beautiful house,
their only fight to get and keep their husbands
.... They gloried in their role as

women, and wrote proudly on the census
blank: "Occupation, housewife" (Friedan
1963, 18).

Most women lived up to these norms. Some of
these were dissenters, like Friedan herself, who
disagreed with them, but felt compelled, nevertheless,
to follow a norm with which they disagreed.
Friedan says they suffered from "the
problem without a name." In our terms, they were
losing utility because they were failing to live up
to what one part of them thought they should do.
We may appeal to religious texts, to work
ethnographies, and, like Friedan, to women's
magazines to see the role of norms. But is there
yet harder data, some form of natural experiment,
that indicates the importance of norms?
The sociologist Erving Goffman has found such
an example. He observed the behavior of children
of different ages when they were brought
to the local merry-go-round. Because appropriate
activity differs by age, the children should
have predictably different reactions. For the
toddlers, riding a wooden horse is an accomplishment.
They show their joy at fulfilling what
they should do with smiles and waves as they
pass by. In contrast, for older children, there is


### ---Economics-2007-0-08.txt---
a gap between their conception of how they
should behave and riding the merry-go-round.
However much they may enjoy it, they also feel
the need to distance themselves from an activity
that is so age inappropriate. They manifest this
distance by riding a frog, rather than a "serious"
animal like a horse; alternatively they show off
by standing up "dangerously" during the ride. In
some way or other they play the clown.
Behavior at the merry-go-round is, of course,
just the stuff of kids. But Goffman supplements
it with a totally serious example. In surgical
operations, because of their inexperience, medical
students are given tasks that are ridiculously
easy.17 They respond in the same way as the
older children at the merry-go-round: they also
act the clown.18

In economics, as elsewhere, $500 bills do not
just lie on the street. If living up to norms is
such an important motivation, it must show up
in many economic examples, even if it is not
identified in exactly our language. Gary S.
Becker's Economics of Discrimination (1957)
offers an example of now-standard economics
that can also be interpreted in terms of such
norms. Becker's theoretical innovation was to
modify plain-vanilla economic utility by the
introduction of a discrimination coefficient. He
defined that as the loss in utility incurred by
exchange with someone from a different racefor
example, the loss of a white from an exchange
with a black. The natural interpretation
is that the discrimination coefficient represents
the loss in utility for the white from physically
engaging in an exchange with a black. But this
representation of the utility function can also be
interpreted in terms of norms. There is a code as
to how blacks and whites should behave toward
each other. The white has a view that she should
not deal with a black. She loses utility equal to
the value of the discrimination coefficient-not
from the physical association-but ipso facto
from the violation of the code. There is reason
to believe that such norm-based interpretation
better reflects the nature of discrimination than
a physical exchange-based theory. In the preCivil
Rights period, when Becker was writing,
there can be no doubt that discrimination, and
the code that upheld it, was stronger in the
South than in the North. Yet exchanges between
blacks and whites were surely much more com-
mon in the South than in the North. At least one
statistic reflects such a difference: there were
significantly lower levels of residential segregation
by race in the South than in the North.
B. Summary

Our examples are illustrative of behavior that
is pervasive. Sociology is dense in examples of
people's views as to how they and others should
behave, their joy when they live up to those
standards, and their discomfort and reactions
when they fail to do so.

We now turn to examining the role of
norms in each of the five macroeconomic neutralities.
20 In each case we shall ask whether
17 Goffman (1961) observed the behavior of such students
in medical operations.

18 Another example, the Milgram experiment (Stanley
Milgram 1963, 1965) demonstrates the strength of such
motivation-by showing the lengths that people will take to
do what they think they should be doing. To see this
interpretation of this experiment, which is only one of many
ways of viewing it, it is useful to give a brief description. On
arrival, subjects were told that they were involved in a
learning experiment. They were put in the role of the
"teacher," who should administer shocks to a "learner"
whenever he gave a wrong answer. The subjects are led to
identify with their role as teacher in this experiment, and
feel that they should obey the experimenter. Rather than
being another subject, and, rather than being wired, as it
appeared, actually the learner was an unwired, trained con-
federate of the experimenter. Subjects were then instructed
to administer shocks of escalating voltage as the learner
made errors. A surprising fraction of subjects escalated their
shocks to the maximum 450 volts-even though such a
dosage in real life would have been lethal. There are many
different versions of the experiment, but the version where
the confederate grunts and moans at 75 volts, asks to be let
out of the experiment at 150 volts, and refuses to give any
more answers at 300 volts, is typical. Here more than 60
percent of subjects went all the way. Nor is such motivation
limited to the laboratory. The rampage of the Nazi Reserve
Police Battalion #101 in Poland during World War II
(Christopher R. Browning 1999) gives a real-world mirror
of the behavior Milgram obtained in the laboratory. Like
Milgram's subjects, the members of this unit, were just
Ordinary Men (Browning's title). They were recruited from
the most prosaic civilian occupations.
19 See Douglas S. Massey and Nancy A. Denton (1993,
table 3.1, 64).

20 Some years ago, at a conference in Spoleto, Italy,
Edmund Phelps gave a still-unpublished lecture wondering
why the economics of the twentieth century had failed to
discover what was central to most of the arts, which was the
role of subjectivity. This paper is about the direct relevance
of such subjectivity for macroeconomics. I have very much
benefitted from enjoyable conversations with Professor


### ---Economics-2007-0-09.txt---
people's views as to how they should behave
will enter their utility function. In each case, we
shall see that such views will nullify the respective
neutrality result. Indeed, we shall also see
that in each case there will be a natural norm
broadly consistent with Keynesians' views of
economic behavior.21

IV. Ricardian Equivalence

We shall begin our detailed discussion with
Ricardian equivalence. It was chronologically
the last of the neutralities to be appreciated by
modern economists. But it is also the simplest.
That makes it the best place to begin.22 If there
is missing motivation in the utility function, it
should be easiest to see here.

A very simple model demonstrates the essence
of Ricardian equivalence, as it was rediscovered
by Robert Barro after a lapse of almost
two centuries.23 In the model, there are just two
periods, periods 1 and 2. There are just two
people, a parent and her child. The utility of the
parent depends directly upon her own consumption,
in period 1; it also depends upon the utility
of her child. That utility depends upon his consumption,
in period 2.

The parent's utility function can be expressed
simply as U,(cl, U2(c2)), where c1 is the consumption
of the parent, C2 is the consumption of
the child, U, is the utility of the parent, and U2
is the utility of the child. The parent chooses her
consumption in period 1 to maximize her utility.
Whatever wealth remains, she bequeaths to her
child.

Ricardian equivalence takes the following form
in this model. Suppose that the government
gives a transfer, which we shall call a social
security payment, to the parent in period 1; but
then in period 2 it taxes the child to retire the
debt caused by this transfer.24 In this case, the
consumption of a parent who maximizes the
utility function U, and who leaves a bequest to
her child will be unaffected by her receipt of
social security.

The logic of this result is simple. With and
without social security the discounted value of
consumption of the parent and of the child is
constrained by the discounted value of the family'
s earnings (plus its initial wealth). Social
security leaves that constraint unchanged. If the
parent found (c,, c2) to be the optimal division
of consumption between herself and her child in
the absence of a social security payment, this
same division of consumption between herself
and her child will optimize her utility with a
social security payment.

A vast literature explains why such Ricardian
equivalence is unlikely to be empirically descriptive.
25 The long list of reasons includes (a)
infinite, rather than finite, horizons; (b) strategic
bequests to obtain the attention of one's heirs
while alive; (c) childless families; (d) uncertainty,
including bequests made because of uncertainty
about the age of death; (e) differential
borrowing rates between the government and
the public; (f) growth of the economy in excess
of the interest rate, allowing steady debt issuance;
(g) lack of foresight regarding the effect
of social security on future taxes; (h) foreign
ownership of debt; (i) tax distortions;26, 27, 28 (j)


### ---Economics-2007-0-10.txt---
constraints on the consumption of parents (so they
do not leave bequests); (k) myopia of the parents
regarding children's future tax payments.29
The preceding list gives empirical reasons for
failure of Ricardian equivalence; but, lengthy as
it is, it still ignores its theoretical challenge.
According to that challenge, under economists'
standard assumptions, with perfect certainty and
with perfect foresight, Ricardian equivalence
will occur. Such a result had previously been
unsuspected by economists.30

Two possible conclusions can be drawn from
this surprise. On the one hand, we might continue
to assume that classical assumptions describe
economic behavior. The five neutralities
that are the subject of this paper concern the
realignment to macroeconomics that occurred
as economists gained understanding of the consequences
of classical assumptions from the

mid-1950s to the mid-1970s.

Economists may have been correct in drawing
the conclusion that the early Keynesian economics
was too simplistic and naive. But they
could have drawn another conclusion from this
surprise. In this view, Ricardian equivalence is
a telltale: we do not believe, even in the presence
of perfect foresight and perfect certainty,
that the parent will make an equal and opposite
offset of her social security transfer in terms of
an increased bequest to her child. Something
must be missing from the motivation in Barro's
model; otherwise, it would not have given rise
to results that are so surprising.

B. Douglas Bernheim and Kyle Bagwell
(1988) give further evidence suggesting that
Ricardian equivalence is such a telltale. They
show how the same logic would apply to a
network of gift-givers. Remarkably, any
member of such a network will be indifferent
whether she receives an extra dollar or any
other participant in the network is the recipient.
Such conclusions, suspect as they are,
suggest a problem with the model beyond the
lack of realism involved in perfect foresight
and perfect certainty. They also suggest missing
motivation.

James Andreoni (1989) has put his finger on
what that missing motivation might be.31 A
bequest is a type of gift. The parent will receive
utility from giving such a gift. Ricardian equivalence
will fail if the parent has utility from
gift-giving. With a social security transfer, more
money is hers, and the same consumption allocation
to herself entails a greater gift to her
child. With declining marginal utility for bequestgiving,
she will then divide an increased social
security transfer between additional consumption
for herself and an additional bequest to her child.32
Andreoni thus describes the utility missing
from the standard utility function as that arising
from the "warm glow" from giving. Such a
characterization may be accurate. It also sounds
as if it is very close to classical assumptionsthat
there is nothing fundamentally different
about this additional motivation. But this segment
of the utility function is, in fact, very
different from economists' usual characterization
of motivation. We know that the "warm
glow" does not come from the utility the parent
This argument suggests that a "bequest" is not really what it
seems. This is an argument where the preferences of the
parent do play a role, but quite different from the type of
reason that I think would have surprised the Keynesians. I
want to show that parents who make bequests for the
conventional reasons, because they care about the welfare of
their children, will still routinely violate Ricardian equiva-
lence, even in the absence of most of the commonplace
frictions that almost surely invalidate exact Ricardian equivalence.
29 This was Ricardo's own reason for dismissal of the
argument. He said that the parent would alter her bequest
because she would not take into account the added tax
payments of the child (see Gerald P. O'Driscoll Jr. 1977).
Uncertainty regarding the size of the future tax payments is
different from such myopia, in which the payment is altogether
ignored. But, with quadratic utility and expected
utility maximization, uncertainty regarding the child's fu-
ture tax payments will have no effect on the size of the
parent's bequest.

30 For example, Feldstein (1974) and Feldstein and Pel-
lechio (1979) engage in no theoretical soul-searching regarding
the negative effects of social security on current
savings. There is a voluminous literature (see Roberto Ric-
ciuti 2003) examining the empirical validity of Ricardian
equivalence. Largely because of the problem of endogene-
ity, it is difficult to come to firm conclusions regarding its
empirical validity. There are studies with findings both for
and against such crowding out.

31 See also John Laitner (2002), Laitner and Henry Ohls-
son (2001), Alan S. Blinder (1975) and Michael D. Hurd
(1989), who have also modeled the bequest motive as coming
from the utility of the parent from giving the bequest.
32 Formally, she trades off the marginal utility of her
own consumption against the marginal utility from gift-
giving and the marginal utility she gets from her child's
consumption. In making this trade-off, she takes due ac-
count of the fact that one unit of consumption today is
traded off against (1 + r) units of consumption next period.


### ---Economics-2007-0-11.txt---
derives from her own consumption; nor, yet
more tellingly, does it derive from the utility of
her child (as the child's utility depends on its
own consumption). It enters the utility function
as a separate term.

What, then, could account for a "warm glow"?
Parent-to-child bequests are a form of gift. If there
is any type of economic transaction that is governed
by norms, it is the giving of gifts.33 Parentto-
child bequests also occur within families.
Therefore, they should also be affected by the
norms of family life. We have already seen one
example of such norms (Friedan's portrait of the
proper place of women in the early 1960s).
The norms of family life are not constant. They
vary by culture. They also change over time. As
the nature of the ideal family has shifted, so has
the ideal bequest. Actual bequests have changed in
tandem. For example, the ideal sixteenth century
Anglo-Saxon family was dynastic. The lineage
passed from father to oldest son.34 Fathers then
left the bulk of their estates to their oldest sons. In
the twenty-first century, in the ideal family, siblings
are equal. Most bequests are now evenly
divided between them.35

Summary.-Economic outcomes, such as the
consumption of the parent and the utility of the
child, are one determinant of bequests. But another
possible determinant is parents' views regarding
how they should behave toward their
children. Just as Friedan's suburban housewives
waxed their floors, because they thought that is
what housewives should do, parents who leave
bequests derive a warm glow from bequests
because that is what they think they should do
for their children. Ricardian equivalence then
illustrates how odd neutralities can occur in
models that fail to take such norms into account.
A comment by David Romer (2001, 539)
tells us where we should venture next. He has
remarked that "quantitatively important" violations
of Ricardian equivalence and of the permanent
income/life-cycle hypothesis occur for
the same reasons. Ricardian equivalence is not
important for us as an empirical aspect of macroeconomics.
There are so many reasons other

than the role of norms for its violation. But it
does give us an initial window on the type of
motivation missing in classical macroeconomics.
Inclusion of such motivation will give us a
new perspective on the consumption function. It
allows us to return to a view in which consumption
will depend on current income, just as its
inclusion makes it natural to believe that social
security transfers will affect savings and consumption,
even in a world without frictions.

V. Consumption and Current Income

This takes us to the second neutrality. According
to this result, other than its contribution
to a consumer's wealth, current income has no
independent effect on the consumption of a
utility-maximizing consumer.

Milton Friedman (1957) derived such
consumption-income neutrality in the twoperiod
model of Irving Fisher. In this model, the
consumer chooses her consumption between
two periods. She maximizes her intertemporal
utility function, given by the function U(cl, C2):
c, denotes her current consumption in the first
period; c2 denotes consumption in the second
period.36 If she maximizes U(c1, c2), a dollar of
income earned today will have the same effect
on her current consumption as a discounted
dollar earned in the next period. Thus, her consumption
will depend only on the discounted

value of her current and future income and the
rate of interest. This proposition is easy to
prove. It generalizes to many different commodities
and to many different time periods,


### ---Economics-2007-0-12.txt---
and, with quadratic utility, to uncertain incomes.
37 In standard terminology, the value of
her discounted income is called her wealth; the
amount of that wealth that can be spent without
its depletion is called permanent income.38 An
alternative expression of Friedman's hypothesis
is that consumption depends on permanent
rather than on current income.39

The permanent income hypothesis may be in
accordance with most standard economic models.
Nevertheless, it contradicted prior thinking
about the consumption function. Keynes, and
his followers, believed that current income
played an especially important role in the determination
of current consumption.

"The fundamental psychological law [emphasis
added], upon which we are entitled

to depend with great confidence both a
priori from our knowledge of human nature
and from the detailed facts of experience,
is that men are disposed, as a rule
and on the average, to increase their consumption
as income increases, but not by

as much as the increase in income" (Keynes,
The General Theory, 1936, 96).

It is true that The General Theory discussed a
long list of other factors that could affect consumption.
The list was sufficiently rich to include
not only current income, but also all the
other determinants of wealth, such as expected
future income and the rate of interest. But that
does not make Keynes's theory identical to
Friedman's. In the Keynesian theory, consumers
are more sensitive to current income than to
other changes in income that have similar effect
on the consumer's wealth.

A. Empirical Results and Their Explanation
A large number of tests have demonstrated the
excess sensitivity of consumption to current income,
in concert with the Keynesian consumption
function. For example, John Y. Campbell and
N. Gregory Mankiw (1989) nested both Friedman'
s view that consumption depends solely on
wealth and the simplified Keynesian view that
consumption depends solely on income. They
suppose that a fraction of consumers A are pure
Keynesians, while a fraction (1 - A) behave according
to the permanent income hypothesis; they
estimate A from the extent to which consumption
overreacts to changes in income that would be
predictable from past changes in income and consumption.
Usefully, then, A gives a natural measure
of the departure from the permanent income
hypothesis. The estimates of A are significant statistically
and also of significant magnitude economically:
between 40 and 50 percent (depending
upon whether three or five periods are used to
predict the change in current income).
Other studies corroborate such excess dependence
on current income: John Shea (1985), for
union members whose contracts specified their
37 The simple proof is that her utility-maximizing con-
sumption will depend upon the intercept and the slope of the
budget line. The budget line states that the present dis-
counted value of consumption is the present discounted
value of her future income, which is what Friedman calls
her wealth. The intercept of the budget line is her wealth.
That is how much she could consume today if she consumed
nothing tomorrow. And the slope of the budget line is
determined by the rate of interest r: on the budget line for
every unit of c, she gives up (1 + r) units of c2. Her
consumption will be on the highest attainable utility indif-
ference curve. That will be the indifference curve that is just
tangent to the budget line. As a result, we see that, given the
utility function, c1 will be a function of W and r. Note that
current income does not come into this expression.
38 Formally, permanent income is the product of the rate
of interest and wealth.

39 The permanent income hypothesis also generalizes to
currently popular models of present bias. In these models
consumers have present bias in the form of "hyperbolic dis-
counting," which means that they put extra weight in their
utility functions on their current consumption. In this case, the
typical consumer's plans will not be consistent, but they can be
analyzed as if she has multiple selves. Her self today decides
on how much to consume today and then passes on the re-
maining assets to her self tomorrow. There is an exact analogy
to the parent's maximization in Barro's model of bequests. In
that model, today's consumer passes on assets to her child in
the next generation; in consumer theory, today's consumer
passes on assets to her new self in the next period. Since the
standard model of intertemporal consumption and Barro's
model of consumption are exactly isomorphic, Ricardian
equivalence then tells us that current consumption-which is
the consumption of the initial self-depends only on the con-
sumer's wealth. David I. Laibson (1997) thus shows that
consumption with forward-looking consumers with hyperbolic
discounting will balance the marginal utility of present con-
sumption out of wealth against the marginal utility of future
consumption according to an Euler condition. Such a condition
is wealth-based. It is the generalization of the tangency of the
utility indifference curve to the budget line in the two-period
model of Irving Fisher. Both Friedman and Laibson obtain
consumption that is determined solely by current income if
there is a constraint on current borrowing, and consumers'
desires for current consumption exceed their current income.
There is nothing inherent in the preferences in either case that
causes current consumption to be based on current income.


### ---Economics-2007-0-13.txt---
future wages; David W. Wilcox (1989), for social
security recipients who had been earlier notified of
changes in cost-of-living adjustments; Jonathan
Parker (1999), for payers of social security taxes
with predictable inter-year changes; Nicholes S.
Souleles (1999), for changes in disposable income
net of tax refunds; and James Banks, Richard
Blundell, and Sarah Tanner (1998), and Bernheim,
Jonathan Skinner, and Steven Weinberg
(2001), for retirees.

Textbooks explain such excess sensitivity by
a variety of frictions, particularly borrowing
constraints. For example, Rudiger Dornbusch
and Stanley Fischer (1987) say: "Given that the
permanent income hypothesis is correct [sic],
there are two possible explanations."40 They are
liquidity constraints for consumers and myopia
in their projections of future income.
Thus, we see the realignment that occurred
because of the life-cycle permanent income hypothesis:
excess sensitivity may occur, but only
in the presence of credit constraints or myopia.
Such a view cannot have been adopted because
of its empirical support. Few studies have tested
this proposition, but those that do have rejected
it. For example, credit constraints cannot explain
the reduction in consumption of retirees.
And, neither myopia nor credit constraint can
explain the reduction in union members' consumption
at the time of wage declines scheduled
in their union contracts (Shea 1995, 996).
The adoption of the permanent income/
life-cycle hypothesis then must rest on theoretical,
not empirical, reasons. But the theory fails
to take into account norms regarding what people
think they should, or should not, consume.
Such a norm-based theory will nest Keynes's
psychological law. Consumption-income neutrality
will occur only in a singular special case.
B. Consumption and the Role of Norms41
Why should consumption be overly sensitive
to income? This section presents an argument in
three steps. First, sociology gives motivations
for consumption that are very different from the
reasons for it in the life-cycle model. A major
determinant of consumption is what people
think they should consume. Second, what people
think they should consume can often be
viewed either as entitlements or as obligations.
Finally, in turn, current income is one of the
major determinants of these entitlements, and
obligations.

Sociology of Consumption.-The motivation
emphasized by sociologists for consumption is
very different from that in the life-cycle model.
Sociologists describe consumption as largely
determined by the norms regarding what people
should consume. These norms, in turn, are dependent
upon the individual's situation and also
who she thinks she is.

Two examples illustrate such dependence on
norms. Following Pierre Bourdieu (1984), people'
s consumption of cultural goods-the literature
they read, the music they hear, and the art
they buy-reflects not just their individual
tastes. The upper class should not make lowerclass
choices. Correspondingly, the lower class
should avoid appearing above their station.42
The epithet "lace curtain Irish" illustrates. To
the users of this phrase, those lace curtains were
indicative of those violating their social place.
Weber's analysis of the relation between religion
and savings further reflects the role of
people's views regarding who they should be.
In The Protestant Ethic and the Spirit of Capitalism,
43 Weber describes Calvinists as aspiring
to be "worldly ascetics." He concludes that
"economic acquisition is no longer subordi-
nated to man as the means for satisfaction of his
material needs.""44 Here the purpose of saving is
to live up to an ideal. The Calvinists are thrifty
because they think they should not be consuming.
That turns the motivation of the life-cycle


### ---Economics-2007-0-14.txt---
model on its head. There people save only because
of their desire for consumption in retirement.
Luigi Guiso, Paola Sapienza, and Luigi Zingales
(2003, 2006) have statistically affirmed
Weber's hypothesis that religion is correlated
both with attitudes toward savings and with
actual savings. In addition, they have more generally
affirmed the quantitative significance of
culture for savings and consumption; in their
regressions, variables reflecting culture have as
much power as variables derived from the lifecycle
hypothesis in explaining cross-country
savings ratios.45

Consumption Entitlements and Obligations.-
While sociology is useful in giving us the general
insight that consumption depends on cultural
norms, we need to be more specific. What
is the nature of those norms? They can frequently
be described in two ways: as entitlements
and, also sometimes, as obligations to
spend. Again some examples will illustrate.
First, oddly, people have obligations to
spend. Social history is full of the obligation to
keep up appearances. Most Wall Street bankers,
for example, do not live like mothers on welfare.
They do not want to. But, even if they did,
it would occasion gossip. It is not what they
should do. History is replete with stories of the
debt of aristocrats struggling to maintain their
social obligations.46 As just one example, the
debts to British merchants by Southern planters,
who were keeping up with the Joneses of the
eighteenth century, are considered a significant
factor underlying the Southern support of the
American Revolution.47

In addition to obligations to spend, there are
also entitlements. The lost-ticket paradox of
Amos Tversky and Daniel Kahneman (1981,
457) gives an illustration. Eighty-eight percent
of respondents to a questionnaire said they would
buy a $10 theater ticket if they arrived at a theater
to see a play and found that they had lost a $10
bill. In contrast, only 46 percent said they would
buy a new $10 ticket in the same situation if
they had lost a previously purchased ticket.
Tversky and Kahneman explain this difference
by "mental accounts," but an explanation
in terms of entitlements is equally valid. Tversky
and Kahneman say that those who have lost
the $10 bill do not connect that loss to the play.
In their mental account, its cost is just $10. But
those who have lost the ticket see themselves as
paying for it twice. In their mental account, its
cost is $20. Those with the lost ticket then tend
to opt out, because they see $20 as too much to
pay to see the play. But the difference in behavior
for those who lost the ticket and those who
lost the $10 bill could also have been interpreted
in terms of entitlements. Most people want to
think of themselves as responsible human beings.
When they lose the ticket, they do not feel
entitled to just buy another one. That is not the
type of person they aspire to be.

We should also observe that it is not coincidental
that the lost ticket paradox could be explained
both by mental accounting and by

norms. Formally, any model of mental accounting
can be translated into a model of norms: just
replace the rules of mental accounting as the
norms that people think they should follow.48
But even though norms and mental accounting
may be equivalent, interpretations in terms of
norms are important for this lecture. Mental accounting
has the connotation, whether rightly or
wrongly, of being a heuristic for quick decisions.
Such a heuristic will, of course, sometimes result
in cognitive error. Whether rightly or wrongly,
most economists would dismiss cognitive error as
unimportant. Why? because in their view people
are smart about what they want, and their decisions
are also very purposeful. But norms cannot
be dismissed so easily. As I argued earlier, people
feel strongly about adherence to them. Their
45 Guiso, Sapienza, and Zingales (2006, 39) report re-
gressions of savings ratios on GDP growth, dependency
ratios, and responses to the question: "Do you consider it
especially important to encourage children to learn thrift
and savings?" A one-standard-deviation difference to GDP
growth and to attitude toward thrift both produce a 1.8-
percentage-point difference in the savings ratio. (A one-
standard-deviation difference in the dependency ratio,
which could be the result both of cultural differences and of
life-cycle considerations, produces a 3.2-percentage-point
difference.)

46 See, for example, David Cannadine (1977).
47 See Woody Holton (1999).

48 But it turns out that there is quite possibly a substan-
tive difference between the two interpretations. With the
mental accounting interpretation the losers of the ticket
could be induced to buy one, if only a wise friend would
make them aware of the logical problems of their reasoning.
In contrast with the norms interpretation the friend cannot
be so helpful. Buying a new ticket is a departure from the
person's norm, and she loses utility by it.


### ---Economics-2007-0-15.txt---
absence from utility constitutes the missing
motivation of macroeconomics.

The Link of Entitlements and Obligations to
Current Income.-It remains to relate current
spending to current income. Norms may be
complex. But a web of evidence still reveals a
strong association between current income and
entitlements and obligations to spend. Such a
link, in turn, produces the excess sensitivity of
consumption on current income in Keynes's
Psychological Law.

A few examples follow.

* It is common practice in the United States for
parents, even for rich ones with no budget
constraint, to expect their children to assume
financial independence after their graduation
from college. They are indicating their belief
in the norm that the child is entitled to spend
what she earns. (Most parents, of course, give
their children a helping hand as they seek
their independence. But that does not mean
that they do not also strongly believe that their
children should live on their earnings, since that
norm is only one of their motivations.)
* In a thought experiment, consider a woman
living on $50,000 a year who learns that
her uncle will die in one year leaving her
$2,000,000. Even if she has considerable savings
in the bank, it would be unseemly for her
to run down her savings in anticipation of the
bequest. She is not entitled to do so. She
should stick to spending from her current
income. This gives another example in which
norms regarding entitlements to spend are
related to current income, in violation of the
life-cycle hypothesis.

* People's expenditures are supposed to reflect
their stations in life, and those stations usually
reflect their earnings. Thus, for example,
college students with little earnings are supposed
to live that way-like college students.
Their current spending is supposed to reflect
their current earnings, not what they will be
earning in the future. (At the other extreme,
as an obligation, the college president is often
expected to live in the presidential mansion.)
* Preliminary results from an experiment by
John Morgan and myself illustrate another
relation between entitlement and earnings. In
this experiment, subjects were asked to donate
to a charity before and after completing
a task. Those who were asked for the donation
afterward were more likely to keep the
money than those who were asked beforehand.
Those who had completed the task felt
that they had earned the money and were thus
entitled to keep it for themselves.49
* The mental accounting model by Hersh M.
Shefrin and Richard H. Thaler (1988) is especially
useful in our quest for a Keynesian consumption
function. Norms take many forms, so
their formal model is not unique.50 But it does
illustrate a possible link between consumption
and current income. In this model, people have
three separate mental accounts: current income,
current assets, and future income combined
with pension wealth. As consumers exhaust one
of these accounts and begin to use the next one
for their current consumption, they incur a discontinuous
"penalty." Those penalties are psychological
in nature-this is a model of mental
accounting-and they take the form of a loss in
utility.51 Corresponding to Shefrin and Thaler's
assumptions regarding the nature of these costs,
as consumption rises, consumers will first finance
it wholly from current income; then,
from current assets; and, finally, from future
income and retirement wealth.

As we discussed earlier, it should be no surprise
that there is an exact translation of such a
model into one with norms regarding entitle-
ments to consume. The rules of mental accounting
become the norms regarding how money
should be spent. The basic norm is that consumption
should come from current income.


### ---Economics-2007-0-16.txt---
And the discontinuous penalties correspond to
the losses of utility due to respective deviations
from that norm. In particular, Shefrin and Thaler
assumed that there is no such cost at all if
consumption comes only from current income.
That means that current income can be considered
as consumers' entitlement to spend, since
any consumption that is less than current in-
come entails no deviation at all from the norm
regarding the account that should finance it.
* Shefrin and Thaler give an impressive array
of econometric facts in support of their
model. Insofar as these facts support their
mental accounting model, they also equally
well support its reinterpretation-with the
norm that current income is an entitlement to
spend. Those facts include: differential savings
out of windfall and current income;52 a
less than one-to-one displacement of discretionary
saving by employee pension contributions;
53 undersaving for retirement;54 and a
marginal propensity to consume out of fully
anticipated bonuses that is much greater than
the marginal propensity to consume out of
monthly income.55

* Retired people are commonly believed to
tailor their consumption to a concept of
income rather than to the value of their
assets. Shefrin and Statman (1984) have
viewed this as another form of mental accounting.
They also present considerable

evidence regarding such behavior.

C. Summary

Considerable evidence suggests that people's
views regarding what they are entitled to spend
play a major role in their consumption choices.
It also suggests strongly that current income
plays a special role in those entitlements. Shefrin
and Thaler have explained such patterns by
mental accounting. A reinterpretation of their
model shows that they also could have explained
this behavior in terms of norms. Once
again we see that the current versions of the
life-cycle hypothesis have left out missing motivation
that easily justifies the excess sensitivity
of consumption to income in Keynes's
psychological law.

VI. Investment and Cash Flow

The debate concerning investment has been
surprisingly close to the debate about consumption.
The early Keynesians emphasized two
variables as determinants of investment: current
cash flow (with profits as a major component),
and the firm's current holdings of liquid assets.
Each of these variables is a measure of funds
available to firms for investment without seeking
outside finance.56 In contrast, the later literature
denied any special role of liquidity in the
investment function.

The first such questioning came from
Modigliani and Miller, who assumed that managers
maximize shareholder value and that markets
are frictionless and competitive. In this
case, a firm's financial position plays no role in
the value of the firm. The argument for this
independence proceeds as follows. By construction,
Modigliani and Miller show how a competitive
equilibrium changes if a firm increases
its debt and buys back shares. In the new equilibrium,
investment will be unchanged, and

shareholders will offset the increase in the
firm's debt by a compensating increase in the
bonds in their respective private portfolios. The
reason the equilibrium changes in this way is
straightforward: if the markets for debt cleared
in the old equilibrium, they will again clear in
the new. If managers' choice of investment
maximized shareholder value in the old equilibrium,
the same choice of investment maximizes
it in the new. Investment is therefore independent
of the firm's current financial position,
including its current liquidity position and its
current cash flow.

The advent of q-theory similarly questioned a
special place for current variables, such as cash
flow and liquid asset holdings in the investment
decision. In the original version of the theory,
James Tobin (1969) suggested that a firm's optimal
investment strategy arbitrages between
52 Shefrin and Thaler (1988, 619-20).
53 Shefrin and Thaler (1988, 622-24).
54 Shefrin and Thaler (1988, 626-27). Especially, they
say that there would be vast undersaving in the absence of
social security and forced private pensions to prevent it.
There is some ambiguity regarding whether there is under-
saving in the presence of these institutions to counteract it.
55 Shefrin and Thaler (1988, 633).

56 See, especially, John R. Meyer and Edwin Kuh
(1957).


### ---Economics-2007-0-17.txt---
the value at which it can sell a unit of its capital
and its investment costs to produce a new unit
of capital. In this case, firms should invest up to
the point where the marginal cost of a new unit
of capital is the valuation of such a unit of
capital in the stock market. That valuation is the
market value of the firm's shares divided by its
capital stock, called the q-ratio. If markets are
efficient, q is also the expected discounted value
of current and expected future profits per unit of
capital.57 Since q-theory says that firms should
invest in capital up to the point where the cost of
an extra unit of capital stock is equal to the
present discounted value of the stream of earnings
from a unit of capital, again, as in ModiglianiMiller,
investment is independent of the firm's
finance decision.58

The empirical testing of q-theory also has a
striking parallel to the empirical testing of the
consumption function. Just as Campbell and
Mankiw showed that there was excess sensitivity
to current income in the consumption function,
Steven M. Fazzari, R. Glenn Hubbard, and
Bruce C. Petersen (1988) showed that investment
depends not just upon q, but also upon the
current cash flows. Furthermore, as in the standard
explanation of excess consumption sensitivity,
Fazzari, Hubbard, and Petersen similarly
suggest that credit constraints are responsible
for the dependence of investment on cash flow.
They continue with the Modigliani-Miller/
q-theory assumption that managers maximize
stockholder value. But they posit that the difference
in information between managers and
financiers results in a wedge between the cost of
internal and external financing. This is clearest
for firms that are credit constrained-so that
credit-constrained firms will be especially sensi-
tive to available liquidity.59 But, as with creditconstraint
explanations of consumption, empirical
evidence, such as there is, rejects this hypothesis.
Steven N. Kaplan and Zingales (1997) analyzed
the subsample of firms that Fazzari, Hubbard,
and Petersen had considered most likely to be
credit constrained. They find credit constraint to
be rare. Furthermore, they also found that those
firms with the least constraint had the greatest
sensitivity to cash flows.60

There is, thus, remarkable similarity between
the consumption function and the investment
function. In both cases, economic theory suggested
rejection of earlier views regarding the
role of current flow variables-current income
in the case of consumption, cash flow in the
case of investment. In both cases, empirical
investigation showed the existence of excess
sensitivity to the current flow variable. In both
cases, these rejections support the previous
Keynesian theory. In both cases, economists
have sought to explain the divergence between
practice and theory by the presence of credit
constraints. In both cases, the empirical evidence,
such as it is, does not support the case
that credit-constraint explanations explain the
theoretical anomaly.

A. Theory of Excess Sensitivity of Investment
to Cash Flow

Whatever the similarities, consumption and
investment differ in one major respect. In the
case of investment, economists are already
aware of a fundamental reason why investment
will depend on current cash flow. ModiglianiMiller
and q-theory both assume that managers


### ---Economics-2007-0-18.txt---
maximize shareholder value. In the now-standard
theory of the firm, the interests of the shareholders
and the interests of the managers are viewed as
different. The managers are only the agents of the
owners, and accordingly they maximize their own
interests instead. Such incentives are said to turn
the managers into "empire-builders,"61 who will
use the resources they control to increase their
own domains.

Empire-building can result from two types of
motivations. On the one hand, managers may
have only strict economic interests in mind:
they care only about their take-home pay, and
their effort on the job. Such managers, for example,
will be biased in favor of investments
whose operation or construction enhances their
firm-specific human capital, and thereby increases
their bargaining power.

On the other hand, empire-building may be
pursued as a goal of its own, for its own sake.
We saw earlier that most workers have views
regarding how they should or should not perform
their jobs. Accompanying such views,
most managers and workers will have the further
view that the firm should be investing in
those jobs. For this reason, the agents making
the investment decision are likely to engage in
empire-building. We can represent such motivation
by adding a term to the utility function of
the agent-decision maker. Her utility function
will not only depend on her own pecuniary
returns and her expenditure of effort. It will also
include an additional term reflective of her
norms. She will lose utility insofar as the firm's
investment fails to live up to her ideal of what
she thinks it should be. In this case, the typical
norm is that she thinks that the firm should
engage in investment that will enhance her job
performance.

Following the logic of Michael Jensen (1986,
1993), empire-building, accompanied by the
abdication of corporate oversight in favor of
management interests, explains a correlation between
investment and cash flow. Furthermore,
this correlation will occur regardless of the motivation
for the empire-building, whether for purely
economic reasons as in the principal-agent model,
or, instead, because of managers' norms for how
they think they should behave. Jensen has given
many instances of lax corporate oversight in favor
of management interests. For example, he has
cited the excess exploration and drilling operations
of oil companies when retained earnings
were high, from 1975 to 1981,62 and the maintenance
of low-return operations in many US industries,
as in the investments of General Motors
throughout the 1980s.63 In Jensen's views, shareholders
would have fared better if profits had been
returned to them, giving them the option of investing
at a higher rate of return, or perhaps if profits
had been used for takeovers outside the industry.
To cure what he calls the "failure of corporate
internal control," Jensen has also suggested that
firms should issue large amounts of debt, perhaps
even by going private. In that case, the added debt
obligations act as a brake on excess investment.
Regarding investment behavior, Jensen is then on
the same page as Keynesian economists such as
Klein and Goldberger. They refer to "the preference
of many businessmen for internal as opposed
to external financing" (1955, 12-13) and also consider
it the major reason for the dependence of
investment on cash flow.

B. Sociology of the Corporation

Once again, we have seen a neutrality result
that depends on the goals of the respective decision
makers. Accordingly, the norms of corporate
decision makers are central to the

sociology of the corporation. For example, Dirk
M. Zorn (2004) has examined how the locus of
control has changed in large US firms over the
past 40 years. He has shown how this control
has shifted away from those with a production
or a sales orientation to those with a financial
orientation.64 Empirically, this is seen in the rise
of the chief financial officer. Prior to the 1960s,
corporate finances were handled by corporate
treasurers, whose duties were mainly restricted
to keeping the accounts and producing the budgets.
Now, most large corporations have replaced
them by a CFO. With the change in title
has come a change in function. CFOs are typically
central to major decisions. Such a change
affects investment decisions. If they are committed
to their missions, managers with sales or
61 Empire-building is especially emphasized by Jeremy
C. Stein (2003), following Jensen (1986, 1993).
62 Jensen (1986, 327).

63 See Jensen (1993, 853).

64 That distinction was emphasized earlier, for example,
by Neil Fligstein (1990).


### ---Economics-2007-0-19.txt---
production orientations will be empire-builders.
In contrast, the role of the conscientious CFO is
to curb those enthusiasms. Fifty years have
elapsed since the publication of ModiglianiMiller.
According to Zorn, when it first appeared,
it did not describe the investment

decision of large corporations. Now, quite possibly,
changes in corporate decision-making
since that time make it more realistic.65
C. Summary

The investment decision demonstrates once
again that the respective neutrality result depends
on the objective function of the decision makers.
VII. Natural Rate Theory

We now turn to natural rate theory. Once
again, the debate concerns the behavior of economic
decision makers. The early Keynesians
viewed wage setters, and possibly also price
setters, as setting nominal wages and prices,
respectively, without taking full account of inflationary
expectations. In contrast, New Classical
revisionists have assumed that wage and
price setters care only about relative wages or
prices, and therefore wage and price setting will
fully incorporate inflationary expectations. Such
behavior yields a long-run neutrality result with
severe limits on the ability of monetary and
fiscal policy to affect unemployment and output.
When wage and price setters care only
about relative wages and relative prices, accelerating
inflation will occur if unemployment is
below a critical level, called the natural rate;
accelerating deflation will occur if unemployment
is above it.

As we shall see, such spirals occur because,
at high levels of demand, the representative firm
will wish to set the price of its product relative
to the price of other firms' products-which we
call its real price-in excess of unity. A standard
natural rate model illustrates why this occurs. That
model assumes that in each period the typical firm
sets a desired real price for the following period;
in each period it also makes a bargain with its
labor regarding next period's real wages. Next
period's nominal price and nominal wage are then
respectively set by adjusting this desired real price
and this bargained real wage according to inflationary
expectations. When demand is higher, the
desired real price of the representative firm is
higher for two reasons: on the demand side, because
the demand for its product is higher, and, on
the cost side, because the bargained real wage is
higher. That bargained real wage is higher both
because the typical employee's opportunity costs,
which take into account her chances of being
unemployed, are higher, and because the firm's
desire for her labor is higher. Since the firm's
owners, customers, and workers care only about
real prices or real wages, a given level of real
aggregate demand will be associated with a given
real wage bargain between the firm and its workers,
and a given desired real price for the firm's
product. If unemployment is sufficiently lowbelow
the natural rate-that desired real price will
be in excess of unity. If unemployment is above
the natural rate, it will be less than unity.
It is now easy to explain the inflationary and
deflationary spirals in natural rate theory. Consider
what happens when the representative firm
wishes to set its price above that of other firms.
In this case, actual inflation will exceed expected
inflation. With such a positive gap between
actual and expected inflation, inflationary
expectations will rise, as inflationary expectations
are adjusted upward to conform to reality.
But the firm's desired real price, and therefore
the difference between actual and expected inflation,
will be unchanged as long as unemployment
is constant. There will be no abatement in
the rise in expected inflation. Inflationary expectations
will be forever increasing, and inflation
will rise with it, as nominal prices and
wages adjust the real wage bargains and the
desired real prices for these increasing inflationary
expectations. By similar logic, if unemployment
is above the natural rate, there will be a
deflationary spiral. The natural rate is the only
sustainable level of unemployment without accelerating
or decelerating inflation. It corresponds
to the exact level of demand where firms
wish to set a real price of exactly one.


### ---Economics-2007-0-20.txt---
A. Acceptance of Natural Rate Theory
Most macroeconomists do not just view natural
rate theory as a useful null hypothesis.
They also see it as a description of reality. Such
a view is revealed in textbook presentations.
Economists accept natural theory for theoretical
and empirical reasons.

Theoretically, they view the assumptions of
natural rate theory as realistic. A standard criterion
for an economic model is that participants
in the economy care only about real outcomes.
That is the fundamental assumption of natural
rate theory. Also, unlike our other neutrality
results, natural rate theory is insensitive to deviations
due to "frictions," such as imperfect
information, taxes, myopia, or transaction costs.
As long as these "frictions" can be expressed
solely in real terms, the neutrality result of
natural rate theory will be robust.
Empirical considerations have also been influential
in economists' acceptance of natural
rate theory. The original Phillips curve showed
a close fit between the rate of change of nominal
wages and the inverse of the unemployment rate
for 97 years of British data, between 1861 and
1957. There was no inflation adjustment in this
equation. In the United States in the late 1960s
and early 1970s, however, such a simple inverse
relation between changes in nominal wages and
unemployment broke down, as both price and
wage inflation rose, along with the unemployment
rate. Natural rate theory offered an explanation
for this occurrence: it explained the rise
in inflation by the large oil supply shock and
also an increase in inflationary expectations,
both of which shifted the Phillips curve outward;
it explained the rise in unemployment by
a decline in demand.

Furthermore, new estimates of Phillips curves
seemed to show that the theory closely fit the data.
If inflationary expectations are formed as a simple
lag of past inflation, estimates of Phillips curves
should find that the coefficients on past inflation
sum to one. Many Phillips curve estimates fail to
reject that this sum is equal to one.66' 67 The stan-
dard errors of such estimates are quite large; thus,
they also fail to reject sums whose departure from
one is of sufficient size to result in departures of
economically significant magnitude from natural
rate theory. But the standard treatment of the Phillips
curve ignores this inconvenient fact.
The textbooks thus typically present natural
rate theory as a "just-so" story. It runs as follows.
The previous Keynesian economists had
posited a Phillips curve without a dependence
on inflationary expectations. Friedman (1968)
and Phelps (1968) perceived that such a theory
could not result from models where the participants
in the economy are concerned only with
real variables. They modified the relationship so
that wage and price equations would be affected
one for one by inflationary expectations. Such
judicious use of economic theory explained the
otherwise-mysterious finding of the simultaneous
increases in inflation and unemployment
of the late 1960s/early 1970s. The theory is also
consistent with most econometric estimates.
B. Nominal Considerations in Wage Behavior
We now turn to the same question regarding
wages that we asked concerning consumption
and investment. Is there "excess sensitivity"
relative to the respective neutrality? Natural
rate theory is based on the assumption that
wages and prices are set only with real considerations
in mind. "Excess sensitivity" here

66 See, for example, Robert J. Gordon (1977, table 3,
lines 6 and 7, 260).

67 Given the importance of such findings, it is remark-
able that their robustness to specifications of time period,
data, and exact specification of the Phillips curve has never
been subjected to tough tests-even though everything else
about the Phillips curve, including the natural rate of un-
employment itself, is considered to be estimated with great
imprecision. Akerlof, William T. Dickens, and George L.
Perry (2000) show a range of estimates for both wage and
price equations with many different specifications. These
estimates, particularly when made for periods of low infla-
tion, show considerable variation in the sum of the coefficients
on lagged inflation, dependent on the specification.
Another bit of evidence that suggests such estimates will be
sensitive to specification comes from the high standard
errors on the natural rate itself (Douglas Staiger, James H.
Stock, and Mark W. Watson 1997); it would be surprising
that the sum of lagged coefficients could be estimated
precisely if another component of the Phillips curve, the
natural rate, could be estimated only with very low preci-
sion. Gordon's own estimates show very different values for
this sum of coefficients. Of course, there is a theoretical
reason why estimates of such a sum should not be robust.
With rational expectations, rather than a simple mechanical
theory of formation of inflationary expectations, Sargent
(1971) shows that there is no theoretical reason that they
should sum to one.


### ---Economics-2007-0-21.txt---
takes the form that nominal considerations affect
real wage or price setting in some way or
other.

Evidence of one form of violation of the
assumptions of natural rate theory is especially
stark. That evidence concerns downward wage
rigidity. Such wage behavior can easily be perceived
statistically by examining distributions
of wage-changes. These distributions are characterized
by a bunching of wage changes at

exactly zero; there are some wage changes just
above zero in these distributions, but almost no
wage changes just below.68 Careful studies have
documented such wage stickiness in Australia,
Canada, Germany, Japan, Mexico, New Zealand,
Switzerland, the United States, and the
United Kingdom.69,70 There seems to be no way
to account for such nominal wage rigidity with
the basic assumptions underlying natural rate
theory: that participants in the economy care
only about real prices and real wages.
Wage stickiness also explains a macroeconomic
observation that is an anomaly for natural
rate theory. Unemployment was so massive in
the Great Depression that inflation should have
been below inflationary expectations throughout
this long period. With any natural-rate adaptiveexpectations
Phillips curve, such high unemployment
would have caused a deflationary spiral.
Data on inflation are available for 12 countries for
the Great Depression. Not a single one of them
shows such a spiral.71 For example, the United
States experienced rapid deflation from 1929 to
1933, but inflation systematically neither rose nor
fell for the next decade. The predictions of natural
rate theory are thus grossly violated. But sticky
wages offer a good explanation for such behavior.
For example, a dynamic simulation of the US
economy with money wage rigidity and with
Depression-level unemployment fits the data all
but exactly (Akerlof, Dickens, and Perry 1996).72
Nominal wage rigidity may not only be statistically
perceptible; it can also be macroeconomically
important, even outside of Great

Depressions. Nominal wage rigidity imparts a
long-run trade-off between unemployment and
long-run inflation. This trade-off is of sufficient
size that it should deter central banks from
targeting very low levels of inflation. For example,
simulations of the US economy (Akerlof,
Dickens, and Perry 1996) show that an increase
of the inflation target from 0 to 2 percent will
permanently reduce unemployment by 1.5 percentage
points.73

Norms as Explanation for Sticky Money
Wages.-It seems to be impossible, or all but
impossible, to explain the existence of sticky
money wages, without relaxation of the basic
assumption that the utility functions of employees
or of employers contain real arguments. A
simple and natural amendment to the standard
model explains such sticky money wages: that
employees have a norm for what wages should
be. According to that norm, they will lose utility
from a money wage decline. Sticky money
wages then result, as the bargains between employers
and employees reflect the presence of
this ideal in the utility function.
Indeed, the study by Bewley (1999) gives


### ---Economics-2007-0-22.txt---
direct evidence that such a norm exists and is
responsible for wage stickiness. His extensive
open-ended interviews sought to elicit why employers
failed to cut money wages in the Connecticut
recession of 1991-1992. Bewley

concludes that, even though substitute labor was
easily available, employers were reluctant to cut
wages because of the negative effects of such
cuts on morale. He says that managers were
afraid that cuts in money wages would cause
workers no longer to "identify" with their companies.
74 There might be no immediate consequences
during the recession. But employers
thought that such cuts would cause workers to
shirk after the recession had ended. They also
feared that their best workers would be more
likely to quit. These stories indicate that workers
are not thinking about their wages only in
real terms, relative to the price level or the
wages received by others. They also have a
special aversion to cuts in wages below their
current nominal levels.75

Norms about Wage Increases.-The motivation
underlying resistance to money wage cuts
is so obvious, and the facts are so unexceptionable,
that most macroeconomists accept the possibility
that money wages are sticky. Even so,
they rarely appreciate the broader implications
of such violation of the assumptions of natural
rate theory. Their adjusted model is that price
and wage decisions are made only with real con-
siderations in mind, but desired wage changes
will be truncated insofar as they entail money
wage decreases. To my mind, such a view entails
a theoretical error. As we have seen, the
existence of money wage rigidity occurs because
workers have a norm, which affects their
utility function, that their employers should not
make such cuts. The message of this finding is
that norms in the utility function yield at least
one clear violation of natural rate theory. That
suggests the further empirical possibility that
workers (and also employers and customers)
may also have other norms regarding what nominal
wages (and prices) should be. All such
violations are exceptions to natural rate theory,
and yield reasons for long-run trade-offs between
inflation and unemployment.

Money wage rigidity is then potentially only
the tip of an iceberg. If there is one way in
which nominal wages enter utility functions,
because of employees' norms regarding what
their employers should or should not do, there
could also be many other ways.

There is another natural way whereby such
norms could enter utility functions: employees
may not only have a norm that they should not
take wage cuts. They may also have norms
regarding the nominal rate of increase of their
wages or salaries. For example, employees may
believe that their employer should give them a
nominal raise.

There is little research on the existence of
such norms. The two questionnaire studies that
have investigated it obtain strong and mutually
reinforcing results. Eldar Shafir, Peter Diamond,
and Tversky (1997) asked respondents to
comment on a vignette about two young women
who take their first jobs with the same initial
income. Specifically they asked respondents
who will be better off: Barbara, who receives a
5-percent raise in the presence of 4-percent inflation;
or Ann, who receives a 2-percent raise
when inflation is zero; 79 percent of respondents
correctly said that Barbara would be
worse off than Ann economically. Nevertheless,
64 percent of respondents also said that Barbara
would be happier.76 Such responses are contrary
to the natural rate hypothesis that employees
only care about real returns. But an
easy explanation for this phenomenon occurs if
74 In more detail, Bewley (1999, 1-2) summarizes his
findings: "Other theories fail in part because they are based
on unrealistic psychological assumptions that people's abil-
ities do not depend on their state of mind and that they are
rational in the simplistic sense that they maximize a utility
that depends only on their consumption and working con-
ditions, not on the welfare of others. Wage rigidity is the
product of more complicated employee behavior, in the face
of which manager reluctance to cut pay is rational. Worker
behavior, however, is not always rational and completely
understandable. A model that captures the essence of wage
rigidity must take into account the capacity of employees to
identify with their firm and to internalize its objectives. This
internalization and workers' mood have a strong impact on
job performance and call for material, moral, and symbolic
reciprocation from company leadership."
75 Following the argument by Raj Chetty and Adam
Szeidl (2006), some employers may have been concerned
with the fact that their employees had fixed mortgages that
they would find difficult to pay with cuts in nominal wages.
This puts the violation of natural rate theory in another
place: why were these financial contracts in nominal rather
than in real terms?

76 Shafir, Diamond, and Tversky (1997, 351-52).


### ---Economics-2007-0-23.txt---
Barbara and Ann both think that their employer
should give them a nominal wage increase.
Another study, with a different form of questionnaire,
independently found a similar response.
Robert Shiller found that 49 percent of
a sample of the general public either fully or
weakly agreed with the following statement: "If
my pay went up, I would feel more satisfaction
in my job, more sense of fulfillment, even if
prices went up as much." An additional 11
percent of the general public were undecided,
while only 27 percent completely disagreed. As
in the case of Ann and Barbara, such opinions
are consistent with the view that workers think
their employers should give them a nominal
wage increase: they will be disappointed when
it does not occur. Shiller's finding may be similar
to the public's view of Ann and Barbara.
But, as he reports, it is also in stark disagreement
with the view of professional economists
that underlies natural rate theory. Ninety percent
of economists weakly or strongly disagreed
with the statement; 77 percent were in complete
disagreement.77

Such norms-regarding the wage or salary increase
that employees think they should receive--
can be economically consequential. They cause
the long-run inflation-unemployment trade-off
to be downward sloping. With such a norm, at
higher levels of inflation workers will not experience
disappointment from receiving lower
nominal wage increases than they think they
should receive; therefore, at higher inflation,
ceteris paribus, wage bargains will result in
lower real wages, which will reduce the relative
price that the firm wants to set, and therefore
raise the rate of sustainable employment. There
is a need for further research following Shafir,
Diamond, and Tversky and Shiller regarding
whether workers have norms regarding the nominal
wage increases they think they should receive.
High Inflation.-The opinions expressed regarding
Barbara and Ann, and also the opinions
of Shiller's respondents, suggest that the longrun
trade-off between inflation and employment
is upward sloping. These answers were elicited
in the United States and thus are reflective of
respondents' views in an environment where
inflation has been low. But if inflation is very
high and therefore also very salient, the answers
to such questionnaires could be very different.
And they could impart a very different shape to
the trade-off between macroeconomic demand
and steady-state inflation.78 In such cases, people
may gain satisfaction only from wage and
salary increases that exceed inflation. Such
norms regarding how employers should behave
will then necessitate higher real wages (to maintain
the same level of satisfaction) at higher
levels of inflation. The long-run inflationemployment
relation will then be downward

sloping. Such behavior gives a much stronger
rationale, even than current rational-expectations
credibility models (Barro and Gordon 1983; Kenneth
Rogoff 1987), why central banks should
maintain price stability. Failure to appreciate this
realistic possibility again may be another case in
which the absence of norms from utility functions
has unduly blinkered macroeconomic thinking.79


### ---Economics-2007-0-24.txt---
C. Prices

We have just seen that employees' norms
regarding nominal wages may affect bargained
real wages, and therefore cause trade-offs between
long-run inflation and long-run unemployment.
Similarly, customers' norms regarding price levels
and price changes may also cause long-run tradeoffs
between output and inflation.

Indeed, models by Katsuhito Iwai (1981),
Julio J. Rotemberg (1982), and Andrew S. Caplin
and John Leahy (1991) all have long-run
trade-offs between inflation and unemployment.
Each of these models assumes that there are real
costs to nominal price changes. If, instead, there
were real costs to real price changes, the assumptions
of natural rate theory would still be
satisfied, and no such trade-off would occur.
These models then pose the question why there
should be such real costs from nominal price
changes. Iwai, Rotemberg, and Caplin and
Leahy all respectively assume that there is a
"menu" cost in making these changes known.80
But the physical costs of making such changes,
as in the printing of new menus, are trivially
small. Norms regarding price changes, however,
give an alternative reason why these costs
might--indeed-be of sufficient size to induce
a significant long-run trade-off between inflation
and unemployment. Customers may think
that firms should not raise prices. In that case,
price increases (or increases of greater size) are
likely to induce angry customers to search for
alternative suppliers. At higher steady-state inflation,
firms will be changing their nominal
prices more, and therefore will face more elastic
demands for their product. Producers' natural
microeconomic response to this increased elasticity-
a lower price for their product-will
produce a macroeconomic trade-off between inflation
and aggregate demand.

Just as sticky money wages indicated that
employees have norms regarding wage change,
similarly, sticky prices indicate that customers
have norms regarding price change. Thus, the
extensive evidence on price stickiness reveals
violation of the assumptions of natural rate theory,
and also the existence of norms regarding
price change. Like wage changes, price changes
also agglomerate at zero. Dennis Carlton (1986)
has shown that prices are often sticky for significant
periods of time.81 Furthermore, prices
seem to be especially sticky in customer markets.
82 Alan Kackmeister (2002) has compared
price changes at the end of the nineteenth century
to such changes a bit more than a century
later. Price changes of specific goods at retail
stores were recorded from June 1889 to September
1891; Kackmeister revisited the same
commodities and their price change for a comparable
period, from June 1997 to September
1999. Price change in the late twentieth century
was five times more frequent than a century
earlier. Furthermore, in the nineteenth century,
the average spell of constant price for an individual
good was very long. It was approximately
80 months.83 Such constancy of prices
can easily be explained by customer norms regarding
price change. The customers have a

notion of the price that they ought to pay at
stores where they are continued and knowing
customers. Kackmeister suggests that the decline
in long-term customer relationships is one
factor responsible for greater frequency of price
change today.

Emi Nakamura and J6n Steinsson (2005)
give an economic reason why customers would
have such a norm that firms should not change
prices. They view consumer purchases as habitforming.
Thus, by buying a particular brand, or
patronizing a particular store, consumers are
putting themselves in a position where they can
be exploited. Their loyalty puts the firm in a
have notions regarding what nominal wage increases should
or should not be. This, of course, is just one of many
anomalies in the form of indexed contracts.
80oMarika Karanassou, Hector Sala, and Dennis J.
Snower (2003) find considerable long-run trade-off between
inflation and unemployment in a model with nominal price
staggering and money growth.

81 See also Blinder and Don Choi (1990) and Blinder et
al. (1998).

82 The meaning of customer markets was especially explored
by Arthur Okun (1978).

83 derive this result from Kackmeister's data in the
following way. He finds that in the nineteenth century, only
5 percent of items changed their prices per month. This
means that the average spell of constant prices would have
been 20 months (the inverse). But that is a biased statistic
for the average length of time between price changes for an
item on the shelf. The difference between the average spell
of employment or unemployment and the average spell
being experienced by an individual suggests a rule of thumb
ratio for four to one. Using this ratio as a rule of thumb
suggests that the spell between price changes averaged over
the individual items on the shelf would be 80 months.


### ---Economics-2007-0-25.txt---
position where it can take advantage of the
consumer by raising prices. Firms then make an
implicit contract with their customers: they will
not change their prices unjustifiably. Since such
an implicit contract is easier to make (and enforce)
regarding nominal prices than real prices,
the implicit guarantee is in nominal terms. Nakamura
and Steinsson have also discovered a
phenomenon that suggests strikingly that firms
do behave this way. Goods in store 126 (chosen
for its completeness of data) of Dominicks Finer
Foods chain frequently go on sale; when the
sale ends, their nominal price returns to the
exact same level. Such behavior is consistent
with the view that consumers think that prices
should not change (for whatever reason); and
that they are also likely to retaliate (change
brands) when prices do change.

I should also remark that in countries where
inflation is very high, customers will expect
price changes to occur frequently, and possibly
be of large magnitude. The inhibitions against
price changes when inflation is low are eroded
at high inflation. Thus, while norms concerning
prices give a negative long-run trade-off between
inflation and unemployment at low inflation,
at high inflation that trade-off could very
well be reversed.

D. Summary

To summarize, there is considerable evidence
of violation of the assumptions and predictions
of natural rate theory. Wages and prices are
nominally rigid; there were no deflationary spirals
in the Great Depression; and questionnaire
respondents act as if they have a positive like for
nominal wage increases.84 This evidence suggests
that wage earners and customers have
views on what wages and prices should be. The
reflection of such views in utility functions produces
trade-offs between inflation and unemployment.
Those trade-offs have significant

implications for economic policy. On the one
hand, central banks should avoid very low targets
for inflation. On the other hand, they should
avoid high inflation, where the trade-offs between
inflation and unemployment may be

reversed.

VIII. Rational Expectations Theory

Our discussion of rational expectations piggybacks
on our previous discussion of the nat-
ural rate.

According to rational expectations theory, insofar
as the central bank changes the money
supply systematically in response to employment
conditions, the public will foresee that
response and change prices and wages exactly
to compensate. The public's anticipation will
then exactly offset the response. Monetary policy
is neutral.85

There are two key assumptions underlying
this neutrality. The obvious one is rational expectations.
To some, rational expectations regarding
the effects of the money supply on

prices and wages would seem to be beyond the
sophistication of most wage and price takers,
and also of most wage and price setters.
Even in the case where all those involved in
buying and selling goods and labor services
have rational expectations, however, the neutrality
results of rational expectations theory
require also that nominal considerations do not
enter into the setting of either wages or prices.
The previous descriptions of the ways in which
nominal wages and prices enter into preference
functions, via employees' views of the wages
that ought to be received and consumers' views
of the prices that ought to be paid, give further
reason why the neutrality results of rational
expectations will be violated. If prices and
wages are affected by people's notions of what
their nominal values should be, monetary policy
can be effective in stabilizing output-and possibly
in raising its long-run level-even in the
presence of rational expectations.

IX. Economic Methodology

We have seen that the absence of norms plays
a key role in each of the five neutralities. Why
have economists made such systematic omissions?
The omission of norms from macroeconomics,
as well as from economics more


### ---Economics-2007-0-26.txt---
generally, can be explained by economists' adherence
to positive economics.86 Friedman's
(1953) essay on positive economics describes
the methodological implications of such belief.
In particular, he says that economic theorists
should strive for parsimonious modeling. According
to Friedman, they should even forsake
realistic assumptions in pursuit of such parsimony.
Maximization models with only objective
arguments of utility have been defined as
more parsimonious than models where people,
additionally, lose utility insofar as they, or others,
fail to live up to their standards. As a result,
whatever the empirical validity or relevance of
such norms, positive economics has a methodological
bias against their consideration. It privileges
models without norms.

The prescriptions of positive economics regarding
the conduct of empirical investigation
compound the bias against norms. Friedman
says that economists should not pay heed to the
stated intentions of decision makers, which
would include their norms as to how they and
others should behave. Instead, empirical work
should test only hypotheses that economists
consider to be based on parsimonious models.
If economic tests had great power, then it
would be easy, of course, to follow Friedman's
dictum of making more and more refined tests
of hypotheses with decreasing parsimony. If
norms really do affect behavior, this method
would reject models without norms and in due
course would arrive at models where people's
views regarding how they should behave affect
decision making. But economic tests lack
power. All economic models are very imprecise
in their specification of the independent variable,
the nature of the dependent variables, the
nature of leads and lags, and the nature of
residuals. Yet worse, most economic problems
involve simultaneity (as in supply and demand),
making establishment of causality difficult. In
almost any instance, such a large number of
models can be fitted statistically that it is extremely
hard-and perhaps impossible-to statistically
reject all the variants of models

without norms. As a result, the program of
positive economics-with its initial nulls of
models based only on utility with objective
variables verified only by statistical hypothesis
testing-has severe bias against explanations of
economic phenomena where norms play a role.
Summers (1986) illustrates the severity of
this bias. The conventional test of the efficient
markets hypothesis-that stock prices are the
expected value of future returns-looks for autocorrelations
of the excess returns on stocks

relative to bonds. Following Summers, it would
take approximately 5,000 years of data with
such a test to obtain as much as 50 percent
rejection of an alternative model where stock
prices are more than 30 percent away from their
fundamentals 35 percent of the time. With such
lack of power, nulls are important. When they
are not rejected, alternative theories, such as
those with norms, are not even considered. This
lecture has illustrated such reversion to normless
nulls. Consumption behavior, investment
behavior, and wage and price behavior-the three
most important components of most macro models-
all display excess sensitivity relative to respective
neutralities. All of these violations could
be easily explained by norms. Yet in each case
economists have sought to explain such violations
of classical theory by norm-less models.
In contrast to reliance on statistical testing,
disciplines other than economics typically put
much greater weight on a naturalistic approach.
This approach involves detailed case
studies. Such observation of the small often
has been the key to the understanding of the
large. To me, the most dramatic example of
such a relation between the small and the
large occurs in the structure of life itself.
Francis Crick and James D. Watson87 conjectured
correctly that if they could describe the
crystalline structure of a single DNA molecule,
they would have unlocked the secret of
life. The duality between the structure of the
DNA molecule and the way in which organisms
are generated and reproduced is one of
the most beautiful findings of human knowledge.
It indicates the sense in which Crick and
Watson were, indeed, profoundly correct.
What are the implications for social science?
Positive economics, with its emphasis on statistical
analysis of populations, would suggest that
86 Some of the thoughts and wording in this section have
been presented in Akerlof (2005).

87 As dramatically described by Watson (1969).


### ---Economics-2007-0-27.txt---
the intensive study of a single molecule would
be an all-but-worthless anecdote. In the case of
DNA, we know that the exact opposite is true:
because DNA is a template that determines all
of the cells of the organism, and also its reproduction,
one molecule may not tell all, but it
does tell a great deal. Form follows function.
Is there some reason to believe that economic
behavior and economic units are any different?
Economic decisions may not be as duplicable as
biological processes, but the basic reason why
science intensively studies the microscopic applies
to economics as well. The individual economic
unit, be it a firm, a consumer, or an
employee, behaves the way it doesfor a reason.
And if these actors behave as they do for a
reason, we can expect to find those reasons from
the structures that we see in close observation;
and because of those structures their behavior
will also tend to be duplicated. This duality
between duplicability and structure explains
why much of science concerns very close observation,
as it also explains why the study of
even a single part of a single DNA molecule
will be revealing.

Standard economic methodology says that
it is impossible to infer motivation of individual
actors from intensive case studies. Anthropologists
and sociologists listen carefully

to individuals in such studies. When people
follow the norms, they use them to explain
their actions; when, on the other hand, they
violate the norms, they become the subject of
local gossip. Those case studies are revealing
because-like a language, which dictates how
one should speak-the norms are common
knowledge. In this lecture, we have seen one
prominent example of the use of such knowledge:
Bewley's interviews uncovered the

common understanding of the norms regarding
wage cuts among Connecticut employers
in the early 1990s.

Summary. -Positive economics systematically
denies that norms can be understood from
intensive case study. Precedence given to models
without norms because they are by definition
more parsimonious and statistical tests of low
power then jointly create a firewall against consideration
that norms play a role in determining
behavior. For these reasons, current economic
methodology inherently has created a biased
economics. In contrast, a more naturalistic approach
would prescribe a different methodology.
In this case, economists would observe
decision makers as closely as possible, with the
express intent of characterizing their motivation,
and would use such characterization as the
basis for modeling of economic structure. Indeed,
sociological and anthropological ethnographers
do precisely that: they depict their
subjects' motivation from close observation.
X. Endogeneity of Norms

It is now time to discuss the endogeneity of
the norms. There is a special reason for its
consideration. Robert Lucas discovered that,
with endogenous rational expectations regarding
inflation, monetary policy that was
intended to stabilize the macroeconomy
would, instead, be exactly neutral. Similarly,
is it not possible that endogeneity of the
norms, like Lucas's endogeneity of inflationary
expectations, will cause the neutralities
again to hold? We shall discuss this question
regarding all five neutralities. For the most
part, we find that the type of government
interventions being considered are usually of
such frequency, or of such order of magnitude,
that they should provoke relatively little
change in the norms. Endogeneity of the
norms should have little effect, then, on our
previous conclusions.

A. Ricardian Equivalence

Let's begin by returning to Ricardian equivalence,
which is still the simplest case. We
found that if people have a norm regarding the
amount of their bequest, then lump-sum transfers
to an older generation will not be neutral.
There remains the possibility that the source of
the warm glow to the older generation is not the
total bequest, but instead the bequest to the
younger generation net of the transfer. In this
case, if the transfers change, then the norm
changes. Ricardian equivalence will again be
valid. While such changes in norms with the
size of transfers are a theoretical possibility,
they also seem highly unlikely. The size of the
transfers involved--especially for those rich
enough to make large nonaccidental bequestswould
seem to be too small to warrant such a


### ---Economics-2007-0-28.txt---
sophisticated calculation. Our earlier discussion
did discuss at least one change in the norms
regarding bequests, but that resulted from a very
large change in people's orientation. It resulted
from changes in their conception of the family--
of their own place within it and of the place of
their heirs. That also occurred over a very long
run-over the course of centuries.

B. Life-Cycle Hypothesis

Regarding the life-cycle hypothesis, we argued
that consumption depends upon current
income because norms regarding how much
people think they should spend are linked to
it. But such a norm Wyould be highly unlikely
to change as a result'of the use of fiscal and
monetary policy for stabilization. In the first
place, such stabilization will make the adherence
to the norm less costly, not more costly,
in purely economic terms. Furthermore, macroeconomic
sources are responsible for only a

small fraction of the variation in individual
incomes. As a result, there is further reason
why the role of current income in norms is
unlikely to change as a result of macroeconomic
stabilization.

C. Cash Flow and Investment

The rise of the CFO suggests that norms
regarding investment have changed in large US
firms. Quite possibly, this change occurred because
firms realized the need for financial controls
that compared the returns on inside and
outside options. Such an endogenous response
would make Modigliani-Miller correct. But,
following Zorn (2004), this change took 40
years. In the meantime, in the short run, following
our earlier logic, investments would have
depended on cash flow. And, of course, even in
the long run the CFO, who is only one voice
among many in corporate decisions, may not be
fully effective.

D. Natural Rate Hypothesis and the Role of
Rational Expectations

Regarding the natural rate hypothesis and
also the rational expectations hypothesis, we
saw that they will no longer hold if norms of
price and wage setting have nominal components.
Regarding prices and wages, the most
powerful evidence in favor of norms comes
from employees' resistance to money wage
cuts and customers' resistance to nominal
price increases. As long as inflation is low, it
is doubtful that small changes in inflation will
affect such norms. People seem to find it
easier to think in nominal, rather than in real,
terms. Indeed the facilitation of such thinking
is one of the benefits of money according to
the textbook mantra on its three uses: for
transactions, as a store of value, and as a unit
of account. Money is useful as a unit of
account especially if people think in nominal,
rather than in real, terms. As a result, as long
as inflation is low, people are unlikely to
forsake making calculations in nominal terms,
especially regarding the norms of what wages
or prices should be. Of course, if inflation
increases to high levels, the norms for wages
and prices and the method of calculating
those norms will change. Exactly how they
change-with the possibility that they underadjust
to increases in inflation when it is low
and overadjust when it is high-should be
empirically investigated.

E. Where Do the Norms Come From?

We do not know the general answer to the
question where norms come from. This lecture
has tried to make the case that norms, such as
they are, could potentially play an important
role in macroeconomics. Hopefully, then, it has
added to the motivation for research on their
microfoundations.88

XI. Conclusion

This lecture has shown that the early Keynesians
got a great deal of the working of the
economic system right, in ways that are denied
by the five neutralities. As quoted from Keynes
earlier, they based their models on "our knowledge
of human nature and from the detailed
facts of experience." They used their intuitions
regarding the norms of how consumers, investors,
and wage and price setters thought they
88 This lecture has been very much influenced by the
insights of the Ph.D. thesis of Robert Akerlof (2006) on
preferences for beliefs. His thinking on this subject has
influenced many of the sections of this paper, especially on
consumption and the endogeneity of norms.


### ---Economics-2007-0-29.txt---
should behave. There is systematic reason why
such knowledge and experience are likely to be
accurate: by their nature, norms are generated
and known by a whole community. They are
known to those who abide by them, and those
who observe them as well.

We have shown ways in which macroeconomic
variables will be affected by norms. The neutralities
say that consumption should have no special
dependence on current income; investment should
be independent of current cash flow; wages and
prices should not depend on nominal considerations.
The very construction of those neutralities
denies the possibility that peoples' decisions
might be influenced by their views regarding how
they, and how others, should behave. In practice,
however, the neutralities are systematically violated.
Insofar as economists have felt it necessary
to explain these violations, they have appealed to
a variety of different frictions, such as myopia and
credit constraint. In so doing, they have failed to
consider that those violations would occur even in
the absence of those frictions: they will occur
because of decision makers' norms.

The incorporation of norms based on careful
observation imparts an appropriate balance to
macroeconomics. The New Classical research
program was correct in viewing models of the
early Keynesians as too primitive. They had not
been sufficiently attentive to the role of human
intent in choices regarding consumption, investment,
wages, and prices. But that research program
itself has failed to appreciate the extent to
which the Keynesians' views of macroeconomics
were also reflective of reality, since they
were based on experience and observation.
A macroeconomics with norms in decision
makers' objective functions combines the best
features of the two approaches. It allows for
observations regarding how people think they
should behave. It also takes due account of the
purposefulness of human decisions.