{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import itertools as it\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_path</th>\n",
       "      <th>file_name</th>\n",
       "      <th>pdf_path</th>\n",
       "      <th>year</th>\n",
       "      <th>num</th>\n",
       "      <th>page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Economics-1942-0-00</td>\n",
       "      <td>Economics-1942-0</td>\n",
       "      <td>The Conditions of Expansion  \\nAuthor(s): Sumn...</td>\n",
       "      <td>data/groups/E5/texts/Economics-1942-0-00.txt</td>\n",
       "      <td>Economics-1942-0-00.txt</td>\n",
       "      <td>data/groups/E5/pdfs/Economics-1942-0-00.pdf</td>\n",
       "      <td>1942</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Economics-1942-0-01</td>\n",
       "      <td>Economics-1942-0</td>\n",
       "      <td>The American Economic Review \\n VOLUME XXXII ...</td>\n",
       "      <td>data/groups/E5/texts/Economics-1942-0-01.txt</td>\n",
       "      <td>Economics-1942-0-01.txt</td>\n",
       "      <td>data/groups/E5/pdfs/Economics-1942-0-01.pdf</td>\n",
       "      <td>1942</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Economics-1942-0-02</td>\n",
       "      <td>Economics-1942-0</td>\n",
       "      <td>2 THE AMERICAN ECONOMIC REVIEW [MARCH \\n II\\n...</td>\n",
       "      <td>data/groups/E5/texts/Economics-1942-0-02.txt</td>\n",
       "      <td>Economics-1942-0-02.txt</td>\n",
       "      <td>data/groups/E5/pdfs/Economics-1942-0-02.pdf</td>\n",
       "      <td>1942</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Economics-1942-0-03</td>\n",
       "      <td>Economics-1942-0</td>\n",
       "      <td>1942 ] SLICHTER: THE CONDITIONS OF EXPANSION ...</td>\n",
       "      <td>data/groups/E5/texts/Economics-1942-0-03.txt</td>\n",
       "      <td>Economics-1942-0-03.txt</td>\n",
       "      <td>data/groups/E5/pdfs/Economics-1942-0-03.pdf</td>\n",
       "      <td>1942</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Economics-1942-0-04</td>\n",
       "      <td>Economics-1942-0</td>\n",
       "      <td>4 THE AMERICAN ECONOMIC REVIEW [MARCH \\n of t...</td>\n",
       "      <td>data/groups/E5/texts/Economics-1942-0-04.txt</td>\n",
       "      <td>Economics-1942-0-04.txt</td>\n",
       "      <td>data/groups/E5/pdfs/Economics-1942-0-04.pdf</td>\n",
       "      <td>1942</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>Economics-1968-0-15</td>\n",
       "      <td>Economics-1968-0</td>\n",
       "      <td>FRIEDMAN: MONETARY POLICY 13 \\n self from bei...</td>\n",
       "      <td>data/groups/E5/texts/Economics-1968-0-15.txt</td>\n",
       "      <td>Economics-1968-0-15.txt</td>\n",
       "      <td>data/groups/E5/pdfs/Economics-1968-0-15.pdf</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>Economics-1968-0-16</td>\n",
       "      <td>Economics-1968-0</td>\n",
       "      <td>14 THE AMERICAN ECONOMIC REVIEW \\n Finally, m...</td>\n",
       "      <td>data/groups/E5/texts/Economics-1968-0-16.txt</td>\n",
       "      <td>Economics-1968-0-16.txt</td>\n",
       "      <td>data/groups/E5/pdfs/Economics-1968-0-16.pdf</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>Economics-1968-0-17</td>\n",
       "      <td>Economics-1968-0</td>\n",
       "      <td>FRIEDMAN: MONETARY POLICY 15 \\n it will be li...</td>\n",
       "      <td>data/groups/E5/texts/Economics-1968-0-17.txt</td>\n",
       "      <td>Economics-1968-0-17.txt</td>\n",
       "      <td>data/groups/E5/pdfs/Economics-1968-0-17.pdf</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>Economics-1968-0-18</td>\n",
       "      <td>Economics-1968-0</td>\n",
       "      <td>16 THE AMERICAN ECONOMIC REVIEW \\n moved in t...</td>\n",
       "      <td>data/groups/E5/texts/Economics-1968-0-18.txt</td>\n",
       "      <td>Economics-1968-0-18.txt</td>\n",
       "      <td>data/groups/E5/pdfs/Economics-1968-0-18.pdf</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>Economics-1968-0-19</td>\n",
       "      <td>Economics-1968-0</td>\n",
       "      <td>FRIEDMAN: MONETARY POLICY 17 \\n By setting it...</td>\n",
       "      <td>data/groups/E5/texts/Economics-1968-0-19.txt</td>\n",
       "      <td>Economics-1968-0-19.txt</td>\n",
       "      <td>data/groups/E5/pdfs/Economics-1968-0-19.pdf</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>653 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 page_id            doc_id  \\\n",
       "0    Economics-1942-0-00  Economics-1942-0   \n",
       "1    Economics-1942-0-01  Economics-1942-0   \n",
       "2    Economics-1942-0-02  Economics-1942-0   \n",
       "3    Economics-1942-0-03  Economics-1942-0   \n",
       "4    Economics-1942-0-04  Economics-1942-0   \n",
       "..                   ...               ...   \n",
       "648  Economics-1968-0-15  Economics-1968-0   \n",
       "649  Economics-1968-0-16  Economics-1968-0   \n",
       "650  Economics-1968-0-17  Economics-1968-0   \n",
       "651  Economics-1968-0-18  Economics-1968-0   \n",
       "652  Economics-1968-0-19  Economics-1968-0   \n",
       "\n",
       "                                                  text  \\\n",
       "0    The Conditions of Expansion  \\nAuthor(s): Sumn...   \n",
       "1     The American Economic Review \\n VOLUME XXXII ...   \n",
       "2     2 THE AMERICAN ECONOMIC REVIEW [MARCH \\n II\\n...   \n",
       "3     1942 ] SLICHTER: THE CONDITIONS OF EXPANSION ...   \n",
       "4     4 THE AMERICAN ECONOMIC REVIEW [MARCH \\n of t...   \n",
       "..                                                 ...   \n",
       "648   FRIEDMAN: MONETARY POLICY 13 \\n self from bei...   \n",
       "649   14 THE AMERICAN ECONOMIC REVIEW \\n Finally, m...   \n",
       "650   FRIEDMAN: MONETARY POLICY 15 \\n it will be li...   \n",
       "651   16 THE AMERICAN ECONOMIC REVIEW \\n moved in t...   \n",
       "652   FRIEDMAN: MONETARY POLICY 17 \\n By setting it...   \n",
       "\n",
       "                                        text_path                file_name  \\\n",
       "0    data/groups/E5/texts/Economics-1942-0-00.txt  Economics-1942-0-00.txt   \n",
       "1    data/groups/E5/texts/Economics-1942-0-01.txt  Economics-1942-0-01.txt   \n",
       "2    data/groups/E5/texts/Economics-1942-0-02.txt  Economics-1942-0-02.txt   \n",
       "3    data/groups/E5/texts/Economics-1942-0-03.txt  Economics-1942-0-03.txt   \n",
       "4    data/groups/E5/texts/Economics-1942-0-04.txt  Economics-1942-0-04.txt   \n",
       "..                                            ...                      ...   \n",
       "648  data/groups/E5/texts/Economics-1968-0-15.txt  Economics-1968-0-15.txt   \n",
       "649  data/groups/E5/texts/Economics-1968-0-16.txt  Economics-1968-0-16.txt   \n",
       "650  data/groups/E5/texts/Economics-1968-0-17.txt  Economics-1968-0-17.txt   \n",
       "651  data/groups/E5/texts/Economics-1968-0-18.txt  Economics-1968-0-18.txt   \n",
       "652  data/groups/E5/texts/Economics-1968-0-19.txt  Economics-1968-0-19.txt   \n",
       "\n",
       "                                        pdf_path  year  num  page  \n",
       "0    data/groups/E5/pdfs/Economics-1942-0-00.pdf  1942    0     0  \n",
       "1    data/groups/E5/pdfs/Economics-1942-0-01.pdf  1942    0     1  \n",
       "2    data/groups/E5/pdfs/Economics-1942-0-02.pdf  1942    0     2  \n",
       "3    data/groups/E5/pdfs/Economics-1942-0-03.pdf  1942    0     3  \n",
       "4    data/groups/E5/pdfs/Economics-1942-0-04.pdf  1942    0     4  \n",
       "..                                           ...   ...  ...   ...  \n",
       "648  data/groups/E5/pdfs/Economics-1968-0-15.pdf  1968    0    15  \n",
       "649  data/groups/E5/pdfs/Economics-1968-0-16.pdf  1968    0    16  \n",
       "650  data/groups/E5/pdfs/Economics-1968-0-17.pdf  1968    0    17  \n",
       "651  data/groups/E5/pdfs/Economics-1968-0-18.pdf  1968    0    18  \n",
       "652  data/groups/E5/pdfs/Economics-1968-0-19.pdf  1968    0    19  \n",
       "\n",
       "[653 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.chdir(\"/Users/BeckyMarcusMacbook/Thesis/EconTextCleaning/\")\n",
    "text_dir = \"data/groups/E5/texts\"\n",
    "dest_dir = \"data/groups/E5/temp/first_pages\"\n",
    "#dest_dir = \"data/groups_cleaned/E5\"\n",
    "def reset():\n",
    "    rm_ds(text_dir)\n",
    "    if os.path.exists(dest_dir):\n",
    "        shutil.rmtree(dest_dir)\n",
    "    shutil.copytree(text_dir,dest_dir)\n",
    "def rm_ds(dir):\n",
    "    for file in os.listdir(dir):\n",
    "        if file==\".DS_Store\":\n",
    "            os.remove(os.path.join(dir,file))\n",
    "## Read folder to df\n",
    "\n",
    "page_ids = []\n",
    "texts = []\n",
    "doc_ids = []\n",
    "for file in sorted(os.listdir(text_dir)):\n",
    "    if not \".txt\" in file:\n",
    "        continue\n",
    "    page_id = file.split(\".\")[0]\n",
    "    doc_id = page_id[:-3]\n",
    "    doc_ids.append(doc_id)\n",
    "    page_ids.append(page_id)\n",
    "    with open(os.path.join(text_dir,file),'r') as f:\n",
    "        text = f.read()\n",
    "    texts.append(text)\n",
    "\n",
    "\n",
    "df=pd.DataFrame({\"page_id\":page_ids,'doc_id':doc_ids,\"text\":texts})\n",
    "def get_info(name):\n",
    "    _,year,num,page = name.split(\"-\")\n",
    "    return int(year),int(num),int(page)\n",
    "df['text_path'] = df['page_id'].apply(lambda x: os.path.join(text_dir,x+\".txt\"))\n",
    "df['file_name'] = df['page_id']+\".txt\"\n",
    "df['pdf_path'] = df['text_path'].apply(lambda x: x.replace(\"texts\",\"pdfs\").replace(\".txt\",\".pdf\"))\n",
    "df[['year','num','page']] = df['page_id'].apply(get_info).to_list()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_preprocessing(text:str):\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines_stripped = [l.strip() for l in lines]\n",
    "    text = \"\\n\".join(lines_stripped)\n",
    "    \n",
    "    jstor_split = text.split(\"This content downloaded from\")\n",
    "    return jstor_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Economics-1942-0-02.txt likely a full page with length 2963\n",
      "Economics-1942-0-03.txt likely a full page with length 4365\n",
      "Economics-1943-0-03.txt likely a full page with length 2766\n",
      "Economics-1944-0-02.txt likely a full page with length 2820\n",
      "Economics-1944-0-03.txt likely a full page with length 2962\n",
      "Economics-1945-0-03.txt likely a full page with length 2738\n",
      "Economics-1947-0-02.txt likely a full page with length 3010\n",
      "Economics-1947-0-03.txt likely a full page with length 3013\n",
      "Economics-1953-0-02.txt likely a full page with length 2943\n",
      "Economics-1953-0-03.txt likely a full page with length 2883\n",
      "Economics-1965-0-02.txt likely a full page with length 2860\n",
      "Economics-1965-0-03.txt likely a full page with length 2986\n"
     ]
    }
   ],
   "source": [
    "temp_dir = \"data/groups/E5/temp/first_pages\"\n",
    "text_dir = \"data/groups/E5/texts\"\n",
    "if os.path.exists(temp_dir):\n",
    "    shutil.rmtree(temp_dir)\n",
    "os.makedirs(temp_dir)\n",
    "title_pages = []\n",
    "first_page_dir = \"data/groups/E5/temp/front_pages\"\n",
    "delete_page_dir = \"data/groups/E5/temp/pages_to_delete\"\n",
    "other_page_dir = \"data/groups/E5/temp/long_text_pages\"\n",
    "os.makedirs(other_page_dir,exist_ok=True)\n",
    "os.makedirs(first_page_dir,exist_ok=True)\n",
    "os.makedirs(delete_page_dir,exist_ok=True)\n",
    "for file in sorted(os.listdir(text_dir)):\n",
    "    if file[0]=='.' or file == 'Economics-1960-1-01.txt':\n",
    "        continue\n",
    "    disc,year,num,pagetxt = file.split(\"-\")\n",
    "    page=int(pagetxt[:-4])\n",
    "    if page<4:\n",
    "        old_path = os.path.join(text_dir,file)\n",
    "        new_path = os.path.join(temp_dir,file)\n",
    "        old_text = open(old_path,'r').read()\n",
    "        with open(new_path,'w') as f:\n",
    "            f.write(basic_preprocessing(old_text))\n",
    "for file in sorted(os.listdir(temp_dir)):\n",
    "    disc,year,num,pagetxt = file.split(\"-\")\n",
    "    page=int(pagetxt[:-4])\n",
    "    if page==0:\n",
    "        continue\n",
    "    path = os.path.join(temp_dir,file)\n",
    "    text = open(path,'r').read()\n",
    "    author_line =re.search(\"\\n(By .*\\n)\",text)\n",
    "    if author_line is not None or file =='Economics-1953-0-01.txt':\n",
    "        title_pages.append(file)\n",
    "        new_path=os.path.join(first_page_dir,file)\n",
    "    elif len(text)<500:\n",
    "        new_path = os.path.join(delete_page_dir,file)\n",
    "    else:\n",
    "        print(f\"{file} likely a full page with length {len(text)}\")\n",
    "        new_path = os.path.join(other_page_dir,file)\n",
    "    shutil.copyfile(path,new_path)\n",
    "\n",
    "\n",
    "    \n",
    "        # print(author_line.group(1).strip())\n",
    "        # print(text.split(author_line.group(1))[1][:100])\n",
    "        # print()\n",
    "#1953 first page not found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "built_different = {\n",
    "    'Economics-1966-0-03.txt':'-R. M. WEAVER',\n",
    "    'Economics-1953-0-01.txt':'1894-1952',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E5_long_edited.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"/Users/BeckyMarcusMacbook/Thesis/manual_work\")\n",
    "partially_clean = '/Users/BeckyMarcusMacbook/Thesis/EconTextCleaning/clean_texts/E5'\n",
    "partially_clean_dir = \"data/groups/E5/temp/partially_clean\"\n",
    "if os.path.exists(partially_clean_dir):\n",
    "    shutil.rmtree(partially_clean_dir)\n",
    "shutil.copytree(partially_clean,partially_clean_dir,dirs_exist_ok=True)\n",
    "\n",
    "dest_file = \"E5_long.txt\"\n",
    "dest_file_edit = \"E5_long_edited.txt\"\n",
    "if os.path.exists(dest_file):\n",
    "    os.remove(dest_file)\n",
    "for file in sorted(os.listdir(partially_clean_dir)):\n",
    "    path = os.path.join(partially_clean_dir,file)\n",
    "    text = open(path,'r').read()\n",
    "    new_text=f\"\\n\\n\\n---{file}---\\n{text.strip()}\"\n",
    "    with open(dest_file,'a') as f:\n",
    "        f.write(new_text)\n",
    "shutil.copyfile(dest_file,dest_file_edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on _CallableType in module typing:\n",
      "\n",
      "Callable = typing.Callable\n",
      "    Deprecated alias to collections.abc.Callable.\n",
      "\n",
      "    Callable[[int], str] signifies a function that takes a single\n",
      "    parameter of type int and returns a str.\n",
      "\n",
      "    The subscription syntax must always be used with exactly two\n",
      "    values: the argument list and the return type.\n",
      "    The argument list must be a list of types, a ParamSpec,\n",
      "    Concatenate or ellipsis. The return type must be a single type.\n",
      "\n",
      "    There is no syntax to indicate optional or keyword arguments;\n",
      "    such function types are rarely used as callback types.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "help(Callable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "[(22, 33)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_kmp_table(pattern):\n",
    "    table = [0] * len(pattern)\n",
    "    j = 0\n",
    "    for i in range(1, len(pattern)):\n",
    "        while j > 0 and pattern[i] != pattern[j]:\n",
    "            j = table[j - 1]\n",
    "        if pattern[i] == pattern[j]:\n",
    "            j += 1\n",
    "        table[i] = j\n",
    "    return table\n",
    "\n",
    "def kmp_search(text, pattern):\n",
    "    table = build_kmp_table(pattern)\n",
    "    j = 0\n",
    "    for i in range(len(text)):\n",
    "        while j > 0 and text[i] != pattern[j]:\n",
    "            j = table[j - 1]\n",
    "        if text[i] == pattern[j]:\n",
    "            j += 1\n",
    "        if j == len(pattern):\n",
    "            return i - len(pattern) + 1\n",
    "    return -1\n",
    "\n",
    "def find_substrings_indices(modified_string, mainstring):\n",
    "    indices = []\n",
    "    start_index = 0  # Start from the beginning of the mainstring\n",
    "    start_len = len(modified_string)\n",
    "    while modified_string:\n",
    "        for i in range(len(modified_string), 0, -1):\n",
    "            substring = modified_string[:i]\n",
    "            match_index = kmp_search(mainstring[start_index:], substring)\n",
    "            \n",
    "            if match_index != -1:\n",
    "                actual_start_index = start_index + match_index\n",
    "                actual_end_index = actual_start_index + len(substring)\n",
    "                indices.append((actual_start_index, actual_end_index))\n",
    "                start_index = actual_end_index + 1\n",
    "                modified_string = modified_string[i:]\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Validation check: Ensure that the total length of matched substrings covers the entire modified_string\n",
    "    total_matched_length = sum(end - start for start, end in indices)\n",
    "    if total_matched_length != start_len:\n",
    "        raise ValueError(\"The matched substrings do not cover the entire modified string.\")\n",
    "    return indices\n",
    "\n",
    "# Example usage:\n",
    "mainstring = \"hello fehell into the hello worlds\"\n",
    "modified_string = \"hello world\"\n",
    "def slice_text(ranges:list[tuple[int,int]],text)->str:\n",
    "    new_text= \"\"\n",
    "    for a,b in ranges:\n",
    "        new_text+=text[a:b]+\"\\n\"\n",
    "    return new_text\n",
    "print(len(mainstring))\n",
    "indices = find_substrings_indices(modified_string, mainstring)\n",
    "print(indices)\n",
    "slice_text(indices,mainstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "partially_clean_dir = \"data/groups/E5/temp/partially_clean\"\n",
    "edited =\"E5_long_edited_FINISHED.txt\"\n",
    "\n",
    "look_over_file = \"E5_check_over.txt\"\n",
    "if os.path.exists(look_over_file):\n",
    "    os.remove(look_over_file)\n",
    "\n",
    "edited_text = open(edited).read().strip()\n",
    "split = edited_text.split('---')\n",
    "names = split[1::2]\n",
    "texts = split[2::2]\n",
    "ms:dict[str,list[tuple[int,int]]]  = {}\n",
    "problems = []\n",
    "import difflib\n",
    "\n",
    "def find_indices(old_text:str, edited_text:str):\n",
    "    # Find the longest matching subsequence\n",
    "    start_index = old_text.find(edited_text)\n",
    "    stop_index = start_index + len(edited_text)\n",
    "    return [(start_index, stop_index)] if start_index!=-1 else find_substrings_indices(old_text,edited_text)\n",
    "\n",
    "def slice_text(ranges:list[tuple[int,int]],text)->str:\n",
    "    new_text= \"\"\n",
    "    for a,b in ranges:\n",
    "        new_text+=text[a:b]+\"\\n\"\n",
    "    return new_text\n",
    "for name, edited_text in zip(names,texts):\n",
    "    og_path = os.path.join(partially_clean_dir,name)\n",
    "    try:\n",
    "        old_text = open(og_path,'r').read()\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    indices = find_substrings_indices(edited_text.strip(),old_text.strip())\n",
    "    ms[name] =  indices if len(indices)>0 else [(0,0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1798, 2837), (0, 1792)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [(0, 1792), (1798, 2837)]\n",
    "sorted(l,reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d={1,2}\n",
    "type(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('delete', 0, 1420, 0, 0)]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SequenceMatcher in module difflib:\n",
      "\n",
      "class SequenceMatcher(builtins.object)\n",
      " |  SequenceMatcher(isjunk=None, a='', b='', autojunk=True)\n",
      " |\n",
      " |  SequenceMatcher is a flexible class for comparing pairs of sequences of\n",
      " |  any type, so long as the sequence elements are hashable.  The basic\n",
      " |  algorithm predates, and is a little fancier than, an algorithm\n",
      " |  published in the late 1980's by Ratcliff and Obershelp under the\n",
      " |  hyperbolic name \"gestalt pattern matching\".  The basic idea is to find\n",
      " |  the longest contiguous matching subsequence that contains no \"junk\"\n",
      " |  elements (R-O doesn't address junk).  The same idea is then applied\n",
      " |  recursively to the pieces of the sequences to the left and to the right\n",
      " |  of the matching subsequence.  This does not yield minimal edit\n",
      " |  sequences, but does tend to yield matches that \"look right\" to people.\n",
      " |\n",
      " |  SequenceMatcher tries to compute a \"human-friendly diff\" between two\n",
      " |  sequences.  Unlike e.g. UNIX(tm) diff, the fundamental notion is the\n",
      " |  longest *contiguous* & junk-free matching subsequence.  That's what\n",
      " |  catches peoples' eyes.  The Windows(tm) windiff has another interesting\n",
      " |  notion, pairing up elements that appear uniquely in each sequence.\n",
      " |  That, and the method here, appear to yield more intuitive difference\n",
      " |  reports than does diff.  This method appears to be the least vulnerable\n",
      " |  to syncing up on blocks of \"junk lines\", though (like blank lines in\n",
      " |  ordinary text files, or maybe \"<P>\" lines in HTML files).  That may be\n",
      " |  because this is the only method of the 3 that has a *concept* of\n",
      " |  \"junk\" <wink>.\n",
      " |\n",
      " |  Example, comparing two strings, and considering blanks to be \"junk\":\n",
      " |\n",
      " |  >>> s = SequenceMatcher(lambda x: x == \" \",\n",
      " |  ...                     \"private Thread currentThread;\",\n",
      " |  ...                     \"private volatile Thread currentThread;\")\n",
      " |  >>>\n",
      " |\n",
      " |  .ratio() returns a float in [0, 1], measuring the \"similarity\" of the\n",
      " |  sequences.  As a rule of thumb, a .ratio() value over 0.6 means the\n",
      " |  sequences are close matches:\n",
      " |\n",
      " |  >>> print(round(s.ratio(), 3))\n",
      " |  0.866\n",
      " |  >>>\n",
      " |\n",
      " |  If you're only interested in where the sequences match,\n",
      " |  .get_matching_blocks() is handy:\n",
      " |\n",
      " |  >>> for block in s.get_matching_blocks():\n",
      " |  ...     print(\"a[%d] and b[%d] match for %d elements\" % block)\n",
      " |  a[0] and b[0] match for 8 elements\n",
      " |  a[8] and b[17] match for 21 elements\n",
      " |  a[29] and b[38] match for 0 elements\n",
      " |\n",
      " |  Note that the last tuple returned by .get_matching_blocks() is always a\n",
      " |  dummy, (len(a), len(b), 0), and this is the only case in which the last\n",
      " |  tuple element (number of elements matched) is 0.\n",
      " |\n",
      " |  If you want to know how to change the first sequence into the second,\n",
      " |  use .get_opcodes():\n",
      " |\n",
      " |  >>> for opcode in s.get_opcodes():\n",
      " |  ...     print(\"%6s a[%d:%d] b[%d:%d]\" % opcode)\n",
      " |   equal a[0:8] b[0:8]\n",
      " |  insert a[8:8] b[8:17]\n",
      " |   equal a[8:29] b[17:38]\n",
      " |\n",
      " |  See the Differ class for a fancy human-friendly file differencer, which\n",
      " |  uses SequenceMatcher both to compare sequences of lines, and to compare\n",
      " |  sequences of characters within similar (near-matching) lines.\n",
      " |\n",
      " |  See also function get_close_matches() in this module, which shows how\n",
      " |  simple code building on SequenceMatcher can be used to do useful work.\n",
      " |\n",
      " |  Timing:  Basic R-O is cubic time worst case and quadratic time expected\n",
      " |  case.  SequenceMatcher is quadratic time for the worst case and has\n",
      " |  expected-case behavior dependent in a complicated way on how many\n",
      " |  elements the sequences have in common; best case time is linear.\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, isjunk=None, a='', b='', autojunk=True)\n",
      " |      Construct a SequenceMatcher.\n",
      " |\n",
      " |      Optional arg isjunk is None (the default), or a one-argument\n",
      " |      function that takes a sequence element and returns true iff the\n",
      " |      element is junk.  None is equivalent to passing \"lambda x: 0\", i.e.\n",
      " |      no elements are considered to be junk.  For example, pass\n",
      " |          lambda x: x in \" \\t\"\n",
      " |      if you're comparing lines as sequences of characters, and don't\n",
      " |      want to synch up on blanks or hard tabs.\n",
      " |\n",
      " |      Optional arg a is the first of two sequences to be compared.  By\n",
      " |      default, an empty string.  The elements of a must be hashable.  See\n",
      " |      also .set_seqs() and .set_seq1().\n",
      " |\n",
      " |      Optional arg b is the second of two sequences to be compared.  By\n",
      " |      default, an empty string.  The elements of b must be hashable. See\n",
      " |      also .set_seqs() and .set_seq2().\n",
      " |\n",
      " |      Optional arg autojunk should be set to False to disable the\n",
      " |      \"automatic junk heuristic\" that treats popular elements as junk\n",
      " |      (see module documentation for more information).\n",
      " |\n",
      " |  find_longest_match(self, alo=0, ahi=None, blo=0, bhi=None)\n",
      " |      Find longest matching block in a[alo:ahi] and b[blo:bhi].\n",
      " |\n",
      " |      By default it will find the longest match in the entirety of a and b.\n",
      " |\n",
      " |      If isjunk is not defined:\n",
      " |\n",
      " |      Return (i,j,k) such that a[i:i+k] is equal to b[j:j+k], where\n",
      " |          alo <= i <= i+k <= ahi\n",
      " |          blo <= j <= j+k <= bhi\n",
      " |      and for all (i',j',k') meeting those conditions,\n",
      " |          k >= k'\n",
      " |          i <= i'\n",
      " |          and if i == i', j <= j'\n",
      " |\n",
      " |      In other words, of all maximal matching blocks, return one that\n",
      " |      starts earliest in a, and of all those maximal matching blocks that\n",
      " |      start earliest in a, return the one that starts earliest in b.\n",
      " |\n",
      " |      >>> s = SequenceMatcher(None, \" abcd\", \"abcd abcd\")\n",
      " |      >>> s.find_longest_match(0, 5, 0, 9)\n",
      " |      Match(a=0, b=4, size=5)\n",
      " |\n",
      " |      If isjunk is defined, first the longest matching block is\n",
      " |      determined as above, but with the additional restriction that no\n",
      " |      junk element appears in the block.  Then that block is extended as\n",
      " |      far as possible by matching (only) junk elements on both sides.  So\n",
      " |      the resulting block never matches on junk except as identical junk\n",
      " |      happens to be adjacent to an \"interesting\" match.\n",
      " |\n",
      " |      Here's the same example as before, but considering blanks to be\n",
      " |      junk.  That prevents \" abcd\" from matching the \" abcd\" at the tail\n",
      " |      end of the second sequence directly.  Instead only the \"abcd\" can\n",
      " |      match, and matches the leftmost \"abcd\" in the second sequence:\n",
      " |\n",
      " |      >>> s = SequenceMatcher(lambda x: x==\" \", \" abcd\", \"abcd abcd\")\n",
      " |      >>> s.find_longest_match(0, 5, 0, 9)\n",
      " |      Match(a=1, b=0, size=4)\n",
      " |\n",
      " |      If no blocks match, return (alo, blo, 0).\n",
      " |\n",
      " |      >>> s = SequenceMatcher(None, \"ab\", \"c\")\n",
      " |      >>> s.find_longest_match(0, 2, 0, 1)\n",
      " |      Match(a=0, b=0, size=0)\n",
      " |\n",
      " |  get_grouped_opcodes(self, n=3)\n",
      " |      Isolate change clusters by eliminating ranges with no changes.\n",
      " |\n",
      " |      Return a generator of groups with up to n lines of context.\n",
      " |      Each group is in the same format as returned by get_opcodes().\n",
      " |\n",
      " |      >>> from pprint import pprint\n",
      " |      >>> a = list(map(str, range(1,40)))\n",
      " |      >>> b = a[:]\n",
      " |      >>> b[8:8] = ['i']     # Make an insertion\n",
      " |      >>> b[20] += 'x'       # Make a replacement\n",
      " |      >>> b[23:28] = []      # Make a deletion\n",
      " |      >>> b[30] += 'y'       # Make another replacement\n",
      " |      >>> pprint(list(SequenceMatcher(None,a,b).get_grouped_opcodes()))\n",
      " |      [[('equal', 5, 8, 5, 8), ('insert', 8, 8, 8, 9), ('equal', 8, 11, 9, 12)],\n",
      " |       [('equal', 16, 19, 17, 20),\n",
      " |        ('replace', 19, 20, 20, 21),\n",
      " |        ('equal', 20, 22, 21, 23),\n",
      " |        ('delete', 22, 27, 23, 23),\n",
      " |        ('equal', 27, 30, 23, 26)],\n",
      " |       [('equal', 31, 34, 27, 30),\n",
      " |        ('replace', 34, 35, 30, 31),\n",
      " |        ('equal', 35, 38, 31, 34)]]\n",
      " |\n",
      " |  get_matching_blocks(self)\n",
      " |      Return list of triples describing matching subsequences.\n",
      " |\n",
      " |      Each triple is of the form (i, j, n), and means that\n",
      " |      a[i:i+n] == b[j:j+n].  The triples are monotonically increasing in\n",
      " |      i and in j.  New in Python 2.5, it's also guaranteed that if\n",
      " |      (i, j, n) and (i', j', n') are adjacent triples in the list, and\n",
      " |      the second is not the last triple in the list, then i+n != i' or\n",
      " |      j+n != j'.  IOW, adjacent triples never describe adjacent equal\n",
      " |      blocks.\n",
      " |\n",
      " |      The last triple is a dummy, (len(a), len(b), 0), and is the only\n",
      " |      triple with n==0.\n",
      " |\n",
      " |      >>> s = SequenceMatcher(None, \"abxcd\", \"abcd\")\n",
      " |      >>> list(s.get_matching_blocks())\n",
      " |      [Match(a=0, b=0, size=2), Match(a=3, b=2, size=2), Match(a=5, b=4, size=0)]\n",
      " |\n",
      " |  get_opcodes(self)\n",
      " |      Return list of 5-tuples describing how to turn a into b.\n",
      " |\n",
      " |      Each tuple is of the form (tag, i1, i2, j1, j2).  The first tuple\n",
      " |      has i1 == j1 == 0, and remaining tuples have i1 == the i2 from the\n",
      " |      tuple preceding it, and likewise for j1 == the previous j2.\n",
      " |\n",
      " |      The tags are strings, with these meanings:\n",
      " |\n",
      " |      'replace':  a[i1:i2] should be replaced by b[j1:j2]\n",
      " |      'delete':   a[i1:i2] should be deleted.\n",
      " |                  Note that j1==j2 in this case.\n",
      " |      'insert':   b[j1:j2] should be inserted at a[i1:i1].\n",
      " |                  Note that i1==i2 in this case.\n",
      " |      'equal':    a[i1:i2] == b[j1:j2]\n",
      " |\n",
      " |      >>> a = \"qabxcd\"\n",
      " |      >>> b = \"abycdf\"\n",
      " |      >>> s = SequenceMatcher(None, a, b)\n",
      " |      >>> for tag, i1, i2, j1, j2 in s.get_opcodes():\n",
      " |      ...    print((\"%7s a[%d:%d] (%s) b[%d:%d] (%s)\" %\n",
      " |      ...           (tag, i1, i2, a[i1:i2], j1, j2, b[j1:j2])))\n",
      " |       delete a[0:1] (q) b[0:0] ()\n",
      " |        equal a[1:3] (ab) b[0:2] (ab)\n",
      " |      replace a[3:4] (x) b[2:3] (y)\n",
      " |        equal a[4:6] (cd) b[3:5] (cd)\n",
      " |       insert a[6:6] () b[5:6] (f)\n",
      " |\n",
      " |  quick_ratio(self)\n",
      " |      Return an upper bound on ratio() relatively quickly.\n",
      " |\n",
      " |      This isn't defined beyond that it is an upper bound on .ratio(), and\n",
      " |      is faster to compute.\n",
      " |\n",
      " |  ratio(self)\n",
      " |      Return a measure of the sequences' similarity (float in [0,1]).\n",
      " |\n",
      " |      Where T is the total number of elements in both sequences, and\n",
      " |      M is the number of matches, this is 2.0*M / T.\n",
      " |      Note that this is 1 if the sequences are identical, and 0 if\n",
      " |      they have nothing in common.\n",
      " |\n",
      " |      .ratio() is expensive to compute if you haven't already computed\n",
      " |      .get_matching_blocks() or .get_opcodes(), in which case you may\n",
      " |      want to try .quick_ratio() or .real_quick_ratio() first to get an\n",
      " |      upper bound.\n",
      " |\n",
      " |      >>> s = SequenceMatcher(None, \"abcd\", \"bcde\")\n",
      " |      >>> s.ratio()\n",
      " |      0.75\n",
      " |      >>> s.quick_ratio()\n",
      " |      0.75\n",
      " |      >>> s.real_quick_ratio()\n",
      " |      1.0\n",
      " |\n",
      " |  real_quick_ratio(self)\n",
      " |      Return an upper bound on ratio() very quickly.\n",
      " |\n",
      " |      This isn't defined beyond that it is an upper bound on .ratio(), and\n",
      " |      is faster to compute than either .ratio() or .quick_ratio().\n",
      " |\n",
      " |  set_seq1(self, a)\n",
      " |      Set the first sequence to be compared.\n",
      " |\n",
      " |      The second sequence to be compared is not changed.\n",
      " |\n",
      " |      >>> s = SequenceMatcher(None, \"abcd\", \"bcde\")\n",
      " |      >>> s.ratio()\n",
      " |      0.75\n",
      " |      >>> s.set_seq1(\"bcde\")\n",
      " |      >>> s.ratio()\n",
      " |      1.0\n",
      " |      >>>\n",
      " |\n",
      " |      SequenceMatcher computes and caches detailed information about the\n",
      " |      second sequence, so if you want to compare one sequence S against\n",
      " |      many sequences, use .set_seq2(S) once and call .set_seq1(x)\n",
      " |      repeatedly for each of the other sequences.\n",
      " |\n",
      " |      See also set_seqs() and set_seq2().\n",
      " |\n",
      " |  set_seq2(self, b)\n",
      " |      Set the second sequence to be compared.\n",
      " |\n",
      " |      The first sequence to be compared is not changed.\n",
      " |\n",
      " |      >>> s = SequenceMatcher(None, \"abcd\", \"bcde\")\n",
      " |      >>> s.ratio()\n",
      " |      0.75\n",
      " |      >>> s.set_seq2(\"abcd\")\n",
      " |      >>> s.ratio()\n",
      " |      1.0\n",
      " |      >>>\n",
      " |\n",
      " |      SequenceMatcher computes and caches detailed information about the\n",
      " |      second sequence, so if you want to compare one sequence S against\n",
      " |      many sequences, use .set_seq2(S) once and call .set_seq1(x)\n",
      " |      repeatedly for each of the other sequences.\n",
      " |\n",
      " |      See also set_seqs() and set_seq1().\n",
      " |\n",
      " |  set_seqs(self, a, b)\n",
      " |      Set the two sequences to be compared.\n",
      " |\n",
      " |      >>> s = SequenceMatcher()\n",
      " |      >>> s.set_seqs(\"abcd\", \"bcde\")\n",
      " |      >>> s.ratio()\n",
      " |      0.75\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |\n",
      " |  __class_getitem__ = GenericAlias(...)\n",
      " |      Represent a PEP 585 generic type\n",
      " |\n",
      " |      E.g. for t = list[int], t.__origin__ is list and t.__args__ is (int,).\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(difflib.SequenceMatcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Differ in module difflib:\n",
      "\n",
      "class Differ(builtins.object)\n",
      " |  Differ(linejunk=None, charjunk=None)\n",
      " |\n",
      " |  Differ is a class for comparing sequences of lines of text, and\n",
      " |  producing human-readable differences or deltas.  Differ uses\n",
      " |  SequenceMatcher both to compare sequences of lines, and to compare\n",
      " |  sequences of characters within similar (near-matching) lines.\n",
      " |\n",
      " |  Each line of a Differ delta begins with a two-letter code:\n",
      " |\n",
      " |      '- '    line unique to sequence 1\n",
      " |      '+ '    line unique to sequence 2\n",
      " |      '  '    line common to both sequences\n",
      " |      '? '    line not present in either input sequence\n",
      " |\n",
      " |  Lines beginning with '? ' attempt to guide the eye to intraline\n",
      " |  differences, and were not present in either input sequence.  These lines\n",
      " |  can be confusing if the sequences contain tab characters.\n",
      " |\n",
      " |  Note that Differ makes no claim to produce a *minimal* diff.  To the\n",
      " |  contrary, minimal diffs are often counter-intuitive, because they synch\n",
      " |  up anywhere possible, sometimes accidental matches 100 pages apart.\n",
      " |  Restricting synch points to contiguous matches preserves some notion of\n",
      " |  locality, at the occasional cost of producing a longer diff.\n",
      " |\n",
      " |  Example: Comparing two texts.\n",
      " |\n",
      " |  First we set up the texts, sequences of individual single-line strings\n",
      " |  ending with newlines (such sequences can also be obtained from the\n",
      " |  `readlines()` method of file-like objects):\n",
      " |\n",
      " |  >>> text1 = '''  1. Beautiful is better than ugly.\n",
      " |  ...   2. Explicit is better than implicit.\n",
      " |  ...   3. Simple is better than complex.\n",
      " |  ...   4. Complex is better than complicated.\n",
      " |  ... '''.splitlines(keepends=True)\n",
      " |  >>> len(text1)\n",
      " |  4\n",
      " |  >>> text1[0][-1]\n",
      " |  '\\n'\n",
      " |  >>> text2 = '''  1. Beautiful is better than ugly.\n",
      " |  ...   3.   Simple is better than complex.\n",
      " |  ...   4. Complicated is better than complex.\n",
      " |  ...   5. Flat is better than nested.\n",
      " |  ... '''.splitlines(keepends=True)\n",
      " |\n",
      " |  Next we instantiate a Differ object:\n",
      " |\n",
      " |  >>> d = Differ()\n",
      " |\n",
      " |  Note that when instantiating a Differ object we may pass functions to\n",
      " |  filter out line and character 'junk'.  See Differ.__init__ for details.\n",
      " |\n",
      " |  Finally, we compare the two:\n",
      " |\n",
      " |  >>> result = list(d.compare(text1, text2))\n",
      " |\n",
      " |  'result' is a list of strings, so let's pretty-print it:\n",
      " |\n",
      " |  >>> from pprint import pprint as _pprint\n",
      " |  >>> _pprint(result)\n",
      " |  ['    1. Beautiful is better than ugly.\\n',\n",
      " |   '-   2. Explicit is better than implicit.\\n',\n",
      " |   '-   3. Simple is better than complex.\\n',\n",
      " |   '+   3.   Simple is better than complex.\\n',\n",
      " |   '?     ++\\n',\n",
      " |   '-   4. Complex is better than complicated.\\n',\n",
      " |   '?            ^                     ---- ^\\n',\n",
      " |   '+   4. Complicated is better than complex.\\n',\n",
      " |   '?           ++++ ^                      ^\\n',\n",
      " |   '+   5. Flat is better than nested.\\n']\n",
      " |\n",
      " |  As a single multi-line string it looks like this:\n",
      " |\n",
      " |  >>> print(''.join(result), end=\"\")\n",
      " |      1. Beautiful is better than ugly.\n",
      " |  -   2. Explicit is better than implicit.\n",
      " |  -   3. Simple is better than complex.\n",
      " |  +   3.   Simple is better than complex.\n",
      " |  ?     ++\n",
      " |  -   4. Complex is better than complicated.\n",
      " |  ?            ^                     ---- ^\n",
      " |  +   4. Complicated is better than complex.\n",
      " |  ?           ++++ ^                      ^\n",
      " |  +   5. Flat is better than nested.\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, linejunk=None, charjunk=None)\n",
      " |      Construct a text differencer, with optional filters.\n",
      " |\n",
      " |      The two optional keyword parameters are for filter functions:\n",
      " |\n",
      " |      - `linejunk`: A function that should accept a single string argument,\n",
      " |        and return true iff the string is junk. The module-level function\n",
      " |        `IS_LINE_JUNK` may be used to filter out lines without visible\n",
      " |        characters, except for at most one splat ('#').  It is recommended\n",
      " |        to leave linejunk None; the underlying SequenceMatcher class has\n",
      " |        an adaptive notion of \"noise\" lines that's better than any static\n",
      " |        definition the author has ever been able to craft.\n",
      " |\n",
      " |      - `charjunk`: A function that should accept a string of length 1. The\n",
      " |        module-level function `IS_CHARACTER_JUNK` may be used to filter out\n",
      " |        whitespace characters (a blank or tab; **note**: bad idea to include\n",
      " |        newline in this!).  Use of IS_CHARACTER_JUNK is recommended.\n",
      " |\n",
      " |  compare(self, a, b)\n",
      " |      Compare two sequences of lines; generate the resulting delta.\n",
      " |\n",
      " |      Each sequence must contain individual single-line strings ending with\n",
      " |      newlines. Such sequences can be obtained from the `readlines()` method\n",
      " |      of file-like objects.  The delta generated also consists of newline-\n",
      " |      terminated strings, ready to be printed as-is via the writelines()\n",
      " |      method of a file-like object.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      >>> print(''.join(Differ().compare('one\\ntwo\\nthree\\n'.splitlines(True),\n",
      " |      ...                                'ore\\ntree\\nemu\\n'.splitlines(True))),\n",
      " |      ...       end=\"\")\n",
      " |      - one\n",
      " |      ?  ^\n",
      " |      + ore\n",
      " |      ?  ^\n",
      " |      - two\n",
      " |      - three\n",
      " |      ?  -\n",
      " |      + tree\n",
      " |      + emu\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(difflib.Differ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"\"\"which we would normally expect.25\n",
    "In our American studies, the distribution of the actual values about\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't figure out page for Economics-1942-0\n",
      "Couldn't figure out page for Economics-1943-0\n",
      "Couldn't figure out page for Economics-1947-0\n",
      "Couldn't figure out page for Economics-1949-0\n",
      "Couldn't figure out page for Economics-1950-0\n",
      "Couldn't figure out page for Economics-1952-0\n",
      "Couldn't figure out page for Economics-1953-0\n",
      "Couldn't figure out page for Economics-1956-0\n",
      "Couldn't figure out page for Economics-1957-0\n",
      "Couldn't figure out page for Economics-1960-0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     31\u001b[0m idxes\u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mdoc)\u001b[38;5;241m&\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m&\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m4\u001b[39m)]\u001b[38;5;241m.\u001b[39mindex\n\u001b[0;32m---> 32\u001b[0m i1,i2,i3 \u001b[38;5;241m=\u001b[39m idxes\n\u001b[1;32m     33\u001b[0m t1, t2,t3 \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[idxes,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;66;03m#type:ignore\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries of photographs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m t1:\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\"\"\"Look from pages 01-03 for all of the texts except 2020 and 2022:\n",
    "if 01 says \"series of photographs\":\n",
    "    mark 02 as picture and 03 as first page\n",
    "elif 01 doesn't have some indicator that it's a title page (Like the \"By...* line\"):\n",
    "    mark 01 as picture and 02 as first page\n",
    "else: \n",
    "    mark 01 as first page\n",
    "    might also want to check for title page indicator and do something if there's a problem with it\n",
    "    \"\"\"\n",
    "df=pd.DataFrame({\"page_id\":page_ids,'doc_id':doc_ids,\"text\":texts})\n",
    "## Basic info:\n",
    "def get_info(name):\n",
    "    _,year,num,page = name.split(\"-\")\n",
    "    return int(year),int(num),int(page)\n",
    "df['text_path'] = df['page_id'].apply(lambda x: os.path.join(text_dir,x+\".txt\"))\n",
    "df['pdf_path'] = df['text_path'].apply(lambda x: x.replace(\"texts\",\"pdfs\").replace(\".txt\",\".pdf\"))\n",
    "df[['year','num','page']] = df['page_id'].apply(get_info).to_list()\n",
    "df[\"length\"] = df['text'].apply(len)\n",
    "df[\"num_lines\"] = df['text'].apply(lambda x: x.count(\"\\n\")+1)\n",
    "df['text'] = df.pop('text')\n",
    "df['num_pages'] = df.groupby('doc_id')['page'].transform('max')+1\n",
    "df['is_jstor_cover'] = df['page']==0\n",
    "## Take care of exception\n",
    "df.loc[(df['year'].isin([2020,2022]))&(df['page']==0),'is_jstor_cover'] =False\n",
    "# Getting picture and cover pages\n",
    "df[[\"is_photo_intro\",\"is_author_photo\",\"is_first_page\"]]=False\n",
    "for doc in df['doc_id'].unique():\n",
    "    if '2020' in doc or '2022' in doc:\n",
    "        continue\n",
    "    idxes= df.loc[(df['doc_id']==doc)&(df['page']>0)&(df['page']<4)].index\n",
    "    i1,i2,i3 = idxes\n",
    "    t1, t2,t3 = df.loc[idxes,\"text\"].values #type:ignore\n",
    "    if \"series of photographs\" in t1:\n",
    "        df.at[i1,\"is_photo_intro\"] = True\n",
    "        df.at[i2,\"is_author_photo\"] = True\n",
    "        df.at[i3,\"is_first_page\"] = True\n",
    "    elif len(re.findall(r\"By[^\\*]+\\*\",t2,re.DOTALL))>0:\n",
    "        df.at[i1,'is_author_photo'] = True\n",
    "        df.at[i2,'is_first_page'] = True\n",
    "    else:\n",
    "        if len(re.findall(r\"By[^\\*]+\\*\",t1,re.DOTALL))==0:\n",
    "            print(f\"Couldn't figure out page for {doc}\")\n",
    "        df.at[i1,\"is_first_page\"]=True\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
