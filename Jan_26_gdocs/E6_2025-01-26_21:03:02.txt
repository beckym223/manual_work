
# E6
 ## Economics-1969-0


### ---Economics-1969-0-01.txt---
Adam Smith, who has strong claim to
being both the Adam and the Smith of
systematic economics, was a professor of
moral philosophy and it was at that forge
that economics was made. Even when I
was a student, economics was still part of
the moral sciences tripos at Cambridge
University. It can claim to be a moral science,
therefore, from its origin, if for no
other reason. Nevertheless, for many
economists the very term "moral science"
will seem like a contradiction. We are
strongly imbued today with the view that
science should be wertfrei and we believe
that science has achieved its triumph precisely
because it has escaped the swaddling
clothes of moral judgment and has
only been able to take off into the vast
universe of the "is" by escaping from the
treacherous launching pad of the "ought."
Even economics, we learn in the history of
thought, only became a science by escaping
from the casuistry and moralizing of
medieval thought. Who, indeed, would
want to exchange the delicate rationality
of the theory of equilibrium price, for the
unoperational vaporings of a "just price"
controversy? In the battle between mechanism
and moralism generally mechanism
has won hands down, and I shall not be
surprised if the very title of my address
does not arouse musty fears of sermonizing
in the minds of many of my listeners.
Let me first explain, then, what I mean
by moral and by moral science. A moral,
or ethical proposition, is a statement
about a rank order of preference among
alternatives, which is intended to apply to
more than one person. A preference which
applies to one person only is a "taste."
Statements of this kind are often called
"value judgments." If someone says, "I
prefer A to B," this is a personal value
judgment, or a taste. If he says, "A is better
than B," there is an implication that
he expects other people to prefer A to B
also, as well as himself. A moral proposition
then is a "common value."

Every culture, or subculture, is defined
by a set of common values, that is, generally
agreed upon preferences. Without a
core of common values a culture cannot
exist, and we classify society into cultures
and subcultures precisely because it is
possible to identify groups who have common
values.

Most tastes are in fact also common
values and have been learned by the process
by which all learning is done, that is,
by mutation and selection. The most absurd
of all pieces of ancient wisdom is
surely the Latin tag de gustibus non disputandum.
In fact, we spend most of our

lives disputing about tastes. If we want to
be finicky about definitions we might turn
the old tag around and say where there is
disputing, we are not talking about tastes.
Nevertheless, even personal tastes are
learned, in the matrix of a culture or a
subculture in which we grow up, by very
much the same kind of process by which
we learn our common values. Purely personal
tastes, indeed, can only survive in a
culture which tolerates them, that is,
which has a common value that private
tastes of certain kinds should be allowed.
One of the most peculiar illusions of


### ---Economics-1969-0-04.txt---
economists is a doctrine that might be
called the Immaculate Conception of the
Indifference Curve, that is, that tastes are
simply given, and that we cannot inquire
into the process by which they are
formed. This doctrine is literally "for the
birds," whose tastes are largely created
for them by their genetic structures, and
can therefore be treated as a constant in
the dynamics of bird societies. In human
society, however, the genetic component
of tastes is very small indeed. We start off
with a liking for milk, warmth, and dryness
and a dislike for being hungry, cold,
and wet, and we do have certain latent
drives which may guide the formation of
later preferences in matters of sex, occupation,
or politics, but by far and away
the largest part of human preferences are
learned, again by means of a mutation-selection
process. It was, incidentally, Veblen'
s principal, and still largely unrecognized,
contribution to formal economic
theory, to point out that we cannot assume
that tastes are given in any dynamic
theory, in the sense that in dynamics we
cannot afford to neglect the processes by
which cultures are created and by which
preferences are learned.

I am prepared indeed to go much further
and to say that no science of any
kind can be divorced from ethical considerations,
as defined above. The propositions
of science are no more immaculately
conceived than the preferences of individuals.
Science is a human learning process
wlhich arises in certain subcultures in
human society and not in others, and a
subculture as we have seen is a group of
people defined by the acceptance of certain
common values, that is, an ethic
which permits extensive communication
among them.

The scientific subculture is no exception
to this rule. It is characterized by a strong
common value system. A high value, for
instance, is placed on veracity, on curios-
ity, on measurement, on quantification, on
careful observation and experiment, and
on objectivity. Without this common
value structure the epistemological process
of science would not have arisen; indeed
it did not arise in certain societies
where conditions might otherwise have
been favorable but where some essential
common values of the scientific subcultures
did not exist. The question as to exactly
what values and ethical propositions
are essential to the scientific subculture
may be in some dispute. The fact that
there are such values cannot be disputed.
It is indeed one of the most perplexing
questions in intellectual history as to why
the scientific subculture developed in the
time and place that it did in Western Europe.
The common values that are prerequisite
to it are rather rare among human
subcultures. The common values, for instance,
of the military or the people that
run the international system are quite different
from those of science. In this sense,
therefore, science has an essential ethical
basis.

This means that even the epistemological
content of science, that is, what scientists
think they know, has an ethical component.
The proposition, for instance, that
water consists of two molecules of hydrogen
and one of oxygen is not usually
thought of as a proposition with high ethical
content. Nevertheless, any student in
chemistry who decides that he prefers to
think of hydrogen as dephlogisticated
water will soon find out that chemistry is
not just a matter of personal taste. The
fact that there is no dispute going on
about any particular scientific proposition
does not mean to say that it is a matter of
taste; it simply means that the dispute
about it has been resolved through the application
of certain common values and

ethical presuppositions.

There is however a fundamental sense
in which the epistemological process even


### ---Economics-1969-0-05.txt---
in the physical and biological sciences is
now running into situations which have
strong ethical implications outside the
scientific subculture. The myth that science
is simply discovering knowledge
about an objectively unchangeable world
may have had some validity in the early
stages of science but as the sciences develop
this myth becomes less and less
valid. The learning process of science is
now running into two serious difficulties.
The first might be called the generalized
Heisenberg principle. When we are trying
to obtain knowledge about a system by
changing its inputs and outputs of information,
these inputs and outputs will

change the system itself, and under some
circumstances they may change it radically.
My favorite illustration of the Heisenberg
principle is that of a man who inquires
through the door of the bedroom
where his friend is sick, "How are you?"
whereupon the friend replies "Fine," and
the effort kills him. In the social sciences
of course the generalized Heisenberg principle
predominates because knowledge of
the social sciences is an essential part of
the social system itself, hence objectivity
in the sense of investigating a world which
is unchanged by the investigation of it is
an absurdity.

The second difficulty is that as science
develops it no longer merely investigates
the world; it creates the world which it is
investigating. We see this even in the
physical sciences where the evolution of
the elements has now been resumed in this
part of the universe after some six billion
years. We are increasingly going to see
this in the biological sciences, which will
only find out about the evolutionary process
by actively engaging in it, and changing
its course. In the social sciences one
can defend the proposition that most of
what we can really know is what we create
ourselves and that prediction in social
systems can be achieved only by setting
up consciously created systems which will
make the predictions come true. Knowledge
of random systems can only be obtained
by destroying them, that is, by taking
the randomness out of them. There is
a great deal of evidence, for instance, that
the fluctuations of prices in organized
commodity or secuirity markets are essentially
random in nature. All we can possibly
discover therefore by studying these
fluctuations is what bias there might be in
the dice. If we want to predict the future
of prices in such a market we will have to
control it, that is, we will have to set up a
system of counterspeculation which will
guarantee a given future course of prices.
The gold standard is a primitive example
of such a system in which it is possible to
predict that the price of gold will lie
within the gold points as long as the system
remains intact. Similarly, we can predict
the inside temperature of a house
with an effective furnace and thermostat
much better than we can predict the
outside temperature simply because we
control one and not the other.
We cannot escape the proposition that
as science moves from pure knowledge toward
control, that is, toward creating
what it knows, what it creates becomes a
problem of ethical choice, and will depend
upon the common values of the societies
in which the scientific subculture is
embedded, as well as of the scientific subculture.
Under these circumstances science
cannot proceed at all without at least
an implicit ethic, that is, a subculture with
appropriate common values. The problem
exists in theory even in what might be described
as the objective phase of science,
that is, the phase in which it is simply investigating
"what is," because the question
of the conditions under which ignorance
is bliss is not an empty one. The assumption
which is almost universal in academic
circles that ignorance cannot possibly
be bliss might under some circumstances


### ---Economics-1969-0-06.txt---
be proved wrong by the very

methods of science itself. As long as science
is investigating an unchanging world,
however, this problem does not become
acute, for if knowledge does not change
the world, then all ignorance does for us is
to prevent us from satisfying our idle
curiosity. When, however, knowledge
changes the world the question of the content
of the common values, both of the
subculture which is producing knowledge
and of the total society in which that subculture
is embedded, becomes of acute importance.
Under these circumstances the

concept of a value-free science is absurd,
for the whole future of science may well
rest in our ability to resolve the ethical
conflicts which the growth of knowledge is
now creating. Science could create an ethical
dynamic which would bring it to an
end.

Let us return then to economics as a
moral science, not merely in the sense in
which all science is "affected with an ethical
interest," but in the quite specific
sense of asking whether economics itself
can be of assistance in resolving ethical
disputes, especially those which arise out
of the continued increase of knowledge.
Economics specializes in the study of
that part of the total social system which
is organized through exchange and which
deals with exchangeables. This to my
mind is a better definition of economics
than those which define it as relating to
scarcity or allocation, for the allocation of
scarce resources is a universal problem
which applies to political decisions and
political structures through coercion,
threat, and even to love and community,
just as it does to exchange. I have elsewhere
distinguished three groups of social
organizers which I have called the threat
system, the exchange system, and the integrative
system. Economics clearly occupies
the middle one of these three. It
edges over towards the integrative system
insofar as it has some jurisdiction over the
study of the system of one-way transfers
of exchangeables, which I have called the
"cgrants economy," for the grant, or oneway
transfer, is a rough measure of an integrative
relationship. On the other side,
economics edges towards an area between
the threat system and the exchange system
which might be described as the study
of strategy or bargaining.

To complete the circle there is also an
area, between the threat system and the
integrative system, of legitimated threat
which is the principal organizer of political
activity and the main subject matter
of political science. All these systems are
linked together dynamically through the
process of human learning which is the
main dynamic factor in all social systems.
Part of this learning process is the learning
of common values and moral choices,
without which no culture and no social
system is possible. The process by which
we learn otr preference structures indeed
is a fundamental key to the total dynamics
of society.

Economics, as such, does not contribute
very much to the formal study of human
learning, though some philosophical economists
like Frederick Hayek [4] have

made some interesting contributions to
this subject. Our main contribution as
economists is in the description of what is
learned; the preference functions which
embody what is learned in regard to
values, and the production functions
which describe the results of the learning
of technology. XVe may not have thought
much about the genetics of knowledge,
but we have thought about its description,
and this is a contribution not to be despised.


Thus, economics suggests the proposition
that actual choices depend not only
on preferences but on opportunities, and
that under some circumstances quite small
changes in either preferences or opportunities


### ---Economics-1969-0-07.txt---
may result in large changes in actual
choices made. This proposition applies
just as much to ethical choices and
common values as it does to private
tastes. It throws a good deal of light also
on what might be called the evolutionary
ecology of ethical systems. Successful ethical
systems tend to create subcultures,
and these subcultures tend to perpetuate
and propagate the ethical systems which
created them. This principle helps to explain
the persistent division of mankind
into sects, nations, and ideological groups.
If we were to map the ethical preference
systems of the individuals who comprise
mankind, we would not find a uniform
distribution but we would find a very
sharp clustering into cultures and subcultures
with relatively empty spaces between
the clusters. All the members of a
single sect, for instance, tend to think
rather alike in matters of ethical judgment
and differentiate themselves sharply
from the ethical judgments of other sects.
Individuals tend to be attracted to one or
another of these clusters, leaving the social
space between them relatively empty,
like space between the stars. The reasons
for this phenomenon lie deep in the dynamics
of the human learning process, for
our preferences are learned mainly from
those with whom we have the most communication.
This principle accounts for

the perpetuation of such clusters, though
it does not necessarily account for their
original formation, which exhibits many
puzzling phenomena. The splitting of
these clusters in a kind of mitosis is also
an important and very puzzling phenomenon.
Once we realize, however, that these
are highly sensitive systems as economic
analysis suggests, we can see how wide
divergences might arise. Thus, the actual
difference in preferences and even opportunities
between, shall we say, the socialist
countries and the capitalist countries,
may in fact be quite small, but this difference
is enough to produce a very wide
difference in the choices made.
Economics has made its own attempt to
solve some of the problems involved in the
moral judglnent in what we know as welfare
economics. I believe this attempt has
been a failure, though a reasonably glorious
one, and we should take a brief look
at it. Welfare economics attempts to ask
the question "What do we mean when we
say that one state of a social system is
better than another in strictly economic
terms?" The most celebrated answer
given is the Paretian optimum, which
states in effect that Condition A of a social
system is economically superior to
Condition B, if nobody feels worse off in
A than in B, and if at least one person
feels better off. "Better off" or "worse off"
are measured of course by preferences, so
that we could restate the condition as saying
that State A is superior to State B if
one or more persons prefer A and if nobody
prefers B. If we permit internal redistributions
within the system, that is,

compensation, the range of possible superior
states is considerably broadened.
From this simple principle a wide range of
applications has been found possible in a
stirring intellectual drama which might
well be subtitled "Snow White (the fairest
of all) and the Seven Marginal Conditions.
"

Many, if not most, economists accept
the Paretian optimum as almost self-evident.
Nevertheless, it rests on an extremely
shaky foundation of ethical propositions.
The more one examines it, for instance,
the more clear it becomes that
economists must be extraordinarily nice
people even to have thought of such a
thing, for it implies that there is no malevolence
anywhere in the system. It implies,
likewise, that there is no benevolence,
the niceness of economists not quite
extending as far as good will. It assumes
selfishness, that is, the independence of individual


### ---Economics-1969-0-08.txt---
preference functions, such that it
makes no difference to me whether I perceive
you as either better off or worse off.
Anything less descriptive of the human
condition could hardly be imagined. The
plain fact is that our lives are dominated
by precisely this interdependence of utility
functions which the Paretian optimum
denies. Selfishness, or indifference to the
welfare of others, is a knife edge between
benevolence on the one side and malevolence
on the other. It is something that is
very rare. We may feel indifferent towards
those whom we do not know, with whom
we have no relationships of any kind, but
towards those with whom we have relationships,
even the frigid relationship of
exchange, we are apt to be either benevolent
or malevolent. We either rejoice when
they rejoice, or we rejoice when they
mourn.

The almost complete neglect by economists
of the concepts of malevolence and
benevolence cannot be explained by their
inability to handle these concepts with
their usual tools. There are no mathematical
or conceptual difficulties involved in
inter-relating utility functions, provided
that we note that it is the perceptions that
matter [2]. The familiar tools of our
trade, the indifference map, the Edgeworth
box, and so on, can easily be expanded
to include benevolence or malevolence,
and indeed without this expansion
many phenomena, such as one-way transfers,
cannot be explained. Perhaps the
main explanation of ouLr neglect of these
concepts is the fact that we have concentrated
so heavily on exchange as the object
of our study, and exchange frequently
takes place under conditions of at least
relative indifference or selfishness, though
I argue that there is a minimum degree of
benevolence even in exchange without
which it cannot be legitimated and cannot
operate as a social organizer. We exchange
courtesies, smiles, the time of day
and so on with the clerk in the store, as
well as exchanging money for commodities.
The amount of benevolence which exchangers
feel towards each other need not
be large, but a certain minimum is essential.
If exchangers begin to feel malevolent
toward each other exchange tends to
break down, or can only be legitimated
under conditions of special ritual, such as
silent trade or collective bargaining.
Nevertheless, economists can perhaps
be excused for abstracting from benevolence
and malevolence, simply because
their peculiar baby, which is exchange,
tends to be that social organizer which lies
between these two extremes, and which
produces, if not selfishness, at least low
levels of malevolence and benevolence.
The threat system constantly tends to
produ-ce malevolence simply because of
the learning process which it engenders. A
threatener may begin by feeling benevolent
toward the threatened-"I am doing
this for your own good"-but threats almost
invariably tend to produce malevolence
on the part of the threatened towards
the threatener, and this is likely to
produce a type of behavior which will in
turn produce malevolence on the part of
the threatener towards the threatened.
This can easily result in a cumulative process
of increasing malevolence which may
or may not reach some kind of equilibrium.
The breakup of communities into
factions and into internal strife frequently
follows this pattern. At the other end of
the scale, the integrative system tends to
produce benevolence and those institutions
which are specialized in the integrative
system, such as the family, the
church, the lodge, the club, the aluimni association,
and so on, tend also to create
and organize benevolence, even beyond
the circle of their members. This is partly
because benevolence seems to be an importaint
element in establishing a satisfactory
personal identity, especially after the


### ---Economics-1969-0-09.txt---
threat system has been softened by the
development of exchange. Those who live
under threat, who generally occupy the
lower end of the social scale, as well as
those who live by threat at the upper end,
tend to find their personal identities
through malevolence and through the development
of counter-threat or through

the displacement of hatred onto weaker
objects, such as children and animals.
Once this state is passed, however, and society
is mainly organized by exchange,
there seems to be a strong tendency to
miove towards the integrative system and
the integrative institutions. The Rotary
Club is a logical extension of a businessoriented
society, but it is not one that
would necessarily have occurred to economists.


Oddly enough, it is not welfare economics
with its elegant casuistry, subtle distinctions,
and its ultimately rather implausible
recommendations, which has

made the greatest impact on the development
of common values and ethical propositions.
The major impact of economics on
ethics, it can be argued, has come because
it has developed broad, aggregative concepts
of general welfare which are subject
to quantification. We can see this process
going right back to Adam Smith, where
the idea of what we would today call per
capita real income, as the principal measure
of national well-being, has made a
profound impact on subsequent thinking
and policy. The development of the concept
of a gross national product and its
various modifications and components as
statistical measures of economic success,
likewise, has had a great impact in creating
common values for the objectives of
economic policy. Another, less fortunate,
example of a measure which profoundly
affected economic policy was the development
of the parity index by the Bureau of
Agricultural Economics in the United
States Department of Agriculture. As a
measure of the terms of trade -of agricul.
ture, this became an important symbol.
"A hundred per cent of parity" became
the avowed goal of agricultural policy,
even though there is very little reason to
suppose that the terms of trade of a given
historic period, in this case the period
1909-14, have any ultimate validity as
an ideal. Because of differing rates of
change in productivity in different parts
of the economy, we should expect the
terms of trade of different sectors to
change. If, for instance, productivity in
agriculture rises faster than in the rest of
the economy, as it has done in the last
thirty years, we would expect the terms of
trade of agriculture to "worsen" without
any worsening of the incomes of farmers,
and without any sense of social injustice.
Even though economic measurement
may be abused, its effect on the formation
of moral judgments is great, and on the
whole I believe beneficial. The whole idea
of cost-benefit analysis, for instance, in
terms of monetary units, say "real" dollars
of constant purchasing power, is of
enormous importance in the evaluation of
social choices and even of social institutions.
We can grant, of course, that the
"real" dollar which is oddly enough a
strictly imaginary one, is a dangerously
imperfect measure of the quality of human
life and human values. Nevertheless,
it is a useful first approximation,
and in these matters of evaluation of difficult
choices it is extremely useful to have
some first approximation that we can then
modify. Without this, indeed, all evaluation
is random selection by wild hunches.
It is true, of course, that cost-benefit analysis
of all sorts of things, whether of
water projects, other pork barrel items, or
in more recent years weapon systems, can
be manipulated to meet the previous prejudices
of people who are trying to influence
the decisions. Nevertheless, the fundamental
principle that we should count


### ---Economics-1969-0-10.txt---
all costs, whether easily countable or not,
and evaluate all rewards, however hard
they are to evaluate, is one which emerges
squarely out of economics and which is at
least a preliminary guideline in the formation
of the moral judgment, in what might
be called the "economic ethic."
Nevertheless, the economic ethic, or the
total cost-benefit principle, is subject to
sharp challenge. Two principal criticisms
have been made of it, the first of which I
think is probably not valid, and the second
of which may be valid under limited
circumstances. The criticism that I think
is not valid is that cost-benefit analyses in
particular, or economic principles in general,
imply selfish motivation and an insensitivity
to the larger issues of malevolence,
benevolence, the sense of community
and so on. It is quite true, as shown
above, that economists have neglected the
problem of malevolence and benevolence.
Nevertheless, our attitudes towards others
can be measured at least as well as we can
measure other preferences, either by some
principle of "revealed preference" or by
direct questioning. It is entirely within the
competence of economics, for instance, to
develop a concept of the "rate of benevolence"
which is the quantity of exchangeables,
as measured in real dollars, which a
person would be willing to sacrifice in
order to contemplate an increase of one
real dollar in the welfare of another person.
If the rate of benevolence is zero, of
course, we have indifference or pure selfishness;
if the rate of benevolence is negative
we have malevolence, in which case
people need compensation in order to contemplate
without loss the increased welfare
of an enemy, or in reverse would be
willing to damage themselves in order to
damage another. The rate of malevolence
then would be the amount in real dollars
one would be prepared to damage one's
self in order to damage another person to
the extent of one dollar. These rates of
malevolence incidentally are frequently
quite high. It apparently costs the United
States about four dollars to do one dollar's
worth of damage in Vietnam, in which
case our rate of benevolence towards
North Vietnam is at least minus four. In
determining cost-benefit analysis we can
easily include rates of benevolence and
malevolence, adding the benefits and subtracting
the costs to those toward whom
we are benevolent, multiplied of course by
the rate of benevolence, and subtracting
the benefits and adding the costs, similarly
modifed, to those towards whom we
are malevolent.

The concept of a rate of benevolence, incidentally,
is at least a partial solution to
the perplexing question of interpersonal
comparisons of utility around which economists
have been doing a ritual dance for
at least three generations. Any decision
involving other people obviously involves
these interpersonal comparisons. They are
made, of course, inside the mind of the decision-
maker and what his rates of benevolence
or malevolence are likely to be is
determined by the whole social process in
which he is embedded. Surely something
can be said about this. We are, for instance,
likely to be more benevolent to
people who are going to vote for us and
perhaps malevolent to people who are
going to vote against us. The economic
theory of democracy indeed as developed
by Anthony Downs and others is a very
good example of what I have sometimes
called "economics imperialism," which is
an attempt on the part of economics to
take over all the other social sciences.
The second attack on the "economic
ethic" is more fundamental and harder to
repulse. This is the attack from the side
of what I have elsewhere called the "heroic
ethic" [1]. In facing decisions, especially
those which involve other people, as


### ---Economics-1969-0-11.txt---
virtually all decisions do, we are faced
with two very different framneworks of
judgment. The first of these is the economic
ethic of total cost-benefit analysis.
It is an ethic of being sensible, rational,
whatever we want to call it. It is an ethic
of calculation. We cannot indeed count
the cost without counting. Hence, it is an
ethic which depends on the development
of measurement and numbers, even if
these are ordinal numbers. This type of
decision-making, however, does not exhaust
the immense complexities of the
human organism, and we have to recognize
that there is in the world another
type of decision-making, in which the decision-
maker elects something, not because
of the effects that it will have, but
because of what he "is," that is, how he
perceives his own identity.

This "heroic" ethic takes three major
forms-the military, the religious, and the
sporting. The heroic ethic "theirs not to
reason why, theirs but to do and die" is so
fundamental to the operation of the military
that attempts to apply an economic
ethic to it in the form of cost-benefit analysis
or programmed budgeting, or even
strategic science as practiced by Herman
Kahn, T. C. Schelling, or even Robert
McNamara, are deeply threatening to the
morale and the legitimacy of the whole
military system. Religion, likewise, is an
essentially heroic enterprise, even though
there is a strong streak of spiritual costbenefit
analysis in it. The enormous role
which religion has played in the history of
mankind, for good or ill, is based on the
appeal which it has to the sense of identity
and the sense of the heroic even in ordinary
people. "Here I stand and I can do
no other" said Luther; "To give and not to
count the cost, to labor and ask for no reward"
is the prayer of St. Francis. "Do
your own thing" is the motto of our new
secular Franciscans, the Hippies. In our
national religion, President Kennedy said,
"Ask not what your country can do for
you, ask only what you can do for your
country." We find the same principle in
poetry, in art, in architecture, which are
constantly striving to disengage themselves
from the chilling embrace of costbenefit
analysis. I cannot resist quoting
here in full what has always seemed to me
one of the finest expressions in English
poetry of the heroic critique of economics
-Wordsworth's extraordinary sonnet on
King's College Chapel, Cambridge (Ecclesiastical
Sonnet, Number XLIII):

INSIDE OF KING'S COLLEGE CHAPEL,
CAMBRIDGE

Tax not the royal Saint with vain expense,
With ill-matched aims the Architect who
plannedAlbeit

labouring for a scanty band

Of white-robed Scholars only-this immense
And glorious Work of fine intelligence!
Give all thou canst; high Heaven rejects the
lore

Of nicely-calculated less or more;
So deemed the man who fashioned for the sense
These lofty pillars, spread that branching roof
Self-poised, and scooped into ten thousand
cells,

Where light and shade repose, where music
dwells

Lingering-and wandering on as loth to die;
Like thoughts whose very sweetness yieldeth
proof

That they were born for immortality.
Okay, boys, bring out your cost-benefit
analysis now! There is a story, for the
truth of which I will not vouch, that
Keynes once asked the chaplain of King's
College if he could borrow the chapel for
a few days. The chaplain was overjoyed
at this evidence of conversion of a noted
infidel until it turned out that Keynes had
got stuck with a load of wheat in the
course of his speculations in futures contracts


### ---Economics-1969-0-12.txt---
and wanted to use the chapel for
storage.

The "lore of nicely-calculated less or
more," of course, is economics. I used to
think that high heaven rejected this because
its resources were infinite and therefore
did not need to be economized. I have
since come to regard this view as theologically
unsound for reasons which I cannot
go into lhere, but also for a more fundamental
reason. High Heaven, at least as it
exists and propagates itself in the minds
of men, is nothing if not heroic. The
power of religion in human history has
arisen more than anything from its capacity
to give identity to its practitioners and
to inspire them with behavior which arises
out of this perceived identity. In extreme
form, this gives rise to the saints and martyrs
of all faiths, religious or secular, but
it also gives rise to a great deal of quiet
heroism, for instance, in jobs, in marriage,
in child rearing and in the humdrum tasks
of daily life, without which a good deal of
the economy might well fall apart.
A good deal of the criticism of economics
from both left and right arises from
dissatisfaction with its implied neglect of
the heroic. There is a widespread feeling
that trade is somehow dirty, and that merchants
are somewhat undesirable characters,
and that especially the labor market
is utterly despicable as constituting the
application of the principle of prostitution
to virtually all areas of human life. This
sentiment is not something which economists
can neglect. We have assumed all
too easily in economics that because something
paid off it was therefore automatically
legitimate. Unfortunately, the dynamics
of legitimacy are more complex
than this. Frequently it is negative payoffs,
that is, sacrifices, rather than positive
payoffs, which establish legitimacy. It has
been the precise weakness of the institutions
that we think primarily of as economic,
that is, associated with exchange,
such as the stock market, the banking system,
organized commodity markets and so
on, as Schumpeter pointed out, that they
easily lose their legitimacy if they are not
supported by other elements and institutions
in the society which can sustain
them as integral parts of a larger community.
On the right also we find nationalists,
fascists, and the military, attacking
the economic man and economic motivation
from the point of view of the heroic
ethic. It is a wonder indeed that economic
institutions can survive at all, when economic
man is so universally unpopular.
No one in his senses would want his
daughter to marry an economic man, one
who counted every cost and asked for
every reward, was never afflicted with
mad generosity or uncalculating love, and
who never acted out of a sense of inner
identity and indeed had no inner identity
even if he was occasionally affected by
carefully calculated considerations of benevolence
or malevolence. The attack on

economics is an attack on calculatedness
and the very fact that we think of the calculating
as cold suggests how exposed

economists are to romantic and heroic
criticism.

My personal view is that, especially at
his present stage or development, man requires
both heroic and economic elements
in his institutions, in his learning processes
and in his decision-making and the
problem of maintaining them in proper
balance and tension is one of the major
problems of maturation, both of the individual
person and of societies. Economic
man is a clod, heroic man is a fool, but
somewhere between the clod and the fool,
human man, if the expression may be pardoned,
steers his tottering way.

Let me conclude by stealing another
idea from economics and applying it to
general moral science. This is the concept


### ---Economics-1969-0-13.txt---
of a production function, some sort of limited
relationship between inputs and outputs
as expressed in the great biblical
principle that grapes are not gathered
from thorns, or figs from thistles (Matthew
7:16). There are production functions
not only for grapes and figs, but also
for goods and bads, and indeed for the ultimate
Good. We dispute about what is
good, about what outputs we want as a result
of the inputs we put in. We dispute
also however about the nature of the production
functions themselves, what inputs
in fact will produce what outputs. In the
case of physical production functions the
problems can be resolved fairly easily by
experimenting, even though there are
some pretty doubtful cases, as in the case
of cloud seedings, which do not seem to be
demonstrably more effective than rain
dances. In the case of moral production
functions, however, the functions themselves
are much in dispute, and there may
indeed be more disputation about the production
functions than there is about the
nature of the desired outputs themselves.
I was impressed some years ago, when engaged
in a long arduous seminar with
some young Russians and young Americans
with how easy it was to agree on ultimate
goals, even across these widely divergent
ideologies, and how extraordinarily
hard it was to agree about the inputs
which are likely to produce these ultimate
goals.

There is a problem here in human
learning of how do we get to know the
moral production functions in the complex
melee of social, political, and economic
life, when it seems to be pervaded
throughout with a note of almost cosmic
irony in which almost everything we do
turns out different from what we expect
because of our ignorance, so that both the
bad and the good we do is all too often
unintentional. I cannot solve this enistemological
problem in one short paper, but
I recommend it as a major intellectual
challenge to the moral sciences. What I
am concerned with here is with economics
as an input into this moral production
function. Does economics, as George Stigler
has suggested, make people conservative
[3]? If so, it is perhaps because it
simply points out the difficulties and dangers
of heroic action and makes people appreciate
the productivity of the commonplace,
of exchange and finance, of bankers
and businessmen, even of the middle class
which our heroic young so earnestly despise.
Perhaps this is why so many young
radiCals today have abandoned economics
as a poisoned apple of rationality which
corrupts the pure and heroic man of their
identities and sympathlies. Economics is a
reconciler, it brings together the ideologies
of East and West, it points up the many
common problems which they have, it is
corrosive of ideologies and disputes that
are not worth their costs. Even as it acts
as a reconciler, however, does it not undermine
that heroic demand for social mutation
which will not be stilled in the
voices of our young radicals?

I confess I have been deeply disturbed
when I have asked myself these questions
and I have no easy answers to them. Nevertheless,
I am not sorry that I became an
economist, for to belong to a body of people
who have never even thought of introducing
malevolence into their social

theory is somehow in this day and age a
little cheering. The anxieties, the moral
anguish, and the intense dispute which
has racked the American Economic Association
this year and which is symbolized
by the question as to whether we should
move our meeting from Chicago is symptomatic
of the fact that not even the study
of economics can turn people into purely
economic men. Strangely enough it was
the mathematical economists and econometricians


### ---Economics-1969-0-14.txt---
who were most heroically

moved by a sense of outrage against their
personal identity, and who were least affected
by the cost-benefit analysis. In this
year of crisis I havte also learned something
about myself-that it is easier to
make heroic decisions as a member of the
committee than it is as a sole decisionmaker
and that heroism is much less appropriate
in political than it is in personal
decisions. The lessons of this year, therefore,
are that the study of economics does
not produce clods, even if perhaps the
American Economic Association does not
produce undue heroics. So we can hope at
least that economics is one of the inputs
that helps to make us human. If so, the
benefits of this strange activity will be
well worth its undoubted cost, even if in
our heroic mood we dare not calculate
them.
 ## Economics-1970-0


### ---Economics-1970-0-03.txt---
Much of mankind's accumulated knowledge
performs functions other than that of
increasing our command over goods and
services in the usual sense of these words.
The value of such knowledge is not instrumental
in an economic-technological

context. It is equally clear that contributors
to the growth of knowledge have all
along been motivated largely by a desire to
improve their understanding of the world
into which they were born. "From science
to engineering and from engineering to
more goods and services" would be an unduly
drab account of man's quest for knowledge.


Yet in societies of various types, financial
and other incentives have become
established for directing inventive and educational
abilities to specific tasks carrying
economic promise. Moreover, for several
centuries the Western conception of
knowledge that is valuable for its own
sake has been appreciably influenced by
the ideals of natural scientists and mathematicians.
It is a fact of crucial importance
that this has created a cultural environment
in which a reasonably high degree of
correlation may be found between the accumulation
of knowledge per se and subsequent
technological applications. The
methods of measuring the economic yield
of new knowledge and of measuring the
trend of that yield have many inevitable
limitations reflecting the indirectness of
the relation of the knowledge-acquisition
process to its economic consequences. My
paper will be concerned with these yields
and trends.

The inputs which in this study will be
regarded as progress-generating will be so
defined because they increase the economic
productivity of inputs at large. Our progress-
inputs could be viewed as producing
intangible capital that is instrumental to
the production of goods and services and
also intangible consumer capital serving as
a source of direct satisfaction. But this is
merely a simile because the properties of
this intangible investment militate against
treating it as output in the sense proper.
The "intangible investment" results in an
increase of the value of terms which in
static versions of neoclassical production
functions would be parameters. The progress-
generating inputs of each period may
then be viewed as producing additional
output indirectly, via their effect on such
terms of otherwise conventionally defined
production functions. In my appraisal,
interpreting the progress-generating inputs
of any period, and their immediate results,
in this distinctive fashion describes the
most convenient way of separating them
out for specific analysis.

As I shall show later, however, very
similar results are obtained in an analytical
framework of different character in
which practically all long-ruIn increase in
output per man-hour is interpreted as developing
from additional per capita knowledge.'


### ---Economics-1970-0-04.txt---
Even what in the usual neoclassical
model is considered mere factor-substitution
is in this alternative framework viewed
as implying the invention of new types of
goods, viz. of less labor-intensive equipment.
I will explain later why I will
merely keep an eye on the conclusions obtainable
from this construction which is
neither neoclassical nor Cambridgeian,
and why the main part of my analysis
will move in the framework usually referred
to as neoclassical.2 Objections raised
against the neoclassical models are quite
inconclusive in their present form, because
the question is not whether the assumptions
underlying such models are

"'realistic" (a good photograph of reality),
but whether in the real world these assumptions
are violated in such respects

and to such an extent as to render the
analytical results misleading. This the critics
have so far not even tried to show.
I. A Ratio of Benefit-Streamt to Cost and the
Corresponding Average Social

Rate of Return

The costs of generating progress consist
of the costs of producing new knowledge,
and the costs of increased per-capita knowledge-
distribution, and of the extra-costs of
chiangeovers from the use of old to the use of
new knowledge. This seems a reasonable
way of looking at our problem, because
if knowledge in use remained constant
per capita, the rate of progress would be
I In the case of the United States "practically all,"
because here terms-of-trade effects, the consequences of
improved diets, etc. play no major role. There remains
an ambiguity in connection with the consequences of
intersector shifts, on which see the discussion of the two
Kendrick measures in Section III.
2 To illustrate with the simplest kind of CobbDouglas
function applicable to the problem, as well as
this is possible for a single period (t= n):
Q(n) = F( E LR, E KR, LR(n), KE(n),
o o

LA(n), KA(n))L4(n)K$(n)

LR, KR = research-type inputs, in the broad sense.
LE(n), KE(n) = education (knowledge-distribution)
inputs which were applied in the past, or are being applied
in period n, to the labor force of period n, expressed
per capita of this labor force.
LA(n), KA(n) =inputs required for changing over to
improved industrial methods (mainly the obsolescencecomponent
of capital-replacement)..

Lc(n), Kc(n) = those inputs of period n not accounted
for in the argument of F, except that the education inputs
applied in period n to persons who are not yet
members of the labor force belong neither in Lc(n),
Kc(n) nor of course in the argument of F.
The algebraic form and the parameters of F depend on
factors not formalized in the equation, such as abilities
and habits of the population; policies other than those
expressing themselves in the R and E inputs; various
inputs in foreign economies; the relative weight of that
part of the old knowledge which remains useful when
new knowledge is acquired, etc. It will be seen later
that the convenient definition of the rate of technological
progress here is d In F/dt.

In this model avoidance of the implication of economies
of scale would require (3<1-a, instead of the
conventional (3=1 -a, but it may be preferable to face
the problem of such economies as one giving rise to the
qualifications discussed in the opening paragraph of
Section III.

The progress-generating inputs should be defined so
that if the variables in the argument of F assume the
values to which zero progress-generating input corresponds,
then d In F/dt=O (in which case, however,
JLc and IKc would be larger than at positive values of the
progress-inputs and of this derivative). Hence the progress-
generating inputs for period n consist of the
period's additions to the two E terms; and of the increase
of LB and KE expressed per capita of the labor
force of period n, as compared to the immediately preceding
period's Lg and KB expressed per capita of that
period's labor force; and of LA(n) and KA(n). In principle,
one of the magnitudes representing a progressinput
(increased per capita education) could turn out
to be negative, and avoidance of this awkwardness
would then call for some adjustments of terminology,
but in our context this may be disregarded.
If the methods of measuring progress call for limiting
Q to the private sector (usually including governmentowned
enterprise), then Lc and Kc should also be so
limited, but the argument of F should nevertheless include
also the public-sector inputs of the type specified
in the lines under the equation.
The larger cost-base to be defined in Section II involves
"charging" to the progress generated in a period
all progress-generating inputs for the period; the smaller
cost-base to be defined in Section II involves charging
merely the "profit-oriented" progress-inputs, which in
turn implies that the costs of the other progress-inputs
are recovered outside the market process and that the
progress-account should therefore be "credited" with
these nonmarket values.


### ---Economics-1970-0-05.txt---
zero. Whether we wish to distinguish the
specific costs of changeovers as a separate
subcategory is a terminological question,
since technological knowledge is not even
truly acquired before it is introduced on
an industrial scale. But at any rate the
changeover-costs belong among the costs
of progress, and they are quantitatively
significanit though they are frequently
overlooked when benefits are weighed
against costs. The costs of progress so conceived
are the costs of those inputs which
in this paper are referred to as "progressgenerating.
" These inputs become combined
with hiuman talent which is no easier
to define and is certainly no less essential
than are the Ricardian "original" and "indestructible"
powers of the soil.

We may express, as a proportion of the
value of a year's output, the cost of the
goods and services representing the progress-
generating input for the year; and on
the benefit-side, we may set against this
cost the proportion in which technological
progress increases the output flow annually.
3 On specific simplifying assumptions,
the progress-induced proportionate
addition to output will remain permanent
and independent of the output-base. With
a deduction on the productivity-side to
allow for the continuing need to keep per
capita education at any once-attained
higher level,4 we then arrive at the ratio of
an economic benefit-stream to cost; and
after multiplying by 100, we obtain a
magnitude that comes close to what the
economist may consider a measure of the
average social rate of return from the
progress-generating activities of the period
in question.6 Such a concept of an average
rate of return can not, of course, guide us
to the core of the ultimately relevant problems
of resource-allocation, but the concept
will nevertheless be found useful.
Even on assumptions assuring constant
proportionate additions to the output-flow
the delayed start of the benefit stream will
subsequently require allowances in the
nature of discounts, yet for reasons explained
in Appendix A and in Section VI,
this difficulty will not prove critical.6 We
will also have to try to get rid of the effect
of cyclical and erratic forces on our yearly
data.

The simlplifying assumptions required for
defining average social rates of return in
terms of constant proportionate additions
to output are indeed specific. But analysis
based on these assumptions has led to
reasonably realistic results in a good many
other respects. I shall make these assumptions
which should assure at least rough
comparability of the benefit-cost ratios so
obtained with analogous ratios for physical
capital formation on a more-or-less given
level of technological knowledge.7 The
"stepping up" of the absolute contribution
of technological progress when future inputs
(other than progress-generating ones)
raise the output-base must not be allowed
to destroy the comparability of progressreturns
with the returns from investment
in the usual sense. This must be watched
in formulating the assumptions needed for
the justification of such comparisons.


### ---Economics-1970-0-06.txt---
The required simplifying assumptions
are reasonably well satisfied in the macroeconomic
Cobb-Douglas framework. The

CES model also acquires the needed properties
if we work into it a mechanism of
induced invention and of distributiveshares
equilibrium.8 This is explained in
Appendix A. In Sections III and IV we
shall return to the question how much
similarity of the real world with such
models is implied in my paper.
I should add that on both sides of my
benefit-cost ledger I will have to disregard
problems of considerable importance which
either cannot be articulated sufficiently
well, or, if capable of articulation, can
be appraised only in very vague terms. I
will not be able to consider the question
whether all-too-rapid change does not reduce
our well-being "in a fundamental
sense," or, on the other hand, whether at a
stagnant or near-stagnant level of technology
the institutions of Western nations
would be workable at all. Nor will I try to
explore the relationship of technological
progress to population growth and to its
specific consequences.

II. Two Versions of the Social BenefitCost
Ratio, and their Meaning

A substantial part of the progress-generating
costs of each period is incurred by
decision makers subjected to the criterion
of private economic yield. The progresscosts
falling in this category consist mainly
of the costs of privately financed research
and development (R & D) and of the extra
costs of changing over to new industrial
processes. Though we are concerned here
with the problem of social yield, we should
obviously include these private yield-oriented
costs in the cost-base of our benefitcost
ratio.

Another large part of the progress-generating
expenses is incurred by agencies
not institutionally subjected to criteria of
economic yield. The government-financed
R & D9 and that financed by other nonprofit
institutions belong here, and so does
almost all of the cost of increased per
capita education.10 This does not mean
that the nonprofit institutions incurring
these costs are under no political or quasipolitical"
pressure to keep costs down per
unit of service; what it does mean is that
these costs create net income by definition,
i.e., that their net-income-creating property
does not depend on whether they
generate offsetting revenues. Nonprofit institutions,
most prominently illustrated by
governments, but illustrated also by private
teaching organizations, can have defcits
but these deficits are not usually
viewed as negative income such as would
cancel any part of the incomes created by
the institutions. In the language of national
accounting, the services of these institutions
are valued at cost, though, when
appraising the worth of the services, any
individual may find this valuation unconvincing.
In this regard such costs are

treated differently from those of firms,
e.g., differently from that R & D component
which is financed by industry.'2 The
8 See my December 1967 article which is listed in the
Reference section, as are the particulars of the other
references. The bibliography attached to that article
lists earlier writings of several authors on this subject
(including some by the present writer).
9 In the United States the greater part of the government-
financed R & D is performed on contract by industry.
The remainder of the government-financed component
is performed by the government itself and by
other nonprofit institutions (see Appendix D).
10 Training on the job in the business sector, and sales
of books, periodicals and newspapers to consumers
represent exceptions. These are institutionally profitoriented
expenditures. They do not weigh heavily in the
sum total of education costs in the broad sense (and, of
course, these items belong here only if education costs
are interpreted in the broad sense, as they should be for
our purpose).

11 By quasi-political pressure, I mean the pressure to
which trustees and administrators of private nonprofit
organizations are exposed and which they in turn exert
on the staffs of these organizations.
la Industry finances also some of the research performed
by universities and other nonprofit organiza-
tions but more than 98 percent of the R & D financed by
industry is also performed by it. (Continued)


### ---Economics-1970-0-07.txt---
costs of industry show in losses-in deductions
from net income-unless they are
recovered through revenue from sales in
markets.

This difference in accounting methods
corresponds to differences in the strength
of the argument for including the two
types of progress-generating expense in the
costs that make up the denominator of our
ratio of benefit-stream to costs. Where the
expenses need not be recovered through
revenues generated by them, groups of decision
makers were given legal authority to
proceed on the assumption that the knowledge
so produced or distributed has considerable
nonmarket value, i.e., value which
is not measured as part of the economic
benefit-stream. As a rule this is true even
where a nonprofit organization13 makes its
services available through sales in markets,
as is the case in a subsector of the education
sector. Even these nonprofit organizations
are heavily subsidized and/or the
acquisition of the type of service they offer
has been made compulsory by a political
decision limiting the market decision of the
customers to the choice of a source of
supply. The question arises therefore
whether the practice of valuation at cost
to the nonprofit institution, without regard
to recovery of the cost through revenue,
should for our purpose be taken at its face
value in the sense that we net out, and thus
exclude from the cost-side of our social
benefit-cost ledger, the costs of all items
that the national accountant treats as if
they created equivalent values by definition.
In a preliminary step let us do so,
but before trying to appraise the merits
and deficiencies of this version of our ratio,
let us take a look also at another way of
going about the matter.

We may, alternatively, decide to include
in our cost-base the costs of nonprofit institutions
along with those of the profitseeking
ones and thus obtain a ratio based
on all-inclusive costs. This ratio will, of
course, come out lower than the first.
We must remember, however, that even
the cost-base which we call "all-inclusive"
excludes items which are interpreted here
not as costs of progress-generating inputs
but as expenditures influencing the effectiveness
of such inputs.14 In the first
place, we shall have to limit ourselves to
the analysis of American data; hence the
costs incurred abroad belong among those
automatically excluded. Secondly, medical
expenses, other health-related components
of the consumer budget, and types of
public investment bearing on the effectiveness
of progress-generating inputs are
excluded (netted out) even from our larger
cost-base because it is implied that these
expenses become justified by the specific
output which they buy, aside from their
effect on productivity-gains. The same is
not "automatically" taken for granted for
any part of the input we define as progressgenerating.
No part of that input is excluded
from our larger cost-base, and while
part of it is excluded from our smaller
base, we emphasize that both bases require
attention in our analysis. Thereby we
leave a range open for personal (subjective)
valuation of the contribution of progressgenerating
inputs aside from their contribution
to measurable productivity

gains.

The proposition suggests itself that the


### ---Economics-1970-0-08.txt---
average social benefit-cost ratio which a
person is willing to accept as relevant to his
behavior as a voter, adviser, or administrator
is unlikely to fall significantly outside
the range bounded by the two ratios
we have defined. Our lower ratio involving
our all-inclusive cost-base completely
disregards the value of new knowledge
per se and also its instrumental value for
the performance of services made available
to the public through channels other than
those of the market. Our higher ratio involving
merely the costs of the institutionally
profit-oriented progress-inputs implies
that the given scale of the progress-generating
activities of governments and other
nonprofit organizations would be justifiable
even if these activities had no favorable
effect whatever on the market sector;
in view of the actual scale of these activities
in the contemporary American economy,
such a judgment would, I think, be
interpreted by most individuals as describing
valuation at, or near, the upper limit
of the range of reasonable valuations. Thus,
for what we may consider typical valuation,
our lower boundary seems convincing
as such, and our upper boundary seems
not far off the mark as such.

Can these propositions concerning alternatively
computed ratios of social benefit
stream to cost provide guidance for the
appraisal of the results of the American
progress-generating activities? I suggest
that the answer is in the affirmative,
though it must be recognized that an affirmative
answer implies a rather modest
interpretation of what it means to be
guided in one's behavior by an estimated
rate of return. Such a limitation is inherent
in any analysis that focuses on average
rates of return for successive periods. Yet
there is good reason for choosing this focus
in the central part of the present analysis.
We are concerned here with activities
for which the marginal social returns, and
hence the ultimately relevant social allocation
criteria, are not capable of being
estimated by examining private or national
accounts. Even with regard to the progressgenerating
inputs financed by profit-seeking
institutions, significant external effects
and shifts in competitive positions create
differentials between social returns and the
private marginal returns of the innovating
firms. For the diffusion of knowledge in the
market sector reduces their profits, while
gains in competitive position achieved during
the intervening period may prove enduring.
These gains increase the private
returns. At the same time, the spillover
effect of the progress-activities originating
in the nonprofit institutions expresses itself
in part in private market-returns. As
for the progress-generating activities of
nonprofit institutions, obviously there exists
no way of approximating from accounting
data the value of their nonmarket
contribution or that of their spillover effect
on the market economy. When appraising
marginal social returns each individual is
entitled to his own standards, and where
the problem is one of politically determined
inputs, individual valuations will not lead
to the quantity-adjustments which would
work toward equalization of returns at the
margin."5

In such circumstances only one statement
can be made about the marginal
criteria of desirable allocation on a reasonably
"objective" (or general) level of discourse.
The statement is that once a very
large number of individual preferences concerning
the ranking of the specific objectives
of the progress-activities have been bridged
16 As was pointed out above, in some sectors of the
progress-activities-particularly in a subsector of
"education"-political-type decisions, which greatly influence
the aggregate quantity of resources used for a
general purpose, nevertheless leave room for the individual'
s market-choices as to the specific quantities of
service acquired from alternative suppliers. I consider
extension of the range of such choices desirable but
whatever the range is (or might become) its existence
does not contradict the statements in the text.


### ---Economics-1970-0-09.txt---
by political or quasipolitical compromise, it
is in the common interest of the community
to avoid waste. The content of the
political compromise plays a significant
role in shaping the waste-avoidance problem
with which the private as well as the
public sector is faced. Given some specific
compromise on the priority-rankings, the
avoidance of waste requires the solution of
technical problems that are far from trivial
from a professional point of view. Yet any
substantive suggestion on how to satisfy
the marginal criteria of socially desirable
allocation for the progress-inputs inevitably
implies distinctly personal preferences
which need to be reconciled with the
preferences of others. Preferences of this
sort are essentially preferences of an individual
for one variety of political compromise
("as if" utility function) rather
than another."6

This very high degree of subjectivity
does not attach to all substantive propossitions
that may be developed on the yield
of the progress-effort. It is possible to suggest
substantive propositions not only
about the average social rate of return
from each period's progress-activities but
also about tlle algebraic sign of the difference
between the average and the marginal
social rate to which a much lesser
degree of subjectivity attaches. Event if an
individual were in a position to exert decisive
influence on the composition of the
progress-generating inputs-or if for any
reason he ideiitified himself fully with politically
determined priority-rankings-he
would still be apt to find a considerable
gap between the average and the marginal
social rates from the progress-activities of
a period, provided that these activities are
pursued on a significant scale. The gap
between average and marginal rates develops
because high talent shades over
into lesser talent, and because the solution
of problems is an inherently sequential pro.
cess. This is ain essential property-we
may perhaps say a Schumpeterian property
-of the progress-generating activities. By
way of "transitivity," the following propositions
follow from the average-marginal
gap in the progress activities:
a) Each person has reason to consider it
a necessary condition of his approval of a
set of allocative decisions that by his
standards the average social rate of return
from the progress-activities should exceed
the marginal social rate of return from
physical investment "at a given level of
technological knowledge";

b) the necessary condition requires also
that the average rate of progress-return
should exceed the marginal market rate
from the current flow of physical investment,
since we may assume that this

market rate is no higher than the marginal
social rate from physical investment;
c) finally, no undue stretching is involved
in extending this condition to the
requirement that the average rate of progress-
return should exceed the rate of return
on the capital stock-say, the algebraic
product of the share of capital in income
with the ratio of yearly output to
capital"7-since we may assume that normally
this rate is not very different from
the marginal market rate on physical investment.


It therefore seems reasonable to conclude
that from each individual's point of
view it is a necessary condition of good
allocation that by his standards the average
social rate of return from the progressgenerating
activities of the successive

periods should tend to exceed the profit
rate on physical capital, i.e., should exceed
the rate which in traditional theory is interpreted
as the net marginal productivity


### ---Economics-1970-0-10.txt---
of capital. About the relationship between
the average rate from the progress-activities
and the conventionally estimated marginal
product of capital it is possible to
make reasonably "objective" numerical
statements, though unfortunately not very
precise ones, because some statements that
can be made about the average rates from
the progress-activities imply merely value
judgments that are very widely shared.
In particular, a great many individuals
would agree that an unduly strict criterion
of overall social profitability, and
hence of required average social rates of
return, is set by charging the full cost of
the knowledge-acquisition activities of
governments and of teaching and research
institutions to measured productivity
trends, as if the nonmarket contribution
of these activities were zero. At the
same time, most individuals would, I
think, agree that it is useful to inquire into
the question of average social rates of return
also by valuing the nonmarket contribution
of the politically or quasi-politically
determined progress-inputs at full
cost to the nonprofit institutions as if no
favorable effect on the market-sector were
required to justify these inputs on their
present scale. I surmise that the valuation
convincing to most persons would be located
either within, or not significantly
outside, the range described by these two
frames of reference for valuation. In this
sense, our two benefit-cost ratios describe a
range which should be of some use to most
decision makers, particularly because the
usefulness of the framework is admittedly
limited. A verdict of social profitability as
expressed by adequate average rates of
return is compatible with anyone's personal
judgment that the progress-activities
could be made even more profitable by
changing the controversial content of
political compromises concerning the composition
of the inputs. While the present
paper will not lose sight of these highly
controversial questions involving marginality,
I will try to separate these from
the problems that can be analyzed on a
reasonably "objective" level. In practice,
this objectivity becomes reduced by the
need to fill in gaps in the available data by
operations that at times become very
speculative.

More ambitious contributions of other
authors are revealing in many ways but
they do not remove the limitations we
need to observe in our present context. For
example, from empirical investigations one
may conclude that additional educationeducation
at the margin-is profitable by
market criteria alone, even without regard
to socially desirable external effects and to
the nonmarket value which is acquired.'8
This is a significant result, and it has important
applications, but it cannot be put
to use in our present analysis. Additional
education is profitable very largely be
cause our technology is becoming increasingly
complex, a process which in turn
presupposes additional education. Recognition
of this takes us back to the problem
of the social worth of the activities that
lead to the increasing complexity of our
technological and organizational methods.
III. The Residual: A Roundabout Way of
Measuring Productivity-Increases
The method here to be used for estimating
productivity gains leads to higher
estimates than do some procedures that
have occasionally been suggested. Yet I
feel that the method needs to be defended
not so much against methods that attribute
a smaller component of growth to
progress, but against methods that attribute
to progress an even larger growthcomponent.
As concerns the United States,
an argument could be made for the view
that practically all long-run increase in
output per man-hour (or possibly more
than this) involves invention and/or
18 See particularly Gary S. Becker.


### ---Economics-1970-0-11.txt---
additional knowledge-distribution because
even insofar as the neoclassical approach
attributes a growth-component to mere
factor-substitution-i.e., insofar as physical
investment exceeds the equivalent of
mere "widening"-such investment presupposes
the invention of new equipment
of lesser labor-intensity.'9 Practically all
long-run increase of output per man-hour
(and even the prevention of some decrease
resulting from land-scarcity) requires at
least some amount of additional per capita
knowledge which may or may not express
itself in the production of new types of
equipment20; the reverse proposition that
making use of new ideas requires new capital
goods is hazy because new capital goods
result from new ideas applied to already
existing types of equipment. If one wanted
to follow through consistently the implications
of the conception that all long-run
increase in man-hour output reflects technological
progress, one would arrive at even
higher productivity-gains than those which
we shall assume (though at more costly
ones); the progress-generated productivity-
gains so defined would show a rising
tendency, as will ours; and these progressgains
too would point to high but somewhat
declining rates of social return, similar
to ours.2' We will not follow through all
the implications of this "inclusive" conception
of progress though one of its several
implications will be carried over into the
framework I am using because in mine, too,
economies of scale will become merged
with "progress." My main reason for not
following through all the implications of
the most inclusive conception is that some
possible ways of increasing man-hour
output require new ideas only in a very
modest sense of this term, i.e., require
merely somewhat trivial new ideas (resulting
in slow changes of the physical
composition of the capital stock). Thisis
what in my appraisal justifies the "neoclassical"
procedure of attributing part of
the increase in output per man-hour to
changes in input-proportions aside from
technological progress. But this way of
looking at the problem still leaves one with
a preference for principles that lead to
higher progress-estimates than do the principles
suggested in some other contributions
to growth theory.

My analysis will rely on the method of
the Residual as it is employed by John
Kendrick for measuring the proportionate
rate of increase of productivity. From this
productivity-increase I will derive the
magnitude I consider the benefit resulting
from the progress-generating costs.
As is well known, the method of the
Residual attributes to technological advance
that increase in output which is not
attributed to the increase in physical
inputs given their base-period productivities.
The measurement of productivity assumes
measurement of the price-corrected market


### ---Economics-1970-0-12.txt---
value of output, and in the United States
this is practically the same thing as to say
that we are concerned here with a Residual
observed as a proportion of the privately
produced output.22 The procedure has the
implication that the analysis of the determinants
of technological progress is as
yet in a tentative stage, so that if for example
R & D, the increase of per capita
education, the extra-inputs needed for
industrial changeovers, and some other
items to be discussed in this paper make up
the identifiable progress-inputs (as I
suggest they do), we cannot at present
expect good results from regressing output
on all these specific inputs along with the
conventional inputs specified in the production
functions. I feel convinced that
realism does require recognizing this limitation
but that this does not mean that we
are left with no good argument for relating
the Residual to the inputs and costs I
defined as progress-generating.
Regression analysis of cross-sections of
individual industry groups and of firms
enabled investigators to obtain confirmation
of the hypothesis that productivityincreases,
as estimated by the method of

the Residual, are positively correlated with
R & D-intensity.23 On the other hand,
spillover effects from the public into the
private sector, the diffusion of knowledge
within the private sector, and the narrowness
of the concept of R & D-intensity
as compared to our concept of progressgenerating
input make it desirable to ask
the question of the relationship of the
Residual to progress-inputs also in broader
intertemporal terms. When the question is
so posed, it calls for exploring whether the
time-sequence of productivity increases, as
measured by the Residual, bears an understandable
relation to the time-sequence

of progress-generating inputs. We shall see
that this is indeed the case, but in such
work there is at present only limited room
for regression analysis and for "testing" in
the technical sense. This is because for
some purposes for which time-sequence
comparisons of productivity increase with
inputs are used it is advisable to rely on a
rather small number of typical values of
the variables for successive periods of
considerable duration; high serial correlation
and lags reduce the usefulness of data
reflecting yearly changes.24 Emphasis will
22 However, the base in relation to which the pro-
ductivity-increase is measured is somewhat inflated as a
result of the fact that the private sector includes some
activities for which no productivity-increase can be
measured (such as the activities of private nonprofit
organizations, and paid household work); and it includes
some activities for which the output indices
actually used do not measure productivity-increases
adequately. A minor terminological awkwardness to be
noted here is that government-enterprise, in contrast to
general government, belongs in the "private" sector
See also fn. 33.

23 Kendrick (pp. 182-186) has found a statistically
significant correlation between the increase in his total
factor productivity and the "R & D-intensity" of
specific industry groups. His discussion of this problem
is based partly on his own research and partly on that of
Nestor Terlecky.

Jora Minasian in his May 1969 article, and in his
earlier work there quoted, reports on a positive re-
lationship between R & D and productivity-increase in
a cross-section of chemical firms. He regressed Value
Added on labor, capital and R & D. His estimate of the
rates of return from R & D disregards a substantial
proportion of the progress-costs with which we shall be
concerned in this paper, but what matters here is the
existence of a positive relationship between R & D and
productivity-increase. However, the fact that on the
cost-side merely R & D enters into Minasian's analysis
when he estimates rates of return may provide part of
the explanation why these rates are hard to reconcile
with the results of a survey (questionnaire study) on
which Edwin Mansfield reported also in the May 1969
Papers and Proceedings. Firms must be acutely aware
of the changeover costs and of various other costs to
be considered in our analysis.
24 This, however, is not all "black and white." For
example, regression analysis based on yearly data is not
useless when it comes to examining the question whether
in the United States the rate of increase of productivity
has been rising.

Testing the upward tendency of the rate of produc-
tivity increases (semi-logarithmic nonlinearity) with
yearly data is of interest only for the period following
1919 (or conceivably for a period ending in 1919).
Kendrick has shown conclusively that if one decides to
limit oneself to semi-logarithmically linear relations of
productivity to time (i.e., of the logarithm of productherefore


### ---Economics-1970-0-13.txt---
be placed on tendencies observable
by comparing typical data for a
limited number of consecutive periods, and
importance will be attributed to the fact
that when the Residual is used as a point of
departure for estimating benefits, and the
costs are defined as those of our progressgeneratinog
inputs, the benefit-trends and
cost-trends show a fairly consistent relationship
that "makes sense."

The method of the Residual can be employed
in different ways. Kendrick has
measured a magnitude he calls total factor
productivity, this being a productivity
measure from which he excludes the consequences
of allocational shifts among sectors
(his industry groupings). Hence here the
shifts, which in fact have favored the
higher-productivity sectors (industry
groups), are made to express themselves as
increases of weighted physical inputs rather
than of productivity. Further, Kendrick
has measured a magnitude which he calls
"output per unit of total input" and which
differs from his "total factor productivity"
only in that it registers the results of the
intersector shifts as productivity-increases
rather than as increase of weighted inputquantity.
Both concepts relate to the private
domestic economy.

Both measures used by Kendrick register
larger productivity-increases than those
consistent with the usual two-factor classroom
models, because his input-estimates
include estimates of farm land and of real
estate (site land) as part of his estimate of
capital. The relative fixity of this input
reduces the growth-rates of physical inputs,
thus increasing the Residual. Aside
from this, some technical considerations
suggest that, by their own standards, the
Kendrick measures of productivity-gains
err somewhat on the high side but other
considerations point in the opposite direction.


From 1900 to 1929, factor productivity,
including the favorable effect of intergroup
shifts, rose at an annual compound
rate of 1.8 percent (during the last ten
years of the period it rose at 2.0 percent);
from 1929 to 1948 the rise occurred at the
rate of 2.3 percent; from 1948 to 1966 at
2.8 percent, with no acceleration during
these past eighteen years. If we exclude
from the productivity-increase the favorable
effect of Kendrick's intergroup shifts,
we obtain somewhat smaller productivity
increases but broadly similar trends for
these, except that some further acceleration


### ---Economics-1970-0-14.txt---
is observed for the comparison of
1957-1966 with 1948-1957.25 Kendrick's
published series end with estimated values
for 1957, but I made use of his somewhat
revised figure for 1957 and of his figures for
later years, i.e., of unpublished data he
kindly let me see, though with emphasis
on their preliminary character.
Thus, from the period 1929-1948 to the
present, the rate of increase of productivity
has risen by about 20 percent of itself; or
by about 30 percent, if the effects of the
intergroup shifts are excluded from the
productivity-increase. From the opening
decades of the century to the present, the
rate of productivity-increase has risen by
about 60 percent (a figure which becomes
75 percent with exclusion of the intergroup
shifts) .26

Indications of a rising tendency of the
rate of productivity-increase are consistent,
though the rise is small as compared
to the rise of the recorded progress-generating
inputs, particularly of R & D, in
relation to output. The discrepancy becomes
significantly reduced by reasonable
allowance for progress-inputs that are not
recorded as such.

IV. The Framework for Productivity and
Benefit Measurement: How Serious
are its Pitfalls?

1. Using the All-Inclusive Cost-Base Requires
a Deduction on the Productivity-Side.
We start with a pitfall for the avoidance
of which a definite prescription can be
given. If we say, as we do when using our
all-inclusive cost-base, that productivityincreases
cost us inter alia the inputs

needed for increased per capita education,
then we are implying that on the benefitside
the bulk of this particular cost must
be deducted from the productivity-increases.
The reason is that a once-attained
higher level of per capita education must
subsequently be maintained to keep the
level of knowledge from becoming reduced.
In this regard the characteristics of education
are different from those of R & D or
from those of the costs of changeovers.
What needs to be deducted on the benefit-
side is the "bulk" of the cost that had
been entered for education-not the entire
cost-because we are relating our magnitudes
to output, and constancy per unit of
the population means a gradual reduction
per unit of output. However, on reasonable
quantitative assumptions the "discounted
value" of the difference between these two
growth rates is small, and hence I will
indeed deduct the bulk.27

2. The Embodiment-Disembodiment Issue,
the Quality-Quantity Controversy and the
Importance of "Dimensions of Improvement.
"'

We shall now consider the main problem
of the present section. Our reasoning can
be Dlaced into frameworks of different
25 On this basis the following results are obtained:
from 1900 to 1929 the rise was 1.5 percent (for the subperiod
1919-29: 2.0 percent); from 1929 to 1948 it was
2.0 percent; from 1948-57 it was 2.3 percent; and for the
following nine years 2.6 percent.
26 Dension, who has made an attempt to separate
quantitatively the effect of different sources of growthan
attempt which because of interactions between his
"sources" I am inclined to consider too ambitiousused
his estimates also for explaining what results he
would obtain by a method much more similar to that of
Kendrick and of other investigators. It turns out that
from the period 1909-29 to 1929-47 he would obtain a
45 percent rise of the rate of productivity-increase. See
Edward F. Denison, pp. 148-49.
27 If progress were limited to a brief interval, then
not merely the "bulk" but, for practical purposes, all of
the cost-item entered for increased per-capita education
would have to be deducted on the benefit-side of the
ledger. The increase in per capita output resulting
merely from, say, a 2 percent yearly increase of the
capital-labor ratio "at a given level of technology"
would require a negligible correction of the kind described
in the text. But if we consider a very long sequence
of periods with technological progress and assume,
say a 2 percent yearly increase in per capita output
then, with a 10 percent discount rate, roughly
four-fifths (or a shade less) of the cost-item entered for
increased per capita education should be deducted on
the benefit-side. I will in fact deduct about four-fifths,
with some rounding where this is called for (see fn. 40,
for the calculation of the 1966 and the 1953 yield).


### ---Economics-1970-0-15.txt---
kinds as long as these have enough in
common with the models discussed in
Appendix A. For us the essential property
of these constructs is that they are models
of disembodied progress in which the
elasticities of output with respect to the
inputs are reasonably well approximated
by the distributive shares of the inputs,
and in which distributive shares remain
reasonably stable. To the extent that these
assumptions are unrealistic our analysis
needs to be adjusted, and in Section III, I
have already pointed out that one type of
adjustment does not change the conclusions
very much. What should we think of
the "disembodiment" implication of our
framework, i.e., of the implication that
progress leads inter alia to the production
of new types of goods rather than depends
on new equipment?

Models of so-called embodied progress do
not seem convincing to me on logical
grounds, and to my knowledge, no claim of
empirical superiority has been made, let
alone established, for these frameworks. As
I said, all progress is necessarily disembodied
in the sense that new ideas must
always be put into effect with reliance on
the initially given resources. This is an
essential constraint under which all economies
operate. Improved production with
the initially given resources then leads to
more and better capital goods-hence to
the replacement of old with superior equipment
and structures ("obsolescence")-
and it yields more and better consumer
goods. But all this represents forward, not
backward, embodiment.

For various purposes it has proved rewarding
to interpret progress as "factoraugmenting"
; yet even if progress is interpreted
as purely capital-augmenting, this
alone does not change the framework into
one of embodied progress. Using a framework
in which some input is "augmented"
merely means expressing the productivityincrease
of that input as the equivalent of a
specific quantity-increase and expressing
the productivity-increases of other inputs
as resulting from complementarity. No
empirical superiority has been claimed for
this degree of unilateral emphasis on
capital-i.e., for assuming pure capitalaugmentation-
but at any rate more is

needed for obtaining "embodiment," in a
distinctive sense of the term.
If the embodiment hypothesis is not to
lose what I consider its essential characteristic,
the additional assumption must be
made that each period's capital-goods
output can become subject merely to a
single dose of augmentation which describes
a specific level of technology; and
that only the next period's capital goodsthe
next vintage-can become subject to
the next dose. This, I submit, is an unconvincing
assumption if we take into

account why next period's capital goodsnext
period's equipment-models-are superior
to this period's equipment-models.
They are superior because this period's
initially given equipment-models and labor
perform better than how these inputs
would have performed without the new
ideas. So why should the possibilities with
a period's capital goods-a given vintage
become exhausted during a specific period
for which a specific level of technological
knowledge is defined? If the possibilities do
not become exhausted in this fashion, then
a level of technological knowledge ceases
to be uniquely described by (or embodied
into) a specific vintage of capital goods.
In the absence of strong empirical support
for the hypothesis of pure capitalaugmentation
and of the required additional
assumption, I have fewer misgivings
about disembodiment than about embodiment.
Yet while so-called embodiment
makes too much of the role of new physical
capital formation in bringing about progress,
disembodiment implies overlooking
its progress-promoting effect which probably
does exist. The existence of this effect


### ---Economics-1970-0-16.txt---
complicates matters, and I shall soon
return to this difficulty.

We must be mindful also of another
property of the framework here employed.
We are implying, as does the method of the
Residual in general, that it is advisable to
try to translate quality-improvements of
consumer goods into additional quantities
and yet to express productivity-increases
of capital goods and of inputs in general as
quality-improvements for given physical
quantities.

In my appraisal there is no reason to
become disturbed about this lack of symmetry
between the interpretation of improvements
of inputs and that of improvements
of consumer goods. The alternative
would be that of lumping together our
"progress-generating inputs" with the
other inputs, and of interpreting the research
worker not as one who increases the
productivity, and hence the quality of the
inputs with which measured output is
produced, but as one who has the same
kind of productivity in the production of
measured output as do (say) production
workers.28 However, for my present purpose
it would be wrong to cut the story
short in this fashion. I am concerned with
the question how the social yield of progress-
inputs compares with that of other
inputs, and this concern calls for placing
emphasis on the indirectness with which
the progress-inputs produce additional
measured oUtput.29 Thus, for the present
purpose it is useful to have a framework
in which the progress-inputs increase the
" quality" of the other inputs and in which
the other inputs produce quantities of
output.

In practice the effort to let the productivity-
increases of inputs express themselves
in quality-changes for given physical
quantities, and to translate the qualityimprovements
of consumer goods into

quantities, gives rise to many procedural
difficulties and the methods for taking care
of these are admittedly very imperfect. In
particular, it has been argued that the
quantitative allowances for the improvement
of consumer-good quality are not
only very crude (which they certainly are),
but in all probability also insufficient. In a
general way it makes good sense to suspect
that the allowances are quantitatively
insufficient, but we should nevertheless
note that the standard of living of Western
countries is rising rapidly, and it is particularly
true at higher levels of income
that the market basket illustrates instances
of consistent quality-deterioration
as well as of improvement. Nevertheless,
for consumer goods I shall make an allowance
for the assumed underestimate of net
quality improvement (see Appendix C).
Last but not least, an inevitable imperfection
of frameworks of the kind here
employed results from two-way interactions
between physical capital formation
and technological progress. As I said above,
my misgivings about the method by
which vintage models deal with this relationship
does not mean that I consider it
harmless to disregard the interactions.
Starting with initially given resources,
the chances of substantial and continued
improvement are undoubtedly greater if
the processes used are rich in changeable
properties-i.e., in what I elsewhere called
dimensions of improvement-than if these
processes offer improvement possibilities
only in a few directions; and the use of a
large variety of instruments of production
opens up a large number of dimensions of
potential improvement. In another study
I found strong indications that the results
of learning-by-doing are significantly influenced
by differences in the available
28 See Zvi Griliches and Dale Jorgensen.
29 Looking at the matter in this fashion is of course
compatible with using models that represent progress
as the "augmentation" of inputs. Indeed, models of
this sort usually draw a sharp distinction between "inputs
measured in physical units" and "inputs measured
in (augmented) efficiency units."


### ---Economics-1970-0-17.txt---
number of dimensions of improvement.30
So physical capital formation helps the
improvement process, if for no other reason
than because it increases the number
of directions in which improvement can
proceed. The interaction is mutual: we
have seen that without any technological
progress whatever, net physical capital
formation could not exceed the equivalent
of the growth of other inputs because less
labor-intensive machines are different machines.
Some readers might therefore prefer
the alternative framework outlined at
the outset of Section III (see also fn. 21).
On the whole, I do not believe that we
would become guilty of gross error by disregarding
these mutual interactions when
using our framework for distinguishing
between progress-generating activities and
physical investment provided we stay
aware of the fact that large changes in the
scale on which either of these activities is
pursued have implications for the conditions
under which it is possible to engage
in the other type of activity. Of this we
must remain aware.

V. Progress-Generating Inputs and Costs:
The Significance of the

"Unrecorded" Items3"

From 1953 to 1966 total recorded R & D
rose from $5.2 billion to $22.2 billion, and
this represents a better than twofold inincrease
when R & D is expressed as a

proportion of the privately produced
GNP. This proportion rose from 1.6 percent
to 3.3 percent.32 While an appreciable
part of this increase was "real"-i.e.,
"physical"- considerable importance attaches
to changes in relative values.
The question of the weight of the realinput
component versus the relative-valuation
component of the increase of the ratio
of R & D-cost to private GNP is discussed
quantitatively in Appendix B. This question
is of some importance because the
relative-valuation component represents
in part rent-formation-a mere transfer of
income-rather than a rise of social costs.
However, in any event, even the relativevaluation
component represents in good

part rising social costs, because the relatively-
rising salaries of research workers
(and also of teachers) have attracted into
the progress activities individuals whose
qualifications and productivity would be
higher also in other occupations. This must
have been the driving force of the transfer.
Such a phenomenon is accompanied by
rent-formation, but I suspect that in the
progress-activities-particularly in reseach
and higher education-the rent-formation
accompanying the rise of social costs is
smaller than in the types of activity mostly
referred to when the nature of the transferrent
problem is illustrated.33


### ---Economics-1970-0-18.txt---
We have just seen that from 1953 to
1966 the ratio of total recorded R & D cost
to private GNP rose by more than 100
percent. For the period 1929-1953 only
sporadic and incomplete data are available,
but there is good reason to conclude
that during this period there occurred a
very significant increase of R & D inputs in
relation to real private GNP, and that in
terms of undeflated values the increase was
even more substantial than in real terms
(see Appendix B). These increases are out
of line with the rise of the rate of productivity-
increase. The latter is a 60 percent
or, at most, a 75 percent increase even if we
go back to 1900-1929 and thus compare
the pre-1929 period with the present.34
The efficiency of the edutcational complement
of research would to some extent
become adjusted to the requirements set
by new knowledge-production even if
per capita knowledge inputs did not rise in
quantity. In reality the quantity of these
inputs has risen appreciably. Partly from
HEW data and partly from those of Fritz
Machlup, I infer that from the calendar
year 1953 to 1966 the cost of increased per
capita education rose from the equivalent
of 0.3 percent of private GNP to the
equivalent of about 0.5 percent (see Appendix
B). Increased education works itself
out with significant lags, but an appreciable
rate of increase dates back far
into the past. Therefore, trend-appraisals
are not appreciably distorted if we associate
0.3 percent with the 1953 benefit, and
0.5 percent with the 1966 benefit, though
the levels of the rates of progress-return
are somewhat distorted-i.e., are calculated
as if the lag did not exist-a fact that
need not disturb us much because the
smaller cost of the past, which in view of
the lag would represent a more appropriate
present charge, arose correspondingly
earlier and for that reason should be
viewed as the equivalent of more than the
undiscounted figure suggests.35
As for the industry-financed (institutionally
profit-oriented) R & D, this rose
from $2.2 billions in 1953 to $7.2 billions in
1966. When expressed in relation to private
GNP we obtain a rise from 0.7 percent to
1.1 percent. In Appendix B it is shown that
the post-1953 rise of this particular proportion
is likely to have been mainly (perhaps
entirely) the result of the increase in the
relative prices of R & D inputs. That is to
say, here the relative-valuation component
accounts for practically the entire effect,
while for total R & D, the real-input component
also was significant (even for 1953-
1966).

Regardless of whether we are interested
in yields on our all-inclusive cost-base or in
yields on a cost-base limited to institutionally
yield-oriented progress-inputs, exclusive
concern on the cost-side with

recorded R & D and with increased per
capita education would lead to the twin
conclusions that yields have been declining
steeply and that they nevertheless still are
at a "perplexing" level. Such calculations
are misleading because they disregard the
fact that obsolescence in the conventional
sense, as well as some costs of a character
similar to that of obsolescense, and also
some costs of a different kind, are omitted
from the recorded progress-generating costs.
This is so even if for the present purpose we
regard the education costs in Machlup's
broadened sense as "recorded." Many of
the unrecorded costs, including obsolescense
itself, fall in the category which in
Section I, I called costs of changeovers to
gress-sectors are the favored ones the significance of
this phenomenon is reduced by the fact that the labor
force of research establishments is far from homogene-
ous. The reason why a high proportion of these workers
is facing increasingly favorable market conditions is
that a high proportion is gradually improving its qualifi-
cations which are getting to be considered increasingly
valuable also in other occupations.
34 See the concluding part of Section III.
36 The appropriate discount rate is obviously in excess
of the growth rate of the output base.


### ---Economics-1970-0-19.txt---
new methods. Their weight is substantial,
but for a given rate of technological progress
these costs are unlikely to change
much in relation to private GNP. Hence
allowances for these costs reduce not
merely the rates of return but also their
downtrend.

It is easy to become misled into the
belief that the method of the Residual
automatically implies proper recognition
of obsolescence. In a sense it does recognize
obsolescence but it does not do so in the
sense relevant to our problem. The method
is not intended to, and it does not, take
account of the fact that in any of the "base
periods" with which the "present period"
is compared, the now old technology was
new.36 Hence at that time the now old
technology gave rise to obsolescence and
it required various instrumental activities
that represented costs of changing over to
new methods. These costs would not have
been incurred in the base-period and they
would not be incurred now if there had
been no technological advance in either
period. In a continuously progressing
economy the inputs used for the obsolescence-
component of capital-replacement
are continuously withheld from the production
of consumer goods or from net

investment (or from the public-sector
counterparts of these). 37

A list of the items that may be regarded
as unrecorded costs is presented in Appendix
C, along with explanations. In spite
of the large number of items on the list,
we may take it for granted that only one
item-conventional obsolescence-corresponds
individually to an appreciable proportion
of private GNP. As for the other
items, in some cases it is doubtful whether
they have a legitimate place on the list and
at any rate, even their joint weight cannot
be appreciable when expressed in relation
to private GNP. Among the items listed
in Appendix C there are only a few that
would be suspect, even at first sight, of
possessing large weight. Item 5 of the
Appendix-the cessation of learning-bydoing
with the old equipment and for the
old product when new methods are introduced-
belongs among the suspects, but
in reality it is unlikely to be of great quantitative
importance because in each period
the discarded methods are apt to be those
for which learning-by-doing has largely
run its course. Item 6-the technical imperfections
of recording-must have been

large in the early decades, but we are concerned
here mainly with the post-1953
period, and our few references to pre-1953
periods were based on attempts to get
indirectly at what now would be "recorded"
R & D. For the present purpose I shall
assume that a deliberately high estimate is
obtained of the joint weight of the unrecorded
costs other than obsolescence in the
usual sense if these other costs are put
at 2 percent in relation to private GNP;
and that a deliberately low estimate is
obtained by putting these items at 1 percent.


What mainly matters is obsolescence in
the usual sense. The fact that we consider a
component of capital consumption a cost
even though we are moving in a GNP
framework may seem paradoxical, but for
no good reason, because while for our
periods GNP may safely be used as a
proxy for NNP, we must take into account
that if capital consumption had all along
been smaller, then any given GNP would
have been a proxy for a higher NNP.
Experts have repeatedly expressed the
view that practically all of the capital
consumption and replacement, which corresponds


### ---Economics-1970-0-20.txt---
to more than 10 percent of GNP,38
reflects obsolescence, i.e., a result of technological
progress. Such a statement needs
to be reinterpreted for our purpose. If the
service lives of capital goods were to be
prolonged significantly, then, unless the
equipment-goods and the construction
industry were to raise their costs and selling
prices for the sake of durability, the
users of capital goods would be spending
more on repairs and maintenance. The
question here is: how much more? This, I
believe, has remained an unexplored question
which would deserve a research effort.
In the absence of dependable information,
I suggest two guidelines for the present
purpose. I will base my subsequent reasoning
more on the second than on the
first of these.

1) By assuming that total capital consumption
typically amounts to about 12

percent as expressed in relation to private
GNP, and by charging one-fourth of this to
technological advance for obsolescence, one
is unlikely to overcharge the progressaccount.
One way of visualizing the implications
of a charge of such size-3 percent
of private GNP-is to note that it is
consistent with the hypothesis that in
order to arrive at a doubling of the useful
service lives, the users of capital goods
would have to increase their present expenditures
on current repairs and maintenance
by the equivalent of about 50

percent of what would then become their
reduced current capital-consumption cost.39
For structures which represent a very
weighty component of the capital stock,
this is quite likely to be an overstatement
of the costs of extending the now usual service
lives, hence an understatement of the
costs to be set against technological advance
and against the rise of living standards
in which it expresses itself. The same
may be true of some types of equipment,
but there undoubtedly exist types for
which these numerical assumptions understate
the costs of the extension of service
life, and thus overstate the cost of progress.
On the whole, charging 3 percent of private
GNP is unlikely to err (or to err much) on
the high side of costs, and this charge
might well err on the low side. As long as
we try not to overcharge we may add
merely 1 percent of the private GNP for
unrecorded costs other than obsolescence,
i.e., for the other costs listed in Appendix C.
We arrive at 4 percent of private GNP for
what I consider a "moderate" estimate of
the unrecorded costs, though I will not
build much on this figure.

2) An immoderate overstatement of the
obsolescence-cost of progress would be
obtained by charging to progress the
entire capital-consumption-i.e., about 12
percent of private GNP-and by adding 2
38 From 1919 to 1955 Kuznets' capital consumption
allowances move in the range from 10.9 to 15.1 in relation
to GNP (constant prices); at the beginning and
also at the end of the period the figure was almost precisely
in the middle of the range. This implies Kuznets'
Variant III of output. Using other Variants of Kuznets,
wider ranges are found for movements of the depreciation
ratio in the region above 10 percent. See Kuznets'
Appendix A, Table R-2, p. 487.
The capital-consumption ratio implied in the Capital
Stock Study of the U.S. Department of Commerce,
Office of Business Economics seems to be somewhat
smaller than Kuznets', but also typically somewhat
higher than 10 percent. See for this e.g. Robert C. Wasson'
s articles in the Survey of Current Business, particularly
in the December 1966 and in the February 1969
issue. However, proper interpretation of the relationships
here considered requires also use of computer
output available from Mr. Wasson's office. My evaluation
implies a reasonable "blowing up" of the Commerce
Capital Stock Study depreciation figures, in view of the
fact that the figures of the Study relate only to the
business sector including farms, i.e., exclude the residential
sector as well as general government. I was using
the Capital Stock Study's straight-line depreciation
estimates and was using them on "Cost 1" basis. This
cost-basis makes the method of deflating comparable to
that applied to the Commerce estimates of the GNP.
39 Any hypothesis of this kind has the realistic impli-
cation that in a progressive economy the "representa-
tive" physical asset is produced in such a way as to be
in some sense "too durable." This has several reasons,
one of which presumably is that lack of durability is
not the only unfavorable property of a "flimsy object."


### ---Economics-1970-0-21.txt---
percent for the other unrecorded costs,
thus obtaining a charge corresponding to
14 percent of private GNP. But it will
prove to be important to take a look also
at the progress-yields obtained on this
immoderate assumption. I will build on
the fact that the 14 percent charge is excessive
by a significant margin.

VI. Levels and Trends of "Objective" Yields
and Observations on Matters Calling
for Subjective Appraisal

The method by which the quantitative
yield-appraisals were obtained is based
on the discussion in Sections III and V
and the details are explained in the footnote
below.40 Note that on the productivity-
side we rely on Kendrick's estimate of
annual productivity-increase including the
effect of intergroup shifts (2.8 percent),
but the reader can convince himself that
the result would be influenced little by
the use of estimates that exclude the intersector
effect (for the most recent period
this would have been an increase of 2.6
percent p.a.). To me it seems reasonable to
relate also the intersector effect to progress-
generating inputs.41

Many statements one would like to be
able to make about these results would be
much too risky for presentation. But there
are two statements which I consider to be
safe.

The first of these is that the present
average social rate of return from the
progress-activities is substantially in excess
of 13 percent on the all-inclusive
cost-base, and substantially in excess of
18 percent on the cost-base limited to
institutionally profit-oriented progress-inputs
("reduced" cost-base). The rates of
13 percent and of 18 percent would imply
charging all capital consumption to progress,
as if extension of service-life were
entirely costless ad infinitum except for
continued maintenance costs at the now
usual level. If we charged one-fourth of the
capital consumption to progress, and made
a moderate charge also for other unrecorded
costs, we would obtain a 31 percent
rate of return on the all-inclusive cost-base,
and a 55 percent rate on the reduced costbase.
These particular charges rest on


### ---Economics-1970-0-22.txt---
guesswork (I believe of a plausible kind),
but it may be regarded as a fact that 13
percent and 18 percent understate the
average rate of progress-return by a substantial
margin and this will prove significant
for the present analysis.

The second statement I consider to be
reasonably safe is that the rates of return
show some degree of downward tendency
in the long run (see here also Appendix B).
The qualifications one might want to add
to this statement are not sufficiently
persuasive to arouse much suspicion in the
diagnosis that such a tendency is observable,
though I would not want to assert
that my figures give an accurate idea of
the extent to which a downtrend has manifested
itself. On what I defined as the reduced
cost-base this downward tendency
has been very mild, but it would not
entirely disappear on any cost-base that to
me seems reasonably chosen.

The suggestion of a long-run downward
tendency of the average rate of return is
not misleading, because the long-run increase
of the costs of progress-inputs has
all along exceeded the rise of the productivity-
gains when both are expressed in
relation to the same output-base. In particular,
the approximately constant uptrend
of productivity during the post-war
decades has been associated with increasing
progress-costs even if we consider
merely the costs of the privately financed
progress-inputs. The second half of the
present decade does not fit well into the
pattern, but it is too early to try to interpret
the record of the very recent inflationary
years, not only because the rate of
resource-utilization in the American economy
has risen suddenly but also because
the rate of increase of the progress-inputs
has been tapering off. This tapering off
occurred partly because the rate of increase
of space research tapered off and
then space research reached a peak in 1966;
and partly because even more recently
there also took place anti-inflationary
downward revisions of government programs
(which is not to say that the rate of
increase of R & D might not have declined
even aside from these circumstances).
Consequently it is to early to ask whether
in the mid-sixties the downward tendency
of our benefit-cost ratios was or was not
interrupted, but further productivityincreases
would have to assume a very

unlikely course to invalidate the conclusion
that there did occur some degree of
long-run decline, even with reasonable
allowances for lags.

We should note that the downward
tendency of the progress-yields would be
slightly smaller than our figures suggest if
we assumed a small rise of the rate of
increase of factor productivity from 1953
to 1966. In fact there is valid reason for
assuming a small acceleration of progress
but, as concerns the downward tendency
of the yields, allowance for this would
make little difference, because such a
modification would call at the same time
for a small increase of our charge for the
unrecorded costs.42

42 I assumed a 2.8 percent rate of increase of factor
productivity for the entire period 1948-66. This is in
accordance with the Kendrick estimates when the favorable
productivity-effects of intersector (intergroup)
shifts are included. On this basis there was no increase
from 1948-57 to 1957-66. However, if the effect of these
shifts is excluded, then from 1948-57 to 1957-66 a rise
is observed from a rate of increase of 2.3 percent to a
rate of 2.6 percent. Furthermore, even the first of these
two types of measure of productivity-increase rose from
the period 1929-48 to 1948-66 (it rose from 2.6 percent
to 2.8 percent), and it may be argued that this rise has
resulted in part from the post-1953 increase in progress-
inputs.

When comparing 1966 with 1953 I took no account
of any such increase on the benefit-side of the ledger. I
also used the same percentage allowance in relation to
private GNP for the unrecorded costs of 1966 as for
those of 1953, since there exists a general presumption
that these costs tend to move in proportion to the rate
of progress. During these particular thirteen years there
may conceivably have occurred a somewhat more than
proportionate increase in specific costs appearing in the
list of Appendix C. One may be led to suspect this for
example because of the increase of the white-colla
proportion of the labor force. On the other hand, the


### ---Economics-1970-0-23.txt---
A downtrend of rates of return in the
progress-generating activities is, of course,
not the same phenomenon as the previously
discussed gap between the average
and the marginal rate on the progressinputs
of single periods; the downtrend
over time calls for a different interpretation.
It may be interpreted as meaning
that researchers, teachers and other workers
engaged in knowledge-acquisition and
distribution activities of growing complexity
have increased the efficiency of their
own group somewhat less than would have
been required for unchanging performance
at unchanging costs. Even the relativevaluation
(or relative-wage) component of
a downtrend can in good part be so interpreted,
because while this component

becomes enlarged by rent-formation, it
nevertheless is true that the relative increase
of the incomes of progress-workers
typically results from the need to use very
highly qualified personnel on a rising
scale."' Trying to avoid a downtrend
of the rates of return on these particular
inputs by limiting the scale of the progressactivies
of each successive period accordingly
would in the long run prove incompatible
with a nondiminishing rate of technological
advance in the economy at large.
In view of the scarcity of the ultimate resources
needed for any economic processincluding
the process of self-improvement
of progress-workers-a secular downward
tendency of yields should indeed be expected,
and Kenneth Arrow was right in
reminding us at last year's annual meeting
that "eternal exponential technological
growth is just as unreasonable as eternal
exponential population growth."44 But it is
equally true and important that such a
slow, secular downward tendency of the
progress-yields may become interrupted
for very long periods. Recurring "breakthroughs"
play an important role in the

history of the sciences.

Quite aside from the question of a downward
tendency with the actually observed
composition of the progress-activities, we
should recall that, given the present scale
of these activities, even a person in full
agreement with the community-decisions
concerning composition would in all probability
have to consider the marginal social
rates of return of each period lower than
our average rates. This gap arises partly
because high-quality personnel is limited,
and partly because the solution of problems
presupposes the solution of earlier ones.
The least promising project of any period
is apt to be appreciably less promising
than the period's average project. But as
I said in Section II, I am abstaining here
from efforts to apply formal analysis to the
problem of marginal rates, since each individual
is entitled to his own marginal
calculus and in most of the area in which
this paper has been moving individual
valuations become submerged in compromise
rather than become validated at the
margin by means of purchase, sale and
production.45 This is why I have so far


### ---Economics-1970-0-24.txt---
focused on average rates of return and on
"necessary conditions," using what I believe
to be very widely shared assumptions
as to the meaning of low and of high valuation
of controversial nonmarket contributions.


At this point, attention should be directed
at those aspects of our problem
which call for highly subjective judgment.
My illustrations will be found in Appendix
D, which contains observations on significant
structural and allocational problems
within the area of R & D. Prominent
among these problems is that of "concentration"
in several senses of this term. As
for educational policy, it has become all too
obvious that the future of Western civilization
depends on the personal attitudes
millions of individuals will develop to
structural problems belonging in that sector
of the progress-generating activities.
We may now return to our point of departure.
As long as we want to remain on a
level of reasonable objectivity, we can only
try to play into the hands of those who
must make their own value judgments. On
alternative cost-bases it is possible to make
reasonable statements of fairly general
validity on the social profitability of the
progress-generating activities as a whole
and on the trends of this profitability. For
the time being the average social yields
are high on both our cost-bases, very much
higher than 13 percent and 18 percent,
respectively, as was seen. It should of
course not be overlooked that a waiting
period of considerable duration elapses between
some progress-generating inputs and
the onset of their yield. But this is true
only of part of the progress-inputs as we
have defined them. Furthermore, when
comparing these yields with yields from
plhysical capital formation, another lagadjustment
is also needed, and as explained
in Appendix A, this works in the
opposite direction (in favor of the progressinputs)
. We should remember also that
while on the one hand the lag between
progress-inputs and their results would call
for upward revaluation of the inputs in
relation to their results, on the other hand,
earlier progress-inputs have all along been
smaller progress-inputs.

At any event, regardless of what lagadjustment
we make within reason, the

average social rate of return-even the
rate on the all-inclusive cost-base-satisfies
our necessary condition with ease. This
"real rate" is at present much higher than
the marginal rate from physical investment
at a more or less given level of knowledge.
The latter rate should be estimated
at less than 10 percent; a good case can be
made for estimating it at a figure located
in the range between 5 percent and 10 percent.
46 Moreover, the rates on progressinputs
suggested by our analysis are appreciably
higher also than the typical pretax
corporate profit-rates, a fact which is
worth noting even though no comparability
is claimed for our rates of return with
accounting profits.47

46 A rough estimate of the rate in question, which is
obtained by way of multiplying the ratio of yearly output
to the capital stock by the share of reproducible
capital in income, points to less than 10 percent. See
Section II.

47 The published corporate profit rates (which are
related to the book value of equity) are of course private
rather than social rates. This would not make too much
difference if the private rates were rates "at a given
level of technological knowledge" but in reality they
are earned partly on private progress-generating inputs
which do not enter into the book value. Also, in contrast
to our "real" rates these corporate rates are money
rates earned in large part on equity such as has been
paid up or accumulated in much earlier years than those
in which the profits were earned. There are several
further reasons for noncomparability. On balance there
exists a strong presumption that the corporate rates on
book value are considerably higher than the real rates
on physical investment. The published pre-tax profit
rate on the book value of American manufacturing
corporations as a whole has of late been about 20 percent,
and even this is less than the rates of progressreturn
to which our calculations in the text point.
Again with no claim to comparability, it may be added
that the earnings-rate on the market value of Standard
and Poor's common-stock sample (not wholly limited
to manufacturing) is in the neighborhood of 6 percent,


### ---Economics-1970-0-25.txt---
For an appreciable period to come, even
a slow downtrend of the progress-yields
would leave a significant differential between
the average social rate of return
from the progress-activities and the conventionally
defined marginal rate of return
from physical investment. One is
tempted to add that, given the size of this
differential, it should be possible to promote
the gradual shifting of a larger proportion
of our inputs into the various progress-
activities without forcing the marginal
social rates on these below the marginal
rates on physical investment (the latter
being an activity which does not come in
fixed proportions with technological progress)
. In my appraisal, this statement, too,
makes very good sense. But even if most
people should agree with this statement,
and yet there should be substantial disagreement
on the nature of the projects

that satisfy the marginal criteria, the possibility
of increasing the weight of the newknowledge
sector would still depend on

how well the political mechanism is capable
of bridging the differences. In view of
this, I will end with a question in bargaining
theory: Is it realistic to expect that the
propensity to reach compromises can be
increased by making the bargaining parties
aware of the fact that the joint payoff on
reaching agreement is high?
 ## Economics-1971-0


### ---Economics-1971-0-03.txt---
Economics today rides the crest of
intellectual respectability and popular
acclaim. The serious attention with which
our pronouncements are received by the
general public, hard-bitten politicians, and
even skeptical businessmen is second only
to that which was given to physicists and
space experts a few years ago when the
round trip to the moon seemed to be our
only truly national goal. The flow of
learned articles, monographs, and textbooks
is swelling like a tidal wave; Econometrica,
the leading journal in the field
of mathematical economics, has just
stepped up its publication schedule from
four to six issues per annum.

And yet an uneasy feeling about the
present state of our discipline has been
growing in some of us who have watched
its unprecedented development over the
last three decades. This concern seems to
be shared even by those who are themselves
contributing successfully to the
present boom. They play the game with
professional skill but have serious doubts
about its rules.

Much of current academic teaching and
research has been criticized for its lack of
relevance, that is, of immediate practical
impact. In a nearly instant response to
this criticism, research projects, seminars
aind undergraduate courses have been
set up on poverty, on city and small town
slums, on pure water and fresh air. In an
almost Pavlovian reflex, whenever a new
complaint is raised, President Nixon appoints
a commission and the university
announces a new course. Far be it from
me to argue that the fire should not be
shifted when the target moves. The trouble
is caused, however, not by an inadequate
selection of targets, but rather by our
inability to hit squarely any one of them.
The uneasiness of which I spoke before is
caused not by the irrelevance of the practical
problems to which present day economists
address their efforts, but rather by
the palpable inadequacy of the scientific
means with which they try to solve them.
If this simply were a sign of the overly
high aspiration level of a fast developing
discipline, such a discrepancy between ends
and means should cause no worry. But I
submit that the consistently indifferent
performance in practical applications is in
fact a symptom of a fundamental imbalance
in the present state of our discipline.
The weak and all too slowly growing empirical
foundation clearly cannot support
the proliferating superstructure of pure, or
should I say, speculative economic theory.
Much is being made of the widespread,
nearly mandatory use by modern economic
theorists of mathematics. To the
extent to which the economic phenomena
possess observable quantitative dimensions,
this is indisputably a major forward
step. Unfortunately, any one capable of
learning elementary, or preferably advanced
calculus and algebra, and acquiring
acquaintance with the specialized terminology
of economics can set himself up as a
theorist. Uncritical enthusiasm for mathematical
formulation tends often to conceal


### ---Economics-1971-0-04.txt---
the ephemeral substantive content of
the argument behind the formidable front
of algebraic signs.

Professional journals have opened wide
their pages to papers written in mathematical
language; colleges train aspiring
young economists to use this language;
graduate schools require its knowledge and
reward its use. The mathematical modelbuilding
industry has grown into one of the
most prestigious, possibly the most prestigious
branch of economics. Construction of
a typical theoretical model can be handled
now as a routine assembly job. All principal
components such as production functions,
consumption and utility functions
come in several standard types; so does
the optional equipment as, for example,
"factor augmentation"-to take care of
technological change. This particular device
is, incidentally, available in a simple
exponential design or with a special automatic
regulator known as the "Kennedy
function." Any model can be modernized
with the help of special attachments. One
popular way to upgrade a simple one-sector
model is to bring it out in a two-sector version
or even in a still more impressive form
of the "n-sector," that is, many-sector
class.

In the presentation of a new model,
attention nowadays is usually centered on
a step-by-step derivation of its formal
properties. But if the author-or at least
the referee who recommended the manuscript
for publication-is technically competent,
such mathematical manipulations,
however long and intricate, can even without
further checking be accepted as
correct. Nevertheless, they are usually
spelled out at great length. By the time it
comes to interpretation of the substantive
conclusions, the assumptions on which the
model has been based are easily forgotten.
But it is precisely the empirical validity
of these assumrptions on which the usefulness
of the entire exercise depends.
What is really needed, in most cases, is
a very difficult and seldom very neat
assessment and verification of these assumptions
in terms of observed facts. Here
mathematics cannot help and because of
this, the interest and enthusiasm of the
model builder suddenly begins to flag:
"If you do not like my set of assumptions,
give me another and I will gladly make you
another model; have your pick."
Policy oriented models, in contrast to
purely descriptive ones, are gaining favor,
however nonoperational they may be.
This, I submit, is in part because the
choice of the final policy objectives-the
selection and justification of the shape of
the so-called objective function-is, and
rightly so, considered based on normative
judgment, not on factual analysis. Thus,
the model builder can secure at least some
convenient assumptions without running
the risk of being asked to justify them on
empirical grounds.

To sum up with the words of a recent
president of the Econometric Society,
" . . . the achievements of economic theory
in the last two decades are both impressive
and in many ways beautiful. But it cannot
be denied that there is something scandalous
in the spectacle of so many people
refining the analysis of economic states
which they give no reason to suppose will
ever, or have ever, come about.... It
is an unsatisfactory and slightly dishonest
state of affairs."

But shouldn't this harsh judgment be
suspended in the face of the impressive
volume of econometric work? The answer
is decidedly no. This work can be in
general characterized as an attempt to
compensate for the glaring weakness of
the data base available to us by the widest
possible use of more and more sophisticated
statistical techniques. Alongside the
mounting pile of elaborate theoretical
models we see a fast-growing stock of
equally intricate statistical tools. These


### ---Economics-1971-0-05.txt---
are intended to stretch to the limit the
meager supply of facts.

Since, as I said before, the publishers'
referees do a competent job, most modeltesting
kits described in professional
journals are internally consistent. However,
like the economic models they are
supposed to implement, the validity of
these statistical tools depends itself on the
acceptance of certain convenient assumptions
pertaining to stochastic properties of
the phenomena which the particular
models are intended to explain; assumptions
that can be seldom verified.

In no other field of empirical inquiry has
so massive and sophisticated a statistical
machinery been used with such indifferent
results. Nevertheless, theorists continue
to turn out model after model and mathematical
statisticians to devise complicated
procedures one after another. Most of these
are relegated to the stockpile without any
practical application or after only a perfunctory
demonstration exercise. Even

those used for a while soon fall out of favor,
not because the methods that supersede
them perform better, but because they
are new and different.

Continued preoccupation with imaginary,
hypothetical, rather than with
observable reality has gradually led to a
distortion of the informal valuation scale
used in our academic community to assess
and to rank the scientific performance of its
members. Empirical analysis, according to
this scale, gets a lower rating than formal
mathematical reasoning. Devising a new
statistical procedure, however tenuous,
that makes it possible to squeeze out one
more unknown parameter from a given
set of data, is judged a greater scientific
achievement than the successful search
for additional information that would
permit us to measure the magnitude of the
same parameter in a less ingenious, but
more reliable way. This despite the fact
that in all too many instances sophisticated
statistical analysis is performed on a
set of data whose exact meaning and
validity are unknown to the author or
rather so well known to him that at the
very end he warns the reader not to take
the material conclusions of the entire
''exercise" seriously.

A natural Darwinian feedback operating
through selection of academic personnel
contributes greatly to the perpetuation of
this state of affairs. The scoring system
that governs the distribution of rewards
must naturally affect the make-up of the
competing teams. Thus, it is not surprising
that the younger economists, particularly
those engaged in teaching and in academic
research, seem by now quite content with a
situation in which they can demonstrate
their prowess (and incidentally, advance
their careers) by building more and more
complicated mathematical models and
devising more and more sophisticated
methods of statistical inference without
ever engaging in empirical research. Complaints
about the lack of indispensable primary
data are heard from time to time,
but they don't sound very urgent. The
feeling of dissatisfaction with the present
state of our discipline which prompts me
to speak out so bluntly seems, alas, to be
shared by relatively few. Yet even those
few who do share it feel they can do little
to improve the situation. How could they?
In contrast to most physical sciences, we
study a system that is not only exceedingly
complex but is also in a state of constant
flux. I have in mind not the obvious change
in the variables, such as outputs, prices or
levels of employment, that our equations
are supposed to explain, but the basic
structural relationships described by the
form and the parameters of these equations.
In order to know what the shape of
these structural relationships actually are
at any given time, we have to keep them
under continuous surveillance.
By sinking the foundations of our analytical


### ---Economics-1971-0-06.txt---
system deeper and deeper, by

reducing, for example, cost functions to
production functions and the production
functions to some still more basic relationships
eventually capable of explaining
the technological change itself, we should
be able to reduce this drift. It would,
nevertheless, be quite unrealistic to expect
to reach, in this way, the bedrock of invariant
structural relationships (measurable
parameters) which, once having been
observed and described, could be used
year after year, decade after decade, without
revisions based on repeated observation.


On the relatively shallow level where the
empirically implemented economic analysis
now operates even the more invariant
of the structural relationships, in terms of
which the system is described, change
rapidly. Without a constant inflow of new
data the existing stock of factual information
becomes obsolete very soon.

What a contrast with physics, biology or
even psychology where the magnitude of
most parameters is practically constant
and where critical experiments and measurements
don't have to be repeated every
year!

Just to keep up our very modest current
capabilities we have to maintain a steady
flow of new data. A progressive expansion
of these capabilities would be out of the
question without a continuous and rapid
rise of this flow. Moreover, the new, additional
data in many instances will have to
be qualitatively different from those provided
hitherto.

To deepen the foundation of our analytical
system it will be necessary to reach
unhesitatingly beyond the limits of the
domain of economic phenomena as it has
been staked out up to now. The pursuit
of a more fundamental understanding of
the process of production inevitably leads
into the area of engineering sciences. To
penetrate below the skin-thin surface of
conventional consumption functions, it
will be necessary to develop a systematic
study of the structural characteristics and
of the functioning of households, an area in
which description and analysis of social,
anthropological and demographic factors
must obviously occupy the center of the
stage.

Establishment of systematic cooperative
relationships across the traditional
frontiers now separating economics from
these adjoining fields is hampered by the
sense of self-sufficiency resulting from what
I have already characterized as undue reliance
on indirect statistical inference as
the principal method of empirical research.
As theorists, we construct systems in which
prices, outputs, rates of saving and investment,
etc., are explained in terms of
production functions, consumption functions
and other structural relationships
whose parameters are assumed, at least for
arguments' sake, to be known. As econometricians,
engaged in what passes for

empirical research, we do not try, however,
to ascertain the actual shapes of these
functions and to measure the magnitudes
of these parameters by turning up new
factual information. We make an about
face and rely on indirect statistical inference
to derive the unknown structural
relationships from the observed magnitudes
of prices, outputs and other variables
that, in our role as theoreticians, we
treated as unknowns.

Formally, nothing is, of course, wrong
with such an apparently circular procedure.
Moreover, the model builder in
erecting his hypothetical structures is free
to take into account all possible kinds of
factual knowledge and the econometrician
in principle, at least, can introduce in the
estimating procedure any amount of what
is usually referred to as "exogenous"
information before he feeds his programmed


### ---Economics-1971-0-07.txt---
tape into the computer. Such

options are exercised rarely and when they
are, usually in a casual way.

The same well-known sets of figures are
used again and again in all possible combinations
to pit different theoretical models
against each other in formal statistical
combat. For obvious reasons a decision is
reached in most cases not by a knock-out,
but by a few points. The orderly and
systematic nature of the entire procedure
generates a feeling of comfortable selfsufficiency.


This complacent feeling, as I said before,
discourages venturesome attempts to
widen and to deepen the empirical foundations
of economic analysis, particularly
those attempts that would involve crossing
the conventional lines separating ours from
the adjoining fields.

True advance can be achieved only
through an iterative process in which improved
theoretical formulation raises new
empirical questions and the answers to
these questions, in their turn, lead to new
theoretical insights. The "givens" of today
become the "unknowns" that will have to
be explained tomorrow. This, incidentally,
makes untenable the admittedly
convenient methodological position according
to which a theorist does not need
to verify directly the factual assumptions
on which he chooses to base his deductive
arguments, provided 'his empirical conclusions
seem to be correct. The prevalence
of such a point of view is, to a large extent,
responsible for the state of splendid
isolation in which our discipline nowadays
finds itself.

An exceptional example of a healthy
balance between theoretical and empirical
analysis and of the readiness of professional
economists to cooperate with experts in
the neighboring disciplines is offered by
Agricultural Economics as it developed in
this country over the last fifty years. A
unique combination of social and political
forces has secured for this area unusually
strong organizational and generous financial
support. Official agricultural statistics
are more complete, reliable, and systematic
than those pertaining to any other major
sector of our economy. Close collaboration
with agronomists provides agricultural
economists with direct access to information
of a technological kind. When they
speak of crop rotation, fertilizers, or alternative
harvesting techniques, they usually
know, sometimes from personal experience,
what they are talking about. Preoccupation
with the standard of living of the rural
population has led agricultural economists
into collaboration with home economists
and sociologists, that is, with social scientists
of the "softer" kind. While centering
their interest on only one part of the economic
system, agricultural economists
demonstrated the effectiveness of a systematic
combination of theoretical approach
with detailed factual analysis. They
also were the first among economists to
make use of the advanced methods of
mathematical statistics. However, in their
hands, statistical infereince became a
complement to, not a substitute for,
empirical research.

The shift from casual empiricism that
dominates much of today's econometric
work to systematic large-scale factual
analysis will not be easy. To start with, it
will require a sharp increase in the annual
appropriation for Federal Statistical Agencies.
The quality of government statistics
has, of course, been steadily improving.
The coverage, however, does not keep up
with the growing complexity of our social
and economic system and our capability
of handling larger and larger data flows.
The spectacular advances in computer
technology increased the economists' potential
ability to make effective analytical
use of large sets of detailed data. The time


### ---Economics-1971-0-08.txt---
is past when the best that could be done
with large sets of variables was to reduce
their number by averaging them out or
what is essentially the same, combining
them into broad aggregates; now we can
manipulate complicated analytical systems
without suppressing the identity of
their individual elements. There is a
certain irony in the fact that, next to the
fast-growing service industries, the areas
whose coverage by the Census is particularly
deficient are the operations of
government agencies, both federal and
local.

To place all or even the major responsibility
for the collection of economic data
in the hands of one central organization
would be a mistake. The prevailing decentralized
approach that permits and

encourages a great number of government
agencies, non-profit institutions and private
businesses engaged in data gathering
activities acquitted itself very well. Better
information means more detailed information
and detailed specialized information
can be best collected by those immediately
concerned with a particular field. What is,
however, urgently needed is the establishment,
maintenance and enforcement of
coordinated uniform classification systems
by all agencies, private as well as public,
involved in this work. Incompatible data
are useless data. How far from a tolerable,
not to say, ideal state our present economic
statistics are in this respect, can be judged
by the fact that because of differences in
classification, domestic output data cannot
be compared, for many goods, with the
corresponding export and import figures.
Neither can the official employment statistics
be related without laborious adjustments
to output data, industry by industry.
An unreasonably high proportion
of material and intellectual resources devoted
to statistical work is now spent not
on the collection of primary information
but on a frustrating and wasteful struggle
with incongruous definitions and irreconcilable
classifications.

Without invoking a misplaced methodological
analogy, the task of securing a
massive flow of primary economic data can
be compared to that of providing the high
energy physicists with a gigantic accelerator.
The scientists have their machines
while the economists are still waiting for
their data. In our case not only must the
society be willing to provide year after
year the millions of dollars required for
maintenance of a vast statistical machine,
but a large number of citizens must be prepared
to play, at least, a passive and occasionally
even an active part in actual

fact-finding operations. It is as if the electrons
and protons had to be persuaded to
cooperate with the physicist.

The average American does not seem to
object to being interviewed, polled, and
surveyed. Curiosity, the desire to find out
how the economic system (in which most of
us are small gears, and some, big wheels)
works might in many instances provide
sufficient inducement for cooperation of
this kind.

One runs up, of course, occasionally
against the attitude that "what you don't
know can't hurt you" and that knowledge
might be dangerous: it may generate a
desire to tinker with the system. The
experience of these years seems, however,
to have convinced not only most economists-
with a few notable exceptions-but
also the public at large that a lack of
economic knowledge can hurt badly. Our
free enterprise system has rightly been
compared to a gigantic computing machine
capable of solving its own problems automatically.
But any one who has had some

practical experience with large computers
knows that they do break down and can't
operate unattended. To keep the automatic,
or rather the semi-automatic, engine
of our economy in good working order we
must not only understand the general


### ---Economics-1971-0-09.txt---
principles on which it operates, but also
be acquainted with the details of its actual
design.

A new element has entered the picture
in recent years-the adoption of methods
of modern econiomic analysis by private
business. Corporate support of economic
research goes as far back as the early
1920's when Wesley Mitchell founded the
National Bureau. However, it is not this
concern for broad issues of public policies
or even the general interest in economic
growth and business fluctuations that I
have in mind, but rather the fast-spreading
use of advanced methods of Operations
Research and of so-called Systems' Analysis.
Some of the standard concepts and
analytical devices of economic theory first
found their way into the curricula of our
business schools and soon after that, sophisticated
management began to put

them into practice. While academic theorists
are content with the formulation of
general principles, corporate operations
researchers and practical systems' analysts
have to answer questions pertaining to
specific real situations. Demand for economic
data to be used in practical business
planning is growing at an accelerated pace.
It is a high quality demand: business users
in most instances possess first-hand technical
knowledge of the area to which the data
they ask for refer. Moreover, this demand
is usually "effective." Profit-making business
is willing and able to pay the costs of
gathering the inlformation it wants to have.
This raises the thorny question of public
access to privately collected data and of
the proper division of labor and cooperation
between government and business in
that fast-expanding field. Under the inexorable
pressure of rising practical demand,
these problems will be solved in one
way or another. Our economy will be surveyed
and mapped in all its many dimensions
on a larger and larger scale.

Economists should be prepared to take
a leading role in shaping this major social
enterprise not as someone else's spokesmen
and advisers, but on their own behalf.
They have failed to do this up to now. The
Conference of Federal Statistics Users
organized several years ago had business,
labor, and many other groups represented
among its members, but not economists as
such. How can we expect our needs to be
satisfied if our voices are not heard?
We, I mean the academic economists,
are ready to expound, to any one ready to
lend an ear, our views on problems of public
policy: give advice on the best ways to
maintain full employment, to fight inflation,
to foster economic growth. We should
be equally prepared to share with the
wider public the hopes and disappointments
which accompany the advance of
our own often desperately difficult, but
always exciting intellectual enterprise.
This public has amply demonstrated its
readiness to back the pursuit of knowledge.
It will lend its generous support to our
venture too, if we take the trouble to
explain what it is all about.

REFERENCE

F. H. Hahn, "Some Adjustment Problems,"
Econometrica, Jan. 1970, 38, 1-2.
 ## Economics-1972-0


### ---Economics-1972-0-03.txt---
The world economy today is vastly
different from the 1930's, when Seymour
Harris, the chairman of this meeting, infected
me with his boundless enthusiasm
for economics and his steadfast confidence
in its capacity for good works. Economics
is very different, too. Both the science and
its subject have changed, and for the
better, since World War II. But there are
some notable constants. Unemployment
and inflation still preoccupy and perplex
economists, statesmen, journalists, housewives,
and everyone else. The connection
between them is the principal domestic
economic burden of presidents and prime
ministers, and the major area of controversy
and ignorance in macroeconomics.
I have chosen to review economic thought
on this topic on this occasion, partly because
of its inevitable timeliness, partly
because of a personal interest reaching
back to my first published work in 1941.
I. The Meanings of Full Employment
Today, as thirty and forty years ago,
economists debate how much unemployment
is voluntary, how much involuntary;
how much is a phenomenon of equilibrium,
how much a symptom of disequilibrium;
how much is compatible with competition,
how much is to be blamed on monopolies,
labor unions, and restrictive legislation;
how much unemployment characterizes
"full" employment.

Full employment imagine macroeconomics
deprived of the concept. But

what is it? What is the proper employment
goal of policies affecting aggregate demand?
Zero unemployment in the monthly
labor force survey? That outcome is so
inconceivable outside of Switzerland that
it is useless as a guide to policy. Any other
numerical candidate, yes even 4 percent,
is patently arbitrary without reference to
basic criteria. Unemployment equal to
vacancies? Measurement problems aside,
this definition has the same straightforward
appeal as zero unemployment, which
it simply corrects for friction.1
A concept of full employment more
congenial to economic theory is labor
market equilibrium, a volume of employment
which is simultaneously the amount
employers want to offer and the amount
workers want to accept at prevailing wage
rates and prices. Forty years ago theorists
with confidence in markets could believe
that full employment is whatever volume
of employment the economy is moving
toward, and that its achievement requires
of the government nothing more than
neutrality, and nothing less

After Keynes challenged the classical
notion of labor market equilibrium and
the complacent view of policy to which it
led, full employment came to mean max;-
mum aggregate supply, the point at which
expansion of aggregate demand could not
further increase employment and output.
Full employment was also regarded as
the economy's inflation threshold. With a
deflationary gap, demand less than full
employment supply, prices would be declining
or at worst constant. Expansion of
aggregate demand short of full employment
would cause at most a one-shot


### ---Economics-1972-0-04.txt---
increase of prices. For continiuing inflation,
the textbooks tol(I us, a necessary and
sufficient conditioin was an inflationary
gap, real aggregate (lemand in excess of
feasible supply. T he modlel was tailormade
for wartime inflation.

Postwar experience destroyed the identification
of full employmeint with the

economy's inflation threshold. The profession,
the press, andI the public discovered
the "new inflation" of the 1950's, inflation
without beniefit of gap), labelled but
scarcely illuminated by the term "costpush.
" Subsequently the view of the world
suggested by the Phillips curve merged
demand-pull and cost-push inflation and
blurred the distinction between them.
This view containe(d no concept of full employment.
In its place came the tradeoff,
along which society supposedly can choose
the least undesirable feasible combination
of the evils of unemployment and inflation.
Many economists deny the existence of
a durable Phillips tradeoff. TIheir numbers
and influence are increasing. Some of them
contendl that there is only one rate of
unemployment compatible with steady
inflation, a "natural rate" consistent with
any steadly rate of change of prices, positive,
zero, or negative. The natural rate is
another full employment candidate, a
policy target at least in the passive sense
that monetary and fiscal policy makers
are advised to eschew any numerical unemployment
goal and to let the economy

gravitate to this equilibrium. So we have
come full circle. Full employment is once
again nothing but the equilibrium reached
by labor markets unaidedl andl undlistorted
by governmental fine tuning.

In discussing these issues, I shall make
the following points. First, an observed
amount of unemployment is not revealed
to be voluntary simply by the fact that
money wage rates are constant, or rising,
or even accelerating. I shall recall and extend
Keynes's dlefinition of involuntary
unemployment and his explanation why
workers may accept price inflation as a
method of re(lucing real wages while rejecting
money wage cuts. The second

point is related. Involuntary unemployment
is a disequilibrium phenomenon;
the behavior, the persistence, of excess
supplies of labor depend on how and how
fast markets adjust to shocks, and on how
large and how frequent the shocks are.
Higher prices or faster inflation can
(liminish involuntary, disequilibrium unemployment,
even though voluntary, eqluilibrium
labor supply is entirely free of
money illusion.

Third, various criteria of full employment
coincide in a theoretical full stationary
eqjuilibrium, but diverge in persistent
disequilibrium. These are 1) the
natural rate of unemployment, the rate
compatible with zero or some other constant
inflation rate, 2) zero involuntary
unemployment, 3) the rate of unemployment
needed for optimal job search and
placement, and 4) unemployment equal
to job vacancies. The first criterion dictates
higher unemployment than any of
the rest. Instead of commending the natural
rate as a target of employment policy,
the other three criteria suggest less unemployment
and more inflation. Therefore,
fourth, there are real gains from additional
employment, which must be

weighed in the social balance against the
costs of inflation. I shall conclude with a
few remarks on this choice, and on the
possibilities of improving the terms of the
tradeoff.

II. Keynesian and Classical Interpretations
of Unemployment

To begin with the General Theory is not
just the ritual piety economists of my
generation owe the book that shaped their
minds. Keynes's treatment of labor market
equilibrium and disequilibrium in his
first chapter is remarkably relevant today.


### ---Economics-1972-0-05.txt---
Keynes attacked what he called the
classical presumption that persistent unemployment
is voluntary unemployment.

The presumption he challenged is that in
competitive labor markets actual employment
and unemployment reveal workers'
true preferences between work and
alternative uses of time, the presumption
that no one is fully or partially unemployed
whose real wage per hour exceeds
his marginal valuation of an hour of free
time. Orthodox economists found the observed
stickiness of money wages to be
persuasive evidence that unemployment,
even in the Great Depression, was voluntary.
Keynes found decisive evidence
against this inference in the willingness of
workers to accept a larger volume of employment
at a lower real wage resulting
from an increase of prices.

Whenever unemployment could be reduced
by expansion of aggregate demand,
Keynes regarded it as involuntary. He expected
expansion to raise prices and lower
real wages, but this expectation is not
crucial to his argument. Indeed, if it is possible
to raise employment without reduction
in the real wage, his case for calling the unemployment
involuntary is strengthened.

But why is the money wage so stubborn
if more labor is willingly available at the
same or lower real wage5? Consider first
some answers Keynes did not give. He did
not appeal to trade union monopolies or
minimum wage laws. He was anxious, perhaps
over-anxious, to meet his putative
classical opponents on their home field,
the competitive economy\ He did not rely
on any failure of workers to perceive what
a rise in prices does to real wages. The unemployed
take new jobs, the employed

hold old ones, with eyes open. Otherwise
the new situation would be transient.
Instead, Keynes emphasized the institutional
fact that wages are bargained

and set in the monetary unit of account.
Money wage rates are, to use an unKeynesian
term, "administered prices." I'hat is,
they are not set and reset in daily auctions
but posted and fixed for finite periods of
time. This observation led Keynes to his
central explanation: Workers, individually
and in groups, are more concerned with
relative than absolute real wages. They
may withdraw labor if their wages fall
relatively to wages elsewhere, even though
they would not withdraw any if real wages
fall uniformly everywhere. Labor markets
are decentralized, and there is no way
money wages can fall in any one market
without impairing the relative status of
the workers there. A general rise in prices
is a neutral and universal method of reducing
real wages, the only method in a
decentralized and uncontrolled economy.
Inflation would not be needed, we may
infer, if by government compulsion, economy-
wide bargaining, or social compact,
all money wage rates could be scaled down
together.

Keynes apparently meant that relative
wages are the arguments in labor supply
functions. But Alchian (pp. 27-52 in Phelps
et al.) and other theorists of search activity
have offered a somewhat different
interpretation, namely that workers whose
money wages are reduced will quit their
jobs to seek employment in other markets
where they think, perhaps mistakenly,
that wages remain high.

Keynes's explanation of money wage
stickiness is plausible and realistic. But two
related analytical issues have obscured the
message. Can there be involuntary unemployment
in an equilibrium, a proper, fullfledged
neoclassical equilibrium? Does the
labor supply behavior described by Keynes
betray "money illusion"? Keynes gave a
loud yes in answer to the first question,
and this seems at first glance to compel an
affirmative answer to the second.
An economic theorist can, of course,
commit no greater crime than to assume
money illusion. Comparative statics is a


### ---Economics-1972-0-06.txt---
nonhistorical exercise, in which different
price levels are to be viewed as alternative
rather th bn sequential. Compare two
situations that differ only in the scale of
exogenous monetary variables; imagine,
for example, that all such magnitudes are
ten times as high in one situation as in the
other. All equilibrium prices, including
money wage rates, should differ in the
same proportion, while all real magnitudes,
including employment, should be the same
in the two equilibria. To assume instead
that workers' supply decisions vary with
the price level is to say that they would
behave differently if the unit of account
were, and always had been, dimes instead
of dollars. Surely Keynes should not be
interpreted to attribute to anyone money
illusion in this sense. He was not talking
about so strict and static an equilibrium.
Axel Leijonhufvud's illuminating and
perceptive interpretation of Keynes argues
convincingly that, in chapter 1 as throughout
the General Theory, what Keynes calls
equilibrium should be viewed as persistent
disequilibrium, and what appears to be
comparative statics is really shrewd and
incisive, if awkward, dynamic analysis.
Involuntary unemployment means that
labor markets are not in equilibrium. The
resistance of money wage rates to excess
supply is a feature of the adjustment process
rather than a symptom of irrationality.


The other side of Keynes's story is that
in depressions money wage deflation, even
if it occurred more speedily, or especially
if it occurred more speedily, would be at
best a weak equilibrator and quite possibly
a source of more unemployment rather
than less. In contemporary language, the
perverse case would arise if a high and
ever-increasing real rate of return on
money inhibited real demand faster than
the rising purchasing power of monetary
stocks stimulated demand. To pursue this
Keynesian theme further here would be a
digression.

What relevance has this excursion into
depression economics for contemporary
problems of unemployment and wage inflation?
The issues are remarkably similar,
even though events and Phillips have
shifted attention from levels to time rates
of change of wages and prices. Phillips
curve doctrine2 is in an important sense
the postwar analogue of Keynesian wage
and employment theory, while natural
rate doctrine is the contemporary version
of the classical position Keynes was opposing.


Phillips curve doctrine implies that
lower unemployment can be purchased at
the cost of faster inflation. Let us adapt
Keynes's test for involuntary unemployment
to the dynamic terms of contemporary
discussion of inflation, wages, and
unemployment. Suppose that the current
rate of unemployment continues. Associated
with it is a path of real wages,
rising at the rate of productivity growth.
Consider an alternative future, with unemployment
at first declining to a rate one
percentage point lower and then remaining
constant at the lower rate. Associated
with the lower unemployment alternative
will be a second path of real wages. Eventually
this real wage path will show, at
least to first approximation, the same rate
of increase as the first one, the rate of
productivity growth. But the paths may
differ because of the transitional effects of
increasing the rate of employment. The
growth of real wages will be retarded in
the short run if additional employment
lowers labor's marginal productivity. In
any case, the test question is whether with
full information about the two alternatives
labor would accept the second one-
2 Phillips himself is not a prophet of the doctrine associated
with his curve. His 1958 article was probably the
most influential macro-economic paper of the last
quarter century. But Phillips simply presented some
striking empirical findings, which others have replicated
many times for many economies. He is not responsible
for the theories and policy conclusions his findings
stimulated.


### ---Economics-1972-0-07.txt---
whether, in other words, the additional
employment would be willingly supplied
aloing the second real wage path. If the
answer is affirmative, then that one percentage
point of unemployment is involuntary.


For Keynes's reasons, a negative answer
cannot necessarily be inferred from
failure of money wage rates to fall or even
decelerate. Actual unemployment and the
real wage path associated with it are not
necessarily an equilibrium. Rigidities in
the path of money wage rates can be explained
by workers' preoccupation with
relative wages and the absence of any
cetntral economy-wide mechanism for altering
all money wages together.

According to the natural rate hypothesis,
there is just one rate of unemployment
compatible with stea(ly wage and price
inflation, andl this is in the long run compatible
with any constant rate of change of
prices, positive, zero, or negative. Only
at the natural rate of unemployment are
workers content with current and prospective
real wages, content to have their real
wages rise at the rate of growth of productivity.
Along the feasible path of real
wages they would not wish to accept any
larger volume of employment. Lower unemployment,
therefore, can arise only from
economy-wide excess demand for labor
and must generate a gap between real
wages (lesired and real wages earned. The
gap evokes increases of money wages designed
to raise real wages faster than productivity.
But this intention is always

frustrated, the gap is never closed, money
wages and prices accelerate. By symmetrical
argument, unemployment above

the natural rate signifies excess supply in
labor markets and ever accelerating deflation.
Older classical economists regarded
constancy of money wage rates as indicative
of full employment equilibrium, at
which the allocation of time between work
and other pursuits is revealed as voluntary
and optimal. Their successors make the
same claims for the natural rate of unemployment,
except that in the equilibrium
money wages are not necessarily
constant but growing at the rate of productivity
gain plus the experienced and

expected rate of inflation of prices.
III. Is Zero-Inflation Unemployment
Voluntary and Optimal?

There are, then, two conflicting interpretations
of the welfare value of employment
in excess of the level consistent with
price stability. One is that additional
employment does not produce enough to
compensate workers for the value of other
uses of their time. The fact that it generates
inflation is taken as prima facie
evidence of a welfare loss. The alternative
view, which I shall argue, is that the responses
of money wages and prices to

changes in aggregate demand reflect
mechanics of adjustment, institutional
constraints, and relative wage patterns
and reveal nothing in particular about
individual or social valuations of unemployed
time vis-a-vis the wages of employment.


On this rostrum four years ago, Milton
Friedman identified the noninflationary
natural rate of unemployment with "equilibrium
in the structure of real wage

rates" (p. 8). "The 'natural rate of unemployment,
' " he said, ". . . is the level
that would be ground out by the Walrasian
system of general equilibrium equations,
provided that there is embedded in them
the actual structural characteristics of the
labor and commodity markets, including
market imperfections, stochastic variability
in demands and supplies, the costs of
getting information about job vacancies
and labor availabilities, the costs of mobility,
and so on." Presumably this

Walrasian equilibrium also has the usual
optimal properties; at any rate, Friedman
advised the monetary authorities not to
seek to improve upon it. But in fact we
know little about the existence of a


### ---Economics-1972-0-08.txt---
Walrasian equilibrium that allows for all
the imperfections and frictions that explain
why the natural rate is bigger than
zero, ancl eveni less about the optimality
of such an equilibriunm if it exists.
In the new microeconomics of labor
markets and inflatioin, the principal activity
whose marginal value sets the reservation
price of employment is job search.
It is not pure leisure, for in principle persons
who choose that option are not reported
as unemployed; however, there may
be a leisure component in job seeking.
A crucial assumption of the theory is
that search is significantly more efficient
when the searcher is unemployed, but
almost no evidence has been advanced on
this point. Members of our own profession
are adept at seeking and finding new jobs
without first leaving their old ones or
abandoning not-in-labor-force status. We
do not know how many quits and new hires
in manufacturing are similar transfers, but
some of them must be; if all reported
accessions were hires of unemployed workers,
the mean duration of unemployment
would be only about half what it is in fact.
In surveys of job mobility among blue
collar workers in 1946-47 (see Lloyd
Reynolds, pp. 2 14-15, and Herbert Parnes,
pp. 158-59), 25 percent of workers who
quit had new jobs lined up in advance.
Reynolds found that the main obstacle
to mobility without unemployment was
not lack of information or time, but simply
"anti-pirating" collusion by employers.
A considerable amount of search activity
by unemployed workers appears to be
an unpro(luctive consequence of dissatisfaction
and frustration rather than a

rational quest for improvement. This was
the conclusion of Reynolds' survey twentyfive
years ago, p. 215, and it has been reemphasized
for the contemporary scene by

Robert Hall, and by Peter Doeringer and
Michael Piore for what they term the
secondary labor force. Reynolds found
that quitting a job to look for a new one
while unemployed actually yielded a better
job in only a third of the cases. Lining up a
new job in advance was a more successful
strategy: two-thirds of such changes
turned out to be improvements. Today,
according to the dual labor market hypothesis,
the basic reason for frequent and
long spells of unemployment in the secondary
labor force is the shortage of good jobs.
In any event, the contention of some
natural rate theorists is that employment
beyond the natural rate takes time that
would be better spent in search activity.
Why do workers accept such employment?
An answer to this question is a key element
in a theory that generally presumes
that actual behavior reveals true preferences.
The answer giveIn is that workers
accept the additional employment only
because they are victims of inflation illusion.
One form of inflation illusion is overestimation
of the real wages of jobs they
now hold, if they are employed, or of jobs
they find, if they are unemployed and
searching. If they did not under-estimate
price inflation, employed workers would
more often quit to search, and unemployed
workers would search longer.

The force of this argument seems to me
diluted by the fact that price inflation
illusion affects equally both sides of the
job seeker's equation. He over-estimates
the real value of an immediate job, but he
also over-estimates the real values of jobs
he might wait for. It is in the spirit of this
theorizing to assume that money interest
rates respond to the same correct or incorrect
inflationary expectations. As a
first approximation, inflation illusion has
no substitution effect on the margin between
working and waiting.

It does have an income effect, causing
workers to exaggerate their real wealth.
In which direction the income effect
would work is not transparent. 1)oes
greater wealth, or the illusion of greater


### ---Economics-1972-0-09.txt---
wealth, make people more choosy about
jobs, more inclined to quit and to wait?
Or less choosy, more inclined to stay in
the job they have or to take the first one
that comes along? I should have thought
more selective rather than less. But natural
rate theory must take the opposite
view if it is to explain why under-estimation
of price inflation bamboozles workers
into holding or taking jobs that they do
not reallv want.

Another form of alleged inflation illusion
refers to wages rather than prices.
Workers are myopic anl (1o not perceive
that wages elsewhere are, or soon will be,
rising as fast as the money wage of the
job they now hold or have just found.
Consequently they under-estimate the
advantages of quitting and searching.
This explanationi is convincing only to the
extent that the payoff to search activity
is determined by wage differentials. The
payoff also depends on the probabilities of
getting jobs at quoted wages, therefore on
the balance between vacancies and job
seekers. Workers know that perfectly well.
Quit rates are an index of volunitary
search activity. They do not diminish
when unemployment is low and wage
rates are rapidly rising. They increase,
quite understandably. This fact contradicts
the inflation illusion story, both
versions. 1 conclude that it is not possible
to regard fluctuations of unemployment
on either side of the zero-inflation rate as
mainly voluntary, albeit mistaken, extensions
and contractions of search activity.
The new microeconomics of job search
(see Edmund Phelps et al.), is nevertheless
a valuable contribution to understanding
of frictional unemployment. It
provides reasons why some unemployment
is voluntary, and why some unemployment
is socially efficient.

Does the market produce the optimal
amount of search unemployment? Is the
natural rate optimal? I do not believe the
new microeconomics has yet answered
these questions.

An omniscient and beneficent economic
dictator would not place every new job
seeker immediately in any job at hand.
Such a policy would create many mismatches,
sacrificing efficiency in production
or necessitating costly job-to-job shifts later
on. The hypothetical planner would prefer
to keep a queue of workers unemployed,
so that he would have a larger choice of
jobs to which to assign them. But he would
not make the queue too long, because
workers in the queue are not producing
anything.

Of course he could shorten the queue of
unemployed if he could dispose of more
jobs and lengthen the queue of vacancies.
With enough jobs of various kinds, he
would never lack a vacancy for which any
worker who happens to come along has
comparative advantage. But because of
limited capital stocks and interdependence
among skills, jobs cannot be indefinitely
multiplied without lowering their marginal
productivity. Our wise and benevolent
planner would not place people in jobs
yielding less than the marginal value of
leisure. Given this constraint on the
number of jobs, he would always have to
keep some workers waiting, and some jobs
vacant. But he certainly would be inefficient
if he had fewer jobs, filled and
vacant, than this constraint. This is the
common sense of Beveridge's rule-that
vacancies should not be less than unemployment.


Is the natural rate a market solution of
the hypothetical planner's operations research
problem?/ According to search

theory, an unemployed worker considers
the probabilities that he can get a better
job by searching longer and balances the
expected discounted value of waiting
against the loss of earnings. The employed
worker makes a similar calculation when
he considers quitting, also taking into account


### ---Economics-1972-0-10.txt---
the once and for all costs of movement.
These calculations are like those of
the planner, but witlh an important difference.
An individual does not initernalize
all the considerations the planner takes
into account. The external effects are the
familiar ones of congestion theory. A
worker decidling to join a queue or to stay
in one consi(lers the probabilities of getting
a job, but not the effects of his decision on
the probabilities that others face. He
lowers those probabilities for people in
the queue he joins and raises them for persons
waiting for the kind of job he vacates
or turns (lown. tI0oo many persons are
unemployed waiting for good jobs, while
less desirable ones go begging. However,
externial effects also occur in the
(lecisions of employers whether to fill a
vacancy with the applicant at hand or to
wait for someone more qualified. It is not
obvious, at least to me, whether the market
is biased toward excessive or inadlequate
search. But it is doubtful that it
produces the optimal amounit.

Empirically the proposition that in the
United States the zero-inflation rate of
unemployment reflects voluntary and efficienit
job-seeking activity strains credulity.
If there were a natural rate of unemployment
in the United States, what would it
be? It is hard to say because virtually all
econometric Phillips curves allow for a
whole menu of steady inflation rates. But
estimates constrained to produce a vertical
long-run Phillips curve suggest a natural
rate between 5 and 6 percent of the labor
force.3

So let us consider some of the features of
an overall unemployment rate of 5 to 6 percent.
First, about 40 percent of accessions
in manufacturing are rehires rather than
new hires. Temporarily laid off by their
employers, these workers had been awaiting
recall and were scarcely engaged in
voluntary search activity. Their unemployment
is as much a deadweight loss as
the disguised unemployment of redundant
workers oni payrolls. This number declines
to 25-30 percent when unemployment is
4 percent or below. Likewise, a 5-6 perceint
unemployment rate means that voluntary
quits amount only to about a third of
separations, layoffs to two-thir(-ds. The proportions
are rever-sed at low unemployment
rates.

Second, the unemployment statistic is
not an exhaustive count of those with time
and inceintive to search. An additional
3 percent of the labor force are involuntarily
confinedI to part-time work, atid another
3 4 of t percent are out of the labor
force because they "could not find job" or
"think no work available"---discouraged
by market con(litions rather than personal
incapacities.

Third, with unemployment of 5-6 percent
the number of reported vacancies is
less than 1/ 2 of 1 percent. Vacancies appear
to be understated relative to unemployment,
but they rise to l2 percent when
the unemployment rate is below 4 percent.
At 5-6 percent unemployment, the
economy is clearly capable of generating
many more jobs with marginal productivity
high enough so that people prefer
them to leisure. TI he capital stock is Ino
limitation, siince 5-6 percent unemployment
has beeni associated with more than
20 percent excess capacity. Mioreover,
when more jobs are createdI by expansion
of demand, with or without inflation, labor
force participation increases; this would
hardly occur if the aclditional jobs were low
in quality and productivity. As the parable
of the central employment plannier indicates,
there will be excessive waiting for
jobs if the roster of jobs an(d the meniu of
vacancies are suboptimal.

In summary, labor markets characterized
by 5-6 percent unemployment do
not display the symptoms one would exI
See Lucas and Rapping, pp. 257-305, in Phelps et al.


### ---Economics-1972-0-11.txt---
pect if the unemployment were voluntary
search activity. Even if it were voluntary,
search activity on such a large scale would
surely be socially wasteful. The only
reason anyone might regard so high an
unemployment rate as an equilibrium
and social optimum is that lower rates
cause accelerating inflation. B3 ut this is
almost tautological. TIhe inferences of equilibrium
anid optimality would be more

conivincing if they were corroboratecI by
direct evidence.

IV. Why is There Inflation without
Aggregate Excess Demand?

Zero-inflation unemployment is not
wholly voluntary, not optimal, I might
eveni say not natural. In other words, the
economy has an inflationary bias: WNhen
labor markets provide as many jobs as
there are willing workers, there is inflation,
perhaps accelerating inflation. Why?
The Phillips curve has been an empirical
finding in search of a theory, like Pirandello
characters in search of an author.
One rationalization might be termecl a
theory of stochastic macro-equilibrium:
stochastic, because random intersectoral
shocks keep individual labor markets in
diverse states of disequilibrium; macroequilibrium,
because the perpetual flux

of particular markets produces fairly
defnite aggregate outcomes of unemployment
and wages. Stimulated by Phillips's
1958 findings, Richard Lipsey proposed a
model of this kind in 1960, and it has
since been elaborated by Archibald, pp.
212-23 and Holt, pp. 53-123 and 224-56
in Phelps et. al., and others. I propose
now to sketch a theory in the same
spirit.

It is an essential feature of the theory
that economy-wide relations among employment,
wages, and prices are aggregations
of diverse outcomes in heterogeneous
markets. The myth of macroeconomics is
that relations among aggregates are enlarged
analogues of relations among corresponding
variables for individual households,
firms, industries, markets. The myth
is a harmless and useful simplification in
many contexts, but sometimes it misses
the essence of the phenomenon.
Unemployment is, in this model as in
Keynes reinterpreted, a disequilibrium phenomenon.
Money wages do not adjust

rapidly enough to clear all labor markets
every clay. Excess supplies in labor markets
take the form of unemployment, and
excess demands the form of unfilled
vacancies. At any moment, markets vary
widlely in excess demand or supply, and
the economy as a whole shows both
vacancies and unemployment.

The overall balance of vacancies and
unemployment is determined by aggregate
demand, and is therefore in principle subject
to control by overall monetary and
fiscal policy. Higher aggregate demand
means fewer excess supply markets and
more excess demand markets, accordingly
less unemployment and more vacancies.
In any particular labor market, the rate
of increase of money wages is the sum of
two components, an equilibrium component
and a disequilibrium component. The
first is the rate at which the wage would
increase were the market in equilibrium,
with neither vacancies nor unemployment.
The other component is a function of excess
demand and supply-a monotonic

function, positive for positive excess demand,
zero for zero excess demand, nonpositive
for excess supply. I begin with
the disequilibrium component.

Of course the disequilibrium components
are relevant only if disequilibria
persist. Why aren't they eliminated bv the
very adjustments they set in motion ?
Workers will move from excess supply
markets to excess demand markets, and
from low wage to high wage markets.
Unless they overshoot, these movements
are equilibrating. The theory therefore


### ---Economics-1972-0-12.txt---
requires that new disequilibria are always
arising. Aggregate demand may be stable,
but beneath its stability is never-ending
flux: new products, new processes, new
tastes and fashions, new developments of
land and niatural resources, obsolescent
industries and (leclining areas.
The overlap of vacancies and unemployment-
-say, the sum of the two for

any given difference between them--is a
measure of the heterogeneity or dispersion
of individual markets. The amount of
(lispersion (lepen(1s directly on the size of
those shocks of demand anid technology
that keep markets in perpetual disequilibriumn,
and inversely on the responsive mobility
of labor. The one increases, the other
diminishes the frictional component of
unemployment, that is, the number of unfilled
vacancies coexisting with any given
unemployment rate.

A central assumptioin of the theory is
that the functions relating wage change
to excess demand or supply are non-linear,
specifically that unemployment retards
money wages less than vacancies accelerate
them. Noinlinearity in the response of
wages to excess demand has several important
implications. First, it helps to
explain the characteristic observed curvature
of the Phillips curve. Each successive
increment of unemployment has less effect
in reducing the rate of inflation. Linear
wage response, on the other hand, would
mean a linear Phillips relation.
Second, given the overall state of aggregate
demand, economy-wide vacancies less
unemployment, wage inflation will be
greater the larger the variance among
markets in excess (lemand and supply.
As a number of recent empirical studies,
have confirmed (see George Perry and
Charles Schultze), dispersion is inflationary.
Of course, the rate of wage

inflation will depend not only on the
overall (lispersion of excess demands and
supplies across markets but also on the
particular markets where the excess supplies
and demands happen to fall. An unlucky
random (Irawing might put the

excess demands in highly responsive markets
and the excess supplies in especially
unresponsive ones.

Third, the nonlinearity is an explanation
of inflationary bias, in the following
sense. Even when aggregate vacancies are
at most equal to unemployment, the average
disequilibrium component will be
positive. Full employment in the sense of
equality of vacancies and unemployment
is not compatible with price stability.
Zero inflation requires unemployment in
excess of vacancies.

Criteria that coincide in full long-run
equilibrium zero inflation and zero aggregate
excess demand diverge in stochastic
macro-equilibrium. Full long-run
equilibrium in all markets would show no
unemployment, no vacancies, no unanticipated
inflation. But with unending sectoral
flux, zero excess (lemand spells inflation
and zero inflation spells net excess
supply, unemployment in excess of vacancies.
In these circumstances neither
criterion can be justified simply because it
is a property of full long-run equilibrium.
Both criteria automatically allow for frictional
unemployment incident to the required
movements of workers between

markets; the no-inflation criterion requires
enough additional unemployment to wipe
out inflationary bias.

' I turn now to the equilibrium component,
the rate of wage increase in a market
with neither excess demand nor excess
supply. It is reasonable to suppose that the
equilibrium component depends on the
trend of wages of comparable labor elsewhere.
A "competitive wage," one that
reflects relevant trends fully, is what employers
will offer if they wish to maintain
their share of the volume of employment.
TI his will happen where the rate of growth
of marginal revenue product the compound


### ---Economics-1972-0-13.txt---
of productivity increase and price
inflation-is the same as the trend in
wages. But in some markets the equilibrium
wage will be rising faster, and in
others slower, than the economy-wide
wage trend.

A "natural rate" result follows if actual
wage increases feed fully into the equilibrium
components of future wage increases.
There will be acceleration whenever the
non-linear disequilibrium effects are on
average positive, and steady inflation, that
is stochastically steady inflation, only at
unemployment rates high enough to make
the disequilibrium effects wash out. Phillips
tradeoffs exist in the short run, and
the time it takes for them to evaporate
depends on the lengths of the lags with
which today's actual wage gains become
tomorrow's standards.

A rather minor modification may preserve
Phillips tradeoffs in the long run.
Suppose there is a floor on wage change in
excess supply markets, independent of the
amount of excess supply and of the past
history of wages and prices. Suppose, for
example, that wage change is never negative;
it is either zero or what the response
function says, whichever is algebraically
larger. So long as there are markets where
this floor is effective, there can be determinate
rates of economy-wide wage inflation
for various levels of aggregate demand.
Markets at the floor do not increase their
contributions to aggregate wage inflation
when overall demand is raised. Nor is their
contribution escalated to actual wage
experience. But the frequency of such
markets diminishes, it is true, both with
overall demand and with inflation. The
floor phenomenon can preserve a Phillips
tradeoff within limits, but one that becomes
ever more fragile and vanishes as
greater demand pressure removes markets
from contact with the zero floor. The
model implies a long-run Phillips curve
that is very flat for high unemployment
and becomes vertical at a critically low
rate of unemployment.

These implications seem plausible and
even realistic. It will be objected, however,
that any permanent floor independent of
general wage and price history and expectation
must indicate money illusion.

The answer is that the floor need not be
permanent in any single market. It could
give way to wage reduction when enough
unemployment has persisted long enough.
But with stochastic intersectoral shifts of
demand, markets are always exchanging
roles, and there can always be some markets,
not always the same ones, at the floor.
This model avoids the empirically questionable
implication of the usual natural
rate hypothesis that unemployment rates
only slightly higher than the critical rate
will trigger ever-accelerating deflation.
Phillips curves seem to be pretty flat at
high rates of unemployment. During the
great contraction of 1930-33, wage rates
were slow to give way even in the face of
massive unemployment and substantial
deflation in consumer prices. Finally in
1932 and 1933 money wage rates fell more
sharply, in response to prolonged unemployment,
layoffs, shutdowns, and to

threats and fears of more of the same.
I have gone through this example to
make the point that irrationality, in the
sense that meaningless differences in
money values permanently affect individual
behavior, is not logically necessary for
the existence of a long-run Phillips tradeoff.
In full long-run equilibrium in all
markets, employment and unemployment
would be independent of the levels and
rates of change of money wage rates and
prices. But this is not an equilibrium that
the system ever approaches. The economy
is in perpetual sectoral disequilibrium
even when it has settled into a stochastic
macro-equilibrium.

I suppose that one might maintain that
asymmetry in wage adjustment and temporary


### ---Economics-1972-0-14.txt---
resistance to money wage decline
reflect money illusion in some sense. Such
an assertion would have to be based on an
extension of the domain of well-defined
rational behavior to cover responses to
change, adjustment speeds, costs of information,
costs of organizing and operating
markets, and a host of other problems
in dynamic theory. These theoretical extensions
are in their infancy, although
much work of interest and promise is being
done. Meanwhile, I doubt that significant
restrictions on disequilibrium adjustment
mechanisms can be deduced from first
principles.

Why are the wage aind salary rates of
employed workers so insensitive to the
availability of potential replacements?
One reason is that the employer makes
some explicit or implicit commitments in
putting a worker on the payroll in the
first place. The employee expects that his
wages and terms of employment will
steadily improve, certainly never retrogress.
He expects that the employer will
pay him the rate prevailing for persons of
comparable skill, occupation, experience,
and seniority. He expects such commitments
in return for his own investments in
the job; arrangements for residence, transportation,
and personal life involve set-up
costs which will be wasted if the job turns
sour. The market for labor services is not
like a market for fresh produce where the
entire current supply is auctioned daily.
It is more like a rental housing market,
in which most existing tenancies are the
continuations of long-term relationships
governed by contracts or less formal understandings.


Employers and workers alike regard the
wages of comparable labor elsewhere as a
standard, but what determines those reference
wages? There is not even an auction
where workers and employers unbound by
existing relationships and commitments
meet and determine a market-clearing
wage. If such markets existed, they would
provide competitively determined guides
for negotiated and administered wages,
just as stock exchange prices are reference
points for stock transactions elsewhere.
In labor markets the reverse is closer to
the truth. Wage rates for existing employees
set the standards for new employees,
too.

The equilibrium components of wage
increases, it has been argued, depend on
past wage increases throughout the economy.
In those theoretical and econometric
models of inflation where labor
markets are aggregated into a single
market, this relationship is expressed as
an autoregressive equation of fixed structure:
current wage increase depends on
past wage increases. The same description
applies when past wage increases enter indirectly,
mediated by price inflation and
productivity change. The process of mutual
interdependence of market wages is a
good deal more complex and less mechanical
than these aggregated models suggest.
Reference standards for wages differ
from market to market. The equilibrium
wage increase in each market will be some
function of past wages in all markets, and
perhaps of past prices too. But the function
need not be the same in every market.
Wages of workers contiguous in geography
industry, and skill will be heavily weighted.
Imagine a wage pattern matrix of coefficients
describing the dependence of the
percentage equilibrium wage increase in
each market on the past increases in all
other markets. The coefficients in each row
are non-negative and sum to one, but their
distribution across markets and time lags
will differ from row to row.

Consider the properties of such a system
in the absence of disequilibrium inputs.
First, the system has the "natural rate"
property that its steady state is indeterminate.
Any rate of wage increase that has
been occurring in all markets for a long
enough time will continue. Second, from
irregular initial conditions the system will


### ---Economics-1972-0-15.txt---
move toward one of these steady states,
but which one depends on the specifics of
the wage pattern matrix and the initial
conditions. Contrary to some pessimistic
warnings, there is nro arithmetic compulsion
that makes the whole system gravitate
in the direction of its most inflationary
sectors. The ultimate steady state inflation
will be at most that of the market
with the highest initial inflation rate, and
at least that of the market with the lowest
initial inflation rate. It need not be equal
to the average inflation rate at the beginning,
but may be either greater or

smaller. Third, the adjustment paths are
likely to contain cyclical conmponents,
damped or at most of constant amplitude,
and during adjustments both individual
and average wage movements may diverge
substantially in both directions from
their ultimate steady state value. Fourth,
since wage decisions and negotiations
occur infrequently, relative wage adjustments
involve a lot of catching up and
leap-frogging, and probably take a long
time. I have sketched the formal properties
of a disaggregated wage pattern system
of this kind simply to stress again the
vast simplification of the one-market
myth.

A system in which only relative magnitudes
matter has only a neutral equilibrium,
from which it can be permanently
displaced by random shocks. Even when a
market is in equilibrium, it may outdo the
recent wage increases in related markets. A
shock of this kind, even though it is not
repeated, raises permanently the steady
state inflation rate. This is true cost-push
-inflation generated neither by previous
inflation nor by current excess demand.
Shocks, of course, may be negative as well
as positive. For example, upward pushes
arising from adjustments in relative wage
levels will be reversed when those adjustments
are completed.

To the extent that one man's reference
wages are another man's wages, there is
something arbitrary and conventional,
indeterminate and unstable, in the process
of wage setting. In the same current market
circumstances, the reference pattern
might be 8 percent per year or 3 percent
per year or zero, depending on the historical
prelude. Market conditions, unemployment
and vacancies and their distributions,
shape history and alter reference
patterns. But accidental circumstances
affecting stragetic wage settlements
also cast a long shadow.

Price inflation, as previously observed,
is a neutral method of making arbitrary
money wage paths conform to the realities
of productivity growth, neutral in preserving
the structure of relative wages.
If expansion of aggregate demand brings
both more inflation and more employment,
there need be no mystery why unemployed
workers accept the new jobs,

or why employed workers do not vacate
theirs. They need not be victims of ignorance
or inflation illusion. They genuinely
want more work at feasible real wages,
and they also want to maintain the relative
status they regard as proper and just.
Guideposts could be in principle the
functional equivalent of inflation, a neutral
method of reconciling wage and productivity
paths. The trick is to find a

formula for mutual deescalation which
does not offend conceptions of relative
equity. No one has devised a way of
controlling average wage rates without
intervening in the competitive struggle
over relative wages. Inflation lets this
struggle proceed and blindly, impartially,
impersonally, and nonpolitically scales
down all its outcomes. There are worse
methods of resolving grotup rivalries and
social conflict.

V. The Role of Monopoly Power

Probably the most popular explanation
of the inflationary bias of the economy is
concentration of economic power in large
corporations and unions. These powerful


### ---Economics-1972-0-16.txt---
monopolies and oligopolies, it is argued,
are immune from competition in setting
wages and prices. The unions raise wages
above competitive rates, with little regard
for the unemployed and under-employed
workers knocking at the gates. Perhaps
the unions are seeking a bigger share of
the revenues of the monopolies and
oligopolies with whom they bargain. But
they don't really succeed in that objective,
because the corporations simply pass the
increased labor costs, along with mark-ups,
on to their helpless customers. The remedy,
it is argued, is either atomization of big
business and big labor or strict public
control of their prices and wages.
So simple a diagnosis is vitiated by confusion
between levels and rates of change.
Monopoly power is no doubt responsible
for the relatively high prices and wages of
some sectors. But can the exercise of
monopoly power generate ever-rising price
and wages? Monopolists have no reason
to hold reserves of unexploited power.
But if they did, or if events awarded them
new power, their exploitation of it would
raise their real prices and wages only
temporarily.

Particular episodes of inflation may be
associated with accretions of monopoly
power, or with changes in the strategies
and preferences of those who possess it.
Among the reasons that wages and prices
rose in the face of mass unemployment
after 1933 were NRA codes and other
early New Deal measures to suppress competition,
and the growth of trade union

membership and power under the protection
of new federal legislation. Recently
we have witnessed substantial gains in the
powers of organized public employees.
Unions elsewhere may not have gained
power, but some of them apparently have
changed their objectives in favor of wages
at the expense of employment.

One reason for the popularity of the
monopoly power diagnosis of inflation is
the identification of administered prices
and wages with concentrations of economic
power. When price and wage increases are
the outcomes of visible negotiations and
decisions, it seems obvious that identifiable
firms and unions have the power to affect
the course of inflation. But the fact that
monopolies, oligopolies, and large unions
have discretion does not mean it is invariably
to their advantage to use it to
raise prices and wages. Nor are administered
prices and wages found only in
high concentration sectors. Very few prices
and wages in a modern economy, even in
the more competitive sectors, are determined
in Walrasian auction markets.

No doubt there has been a secular increase
in the prevalence of administered
wages and prices, connected with the relative
decline of agriculture and other sectors
of self-employment. This development
probably has contributed to the
inflationary bias of. the economy, by enlarging
the number of labor markets

where the response of money wages to
excess supply is slower than their response
to excess demand. The decline of agriculture
as a sector of flexible prices and wages
and as an elastic source of industrial labor
is probably an important reason why the
Phillips trade off problem is worse now
than in the 1920's. Sluggishness of response
to excess supply is a feature of
administered prices, whatever the market
structure, but it may be accentuated by
concentration of power per se. For example,
powerful unions, not actually

forced by competition to moderate their
wage demands, may for reasons of internal
politics be slow to respond to unemployment
in their ranks.

VI. Some Reflections on Policy
If the makers of macro-economic policy
could be sure that the zero-inflation rate
of unemployment is natural, voluntary,
and optimal, their lives would be easy.


### ---Economics-1972-0-17.txt---
Friedman told us that all macro-economic
policy needs to do, all it should try to do, is
to make nominal national income grow
steadily at the natural rate of growth of
aggregate supply. This would sooner or
later result in price stability. Steady price
deflation would be even better, he said,
because it would eliminate the socially
wasteful incentive to economize money
holdings. In either case, unemployment
will converge to its natural rate, and
wages and prices will settle into steady
trends. Under this policy, whatever unemployment
the market produces is the correct
result. No tradeoff, no choice, no
agonizing decisions.

I have argued this evening that a substantial
amount of the unemployment

compatible with zero inflation is involuntary
an(l nonoptimal. This is, in my
opinion, true whether or not the inflations
associated with lower rates of unemployment
are steady or ever-accelerating.
Neither macro-economic policy makers,
nor the elected officials and electorates to
whom they are responsible, can avoid
weighing the costs of unemployment
against those of inflation. As Phelps has
pointed out, this social choice has an intertemporal
dimension. The social costs of
involutionary unemployment are mostly
obvious and immediate. The social costs
of inflation come later.

What are they? Economists' answers
have been remarkably vague, even though
the prestige of the profession has reinforced
the popular view that inflation leads
ultimately to catastrophe. Here indeed is
aT case where abstract economic theory
has a powerful hold on public opinion
and policy. The prediction that at low
unemployment rates inflation will accelerate
toward ultimate disaster is a theoretical
deduction with little empirical
support. In fact the weight of econometric
evidence has been against acceleration,
let alone disaster. Yet the deduction has
been convincing enough to persuade this
country to give up billions of dollars of
annual output and to impose sweeping
legal controls on prices and wages. Seldom
has a society made such large immediate
and tangible sacrifices to avert an ill defined,
uncertain, eventual evil.

According to economic theory, the
ultimate social cost of anticipated inflation
is the wasteful use of resources to
economize holdings of currency and other
noninterest-bearing means of payment.
I suspect that intelligent laymen would
be utterly astounded if they realized that
this is the great evil economists are talking
about. They have imagined a much more
devastating cataclysm, with Vesuvius
vengefully punishing the sinners below.
Extra trips between savings banks and
commercial banks? What an anti-climax!
With means of payment-currency plus
demand deposits-equal currently to 20
percent of GNP, an extra percentage point
of anticipated inflation embodied in nominal
interest rates produces in principle a
social cost of 2/10 of I percent of GNP
per year. This is an outside estimate. An
unknown, but substantial, share of the
stock of money belongs to holders who are
not trying to economize cash balances and
are not near any margin where they would
be induced to spend resources for this purpose.
These include hoarders of large denomination
currency, about one-third of

the total currency in public hands, for
reasons of privacy, tax evasion, or illegal
activity. They include tradesmen and
consumers whose working balances turn
over too rapidly or are too small to justify
any effort to invest them in interestbearing
assets. They include corporations
who, once they have been induced to
undertake the fixed costs of a sharp-pencil
money management department, are already
minimizing their cash holdings.
They include businessmen who are in fact
being paid interest on demand deposits,


### ---Economics-1972-0-18.txt---
although it takes the form of preferential
access to credit and other bank services.
But, in case anyone still regards the waste
of resources in unnecessary transactions
between money and interest-bearing financial
assets as one of the major economic
problems of the day, there is a simple and
straightforward remedy, the payment of
interest on demand deposits and possibly,
with ingenuity, on currency too.
The ultimate disaster of inflation would
be the breakdown of the monetary payments
system, necessitating a currency
reform. Such episodes have almost invariably
resulted from real economic catastrophes-
wars, defeats, revolutions, reparations-
not from the mechanisms of

wage-price push with which we are concerned.
Acceleration is a scare word, conveying
the image of a rush into hyperinflation
as relentlessly deterministic and
monotonic as the motion of falling bodies.
Realistic attention to the disaggregated
and stochastic nature of wage and price
movements suggests that they will show
diverse and irregular fluctuations around
trends that are difficult to discern and
extrapolate. The central trends, history
suggests, can accelerate for a long, long
time without generating hyper-inflations
destructive of the payments mechanism.
Unanticipated inflation, it is contended,
leads to mistaken estimates of relative
prices and consequently to misallocations
of resources. An example we have already
discussed is the alleged misallocation of
time by workers who over-estimate their
real wages. The same error would lead to
a general over-supply by sellers who contract
for future deliveries without taking
correct account of the increasing prices of
the things they must buy in order to fulfill
the contract. Unanticipated deflation
would cause similar miscalculations and
misallocations. Indeed, people can make
these same mistakes about relative prices
even when the price level is stable. The
mistakes are more likely, or the more
costly to avoid, the greater the inflationary
trend. There are costs in setting
and announcing new prices. In an inflationary
environment price changes must
be made more frequently-a new catalog
twice a year instead of one, or some formula
for automatic escalation of announced
prices. Otherwise, with the interval
between announcements unchanged,
the average misalignment of relative prices
will be larger the faster the inflation. The
same problem would arise with rapid
deflation.

Unanticipated inflation and deflationand
unanticipated changes in relative
prices-are also sources of transfers of
wealth. I will not review here the rich and
growing empirical literature on this subject.
Facile generalizations about the progressivity
or equity of inflationary transfers
are hazardous; certainly inflation does
not merit the cliche that it is "the cruelest
tax." Let us not forget that unemployment
has distributional effects as well as deadweight
losses.

Some moralists take the view that the
government has promised to maintain the
purchasing power of its currency, but this
promise is their inference rather than any
pledge written on dollar bills or in the
Constitution. Some believe so strongly in
this implicit contract that they are willing
to suspend actual contracts in the name of
anti-inflation.

I have long contended that the government
should make low-interest bonds of
guaranteed purchasing power available
for savers and pension funds who wish to
avoid the risks of unforeseen inflation. The
common objection to escalated bonds is
that they would diminish the built-in
stability of the system. The stability in
question refers to the effects on aggregate
real demand, ceteris paribus, of a change in
the price level. The Pigou effect tells us
that government bondholders whose
wealth is diminished by inflation will spend
less. This brake on old-fashioned gap


### ---Economics-1972-0-19.txt---
inflation will be thrown away if the bonds
are escalated. These considerations are
only remotely related to the mechanisms of
wage and price inflation we have been
discussing. In the 1970's we know that the
government can, if it wishes, control
aggregate demand-at any rate, its ability
to do so is only trivially affected by the
presence or absence of Pigou effects on
part of the government debt.

In considering the intertemporal tradeoff,
we have no license to assume that the
natural rate of unemployment is independent
of the history of actual unemployment.
Students of human capital have
been arguing convincingly that earning
capacity, indeed transferable earning capacity,
depends on experience as well as
formal education. Labor markets soggy
enough to maintain price stability may
increase the number of would-be workers
who lack the experience to fit them for
jobs that become vacant.

Macro-economic policies, monetary and
fiscal, are incapable of realizing society's
unemployment and inflation goals simultaneously.
This dismal fact has long stimulated
a search for third instruments to do
the job: guideposts and incomes policies,
on the one hand, labor market and manpower
policies, on the other. Ten to fifteen
years ago great hopes were held for both.
The Commission on Money and Credit in
1961, pp. 39-40, hailed manpower policies
as the new instrument that would overcome
the unemployment-inflation dilemma.
Such advice was taken seriously in
Washington, and an unprecedented spurt
in manpower programs took place in the
1960's. The Council of Economic Advisers
set forth wage and price guideposts in
1961-62 in the hope of "talking down" the
Phillips curve (pp. 185-90). It is discouraging
to find that these efforts did not keep
the problem of inflationary bias from
becoming worse than ever.

So it is not with great confidence or
optimism that one suggests measures to
mitigate the tradeoff. But some proposals
follow naturally from the analysis, and
some are desirable in themselves anyway.
First, guideposts do not wholly deserve
the scorn that "toothless jawboning" often
attracts. There is an arbitrary, imitative
component in wage settlements, and maybe
it can be influenced by national standards.
Second, it is important to create jobs for
those unemployed and discouraged workers
who have extremely low probability of
meeting normal job specifications. Their
unemployment does little to discipline
wage increases, but reinforces their deprivation
of human capital and their other
disadvantages in job markets. The National
Commission on Technology, Automation
and Economic Progress pointed

out in 1966 the need for public service jobs
tailored to disadvantaged workers. They
should not be "last resort" or make-work
jobs, but regular permanent jobs capable
of conveying useful experience and inducing
reliable work habits. Assuming
that the additional services produced by
the employing institutions are of social
utility, it may well be preferable to employ
disadvantaged workers directly rather
than to pump up aggregate demand until
they reach the head of the queue.
Third, a number of measures could be
taken to make markets more responsive to
excess supplies. This is the kernel of truth
in the market-power explanation of inflationary
bias. In many cases, government
regulations themselves support prices and
wages against competition. Agricultural
prices and construction wages are wellknown
examples. Some trade unions follow
wage policies that take little or no account
of the interests of less senior members and
of potential members. Since unions operate
with federal sanction and protection, perhaps
some means can be found to insure
that their memberships are open and that
their policies are responsive to the unemployed
as well as the employed.

As for macro-economic policy, I have


### ---Economics-1972-0-20.txt---
argued that it should aim for unemployment
lower than the zero-inflation rate.
How much lower? Low enough to equate
unemployment and vacancies? We cannot
say. In the nature of the case there is no
simple formula-conceptual, much less.
statistical-for full employment. Society
cannot escape very difficult political and
intertemporal choices. We economists can
illuminate these choices as we learn more
about labor markets, mobility, and search,
and more about the social and distributive
costs of both unemployment and inflation.
Thirty-five years after Keynes, welfare
macroeconomics is still a relevant and
challenging subject. I dare to believe it has
a bright future.
 ## Economics-1973-0


### ---Economics-1973-0-03.txt---
The ceremonial address of the President
of the American Economic Association is
an art form which, I imagine like most of
my predecessors, I have thoughtfully reviewed.
On occasion, in the past, the addresses
have dealt with some substantive
problem of our subject or some afflicting
problem of the economy. More often they
have dealt, always a shade critically, with
the methodology of economics. While accepting
the larger science there has been
adverse comment on the detailed elements
of its practice. Economics is insufficiently
normative. Model building has become an
end, not a means. For several recent years
in succession the criticism-which involved
a certain element of personal introspection-
included an exceptionally grave
attack on mathematical economics. The
style of these addresses, I might note in
passing, is as distinctive as the subject
matter. It features the thoughtful solemnity
of men who sense that we are speaking
for the ages. It may be worth a moment's
time, on these great occasions, to recall
that ours is a subject which features defeated
expectations.

I am moved this evening to depart from
the established rites. I should like to concern
myself with basic questions of assumption
and structure. If this breaks with
tradition, it does not break with present
professional tendency. We meet at a time
when criticism is general when the larger
body of established theory is under extensive
attack. Within the last half-dozen
years what before was simply called economics
in the nonsocialist world has come
to be designated neoclassical economics
with appropriate overtures to the Keynesian
and post-Keynesian development.
From being a general and accepted theory
of economic behavior this has become a
special and debatable interpretation of
such behavior. For a new and notably
articulate generation of economists a reference
to neoclassical economics has become
markedly pejorative.

I would judge as well as hope that the
present attack will prove decisive. The
established theory has reserves of strength.
It sustains much minor refinement which
does not raise the question of overall
validity or usefuliness. It survives strongly
in the textbooks although even in this
stronghold one senses anxiety among the
more progressive or commercially sensitive
authors. Perhaps there are limits to
what the young will accept.

And the arrangements by which orthodoxy
is conserved in the modern academy
also remain formidable. In its first half
century or so as a subject of instruction
and research, economics was subject to
censorship by outsiders. Businessmen and
their political and ideological acolytes
kept watch on departments of economics
and reacted promptly to heresy, the latter
being anything that seemed to threaten
the sanctity of property, profits, a proper
tariff policy, a balanced budget, or which
involved sympathy for unions, public
ownership, public regulation or, in any
organized way, for the poor. The growing
power and self-confidence of the educational
estate, the formidable and growing
complexity of our subject and, no doubt,
the increasing acceptability of our ideas
has largely relieve(d us of this intervention.


### ---Economics-1973-0-04.txt---
In leading centers of instruction faculty
responsibility is either secure or increasingly
so. But in place of the old censorship
has come a new despotism. That consists in
defining scientific excellence as whatever is
closest in belief and method to the
scholarly tendency of the people who are
already there. This is a pervasive and
oppressive thing not the less dangerous for
being, in the frequent case, both selfrighteous
and unconscious.

But there are problems even with this
control. Neoclassical or neo-Keynesian
economics, though providing unlimited opportunity
for demanding refinement, has

a decisive flaw. It offers no useful handle
for grasping the economic problems that
now beset the modern society. And these
problems are obtrusive-they will not lie
down and die as a favor to our profession.
No arrangement for the perpetuation of
thought is secure if that thought does not
make contact with the problems that it is
presumed to solve.

I will not omit this evening to mention
the failures of neoclassical theory. But I
want also to urge the means by which we
can reassociate ourselves with reality.
Some of this will summarize past argument,
more a book that is presently to be
published. At this stage even the most conservative
among my listeners will be reassured.
To speak well of one's own published
and unpublished writing, whatever
one's other aberrations, is strongly in our
professional tradition.

I

The most commonplace features of neoclassical
and neo-Keynesian economics are
the assumptions by which power, and
therewith political content, is removed
from the subject. The business firm is subordinate
to the instruction of the market
and, thereby, to the individual or household.
The state is subordinate to the instruction
of the citizen. There are exceptions
but these are to the general and controlling
rule, and it is firmly on the rule
that neoclassical theory is positione(l. If
the business firm is subordinate to the
market--if that is its master--then it does
not have power to deploy in the economy
save as this is in the service of the market
and the consumer. And the winning of
action to influence or rig the behavior of
markets apart, it cannot bring power to
bear on the state for there the citizen is in
charge.

The decisive weakness in neoclassical
and neo-Keynesian economics is not the
error in the assumptions by which it elides
the problem of power. The capacity for
erroneous belief is very great, especially
where it coincides with convenience.
Rather in eliding power--in making economics
a nonpolitical subject---neoclassical
theory, by the same process, destroys
its relation with the real world. The problems
of this world, moreover, are increasing,
both in number and in the depth of their
social affliction. In consequence neoclassical
and neo-Keynesian economics is relegating
its players to the social sidelines
where they either call no plays or urge the
wrong ones.

Specifically the exclusion of power and
the resulting political content from economics
causes it to foretell only two intrinsic
and important economic problems.
One of these is the microeconomic problem
of market imperfection-more specifically
of monopoly or oligopoly in product or
factor markets leading to aberration in
resource and income distribution. The
other is the macroeconomic problem of unemployment
or inflation-of a deficiency

or excess in the aggregate demand for
goods and services, including that associated
with monetary effects. And on both
problems the failure is dramatic. Neoclassical
economics leads to the wrong

solution of the microeconomic problem
and to no solution of the macroeconomic


### ---Economics-1973-0-05.txt---
problem. Meanwhile it leaves a whole
galaxy of other urgent economic issues
largely untouched.

It is now the considered sense of the
community, even of economists when unhampered
by professional doctrine, that
the most prominent areas of market
oligopoly-automobiles, rubber, chemicals,
plastics, alcohol, tobacco, detergents,
cosmetics, computers, bogus health remedies,
space adventure-are areas not of
low but of high development, not of inadequate
but of excessive resource use.
And there is a powerful instinct that in
some areas of monopoly or oligopoly,
most notably in the production of weapons
and weapons systems, resource use is dangerously
vast.

In further contradiction of the established
microeconomic conclusions, we have
an increasing reaction by the community
to deficient resource use in industries that,
at least in the scale and structure of the
firm, approach the market model. Housing,
health services, and local transportation
are among the leading cases. The deprivation
and social distress that follow from
the poor performance of these industries
are also something that, in their nondoctrinal
manifestation, most economists take
for granted.

The defender of the established doctrine
does, of course, argue that excess and
deprivation in resource use in the areas
just mentioned reflect consumer choice.
And in the areas of deprivation he can
rightly insist that the fault lies with firms
that, though small, are local monopolies
or reflect the monopoly power of unions.
These explanations beg two remarkably
obvious questions: Why does the modern
consumer increasingly tend to insanity,
increasingly insist on self-abuse? And why
do little monopolies perform badly and the
big ones too well?

In fact the neoclassical model has no
explanation of the most important microeconomic
problem of our time. That problem
is why we have a highly unequal
development as between industries of
great market power and industries of
slight market power, with the development,
in defiance of all doctrine, greatly
favoring the first.1

The macroeconomic failure has been, if
anything, more embarrassing. Save in its
strictly mystical manifestation in one
branch of monetary theory, modern macroeconomic
policy depends for its validity
and workability on the neoclassical market.
That market, whether competitive,
monopolistic, or oligopolistic, is the ultimate
and authoritative instruction to the
profit-maximizing firm. When output and
employment are deficient, policy requires
that aggregate demand be increased; this
is an instruction to the market to which
firms in turn respond. When the economy
is at or near the effective capacity of the
plant and the labor force and inflation is
the relevant social discomfort, the remedy
is reversed. Demand is curtailed; the result
is either an initial effect on prices or a
delayed one as surplus labor seeks employment,
interest rates fall and lower

factor costs bring stable or lower prices.
Such is the accepted basis of policy. It
follows faithfully from the neoclassical
faith in the market. The practical consequences
of pursuing it need no elucidation.
It has been tried in recent years in
every developed country. The common
result has been politically unacceptable
unemployment, persistent and (in my
view) socially damaging inflation or, more
often, a combination of the two. The extreme
failure has been, not surprisingly,
in the most advanced industrial country,


### ---Economics-1973-0-06.txt---
the United States. But the recent experience
of Britain has been almost equally
disenchanting. One gathers that there may
be Canadian politicians who now believe
that a combination of unemployment and
inflation is not the best platform on which
to fight a general election.

We should not deny ourselves either the
instruction or the amusement that comes
from the recent history of the United
States in this matter. Four years ago Mr.
Nixon came to office with a firm commitment
to neoclassical orthodoxy. In this he
was supported by some of its most distinguished
and devout exponents in all the
land. His subsequent discovery that he
was a Keynesian involved no precipitate
or radical departure from this faith. The
discovery came thirty-five years after The
General Theory; as I have just noted, all
neo-Keynesian policy rests firmly on the
paramount role of the market. But then a
year and. a half ago, facing reelection, he
found that his economists' comnmitment
to neoclassical and Keynesian orthodoxy,
however admirable in the abstract, was a
luxury that he could no longer afford. He
apostatized to wage and price control; so,
with exemplary flexibility of mind, did
his economists although admittedly this
acceptance of the real world has still to
survive its critical test which is the
apostates' return to computers and classrooms.
But our admiration for this pliability
should not keep us from recalling that,
when the President changed course, no
American economists were anywhere
working on the policy he was forced by
circumstances to adopt. And it is even
more disturbing that few are now working
on the policy which we have been forced
to follow.

More economists, in fact, are still concerning
themselves with the effort to reconcile
controls with the neoclassical
market. This has involved an unrewarding
combination of economics and archeology
with wishful thinking. It holds that an
inflationary momentum developed during
the late 1960's in connection with the
financing--or underfinancing of the Vietnam
war. And inflationary expectation
became part of business and trade union
calculation. The momentum and expectation
still survive. The controls are necessary
until these are dissipated. Then the
neoclassical and neo-Keynesian world will
return, along with the appropriate policies,
in all their quiet comfort. We may be sure
that will not happen. Nor will we expect it
to happen if we see the role of power and
political decision in modern economic behavior.


II

In place of the market system, we must
now assume that for approximately half of
all economic output there is a power or
planning system. (The latter term seems
to me more descriptive, less pejorative and
thus preferable.) The planning system consists
in the United States of, at the most,
2,000 large corporations. In their operation
they have power that transcends the
market. They rival where they do not
borrow from the power of the state. My
views on these matters will be familiar at
least to some, and I shall spare myself the
pleasure of extensive repetition. I cannot
think that the power of the modern corporation,
the purposes for which it is used
or the associated power of the moclern
union would seem implausible or even very
novel were they not in conflict with the
vested (loctrine.

Thus we agree that the modern corporation,
either by itself or in conjunction
with others, has extensive influence over
its prices and its major costs. Can we
doubt that it goes beyond its prices and
the market to persuade its customers? Or
that it goes back of its costs to organize
supply? Or that from its earnings or the
possession of financial afiliates it seeks to


### ---Economics-1973-0-07.txt---
control its sources of capital? Or that its
persuasion of the consumer joined with the
similar effort of other firms and with the
more than incidental blessing of neoclassical
pedagogy helps establish the values
of the community, notably the association
between well-being and the progressively
increased consumption of the products of
this part of the economy?

And as citizens, if not as scholars, we
would not deny that the modern corporation
has a compelling position in the
modern state. What it needs in research
and development, technically qualified
people, public works, emergency financial
support, becomes public policy. So does the
military procurement that sustains the
demand for numerous of its products. So,
perhaps, does the foreign policy that
justifies the military procurement. And
the means by which this power is brought
to bear on the state is widely accepted. It
requires an organization to deal with an
organization. And between public and
private bureaucracies between GM and
the Department of Transportation, General
Dynamics and the Pentagon there is
a deeply symbiotic relationship. Each of
these organizations can do much for the
other. There is even, between them, a
large and continuous interchange of executive
personnel.

Finally over this exercise of power and
much enhancing it is the rich gloss of
reputability. The men who guide the
modern corporation, including the financial,
legal, technical, advertising, and other
sacerdotal authorities in corporate function,
are the most respectable, affluent,
and prestigious members of the national
community. They are the Establishment.
Their interest tends to become the public
interest. It is an interest that even some
economists find it comfortable and rewarding
to avow.

That interest, needless to say, is profoundly
concerned with power with wining
acceptance by others of the collective
or corporate purpose. It does not disavow
profits. These are important for ensuring
the autonomy of the management what
I have called the technostructure and for
bringing the supply of capital within the
control of the firm. Profits are also a
source of prestige and therewith of influence.
But of paramount importance is the
much more directly political goal of
growth. Such growth carries a strong economic
reward; it directly enhances the
pay, perquisites, and opportunities for
promotion of the members of the technostructure.
And it consolidates and enhances
authority. It does this for the individualfor
the man who now heads a larger
organization or a larger part of an organization
than before. And it increases the
influence of the corporation as a whole.
Neoclassical economics is not without an
instinct for survival. It rightly sees the
unmanaged sovereignty of the consumer,
the ultimate sovereignty of the citizen and
the maximization of profits and resulting
subordination of the firm to the market
as the three legs of a tripod on which it
stands. These are what exclude the role of
power in the system. All three propositions
tax the capacity for belief. That the
modern consumer is the object of a massive
management effort by the producer is not
readily denied. The methods of such management,
by their nature, are embarrassingly
visible. It can only be argued that
somehow it all cancels out. Elections in
the United States and Canada are now
being fought on the issue of the subordination
of the state to corporate interest. As
voters, economists accept the validity of
the issue. Only their teaching denies it.
But the commitment of the modern corporate
bureaucracy to its expansion is, perhaps,
the clearest of all. That the modern
conglomerate always pursues profit over
aggrandizement is believed by none. It is a
commonplace of these last years, strongly


### ---Economics-1973-0-08.txt---
reflected in securities' prices, that agglomeration
has always been good for growth
but often bad for earnings.

There remains in the modern economyand
this I stress a world of small firms
where the instruction of the market is still
paramount, where costs are given, where
the state is remote and subject through the
legislature to the traditional pressures of
economic interest groups and where profit
maximization alone is consistent with survival.
We should not think of this as the
classically competitive part of the system
in contrast with the monopolistic or
oligopolistic sector from which the planning
system has evolved. Rather, in its
combination of competitive and monopolistic
structures, it approaches the entire
neoclassical model. We have, to repeat,
two systems. In one, power is still, as ever,
contained by the market. In another and
still evolving system, power extends incompletely
but comprehensively to markets,
to the people who patronize them, to
the state and thus, ultimately, to resource
use. The coexistence of these two systems
becomes, in turn, a major clue to economic
performance.

III

Power being so comprehensively deployed
in a very large part of the total economy,
there can no longer, except for reasons
of game-playing or more deliberate
intellectual evasion, be any separation by
economists between economics and politics.
When the modern corporation acquires
power over markets, power in the
community, power over the state, power
over belief, it is a political instrument,
different in form and degree but not in
kind from the state itself. To hold otherwise
to deny the political character of
the modern corporation is not merely to
avoid the reality. It is to disguise the reality.
The victims of that disguise are those
we instruct in error. The beneficiaries are
the institutions whose power we so disguise.
Let there be no question: Economics,
so long as it is thus taught, becomes,
however unconsciously, a part of an
arrangement by which the citizen or
student is kept from seeing how he is, or
will be, governed.

This does not mean that economics now
becomes a branch of political science. That
is a prospect by which we would rightly be
repelled. Political science is also the captive
of its stereotypes including that of citizen
control of the state. Also while economics
cherishes thought, at least in principle,
political science regularly accords reverence
to the man who knows only what has
been done before. Economics does not become
a part of political science. But
politics does and must become a part of
economics.

There will be fear that once we abandon
present theory, with its intellectually
demanding refinement and its increasing
instinct for measurement, we shall lose the
filter by which scholars are separated from
charlatans and windbags. These latter are
always a danger, but there is more danger
in remaining with a world that is not real.
And we shall be surprised, I think, at the
new clarity and intellectual consistency
with which we see our world, once power is
made a part of our system. To such a view
let me now turn.

IV

In the neoclassical view of the economy
a general identity of interest between
the goals of the business firm and those
of the community could be assumed. The
firm was subject to the instruction of
the community, either through the market
or the ballot box. People could not be fundamentally
in conflict with themselvesalways
given some reasonable decency in
income distribution. Once the firm in the
planning system is seen to have comprehensive
power to pursue its own interest,


### ---Economics-1973-0-09.txt---
this assumption becomes untenable. Perhaps
by accident its interests are those of
the public but there is no organic reason
why this must be so. In the absence of
proof to the contrary, divergence of interest,
not identity of interest, must be
assumed.

The nature of the conflict also becomes
predictable. Growth being a principal goal
of the planning system it will be great
where power is great. And in the market
sector of the economy, growth will, at least
by comparison, be deficient. This will not,
as neoclassical doctrine holds, be because
people have an amiable tendency to misunderstand
their needs. It will be because
the system is so constructed as to serve
badly their needs and then to win greater
or less acquiescence in the result. That the
present system should lead to an excessive
output of automobiles, an improbable
effort to cover the economically developed
sections of the planet with asphalt, a lunar
preoccupation with moon exploration, a
fantastically expensive and potentially
suicidal investment in missiles, submarines,
bombers, and aircraft carriers, is as one
would expect. These are the industries
with power to command resources for
growth. And central to public purpose
to sound resource utilization will be a
cutback in such industries, as all instinct
now suggests. Thus does the introduction
of power as a comprehensive aspect of our
system correct present error. Let us not
fail to note that these are exactly the industries
in which an uncomplicated neoclassical
view of monopoly and oligopoly
and of profit maximization at the expense
of ideal resource use would, of all things,
suggest an expansion of output. How
wrong are we allowed to be!

The counterpart of excessive resource
use in the planning system where power is
comprehensively deployed is a relatively
deficient resource use where power is circumscribed.
Such will be the case in the

part of the economy where competition
and entrepreneurial monopoly as distinct
from great organization are the rule. And
if the product or service is closely related
to comfort or survival, the discontent will
be considerable. That housing, health
services, local transportation, some household
services, are now areas of grave inadequacy
is agreed. It is in such industries
that all modern governments seek to expand
resource use. Here, in desperation,
even the devout free enterprisers accept
the need for social action, even of socialism.


Again, we may observe, the error of
economics is prejudicial. Although as
citizens we advocate restraint in the area
of excessive resource use, our teaching does
not. And though as citizens we urge social
action where the firm approaches the neoclassical
norm, our teaching does not. In
this latter case we not only disguise corporate
power but we make remedial action
in such areas as housing, health care,
transportation, also abnormal the consequence
of sui generis error that is never
quite explained. This is unfortunate for
here are tasks that require imagination,
pride and determination.

V

When power is admitted to our calculus,
our macroeconomic embarrassment also
disappears. Economics makes plausible
what governments are forced, in practice,
to do. Corporations have power in their
markets. So, and partly in consequence, do
unions. The competitive claims of unions
can most conveniently be resolved by passing
the cost of settlement along to the
public. Measures to arrest this exercise of
power by limiting aggregate demand must
be severe. And, not surprisingly, the power
of the planning system has been brought to
bear to exclude those macroeconomic measures
that have a primary effect on that
system. Thus monetary policy is entirely


### ---Economics-1973-0-10.txt---
permissible; that is at least partly because
its primary effect is on the neoclassical
entrepreneur who must borrow money.
Monetary constraint is far less painful
for the large established corporation which,
as an elementary exercise of power, has
ensured itself a supply of capital from
earnings or financial affiliates or morally
affiliated banks. The power of the planning
system in the community has also won
immunity for public expenditures important
to itself-highways, industrial research,
rescue loans, national defense.
These have the sanction of a higher public
purpose. A similar if still slightly less successful
effort is being made on behalf of
corporate and personal taxes. So fiscal
policy has also been accommodated to the
interests of the planning system.
Thus the inevitability of controls. The
interaction of corporate and trade union
power can be made to yield only to the
strongest fiscal and monetary restraints.
Those restraints that are available have a
comparatively benign effect on those with
power, but they weigh adversely on people
who vote. When no election is in prospect,
perhaps such a policy is possible. It will
earn applause for its respectability. But it
cannot be tolerated by anyone who must
weigh its popular effect.

As with the need for social action and
organization in the market sector there
are many reasons why it would be well
were economists to accept the inevitability
of wage and price control. It would help
keep politicians, responding to the resonance
of their own past instruction, from
supposing controls to be wicked and unnatural
and hence temporary and to be

abandoned whenever they seemed to be
working. This is a poor mood in which to
develop sound administration. And it
would cause economists themselves to
consider how controls can be made workable
and how the effect on income distribution
can be made equitable. With controls
this last becomes a serious matter.
The market is no longer a disguise for
inequality, however egregious, in income
distribution. Much inequality must
be seen to be the result of relative power.
VI

When power is made part of our system,
yet other matters of considerable current
moment are illuminated. Thus the counterpart
of systemic differences in development
as between the planning and market sectors
of the economy is systemic sectoral
differences in income. In the neoclassical
system, resource mobility is assumed,
broadly speaking, to equalize inter-industry
return. If there is inequality, it is
the result of barriers to movement. Now
we see that, given its comprehensive market
power, the planning system can protect
itself from adverse movements in its terms
of trade. The same power allows it to
accept unions for it need not absorb even
temporarily their demands. In the market
system, some areas of monopoly or union
power apart, there is no similar control of
the terms of trade. Given the absence of
market power there can be no similar
yielding on wage costs for there is no
similar certainty that they can be passed
on. (It is because of the character of the
industry he seeks to organize, not his
original power, that Cesar Chavez is for so
many the new Lenin.) And, in the market
system, the self-employed have the option
not present in the planning system of
reducing their own wages (and sometimes
those of families or immediate employees)
in order to survive.

Thus there is a built-in inequality in
income between the two systems. And
thus also the case for minimum wage
legislation, support to trade unions in
agriculture, price support legislation, and
most important, perhap5, a floor under
family income as antidotes to such interindustry


### ---Economics-1973-0-11.txt---
inequality. Again this view of
matters fits our present concerns. Minimum
wage legislation, price support
legislation, and support to collective bargaining
are all questions of continuing
political controversy as they apply to
small business and agriculture. They are
not serious issues in highly organized industry
in the planning system. And the
question of a floor under family income, a
matter of intense political argumeint, has
recently divided workers in the planning
system who would not be beneficiaries
from those in the market system who
would be. Again there is reassurance in a
view of the economy that prepares us for
the politics of our time.

The inclusion of power in economic calculus
also prepares us for the great debate
over the environment. It is the claim of
neoclassical economics that it foresaw
possible environmental consequences from
economic development-that it, some
time ago, embraced the concept of external
diseconomies of production and, by inference,
of consumption. Alas, this is a modest
claim. The noninclusion of external diseconomies
was long viewed as a minor

defect of the price system-an afterthought
for an hour's classroom discussion.
And, as E. J. Mishan has observed,
it was largely ignored in the textbooks.
Nor does the notion of external diseconomies
now offer a useful remedy. No one
can suppose, or really supposes, that more
than a fraction of the damage especially
that to the beauty and tranquility of our
surroundings -could be compensated in
any useful way by internalizing external
diseconomies.

If growth is the central and rewarding
purpose of the firm and if power is comprehensively
available to impose this goal

on the society, the possibility of conflict
between private growth and public purpose
as regards the environment is immediately
plausible. So, since this power
depends extensively not on force but
persuasion, is the effort to make pollution
seem palatable or worth the cost, including
the effort to nmake advertising of remedial
action a substitute for action. And so is the
remedy to which all industrial countries
are being forced. This is not, primarily, to
internalize external diseconomies. Rather
it is to specify the legal parameters within
which growth may proceed or, as in the
case of automobile use in central cities,
airplane use over urban areas, the SST,
industrial, commercial, and residential
appropriation of countryside andI roadside,
the kinds of growth that are inconsistent
with public purpose. We would have saved
much corruption of our surroundings if
our economics had held such action to be
the predictable consequence of the pursuit
of present economic goals and not the
exceptional result of a peculiar aberration
of the price system.

We had best, in any case, have the right
guide to action for the future for there is a
strong conservative case for such guidance.
While economists toy weakly with external
diseconomies, others are arguing
that growth itself is the villain. They are
seeking its extinction. To see environmental
damage as a natural consequence
of planning power and purpose and to see,
in consequence, the need for confining
growth within parameters that protect the
public interest could be important for ensuring
continued economic growth.

Finally, when power becomes part of our
system, so does Ralph Nader. We are
prepared for the explosion of concern now
called consumerism. If the consumer is the
ultimate source of authority, his abuse is
an occasional fault. He cannot be fundamentally
at odds with an economic system
that he commands. But if the producing
firm has comprehensive power and purposes
of its own, there is every likelihood
of conflict. Technology is then subordinate
to the strategy of consumer persuasion.


### ---Economics-1973-0-12.txt---
Products are changed not to make them
better but to take advantage of the belief
that what is new is better. There is a
high failure rate in engineering not what is
better but what can be sold. The consumer-
the unpersuaded or disenchanted
consumer rebels. This is not a rebellion
against minor matters of fraud or misinformation.
It is a major reaction against a
whole deployment of power by which the
consumer is made the instrument of purposes
that are not his own.

VII

There are two conclusions to which this
exercise-to which incorporation of power
into our system compels us. The first, in
a way, is encouraging. It is that economists'
work is not yet done. On the contrary,
it is just beginning. If we accept the
reality of power as part of our system, we
have years of useful work ahead of us.
And since we will be in touch with real
issues, and since issues that are real inspire
passion, our life will, again, be pleasantly
contentious, perhaps even usefully dangerous.


The other conclusion concerns the state.
For when we make power and therewith
politics a part of our system, we can no
longer escape or disguise the contradictory
character of the modern state. The state is
the prime object of economic power. It is
captured. Yet on all the matters I have
mentioned the restrictions on excessive
resource use, organization to offset inadequate
resource use, controls, action to
correct systemic inequality, protection of
the environment, protection of the consumer-
remedial action lies with the state.
The fox is powerful in the management of
the coop. To this management the chickens
must look for redress.

Thus perhaps our greatest question. Is
emancipation of the state from the control
of the planning system possible? No one
knows. And in the absence of knowledge
no one certainly will suggest that it will be
easy. But there is a gleam of encouragement.
As ever circumstances are forcing
the pace.

In the United States the recent election
was fought, all but exclusively, over issues
in which the purposes of the planning system
or its major participants diverge from
those of the public. The question of
defense expenditures is such an issue. That
of tax reform is another. The deprivation
in housing, mass transportation, health
services, city services, is yet another-one
that reflects the relative inability of these
industries to organize and command resources.
The question of a guaranteed

income is another such issue. Its effect, as
I have noted, is on incomes outside the
planning system on the exploited in the
market system, those who are rejected by
both. The environment is such an issuewith
its conflict between the technostructure'
s goal of growth and the public concern
for its surroundings. Only wage and
price control was not an issue in the recent
election. That was almost certainly because
economists of orthodox tendency
on both sides found the prospect too embarrassing
to discuss.

I do not mention these issues with any
purpose save to show that the questions
that emerge when power is made a part of
our calculus are present and real. We need
hardly remind ourselves that political
issues are made not by parties and politicians
but by circumstance.

Once power is made part of our system,
we will not of course escape the political
contention that comes from dealing with
issues that are real. This brings me to my
last point. I do not plead for partisanship
in our economics but for neutrality. But
let us be clear as to what is neutral. If the
state must be emancipated from economic
interest, a neutral economics would not
deny the need. This is what economics now


### ---Economics-1973-0-13.txt---
does. It tells the young and susceptible
and the old and vulnerable that economic
life has no content of power and politics
because the firm is safely subordinate to
the market and to the state and for this
reason it is safely at the command of the
consumer and citizen. Such an economics
is not neutral. It is the influential and invaluable
ally of those whose exercise of
power depends on an acquiescent public.
If the state is the executive committee of
the great corporation and the planning
system, it is partly because neoclassical
economics is its instrument for neutralizing
suspicion that this is so. I have spoken
of the emancipation of the state from economic
interest. For the economist there
can be no doubt as to where this task begins.
It is with the emancipation of economic
belief.
 ## Economics-1974-0


### ---Economics-1974-0-03.txt---
The content of presidential addresses to
this Association provides a fine example of
a random variable with a high variance. It
might even be a good subject for econometric
analysis; the variation might be
explained in terms of the economic conditions
of the moment, previous intellectual
investments, or even, for boldly interdisciplinary
analysis, the psychological states
or class origins of the speakers or the
audience. But no doubt captious theorists
like myself will object that the endogenous
variable is not cardinally measurable and
probably not even ordinally measurable;
tougher-minded econometricians will worry
about collinearity in the predetermined
variables; and practical-minded policy
analysts will see no discernible effect on
the gross national product, the price level,
or the balance of payments through effects
on either fiscal policy or the stock of
money. The last group, the policy-oriented,
are perhaps the least accurate; at least
according to Keynes, the effect of ideas on
policy is dominant, though the lag may be
as variable as and a good deal longer than
that of the stock of money on money gross
national product.

It is now more fashionable than it used
to be for statisticians to be told to take a
good look at their data before fitting
models. Taking presidential addresses as
our data, we find most frequently a review
of the speaker's main research concerns
but also expressions of methodological or
ethical concerns, historical surveys of varying
degrees of erudition and humor, and,
least frequently, new points of view on signifi-
cant problems of economics.

I am taking a somewhat different tack
today; it will be an expression of discontents
and expectations. As I shall try to
argue, the uncertainties about economics
are rooted in our need for a better understanding
of the economics of uncertainty;
our lack of economic knowledge is, in good
part, our difficulty in modelling the ignorance
of the economic agent.

Critical aspects of this need for reorientation
of theory have been recognized by
many scholars in the last quarter-century
and particularly in the last decade. I view
my remarks today as a summary and perspective
on a widely shared development
of thinking.

The starting point of discussion must
still be the much-abused neoclassical
theory. No really cohesive alternative
which aspires to the same level of completeness
exists. The neoclassical model is
founded on two concepts, which are considerably
different in nature. One is the
notion of the individual economic agent,
whose behavior is governed by a criterion
of optimization under constraints which
are partly peculiar to the agent, such as
production functions, and partly terms of
trade with the economic system as a whole.


### ---Economics-1974-0-04.txt---
The other is the market; here, the aggregate
of individual decisions is acknowledged,
and the terms of trade adjusted until
the decisions of the individuals are
mutually consistent in the aggregate, i.e.,
supply equals demand.

The neoclassical theory, especially in its
competitive form, can be and has been
given a rich formal development. Parenthetically,
one cause for the persistence of
neoclassical theory in the face of its long
line of critics is precisely that for some
reason of mathematical structure, the neoclassical
theory is highly manipulable and
flexible; when faced with a specific issue,
it can yield meaningful implications relatively
easily. Although I intend to air
complaints and desires for change today,
I must express my unabashed admiration
for the accomplishments of the neoclassical
viewpoint. In its most formal statement,
we simply use for analysis the equilibrium
conditions of the individual agent and of
the market, without inquiry as to how they
come to hold. Yet even these statements
turn out to yield revealing insights in the
workings of resource allocation. Why have
medical costs risen so rapidly relative to
other prices since 1967? The upward shift
in demand due to Medicare and Medicaid
with a price-inelastic supply of physicians
and hospitals provides a simple straightforward
answer; I cannot really imagine
how a Marxian or a neo-Ricardian would
even approach the question, though I suppose
they might dismiss it as unimportant.
The explanation of environmental problems
as due to the nonexistence of markets
is similarly an insight of purely neoclassical
origin. The now-demonstrated fact that
flexible exchange rates are a feasible way
of conducting international finance is a
triumph of theoretical insights over practical
men's convictions. More broadly, the
shifts in long-run resource allocation as
motivated by returns and, in particular,
the absence of a secular trend in technological
unemployment to the perpetual

surprise of the layman fit in well with the
neoclassical formulation but have no ready
explanation in alternate models.
Of course, the implications of neoclassical
theory have also been conspicuously
falsified in important ways. Most notably,
the recurrent periods of unemployment
which have characterized the history of
capitalism are scarcely compatible with a
neoclassical model of market equilibrium.
A post-Keynesian world in which unemployment
is avoided or kept at tolerable
levels by recurrent alterations in fiscal or
monetary policy is no more explicable by
neoclassical axioms, though the falsification
is not as conspicuous.

Inequality in economic development
among countries and among groups and
regions within a country provides a second,
somewhat complicated difficulty for neoclassical
theory. A purely neoclassical answer
would explain differences in per
capita income by differences in physical
and human assets per capita. This of
course raises the further question, how
this came to be, a question which would
require a fully dynamic model to answer;
but I think the more compelling problem
is that the differences in income seem
much too vast to be explained by factor
differences. Indeed, in the presence of international
trade and especially international
capital movements, wage differences
should be very strongly reduced compared
with what would occur in autarchic states
where domestic capital is the limiting factor.
Hence, we come immediately to the
explanation that there are differences in
the production possibility sets of the
different countries. This conclusion is a
legitimate and important use of neoclassical
analysis; but obviously it raises new
questions, to which we will return.
I pass by the whole tangle of questions
relating to the holding of money and the
general level of prices. In its pure form,


### ---Economics-1974-0-05.txt---
neoclassical theory is a theory of relative
prices. Monetary theories vaguely related
to it in spirit can be grafted on to it, but
none have succeeded in achieving a
genuine synthesis.

The two failures of the neoclassical explanatory
mechanism reflect on its foundations
in quite different ways. The existence
of unemployment is clearly a direct contradiction
to the notion of the smoothly

clearing market. One must of course be
aware that the official measure of unem-'
ployment is by no means a simple inequality
between supply and demand; it aggregates
a whole range of distinct markets,
it does not separate out voluntary and involuntary
unemployment according to the

tests of economic theory, and it does not
take account of unfilled jobs. I do not subscribe,
I hasten to say, to the sometimes
expressed view that all unemployment is
essentially voluntary, an unwillingness to
search or whatnot; indeed, the official
measure may underestimate the degree of
disequilibrium in the labor market, particularly
with regard both to underutilization
of advanced skills and discouraged job
seekers. With all these qualifications, it is
clear that statistical unemployment does
correspond to a disequilibrium as that
term is used in the basic neoclassical
model; there are two individuals, identical
in productive capacity and both willing
to work at a given wage, but one is working
at that wage and the other is not.
Differential levels of economic development,
on the other hand, point to a difficulty
with the other fundamental concept;
the conditions of optimization. If countries
differ in their production possibility sets,
then firms, occupying similar economic
positions, are facing different constraints
on their optimization. This does not contradict
the fundamental assumption of

optimizing behavior, but it does raise
severe questions about its interpretation.
The simplest hypothesis is to take the
technological conditions as data, possibly
varying over time due to exogenous changes
in scientific knowledge. But here we are
asserting that two contemporaries have
different access to productive knowledge.
Clearly, we are saying something about the
conditions of transmission of knowledge
across national boundaries, and of course
the same questions arise among firms or
workers within a single economy. The constraints
upon the firm's optimization begin
to seem more like variables to be explained
than like constants exogenously given.
Let me look now at the two basic concepts
from the inside, from the point of
view of our direct perceptions which motivate
the modelling. The two are far from
parallel. The optimization by individual
agents has a sense of concreteness about it,
for all the sophisticated mathematical
ability with which we theorists endow the
agents. They behave in ways whose logic
we understand. They seek to achieve goals
which are reasonable to postulate, and we
can specify constraints which clearly are
real. It can be and has been correctly objected
that our models are too simple; we
ignore other arguments in the utility function,
power, status, social approval, or
whatnot that also motivate individuals,
and we ignore some constraints, capacity
for calculation and social controls. But the
model is comprehensible, and the motives
and constraints we deal with are real and
important.

The market, on the other hand, is a
much more ethereal construct. Who exactly
is it that is achieving the balancing of
supply and demand? Where in fact is the
information on bids and offers needed for
equilibration actually collected and stored?
Right from the beginning of neoclassical
theory, the difficulty of explaining markets
in terms of individual self-seeking behavior
was perceived. Parenthetically, this
is one example of the superiority of neoclassical
analysis to its predecessors, despite


### ---Economics-1974-0-06.txt---
the current fashion for exalting
Ricardo over his successors; Ricardo implicitly
equated supply and demand on all
his markets without ever realizing the
problematic nature of this process. Jevons
felt obliged to enunciate explicitly a Law
of Indifference, enforced by arbitrage; but
this does not really meet the problem when
the market is out of equilibrium, for the
arbitrage might well not be feasible.
Menger, at least according to Hayek and
Streissler, concentrated on individual trades
and ignored the market completely. It is
Walras's auctioneer which has proved to
have had the most enduring effect on subsequent
theoretical development, and the
stability theory which flows from that concept
is still the subject of vigorous theoretical
development, though very little
empirical application. What is envisioned
is a feedback mechanism in which errors
in the price are successively corrected by
reference to the disequilibria they generate.
This view specifies and makes feasible
the operations of the market. But on one
hand the stability models are far from
adequate representations even of the dynamics
of the neoclassical models and,
what may be connected, the results are by
no means necessarily favorable to the
stability of the adjustment process; and on
the other hand, the motivations for the
feedback to operate are obscure.
Let me be clear on one methodological
point. The fact that our intuitive understanding,
our verstehen as the German

social methodologists call it, of the market
as an institution is not entirely satisfactory
does not mean that we should not use the
perfect market as a model, at least pending
further development. Certainly, as Popper
and Friedman hold, the acceptability of a
theory is to be judged by its ability to
predict and understand phenomena. The
theory of the perfect market is in an interesting
way complementary to Keynesian
theory. We have never been able to
integrate Keynesian viewpoints into standard
neoclassical theory, in terms of individual
motivation, yet this theory, with
its various modifications, has been a most
serviceable tool of prediction and control.
In fact, it is useful in domains where competitive
theory fails and vice versa. Neither
theory is good, however, at predicting dynamic
processes, the short-run changes
which are responses to disequilibria, and it
is here that the pressure for a more satisfactory
model arises.

Hold in abeyance for a moment our considerations
about the market. Let us return
to the optimization problem of the individual.
One aspect on which we put a

good deal of weight, particularly in our less
formal discussions, is that a market system
is informationally economical. That is, we
tend to regard it as a virtue of the system
that the individual agent need not know
very much. Specifically, he is supposed to
know the motivation and production conditions
which define him, i.e., his utility
function and production possibility set,
together with the prices of the commodities
he buys and sells. The economic system,
taken as a whole, has vastly more in it
than any one individual knows; it contains
the utility functions and production possibilities
of all individual agents. Indeed,
the apparent modesty of the information
needed is one of the most appealing aspects
of the neoclassical model, both in the
descriptive sense that the individual's decision
problems appear manageable for him
and for the economist studying him, and
in the normative sense that the system
permits its members to spend their time
and effort at producing goods rather than
in unnecessary duplication of information.
But clearly this simplification of the individual'
s decision making is made possible
only because the markets have supplied
the information economized on, in the form
of prices. In equilibrium, at least, the system
as a whole gives the impression of


### ---Economics-1974-0-07.txt---
great economy in the handling of information,
presumably because transmission of
prices is in some significant sense much
cheaper than transmission of the whole set
of production possibilities and utility
functions. It is this point which emerged
in the great debate over the feasibility of
socialism begun by Ludwig von iMises's
attack and usually thought of as concluding
with the work of Oskar Lange and
Abba Lerner in the 1930's; though it should
be added many of the essential points had
already been made earlier by Vilf redo
Pareto and Enrico Barone. XVhat was argued,
in effect, was that a socialist system
could use the price system and therefore
achieve whatever economies in information
it does achieve; and if the equilibrium
conditions are written out they do give the
appearance of relative simplicity. But
what was left obscure is a more definite
measure of information and its costs, in
terms of which it would be possible to assert
the superiority of the price system
over a centralized alternative. Though I
feel that current work has brought about a
considerable clarification, we still have no
definite measure. Indeed, in some respects,
more recent developments have made the
answers less clear. Several writers, in both
Western and socialist countries, have noted
that alternative decentralized schemes
exist where quantity messages rather than
price messages are transmitted in the successive
stages of approximation and that
such schemes also have efficient equilibrium
points. Indeed, with the development of
mathematical programming and highspeed
computers, the centralized alternative
no longer appears preposterous. After
all, it would appear that one could mimic
the workings of a decentralized system by
an appropriately chosen centralized algorithm.
While there is more to the story
than these few remarks, they do make the
point that if we are going to take informational
economy seriously, we have to add
to our usual economic calculations an appropriate
measure of the costs of information
gathering and transmission.

But actually the comparisons between
socialist and capitalist resource allocation
systems have tended to overlook some of
the most obvious facts while examining
finer points closely. As we all know, both
production and consumption (lecisions are
in fact made with reference to the future
as well as to the present. A rational production
plan includes very importantly decisions
or at least plans about the future; and
similarlv with consumption plans. Investment
and savings are not only integral
parts of our current decisions but in the
long run shape the possibilities for further
development. As we know, the formal neoclassical
model can be extenided to decisions
over time by dating commodities and
regarding the same commodity at different
dates as different commodities. All previous
conclusions follow; allocative efficiency,
for example, is achieved with the
same appearance of informational efliciency.
But of course there is a slight problem
with this reasoning. The information
about future commodities needed includes
their prices. These prices must be those
found on a suitable market, one in which
future supply and future demand are
equated. Unfortunately, no such markets
exist. Even the futures markets in certain
commodities, limited in extent as they are,
do not in fact lead to balancing all future
decisions. Rather they balance present
commitments to the future; but it is understood
by all parties that when the future
becomes the present, there will be a
spot market on which the futures commitments
may be undone; and indeed those
making no futures commitments at all can
enter and know now that they will be able
to enter.

Even as a graduate student, I was somewhat
surprised at the emphasis on static
allocative efficiency by market socialists,


### ---Economics-1974-0-08.txt---
when the nonexistence of markets for future
goods under capitalism seemed to me
a much more obvious target.

However that may be, the nonexistence
of these markets must be faced. Now in
general equilibrium any part of the system
affects every other part in at least two
different ways. Thus, we may ask two
questions about the nonexistence of futures
goods markets: what are its implications
for the rest of the system and what are the
reasons for its nonexistence.

The implication first of all is that the
information needed by the optimizer is not
provided by an existing market. It will be
provided by a market which will exist in
the future, but that is a bit too late to help
in decisions made today. Hence, the optimizer
must replace the market commitment
to buy or sell at given terms by
expectations: expectations of prices and
expectations of quantities to be bought or
sold. But he cannot know the future.
Hence, unless he deludes himself, he must
know that both sets of expectations may
be wrong. In short, the absence of the
market implies that the optimizer faces a
world of uncertainty.

The exact modelling of behavior under
uncertainty is probably not crucial to the
subsequent discussion; let us use the
conventional expected-utility hypothesis.
When there is uncertainty, risk aversion
implies that steps will be taken to reduce
risks. This partly affects decisions within
the firm, such as the holding of inventories
and preference for flexible capital equipment,
and partly leads to new markets
which will shift risks to those most able
and willing to bear them, particularly
through the equity market. The rich development
of inventory theory and portfolio
theory in the last twenty years or so
reflects growing understanding of these
matters.

But when we speak of expected utilities,
we need to have some probabilities. Where
do these come from? We may in the first
instance regard them as subjective. But
the economic agent observes his world and
has the opportunity to learn from his experience,
for there is a considerable degree
of continuity. By Bayes' Theorem or perhaps
psychological learning theory, the
probabilities, say of future prices, will
gradually adjust so as to conform to the
facts. If indeed the econromic world exhibited
the same structure in some sense
from period to period, and if everybody
observed everything relevant, then the
probabilities ascribed by different individuals
to the same events might be expected
gradually to converge to the correct values
and therefore be the same for all. In fact,
of course, the basic economic facts are
changing, partly endogenously because of
capital accumulation in its most general
sense, partly exogenously with predictable
and unpredictable changes in technology
and tastes; equally if not more important,
though, is the fact that the dispersion of
information which is so economical implies
that different economic agents do not have
access to the same observations. Hence, it
is reasonable to infer that they will never
come into agreement as to probabilities of
future prices.

A further implication is that the past influences
the future. Jevons's well-known
slogan, "bygones are forever bygones,"
ceases to be fully accurate. The past is relevant
because it contains information
which changes the image of the future; the
probabilities which govern future actions
are modified by observations on the past.
It follows that present decisions with implications
for the future are functions of
past values of variables as well as present
values.

This point of view has been exploited in
the econometric models which have used
distributed lags in explaining investment
decisions. What still needs to be exploited
more, however, is that the inference to the


### ---Economics-1974-0-09.txt---
future is necessarily uncertain, and the
decisions made still exhibit risk aversion.
Expectations for the future are related
to quantities as well as prices. The importance
of quantity expectations has been
stressed in macroeconomic models, even in
such pre-Keynesian concepts as the acceleration
principle, and most especially in
relation to inventories. It sometimes is
held that in a neoclassical world only prices
matter; in the absence of prices, presumably
they are replaced by price expectations.
But that is not strictly true. Under
constant returns, at least, quantity information
for the individual firm is needed
even when neoclassical assumptions are
strictly fulfilled. Neoclassically founded investment
theories usually predict capitaloutput
ratios or capital-labor ratios; they
still need output forecasts explicitly or implicitly.
This gives considerable, perhaps
major weight to past quantity information
in predicting the future and therefore in
guiding current investment decisions. It is
perhaps along these lines that Keynesian
theory, with its overwhelming emphasis on
quantity changes as equilibrating variables,
can be founded firmly on individual
optimizing behavior.

I have referred to the fact that information
is dispersed throughout the economy
but have not suggested how. In the pure
neoclassical model, each agent knows only
his own production possibilities and his
tastes, together with market information
on the rest of the economy. In the world I
have just sketched, however, any variables
which improve his ability to predict the
future have a very meaningful economic
value to him. He will seek to acquire additional
information. Such information is
presumably costly; that is the basis for
such great emphasis on the value of informational
economy. But there is clearly

a great incentive to acquire information of
predictive value, and, as neoclassical
theory would predict, there will be an incentive
to produce such information. We
have then an economic information industry:
(lata assembly and analysis, business
journalism, economic forecasting, with
a longer-run perspective business education.
Since information as a commodity
does not satisfy all the neoclassical norms,
it is not surprising that the government
plays a large role in this process. Information-
acquisition activities and information
markets now appear on the economic landscape.
Efficiency in the operation of firms
ceases to be purely productive efficiency;
it involves efficiency in prediction as well.
I would conjecture that the incomplete
diffusion of information -along the lines just
sketched has a good deal to do with the
operations of the securities markets and
the decisions on corporate financing. The
predominant role of internal financing and
indeed the whole special importance of the
managerial factor in corporate decision
making are clearly connected with differential
access to information about the
firm.

You may have forgotten by now, but I
earlier promised to consider not only the
implications of but also the causes for the
absence of markets for future goods. One
might wonder why one should explain the
absence of a phenomenon. Sherlock Holmes
once maintained to the dimwitted local
police inspector so typical of English detective
stories that the significant question
in the case at hand was the dog's barking
at night. "But," said the inspector, "the
dog didn't bark." "That," said Holmes,
"is what is significant." So too is the absence
of these markets significant for a full
neoclassical theory. A truncated theory of
temporary equilibrium in which markets
for future goods are replaced by some form
of expectations, themselves functions of
current prices and quantities, has indeed
been developed, though its empirical content
is necessarily meager if the formation
of expectations is left unanalyzed. But the


### ---Economics-1974-0-10.txt---
true neoclassical spirit is being denied in
such a model. Although we are not usually
explicit about it, we really postulate that
when a market could be created, it will be.
I sometimes think that welfare economics
ought to be considered an empirical discipline.
Implicitly, if an opportunity for a
Pareto improvement exists, then there will
be an effort to achieve it though some social
device or another. In our theories and
to a considerable extent in practice, the
cheapest way in many cases is the creation
of a market; and markets do emerge. If a
market is impractical for one or another of
the reasons we usually call "market failure,
" then very likely some other social
device will at least be tried: government
intervention; codes of professional ethics;
or economic organizations with some
power intermediate between the competitive
firm and the government.

Thus, the failure of markets for future
goods must be regarded as an analytic
problem as well as a presupposition. It
seems to me there are two basic causal factors.
One is that contracts are not enforceable
without cost and forward contracts
are more costly to enforce than contemporaneous
contracts; the other is that because
of the many uncertainties about the
future, neither buyers nor sellers are willing
to make commitments which completely
define their future actions. Let me
take these two points up in turn.
The ability to make enforceable contracts
is a necessary but not sufficient
condition for a market. However, there is
no way to insure complete enforceability.
An individual may make a contract which
he cannot in fact fulfill. Penalties may indeed
be imposed on failure to live up to
one's agreement, but they are not a substitute
for compliance from the viewpoint
of the other party, and there is always a
degree of cost in enforcing the penalties.
The laws of bankruptcy are a social recognition
that complete enforceability is impossible
and that it is even socially desirable
to set limits on the penalties for
failure. However, when the exchange of
values for values is simultaneous or nearly
so, the contracts may almost be selfenforcing.
If a good has been sold and not
paid for, it can be recovered; if there is a
continuing relation of buyer and seller, a
failure to settle bills can be met by refusal
to make further deliveries, in which case
the loss is minimized. With contracts extending
into the distant future, on the
contrary, the possibility of failure to comply
becomes greater, partly because the
self-enforcement aspects become weaker,
partly because unexpected changes may
intervene to make even a sincerely intended
compliance difficult or impossible.
The outstanding examples of forward
contracts are credit instruments. The
buyer, who is taking the risks of default, is
motivated to protect himself by seeking
more information about the seller. The
lender wants to know the borrower's assets,
the prospects for changes in them,
possibly even what he is going to do with
the money. This very individualized information-
seeking relation is quite far

from the arm's length impersonal model of
a market. The so-called capital markets
are in many structural aspects very different
from our model markets. It is of
course an empirical question how far their
behavior departs from the model. But
the recurrent theme of credit rationing and
availability doctrines, the essential imperfections
of the credit market which underlie
monetarist theories of cyclical fluctuations
suggest that the incomplete enforceability
of credit contracts and the

protective steps taken by lenders are significant
factors in explaining the working
of the market.

While the enforceability question explains
why those forward contacts that are
made do not constitute a perfect market,
we need more to understand why even


### ---Economics-1974-0-11.txt---
these are so limited in their coverage of
future goods transactions. There are forward
contracts in money, some commodities,
real estate, but very little else.
The explanation lies in uncertainties of
both buyers and sellers about prices and
quantities and about technology and
tastes. Using uncertainties about prices
and quantities as an explanation for market
failure is a circular argument, though
not necessarily a fallacy. That is, if all
markets for future goods existed and
cleared all transactions, then there would
be no price-quantity uncertainties. But
this much is true; if some markets for future
goods do not exist, then the agents
have uncertainties which are relevant to
their behavior on markets for complementary
or substitute goods. As Hicks showed
a long time ago, complementarity and substitution
can occur over time as well as
simultaneously. If, as I will argue in a
minute, uncertainty can tend to destroy
markets, then we can conclude that the
absence of some markets for future goods
may cause others to fail.

To illustrate, the demand for capital
goods at any point of time is dependent on
the prices and sales of the product at future
points of time. Therefore the demand
for future capital goods will depend on expectations
about the product at some still
more removed time. If we assume only
that we will not have markets for products
at some distant point of time, then the
resulting uncertainty will reflect itself in a
failure of the market for capital goods in
the nearer future, which will in turn create
still further uncertainties.

Thus, if some markets for future goods
are nonexistent, there will be uncertainties
on the other markets; in addition, demand
and supply conditions for the future are
uncertain because of technological and
taste shifts. Assume that both buyers and
sellers are risk averters. Then without
going into details it is reasonable to conclude
that both demand and supply will
have a downward bias as compared with
the situation in which uncertainty is absent.
A buyer will be unwilling to contract
for purchase of a good if a superior or
cheaper substitute may be available; and
the seller will be unwilling to accept a
price sufficiently low to be suitable to the
buyer, particularly if he thereby precludes
himself from a possible opportunity to
shift his resources to other closely related
goods. It would seem possible, at least,
that there will be no price at which transactions
in future goods will take place.
From a theoretical viewpoint, one might
say that the market is in a strange sort of
equilibrium; there is some shadowy sort of
price at which supply and demand are
equated at zero. But this price is not performing
much of a signalling function.
There is one ultra-neoclassical approach
to the market treatment of uncertainties,
in which I take some pride. That is the notion
of a contingent market. Instead of
letting uncertainty ruin existing markets,
we can take it explicitly into account by
buying and selling commitments to be
carried out only if some uncertain event
occurs. We could in principle imagine
agreements to transact which will hold if
and only a given conceivable technological
innovation does not take place, with a
second market for transactions valid if the
innovation does take place. Then we can
restore the possibility of markets.
Such contingent markets are not entirely
unknown; insurance contracts are
the purest example, and equity markets
and cost-plus contracts provide more
muddied illustrations. But they are relatively
rare. Why this should be so follows
again from the general problem of informatioin
costs and dispersal. If contracts are
contingent on the occurrence of some
event, then it must be verified whether or
not the event occurre(l. But this is information,
an(I as the example of a technological


### ---Economics-1974-0-12.txt---
innovation suggests, it is information
likely to be much more easily available to
one party than to the other. Hence, the
range of possible contingent contracts becomes
limited to those for whom the events
are easily verifiable for both parties. The
implications of these limits are known in
the insurance literature as adverse selection
and moral hazard, and they are of
immediate practical significance in such
matters as health insurance. But more
broadly, they so limit the scope of contingent
markets in practice that, as argued
before in connection with markets for future
goods, they prevent the emergence of
even technically possible markets because
of the large unresolved uncertainties.
I hope enough has been said to indicate
the widespread implications of costly, dispersed
information for the process by
which future-oriented economic decisions
are made. Let me remark, briefly in view
of the length of time I have already taken,
that informational costs and values play a
key role in modifying the structure even of
contemporaneous transactions. The individual
optimizing agent is supposed to
know at least his technology or tastes and
the prices he faces. We have already argued
a good deal of uncertainty with respect
to the future economic implications
of present economic choices. But in addition
there is the possibility that technological
information, which would be useful
to him, exists somewhere in the world but
outside his firm. There are grounds for
engaging in the active pursuit of information.
We begin to enter the realm of diffusion
of innovations, to which some sociologists
as well as economists have contributed.
The interesting points here are
the biasses in the information channels,
some of which, at least, can be explained
in terms of differential costs of acquiring
information. For example, the well-documented
role of personal influence in accepting
innovations can be interpreted as
due to a perceived high reliability of such
information; in economic terms, this means
more information per unit of expenditure
of time or money.

The terms of trade with the outside
world should not be regarded as freely
given to the firm. In a world with a large
number of commodities, even knowing the
prices of relevant commodities involves
the costly acquisition of certain kinds of
information. This remark has given rise
to a large literature on search in recent
years. One implication which has been
only slightly explored is that the concept
of the market begins to weaken, and
Jevons's Law of Indifference becomes
more of an equilibrium condition than a
statement valid about a market even in
disequilibrium. At a moment of time,
prices of what would usually be thought of
as the same commodity bought or sold by
different firms can differ because buyers or
sellers may not, in their ignorance and in
the presence of costs of search, find it
worthwhile to shop further. Obviously, the
important application of this principle
may be to the labor market. Clearly, there
are important informational differences
between the employees currently working
for a firm and potential substitutes elsewhere,
although these are interchangeable
in pure neoclassical theory. Indeed, there
are differences both in the information the
firm possesses about its employees as compared
with alternatives and the information
which employees have about the economic
opportunities and the specific production
conditions of the firm as compared
with outsiders. It appears that considerations
of this type must play some role in
understanding the continued possibility of
unemployment and particularly the sluggish
response of wages to market disequilibria.


I am far from exhausting the implications
of an information-economical viewpoint
for the economic world. I look forward
to exciting developments in the next
decade.
 ## Economics-1975-0


### ---Economics-1975-0-01.txt---
I come here with no eye-opening report
from the frontiers of economics, no stirring
cry for reform of conventional economics,
no closely reasoned analysis of an economic
dilemma or puzzle, no scathing or
reproachful scolding of the profession for
its technical preciousness or moral blindness,
no report on painstaking research
results, no valedictory on a lifetime of
theoretical or empirical contributions. The
AEA presidential addresses have been all
of these things.

But tonight, going against our current
fashion of telling the world what's wrong
with economics, I offer a modest contribution
to the immodest subject of what's
right with economics-and, in particular,
what's right with economics as a guide to
public policy. In doing so, I won't ignore
the dark side of the moon-indeed, I can't,
since I will deal at some length with the bedeviling
subject of inflation. But believing
that it is at least as reasonable to judge a
discipline by its successes as by its failures,
I intend to accentuate the positive.
I. The Critical Look Inward

In recent years, as I shall illustrate in a
moment, we have instead accentuated the
negative. In good part, this has taken the
becoming form of mea culpa or rather
nostra culpa. We have, for example, readily
confessed that the inflationary shocks of
1973-74 caught not just the economy but
the economist by surprise. On this and
other fronts, the chorus of self-criticism
has risen to a new crescendo. It is almost
as if we take pride in our humility. Nietzsche
must have been thinking of economists
when he observed that "he who

despises himself nevertheless esteems himself
as a self-despiser."

This is not to imply that economists'
criticisms are all self-inflicted wounds. Far
from it. Often among our colleagues'
favorite targets are the shortcomings of
mainstream economics, the misuse of
modern techniques, the fallacies of conventional
wisdom-in each case, the target
is not the critic's but his colleagues' brand
of economics, not mea culpa but eorum
culpa.

In any event, he who comes to praise
economics risks being buried in the barrage
of indictments that economists have
brought against themselves and their
brethren. Let me give you a sampling of
some that will be ringing in my ears as I
follow the parlous path of economic virtue.
Ceremonial occasions-presidential, memorial,
or inaugural addresses-in particular
seem to evoke musings on the

troubled or even dismal state of our science.
For the AEA faithful, I need only
recall John Kenneth Galbraith condemning
neoclassical and neo-Keynesian economics
for ignoring power and thus losing
contact with the real world; Wassily
Leontief attacking mathematical economics
for building a showy superstructure
on weak empirical foundations and
unverified assumptions, and thus losing
contact with the real world; Kenneth


### ---Economics-1975-0-04.txt---
Boulding assailing welfare economics for
its reliance on that holiest of holies, Pareto
optimality-when in fact "our lives are
dominated by precisely this inter-dependence
of utility functions which the Paretian
optimum denies"-thus also losing
contact with the real world.

In one form or another, variations on
Leontief's lament have been heard in many
another presidential address, to wit:
By F. H. Hahn (Econometric Society,
1968), who decried "the spectacle of so
many people refining the analysis of
economic states which they give no
reason to suppose will ever, or have
ever, come about...."

By G. D. N. Worswick (Section F of
the British Association, 1971), who
viewed the performance of economics
as "curiously disappointing," suggesting
that it has "a marvelous array of pretend
tools which would perform wonders
if ever a set of facts should turn up in
the right form."

By E. H. Phelps Brown (Royal Economics
Society, 1971), who judged the
usefulness of current work in economics
as ''not equal to its distinction" because
it is "built upon assumptions about human
behavior that are plucked from
the air."

By James H. Blackman (Southern
Economic Association, 1971), who noted
that models with sufficiently intriguing
mathematical properties can achieve
lives of their own even if they lead the
investigator further away from reality
and yet, "the profession's incentive
system tends perversely to reward this
kind of endeavor and to deflect the
attention of gifted economists from the
exploration of concrete problems and
the dirty work that entails."

By Sherman Maisel (American Finance
Association, 1973), who concluded
that most of the literature of
monetary economics is "non-operational"
since its prescriptions are too often
based on limited or false assumptions,
it by-passes critical operational problems,
and it ascribes too great validity
to its statistical tests.

By Barbara Bergmann (Eastern Economic
Association, 1974), who prefaced
her plea for more microsimulation to
incorporate "realistically messy information"
in our economic data base with
a few roundhouse swings at the economics
profession and the pointed observation
that instead of studying the

real nature of decision making, we
typically rush to make assumptions
"whose purpose in life is to let the
theorem emerge, all neat and provable."
Another favorite line of criticism and
attack focusses on the implicit value
premises of conventional economics. Gunnar
Myrdal and Robert Heilbroner chide
us for concealing the value judgments that
inevitably enter into our selection of problems
for study, choice of approach, definition
of concepts, and even gathering of
data. So a "value-free" economics is an
illusion-they urge economists to specify
their values and thus avoid biases and
make research more realistic.

Radical economists simply reject the
whole value system of conventional economics-
as they see it, the neoclassical
paradigm in its very bone and marrow
enthrones acquisitiveness and enshrines
the existing order. Paul Sweezy accuses
mainstream economists "of hiding the
facts, of making the uncontrollable appear
under control, of rationalizing a system
which condemns hundreds of millions of
people to lives of despair and starvation.
... "


### ---Economics-1975-0-05.txt---
Inflation is the latest source of critical
volleys, and I will get to these in due
course. Meanwhile, the sampler of economic
masochism I have already provided
should serve as ample insurance against
complacency or smugness in considering
"what's right with economics." At the
same time, it strongly suggests that economics,
more than any other social science,
is afflicted with the common scold.
1 recognize that such a quick sampling
and cryptic quotes, selected to highlight
criticism, do a certain injustice to economics
and to some of the quoted economists
whose kindlier observations have
been neglected in the process. But I am
also aware that my litany omitted a number
of familiar flaws, for example, our
impounding of tastes and preferences in
ceteris paribus; the shortcomings of the
maximization principle in explaining consumer
and producer behavior, especially in
the short run; and our limited ability to
bring the claims of future generations into
our social utility functions.

Were I to serve as defense counsel for the
profession on this wide variety of indictments,
I would urge that we plead guilty
or take the Fifth on some, take to the defense
on others, and take offense at the
rest. Having paid my respects to the critics,
I intend no point-by-point evaluation or
rebuttal. This has been ably undertaken
by others.' Rather, my object is to gain a
more balanced perspective by focussing on
the quality, role, and contributions of
economics, especially to public policy. In
that undertaking, the first step is to examine
the flank we expose to the public.
II. The Economist and the Public
When we turn from inside to outside
critics, the focus changes. We may think,
rightly, that freely confessing our weaknesses
and airing our differences stimulate
responses and adaptations that strengthen
economics. Yet, wearing our purple hearts
on our sleeves has its price. It nourishes
the darkest suspicions about our art and
supplies live ammunition to outside critics
who have declared open season on economists.
Witness the open sesame to the edop
pages for such recent thrusts as B ergmann'
s assault on economists in general
and Friedrich von Hayek's attack on
Keynesians in particular. With everything
from off-the-cuff phrases about being
"caught with our parameters down" to
tracts for the Times, we feed the hand that
bites us.

This is not a plea to do our self-flagellating
in secret or to mute our disputes
and conflicts. Open controversies, openly
arrived at, are part of the therapy that
keeps our profession healthy. Rather, my
plea is to the media and the opinion makers
to understand that appearances are deceiving,
that hard give-and-take is indeed
a symbol of strength, and that our areas
of agreement and consensus are vastly
larger than our areas of difference.
On the first point, observers from other
disciplines are of ten astonished at how
hard economists go at each other, how
readily they run the gauntlet of their colleagues'
criticisms with no quarter asked
and none given-and, with few exceptions,
all this within the framework of professional
respect and friendship. As Charles
Frankel put it, unlike other social sciences,
economics seems to have achieved
"a working etiquette which allows people
to disagree vigorously without engaging
in recrimination about 'unscientific' or
'unprofessional' behavior" (quoted in
Johnson (1973)).

What accounts for this? Part of it, one
can unblushingly say, is simply that so
many competent, tough, and rigorously
I Among those who have sprung to the defense with
varying degrees of fervor are Harry Johnson (1968),
Donald MacDougall, Charles Schultze (1972), Robert
Solow (1970, 1971), and James Tobin (1973, 1974). For
more general appraisals of the criticisms and the state
of economics, see Blackman and Nancy Ruggles.


### ---Economics-1975-0-06.txt---
trained minds have been drawn into economics
in response not just to challenging
policy problems but to the quantitative
revolution since World War II. And part
of it is that the participants can draw on a
hard core of economic theory and methodology,
together with a growing body of
empirical knowledge, to provide standards
for testing the validity (though not necessarily
the relevance and reality) of ideas,
analysis, and empirical findings. The result
is not only a relentless intellectual
policing of the profession that soon exposes
the fool, the quack, and the charlatan, but
a growing capacity "to participate in adversary
debate over public policy issues
without jeopardizing scientific integrity
and freedom" (Johnson (1973)).
That brings me to the second point, the
impression we give outsiders of a house
divided, not to say splintered. It is worth
reminding ourselves and our critics of
several factors that drive a wedge between
image and reality.

One, instead of laying aside our differences
and living contentedly together, we
economists tend to lay aside our agreements
and live contentiously together. We
focus our private and public debates on
unsolved policy problems, tough analytical
nuts, and issues on which we have rival
theories, contradictory evidence, or strong
ideological differences. Just as these are
the questions that intrigue us, they are
the ones that attract the attention of press
and public. What we know-and they may
not is that beneath the visible tip of disagreement
and rivalry lies no huge iceberg
of divisiveness.

Two, it is only occasionally that our
areas of consensus are brought to the surface
in a newsworthy way. One such occasion
was the White House "summit conference"
on inflation last September. Two
dozen leading economists from across a
wide spectrum of American economics
(not wide enough, the radicals would say)
signed a statement which called on the
President and Congress to eliminate
twenty-two restrictive laws and practices
that inhibit competition, inflate costs, and
prop up prices. Only a tiny minority held
out (if any minority that includes Galbraith
can be called "tiny"). Even more
striking, in a sense, was that while the
customary and largely ideological clashes
among, say, Galbraith, Milton Friedman,
and Paul Samuelson caught the public
eye, the real story lay in the minimal dissent
among the participants on (a) the
forecast of a soggy or sagging economy,
(b) the urgency of providing relief to the
victims of inflation and the casualties of
recession, (c) the need to ease monetary
restraint, (d) the small anti-inflationary
payoff on moderate ($5 to $10 billion)
budget cuts, and (e) the advisability of
resisting popular demands for reimposing
full-scale wage and price controls.
Three, even where disagreement flourishes
most visibly, perhaps, between
Keynesians and monetarists-the public
may not discern that the analytical and
empirical ties that bind us are far stronger
than the forces that divide us. Our controversies
take place within the context of
basic consensus on the nature and methods
of economic theory and inquiry, on the
content of the disagreement, and on the
kinds of tests that may one day resolve the
conflict. "Such disagreement within agreement
lies at the heart of the process of
normal development of a science" (Benjamin
Ward, p. 12).

Four, much of what the public perceives
as a clash of economic concepts and findings
is in fact a clash of ideology and
values. Given the way technical economics
and ethical preferences are packaged in
policy debates (and given our lapses in
identifying which is which), this is hardly
surprising. Thus, whoever opens the package
labeled "monetarist" typically finds
not just money supply in full flower, but a


### ---Economics-1975-0-07.txt---
dedication to minimum government intervention,
small budgets, reliance on rules
rather than authority, and price stability.
Contrasting correlations appear in the
Keynesian package. So outsiders can be
excused for slipping into the fallacy of
association and attributing the split to
our unresolved analytical conflicts rather
than to divergent evaluations of social
priorities and competing philosophies of
government. These associational chains are
not linked together by any inexorable
logic in part, they seem to be an accident
of birth as in the case of the Chicago twins
of monetarism and laissez-faire rules. A
belief in the supremacy of monetary over
fiscal tools could quite logically go hand-inhand
with avid interventionism. But this
escapes the jaundiced eye of the outside
observer, who takes the ideological lineup
as further evidence that economics is
riven to its core.

Five, there is an ironic but substantial
inverse correlation between the degree of
consensus among economists and the degree
of public acceptance of their findings.
Thus, in the macro-economic sphere of stabilization
policy, where debate and disputes
among economists flourish, their imprint
on public policy is undeniable. But in
the considerably more peaceful realm of
microeconomics and allocative. efficiency
-where a reliable analytical apparatus
coupled with solid quantitative work, especially
on costs and benefits, has led the
great majority of disinterested economists
to an agreed diagnosis and prescriptionthe
policy box score shows few hits, fewer
runs, and lots of runners lef t on base.
Economists widely, in some cases almost
uniformly, favor tougher antitrust policy,
freer trade, deregulation of transportation,
pollution taxes in place of most prohibitions,
and tax reform to remove income tax
shelters. They oppose fair trade laws, restrictive
labor and management practices,
distortive zoning laws and building codes,
import quotas, ceilings on interest rates,
maritime subsidies, and pure (or impure)
pork barrel projects.

Granted, the diffuse and inchoate consumer
interest has been no match for
the sharply focussed, articulate, and wellfinanced
efforts of producer groups. But
the economist is beginning to pick up some
allies. Public interest groups are increasingly
giving focus and force to the consumer
and general public interest. And the
march of events is providing some windfalls:
Among the apples that have dropped
in our laps are flexible exchange rates, the
dethroning of agricultural price supports,
inroads on import quotas, and moves to
end percentage depletion. Under the pressure
of virulent inflation, government actions
that erode productivity and boost
costs and prices are being subjected to new
and searching scrutiny. So perhaps, on
these micro-economic issues where economists
sing in reasonably close harmony,
the outside world will no longer quite tune
us out. In macro-economic policy, where
cacaphony prevails, we can be sure that
the world will tune us in.

It may also be useful to draw attention
especially the attention of those who
interpret us to the public-to certain other
misperceptions and roadblocks that thwart
good economics and tend to put economists
in bad repute.

First, much of our economic analysis
and the uncommon sense growing out of it
fly in the face of "common sense," for
example: that budget deficits need not
spell inflation, nor national debt a burden
on our grandchildren; that thriftiness can
be a mixed virtue; that while exploding oil
prices inflate costs, they deflate demand;
that in an overheated economy, greater
taxes can be the lesser evil; and so on.
Behind every false dictate of common
sense lies a primitive and misbegotten
economic theory-and for most of our
pains to correct it, we can expect to get


### ---Economics-1975-0-08.txt---
the back of everyman's hand.

Second, a related cross to bear can be
characterized by Kermit Gordon's apt
phrase, "virtue is so much easier when
duty and self-interest coincide." Not only
does that foredoom action on many microeconomic
fronts, as already noted, but it
puts roadblocks in the path of efforts to
make fiscal policy a two-way street. For
forty years, Congress has enacted major
tax increases only under the whiplash of
war. The resulting reliance on tight money
to fight peacetime excess demand, coupled
with expansionary fiscal policy to fight
recession and slack, have had an unmistakeable
ratchet effect that has tilted the
system toward tighter money and easier
budgets. (Small wonder, by the way, that
many economists and policy makers are
unwilling to give up, via indexing, the
increases in effective income tax rates
"legislated" by inflation.)

Third, the public sees economists as the
bearers of hard and unpalatable truths.
And often we are, by the very nature of
our sometimes dismal discipline. Except
when idle resources can be put to work or
productivity increased, our message is the
stern one of tradeoffs, benefits at a cost,
and no one-dimensional daydreaming.
Even worse, at times economics has to
bring the bad tidings that for some problems
there are no satisfactory solutions.
For some thirty years, we have warned
that full employment, price stability, and
full freedom of economic choice cannot coexist
in a world of strongly organized producer
groups. More recently, economic
analysis has brought home the unromantic
truth that failure to cure some of our social
ills traces less to a failure of will, or "rightwing
villains," or a calloused "establishment,
" or powerlessness of the people,
than it does to the prosaic facts that the
problems are tough and complex and the
goals we seek may be irreconcilable-in
short, trace more to conflicts in our national
objectives than to conflicts among
social groups. Welfare reform is a case in
point: no solution can simultaneously provide
a decent minimum income for all,
preserve work incentives, cut no one's
benefits, and avoid huge budget costs (see
Schultze (1972); Rivlin (1973)). We may
view such work as a contribution to
straight thinking and rational choice. Our
critics are more likely to view it, at worst,
as a counsel of defeat (which it is not) or
at best a counsel of inescapable compromise
(which it is).

Since the foregoing misperceptions and
roadblocks thwart the translation of
good economics into good policy, one could
justify, in cold cost-benefit terms, a sizeable
investment to overcome or reduce
them. The most obvious implication is
that the country needs to invest more in
formal economic education at all levels.
But an equally pressing need is for economists
to invest more of their time and
effort in making themselves understood
to the public and policy maker-and that
in turn requires recognition of this skill
in the academic reward system. This
might serve as a useful antidote to the
influence of mathematics and econometrics
which, while heightening the precision of
professional thinking and internal communication,
have apparently dulled the

appetite and eroded the facility to communicate
with the public in intelligible
English prose.

In a very real sense, this confronts the
press with an unusual opportunity and
challenge, perhaps even a responsibility,
to serve as a translator and interpreter of
economics and its offerings. But believing
(probably rightly) that their readers and
listeners prefer to hear of fights and
failures, crises and controversies, rather
than of quiet contributions and consensus,
the conventional or electronic press is not
very likely to rise to this challenge. So it is
still up to economists. But I must not


### ---Economics-1975-0-09.txt---
carry this too far. Just as I am eschewing
any Cassandra-like pronouncements tonight,
so I have promised myself to suppress
the oracular and even the avuncular
(Dutch-type) mood evoked by these occasions.
So I shall press on.

III. Standards of Judgment

From the foregoing, it is evident that I
feel, first, that economists have gone beyond
beguiling humility and welcome selfcriticism
to the point of almost neurotic
self-rebuke, and second, that press and
public have all too lustily taken up the
cry-in part taking us at our word, in
part misinterpreting us, and in part reflecting
their belief that, after the high
promise of the 1960's, we have failed them
in not foreseeing and forestalling the
crises of the 1970's: stagflation, energy
shortage, and the environment.
In my quest for a more balanced perspective
on the state of economics, the
next task is to set up some standards for
judging the quality and performance of
economists. Since we have developed no
measures of output or allocative efficiency,
no capital-output or cost-benefit ratios, for
the economics "industry," I will have to
fall back on more subjective and less quantitative
measures in judging its quality and
contributions.

My mixed bag of criteria includes (1)
the quality of inputs; (2) the demands for
our services; (3) as a proxy for a measure
of outputs, the record of accomplishment
in a given field (public finance); (4) finally,
the cruelest test, our handling of the economics
of inflation.

The potential of economics for informing
and improving public policy depends on
the stock of human capital, technology
and tools at its command. Here, economics
has no difficulty in holding its head high,
especially in terms of the striking advances
of the past three or four decades.
Harry Johnson may be a trifle extravagant
in his assessment that the United
States now has "perhaps fifty economic
departments of an average quality comparable
to the average quality of the four
or five best departments in the whole
world in the pre-World War II period. . ."
(1973), but only a trifle. Another attest to
professional quality, already referred to,
might be put this way: Show me another
field that has enough inner strength to
confess so much remaining weakness (and
to carry on so much open controversy).
Humility where we have things to be
humble about (and we do) is a becoming
trait. But coupling it with pride where we
have things to be genuinely proud about
is hardly a deadly sin.

Accompanying the growth in the quantity
and quality of economic brainpower
have been striking advances in the techniques
and tools with which economists
work. For this audience, I can speak in
shorthand about the strengthened analytical
base of micro- and macroeconomics;
the methodological revolution that moved
us from the rationalist-historical approach
into the age of quantification, with its
insistence on systematic measurement of
the shapes of economic functions and empirical
testing of hypotheses and its use of
econometrics and simulation techniques
(with a powerful assist from the computer)
; and such conceptual advances as
those in the economics of human capital,
of cost-benefit relations, of uncertainty, of
control, of transactions and information
costs, of "second best," and of the allocation
of time.

In normative economics and the analysis
of value-laden social problems, new frontiers
in the study of economic behavior are
being opened up by survey research techniques
(especially by the Michigan Survey
Research Center), by efforts to measure
nonmarket benefits or values (especially
by the National Bureau of Economic Research)


### ---Economics-1975-0-10.txt---
and by "controlled" social experimentation
(for example, by the Brookings
Institution and the University of Wisconsin
Institute for Research on Poverty).
These newer tools and the institutions
that nurture them constitute part of the
rich and expanding resources of economics.


Economics can also draw on a broad
data base, especially in federal statistics.
But here, the quantity, timeliness, and
even the quality of the data are not keeping
pace with either the problems requiring
analysis or the capacity of our quantitative
techniques. Responding to policy
needs and mounting self-criticism, the
profession has opened many new fronts in
the search for realistic micro-data to link
up with macro-data, for cross-section data
to help overcome the curse of collinearity
in time-series analysis, and for custombuilt
data developed by survey and experimental
techniques.

That the human, analytical, and quantitative
resources of economics provide a
huge potential for solving problems seems
undeniable. That more of these powerful
resources than ever before are being put at
the disposal of economic policy makers
also seems undeniable. What we do not
know is what proportion is being misdirected
into arid puzzles, sterile proofs,
and recreational mathematics while the
world's pressing economic and social problems
go begging for answers. Here, we
can only match one observer's impression
against another's. The profession itself has
not come to grips with this question of
allocative efficiency.

A second test in appraising the state of
economics, one not unknown to economics,
is that of the market place. This takes
several forms, none very robust, but none
trivial. The first is the upsurge in enrollments
in economics courses, especially in
introductory economics, that has occurred
in the academic years 1973-74 and 1974-
75. The second is the oft-reported high
ranking of economists' salaries in business,
government, and academic life. A third
is the strong and growing demand for
economists' inputs into the policy-making
process either as staff members or as
expert witnesses for congressional committees,
individual congressmen, and the
executive branch.

With students, business, and government
beating a path to our door, we can
infer that something must be right with
economics, or wrong with the economy,
or both. Either we are building a better
mousetrap or there are more and bigger
mice threatening our customers. Perhaps
it is simply that we have the only mousetraps
in town.

But there must be more to it than that.
Take the policy maker, for example.
What he finds congenial is that he can
hand an economist a problem relating to
changes in taxation, regulations, budget
proposals, pollution control, poverty, social
security, public service jobs, gasoline
taxes, oil prices, and so on and be reasonably
sure of getting a useful appraisal of
alternative paths to his objectives, of
costs and benefits, and of distributional,
allocative, and stabilization impacts.
Many of these judgments will come with
orders of magnitude or reasonably precise
numbers attached. He may not trust our
GNP forecasts, but he has come to respect
our hardheaded analysis and numbers on
the myriad problems of economic choice
with which he is faced.

It seems fair to draw another inference:
notwithstanding the current wave of selfcriticism
and public criticism, even lampooning,
of economists and despite our

highly visible public debates and highly
vulnerable participation in policy making,
economics continues to maintain its standing
as a science. Signs of a reported crisis
of public confidence or of a "recession of
self-confidence" are few and far between.


### ---Economics-1975-0-11.txt---
Reports of the demise of our discipline are
grossly exaggerated.

IV. The "Outputs" of Public Economics
Having considered some indicators of
the quality of our inputs and of the revealed
preferences for our outputs, let me
continue this exercise in casual (andcongenial)
empiricism by taking an unscientific
but not unrepresentative sample of
the outputs of economics, especially those
bearing on policy. For this purpose, I
draw on my chosen field of public finance,
or public economics, to illustrate the telling
conceptual and empirical advances of
economics in recent decades and the resulting
enrichment of its offerings to the policy
maker. Indeed; such an appraisal offers so
many healthy antidotes to "what's wrong
with economics" that I was tempted to
devote my whole discourse to it tonight.
But I resisted the temptation because,
first, much of it has already been done in
carefully documented depth in survey
volumes by Brookings and the National
Bureau;2 second, I figured it might test
your patience and mine; and third, it
would have left no room for a confrontation
with inflation. So I offer instead a
miniaturized assessment of the achievements
of public economics as viewed

through the policy prism.

Public Expenditures

Consider first the striking contributions
economics has made in the past generation
to clear thinking and better informed
decisions on public expenditures. Partly,
this reflects advances in economic science,
for example, in the theory of public goods
and human capital, and partly, creative
new applications of the economist's characteristic
way of looking at problems of

choice, namely, through the lens of opportunity
costs, benefits, and alternative
paths to a stated goal.

Economics can offer much more concrete
guidance on efficient ways of allocating
resources to achieve stated governmental
objectives than it can on what the
public-private sector division of resources
should be. That may be a good thing in
that presidents and congressmen view the
fixing of goals for public health, housing,
welfare, and the like as what they were
elected for, yet at the same time seek, or
at least accept, economic guidance on the
choice among competing methods of
achieving these goals.

Nonetheless, rapid progress in the
theory of public goods since the appearance
of the Samuelson classic on "The
Pure Theory of Public Expenditures" just
twenty years ago has vastly improved on
the simplistic theory it replaced. It has
facilitated straight thinking on the derivation
of conditions for efficient public-sector
allocations from private evaluations and
on the articulation of social priorities
through the political process.
Interwoven with the newer thinking
about public goods has been a resurgent
interest in externalities or spillover effects.
In a sense, the pure collective good is a
case of total externality-all of its benefits
are external and nonmarketable since
nobody can be excluded from them. That
may clarify thinking but gives little policy
guidance.

Yet, the externality concept translates
into hard-headed policy advice in such
disparate areas as pollution, federal aid,
and the law. When pollution became a
national concern, economists quickly drew
on their tool kit to develop proposals for
antipollution taxes (within the context of
2 See Alan Blinder and Robert Solow et al. This is the
capstone volume of the Brookings Studies on Govern-
ment Finance, directed by Joseph A. Pechman, which
has produced 35 books in the past decade. See also Carl
Shoup et al. This was one of several survey volumes un-
der the general heading, Economic Researchl: Retrospect
and Prospect, based on the Bureau's Fiftieth Anniversary
Colloquia.


### ---Economics-1975-0-12.txt---
target air and water quality standards).
Tax penalties of so much per unit would
put price tags on use of the public's air
and water, thus internalizing external
costs and using market incentives to accomplish
depollution rather than relying
on the less efficient route of regulation.
When local governments supply education
and public health services to a mobile
population, many of the benefits spill over
to other units. An important rationale for
federal grants flows from these externalities,
namely, that to get local units to
produce enough education and health
service to achieve a national, not just a
local, cost-benefit optimum requires conditional
grants from the federal purse.
Further, since externalities in the form
of damage to third parties lies at the heart
of many problems in legal justice, economics
is able to make an important contribution
in this area.

When we turn to the empirical outputs
that are now illuminating problems of
public choice, we find the past decade
bristling with new thinking, new techniques,
and new measurements. These

offer the decision maker important new
guides in the seJection and evaluation of
government programs and new insights
into alternative systems of delivering
government services:

Measurement of cost-benefit ratios
has developed from the early metrics
of water projects into, first, a sophisticated
cost-benefit calculus for tangible
investments like dams, roads, pollutioncontrol
projects and, second, cost-benefit
estimates for intangible investments
in human brainpower, skills, and health.
Shadow pricing has been one of the useful
tools in this connection. Cost-benefit
analysis, even with its limits of quantification
and its inability to shed light
on distributional and value questions,
is an important aid to informed decisions.


A related advance is the development
of new and tougher standards for judging
government programs. The former
criteria centered on the question: Is the
program put into effect quickly and with
high fidelity to the congressional intent?
Now, the accountability question is:
Does it deliver the goods? Does it accomplish
the objectives? Inputs used to
be stressed if they conformed with the
intent of the legislation, they tended to
be judged a success. But now we try to
measure outputs, a tougher and more
elusive standard. (The parallel with
judging the performance of economics
and economists is painfully obvious.)
Antipoverty programs, which were
among the first to be evaluated by these
stringent standards, seem to have borne
the brunt of the evaluation boom. By
the old inputs standard, a program like
Head Start would have fared much
better.

The reach of cost-benefit analysis will
be lengthened if a broad range of new
research efforts in nonmarket sectors of
economic activity pays off. I refer not
only to the exciting work on measurement
of the returns on investments in
human capital (T. W. Schultz), but to
efforts to measure the output of the
medical industry, to measure the relations
between crime and punishment,

and to measure the value of nonmarket
economic activity conducted within
firms and households.

The new technique of controlled social
experimentation on proposed welfare
and housing measures, health insurance,
and education vouchers is yielding important
insights. As a result of experiments
on negative income taxation, for
example, the equity versus efficiency, or
equality versus incentives, controversy
will never be conducted in a vacuum


### ---Economics-1975-0-13.txt---
again (Rivlin (1973)). In spite of some
limitations, the New Jersey experiment
yielded strong evidence that fears of
fatal incentive effects of a negative income
tax were grossly overblown.

Another focus of fruitful thinking relates
to alternative strategies for delivering
social services. The in-cash

versus in-kind choice is a basic one.
Economists are predisposed toward the
in-cash approach on grounds that one
can generally depend on people to follow
their own best interests. But there
are significant exceptions where consumer
sovereignty is limited or specific
goods externalities exist or some explicit
social values take priority.

Out of economics also comes the attempt
to develop "market analogs" to
serve as substitutes for market incentives
in reconciling public with private
interests, decentralized individual decisions
with social goals. Pollution taxes
are a case in point. Performance standards
for teacher pay would be another.
Putting medical insurance programs
on an efficiency-based reimbursement
basis would be a third. The big gap is
in the redesign of incentives and institutions
to guide decentralized government
decision making more systematically
toward the aims of our social

programs (Schultze (1971)). Thus far,
the government, like the economics profession,
is largely in the dark about its
own production function.

Taxation

What strikes an old public finance functionary
as forcibly as any change in the
field of public finance is the way in which
modern thinking has knocked the props
out from under the neat and primitive
theories of tax incidence of a generation
ago. The property tax provides a particularly
instructive case in point. The textbooks
of the 1930's and 1940's told us confidently
that the tax on land (fixed supply)
was capitalized and on dwellings (supplyresponsive)
fell like an excise tax on the
occupant, the consumer of housing services.
The policy lesson was clear: Given
the declining proportion of income spent
on housing services as income rises, the
tax was hopelessly regressive. Today? It is
recognized that the old incidence analysis
was wrong, even on its own terms.
The modern theory of incidence (defined
as the impact on distribution of
private real income) draws on general
equilibrium theory, distinguishes between
sources-of-income and uses-of-income effects,
and disentangles the concepts of
specific, differential, and balanced-budget
incidence. The resulting analysis indicates
that much, of the aggregate burden of the
property tax falls on owners of capital
and hence tends to be progressive and
this progressivity is enhanced by the
particular "excise-type" effects of this
tax (Henry Aaron). In short, error has
been exposed and though the debate is not
over, we are now in transit toward truth.3
It is hard to put down the knee-jerk reaction
that prefixes "property tax" with
"regressive." And it will take some time
before policy makers accept the proposition
that, at the very least, the property
tax is now in the unexpected position of
"innocent until proved guilty." But the
implications for policy are profound.
Economists have long been useful and
influential contributors to the design of the
federal tax structure and of particular
taxes. Again, elementary concepts we now
take for granted-for example, horizontal
versus vertical equity, Richard Musgrave'
s three branches of distribution, al-
3Those who view decisions to locate in a particular
community as a conscious choice of one particular
bundle of public services over others conclude that the
property tax on housing is a benefit tax, a payment for
benefits received.


### ---Economics-1975-0-14.txt---
location, and stabilization, the lagged
effect of tax changes, and automatic versus
discretionary tax changes were not even
part of our vocabulary in the pre-World
War II period. Yet, all of these are now
factored into our economic advice on
taxation.

Even more directly impinging on policy
are the empirical advances. One thinks of
searching studies of particular taxes and
tax components (especially in the Brookings
Studies on Government Finance),
and of the relentless identifying and quantifying
of federal income tax preferences
or "loopholes." Much of the thrust of
economists' recent work on these "tax
expenditures" has been (a) to identify the
beneficiaries and specify the size of the
government subsidies provided in the form
of preferential tax treatment, (b) to define
the inequities, both horizontal and vertical,
that they create, and (c) to estimate
the distortions in resource flows caused by
preferential treatment of oil and gas, housing,
real estate partnerships, and the like,
and measure the resulting welfare loss.
Though the congressional response has
been slow and halting, progress has been
made along the lines plotted by economists,
and a solid base has been laid for
the further tax reform that is surely coming.


Out of the countless other advances,
one stands out, namely, the highly informative
work done on the distributional
impacts of taxation with the aid of the
powerful tool of micro-unit data files (for
example, the MERGE file developed by
Joseph Pechman and Benjamin Okner).
Such micro-unit files are a new-generation
statistical missile, MIRVed so that they
can simultaneously hit multiple revenueestimating
and burden-distribution targets.
With their help, for example, economists
have measured the growing burden
of income, payroll, and consumption taxes
on the lower income groups and developed
techniques for removing them-most recently,
in the context of the impact of
inflation on the same groups.

One should add that if revolution rather
than reform becomes the order of the day
in the federal tax structure, the economist
is ever ready with reasonably sophisticated
analytics and a fair amount of empirical
information on such major alternatives
as a value-added tax, a progressive
expenditure tax, and a net-worth tax. One
of the next stages in tax research, a highly
complex one, will be the general equilibrium
analysis of such sweeping changes
in the tax system as, say, the substitution
of a value-added tax for the corporate
income tax or for part of the payroll tax.
Or, if stimulation of private saving becomes
a compelling objective, perhaps the
substitution of an expenditure tax for
part of the income tax will become a live
issue. The skills of the economist will be
front and center in any such redesign of
the tax system.

The negative income tax story is relevant
here. The concept and its rudimentary
principles were developed and discussed
among economists in the early

1940's. Some of us were already using it as
a teaching device in the mid-1940's. A
quarter-century after its origin, it became
the basis for the Family Assistance Plan
developed by Mr. Nixon's economists.
And a more limited version of the plan
seems again to be rustling in the leaves.
Fiscal Policy

In the domain of fiscal policy, it is harder
to answer the question, "What have
you economists done for us lately?" with
a sparkling array of examples. Much of
the theoretical ferment in this field is associated
with the flowering of Keynesian
macroeconomics in the late 1930's and
1940's, the very period when the microeconomics
of tax incidence and public

expenditures languished.


### ---Economics-1975-0-15.txt---
Conceptual advances have continued
throughout the past twenty-five years, but
they have been more in the nature of a
fleshing out and consolidation of the original
breakthroughs with the aid of the
powerful tools of mathematics and econometrics.
Multiplier analysis, for example,
has moved from the theoretical realm into
large computer models of the economywith
the tax cut of 1964 and the surtax of
1968 providing empirical grist for the mill.
While the models differ on the exact value
of the multiplier, "a fiscal policy planner
will not often be led astray if he uses a
multiplier of 2" for government spending
(see Blinder and Solow).

Coupled with multiplier studies is the
even more subtle study of the structure
of the "outside lags," of the timing of responses
in the economy to changes in

fiscal policy. Though the empirical efforts
and debates go on apace, the behavior of
the cumulative multipliers in a clutch of
economic models suggests that for any
given change in fiscal policy, "at least 75
percent, probably much more, of the ultimate
effect is felt within the first year
after the initiation of the policy" (Blinder
and Solow). Although intractable questions
remain concerning investment responses
to fiscal policy changes, enough
has been learned about aggregate demand
responses to provide two broad generalizations
about fiscal policy:

One, the conditions for intelligent
fiscal policy are met if economic f orecasting
can answer two not-very-exacting
questions: Do projected economic
conditions in the ensuing six to nine
months call for restraint or stimulus?
Is the required dosage large or small?
Two, given the limited margin for
error in a high-employment economy,
it is better to rely on many smaller
monetary-fiscal moves than a few large
ones.

Implicit in these two generalizations is a
third one: Given both the internal shifts
and the external shocks with which stabilization
policy has to cope, a discretionary
policy that makes efficient use of feedback
information will be more effective than
an automatic policy that locks in on fixed
fiscal and monetary targets.

Development of a simplified measure of
fiscal impact revolving around the "full
employment surplus" (FES) concept is
another example of the typical process by
which economists expose error, develop approximations
of truth, but continue the

vigorous debate on further improvements.
First, policy makers had to be weaned
away from the annually balanced budget
and the cyclically balanced budget as
policy targets and from actual deficits or
surpluses (especially in budgets other than
the national income accounts budget) as
measures of budget stimulus or restriction.
It was not easy. It took almost a quarter
of a century before a Democratic president
was converted (in 1961) and another
decade to capture a Republican White
House.

But success on the policy frontier has
its own pitfalls, both political and economic.
What was intended as a measure of
policy was instead taken as a goal, namely,
a balanced budget at full employment,
a "self-fulfilling prophecy" as the Nixon
Administration called it. This erroneously
implied that the fiscal target should remain
fixed regardless of changes in monetary
policy and significant shifts in private
demand, for example, a plant-and-equipment
boom. Apart from trying to correct
such misconceptions, economists have had
to wrestle with the problem of the overstatement
of the full employment surplus
when inflation expands revenues faster
than expenditures, not to mention the
problem of weighting for differing multipliers
if tax or expenditure components
change sharply. In brief, the advances over


### ---Economics-1975-0-16.txt---
the bad old days of the annually balanced
budget are enormous, but economists are
aware of the limitations of the FES measure
and are struggling to resolve them.
Just as economics relegated erroneous
budget concepts to the dustbin, so it has
cast a shadow over such former favorites
(of mine, among others) as federal capital
budgeting and the "shelf of public works."
The initial enthusiasm for the capital
budget concept (in the context of a Congress
seeking to balance the budget annually)
was dispelled by second-thought
analysis showing that (a) it rested on some
faulty parallels with private finance, (b)
the implicit fiscal policy rule of always
financing capital projects by borrowing is
in error, and (c) it would bias government
capital spending toward bricks and mortar
instead of brainpower and people. In the
public works case, the concept ran afoul
the findings of prosaic economic research:
recent studies show that the public works
program launched in 1963 to speed recovery
was far from completed before

excess demand overtook us in the 1966-69
period. This is not to rule out the use of
certain types of "public works" that are
nimble on their feet, such as road and
forest maintenance work, for stabilization
purposes. Nor does it rule out speeding
up or delaying the launching of projects
that are to be undertaken for sound costbenefit
reasons in any event. But it is
fair warning not to expect very much
stabilization help from the public works
sector (not to be confused with public
service employment).

In the conscious use of taxes for stabilization
purposes, the huge 1964 income
tax cut delivered economic expansion and
a balanced budget on schedule without inflation
by mid-1965, just before the Vietnam
escalation struck the economy. The
temporary 1968 surtax, buffeted by powerful
demand forces and monetary easing,
left a more ambiguous econometric trail.
Subsequent fiscal policy thinking emphasizes
the advantages of temporary tax
changes that embody not just income effects
but intertemporal substitution effects.
For example, lowering the prices of investment
goods in a recession via a clearly
temporary increase in the investment
credit, or temporary cuts in consumption
taxes on durable goods (or lacking these,
temporary purchase subsidies), would constitute
a powerful incentive to purchase
those goods before the price went up
again.

Further work is needed to measure the
cost-push effects of anti-inflationary tax
increases that offset part of their demanddamping
effect. In recession, the costeasing
and demand-push effects work in
happy harmony. They work at cross purposes
in tax increases (though not in
expenditure cuts) to curb inflation. The
question of how large the offsetting costpush
effects, or aggregate supply effects,
may be, is unresolved. In a high-inflation
economy, this is a serious gap in our fiscal
policy knowledge.

Other Aspects

This kaleidoscope of contributions, long
as it is, leaves out a whole string of developments
in budget concepts, techniques,
and processes-efforts that were
crowned by the congressional budget reforms
recently put into effect. Much of the
guidance and momentum for these reforms
was provided by economic analysis and by
a succession of five economist-budget directors
throughout the 1960's. Also omitted is
the conceptual work on the economics of
the bureaucratic process, of how government
works. Other omissions include the
rebirth of interest and great advances in
the economics of state-local finance, the
rapid growth of the important new field of
urban economics-with its contributions
to regional economics, location research,
and analysis of the city as an economic system


### ---Economics-1975-0-17.txt---
and the enriched economics of fiscal
federalism. I have even eschewed an assessment
of revenue sharing, the rationale
and form of which were developed by
economists. With little imperialism, economists
can also cite the firm quantitative
evidence being developed to demonstrate
the adverse economic effects of many public
regulatory activities.4

For all the advances, the agenda of unresolved
conceptual questions and unfinished
empirical business is huge. But
even this truncated review of progress
and current output in public economics
makes clear that the contributions of
recent decades have enormously enriched
this field not only conceptually but as a
source of hard practical advice to decision
makers who want to shape a better tax
system, do justice to the poor, improve
social programs, reform budget procedures,
fight unemployment, and so on. And in the
process, the frontiers of normative economics,
both theoretical and empirical,
have been pushed out into the areas of
education, health, racism, crime, family
behavior, and even political behavior.
As a result, we have plunged ever deeper
into the realm of values. Not that it was
a value-free inquiry to ask the traditional
questions about the effect of a given policy
on material output. But surely the testing
of policies by the costs they incur and how
effective they are in meeting some generally
accepted criteria of social welfare or
general welfare involves economics directly
in value and distributional problems.
Aid it enables economics to say important
things on social policy issues within the
framework of the conventional economic
paradigm and with rigor of the non-mortis
variety.

We are becoming interdisciplinary in
spite of ourselves. When we do it, of course,
we don't think of it as cross-sterilization
of disciplines. But here is an area where
modesty becomes us. For if we confine
ourselves too narrowly to economics, we
are far too likely to attribute to economic
variables the behavior and results that are
really a response to social variables. Fearing
just that, one observer has been unkind
enough to suggest that we ought to
stick to inflation problems where we all
know what to say.

V. The Economist and Inflation
Inflation may no longer be "Public
Enemy Number One" now that severe recession
is upon us, but it is surely "Economists'
Enemy Number One." Among the

charges of, by, and against economists
that have been touched off by doubledigit
inflation and reported in the public
prints are these:

Economists have confessed (I plead
guilty) that 1973 was "the year of infamy
in inflation forecasting" and, as
already noted, that "we were caught
with our parameters down."

Aaron Gordon puts it more explicitly
when he says that "the forecasters fell
flat on their faces in predicting price
changes because they didn't have any
way of estimating sectoral supply scarcity"
and adds that we have not "even
started to develop a theory of aggregate
supply."

Leontief scolds macroeconomists more
generally: "There is a lot of fancy methodology,
but the macroeconomists get

indigestion if you give them facts."
I As an example of the "Age of Quantification,"
George Stigler cites the sea of studies on regulatory
practices and their costs and benefits in the past dozen
years, where there was a vacuum before. He notes that
thirty-six "quantitative studies of effects of laws" were
reported in two journals alone during this period, the
Joutrnal of Law and Economics and the Journal of
Political Economy. These are promoting a broader
consensus within the profession, informing decision
makers, and posing challenges that will make policy
failures easier to identify. (Personal correspondence.)


### ---Economics-1975-0-18.txt---
We are reminded ad nauseam that the
"new economists" of the 1960's had
promised to fine-tune inflation out of
their full employment economy (a clearcut
triumph of caricature over fact
since Keynesians time and again warned
of precisely the opposite danger).
Myrdal and Heilbroner have pointed
to stagflation as Exhibit A that economists
typically lag rather than lead
their targets, that being "behind its
time" is "the regular methodological
weakness of establishment economics."
Von Hayek recently reentered the
fray to lay the blame for worldwide
inflation squarely at the door of economists,
particularly those "who have

embraced the teachings of Lord
Keynes."

Apart from the charge that Keynesian
economists have caused inflation (which
is much like saying that the cause of forest
fires is trees), the bill of particulars against
macroeconomics runs something like this:
First, it did not forewarn the body politic
that it would have to pay such a high
price in endemic inflation for the attainment
of high employment. Second, its
progress in solving some important puzzles
of endemic inflation relating, for
example, to the Phillips curve, wage inflation,
expectations, and uncertainty is
much too slow. Third, there is no articulated
genefal theory of inflation as such.
Fourth, economists failed to foresee the
1973-74 epidemic inflation because their
forecasting models lacked the central
supply and price parameters. Fifth, macroeconomics
is helpless in the face of epidemic
or external-shock inflation-indeed,
it has not satisfactorily explained the coexistence
of inflation and recession, or
stagflation. Without attempting a pointby-
point assessment of these complaints,
I will touch on all of them in the following
sympathetic interpretation of how economists
are coping with inflation's tough
analytic and empirical challenges.
Addressing myself for a moment to our
reproachful public, let me simply say to
them: "We never promised you a rose
garden without thorns." Over most of the
past thirty years, macroeconomists have
warned again and again, first, that aggressive
fiscal and monetary policy to

manage aggregate demand was bound to
generate inflationary pressures once the
economy entered the full employment
zone, and second, that while full employment
spells inflation, recessions run into
price and wage rigidities that thwart deflation,
an asymmetry bound to produce a
ratchet effect on the price level. Keynes
himself foresaw the basic problem in his
little book, How to Pay for the War, in
1940. Abba Lerner and William Beveridge
also wrote of the problem in the early
1940's. And it has been discussed in the
stabilization theory and policy literature,
in congressional hearings, and in other
policy forums ever since.

This country finally embraced activist
fiscal policies for full employment in the
1960's, most explicitly in the 1964 tax cut.
Following the canons of Keynesian economics,
focussing on the economy's full
employment potential as their target, and
steadfastly rejecting a spate of "structural"
explanations of unemployment,

economists were at first alone in prescribing
tax cuts as a tonic for the stagnant
economy. Enacted early in 1964, the tax
cut delivered the promised expansion and
budget balance without inflation. By
August 1965, when Vietnam escalation
began, unemployment had been brought
to 4.4 percent with only the faintest stirring
of the inflationary beast (i.e., with
consumer prices rising at less than a 2 percent
annual rate).

In a very real sense, economists have
been victims of their own success. Macroeconomic


### ---Economics-1975-0-19.txt---
policy, capped by the tax cut,
was the major force holding the postwar
economy on a vastly higher plane than
the prewar economy.5 On one hand, the
high employment, limited-recession economy
forged with our macro-economic
policy tools is indeed an inflation-prone
economy the formula for successful management
of high-pressure prosperity is far
more elusive than the formula for getting
there. Yet on the other hand, success bred
great expectations on the part of the public
that economics could deliver prosperity
without inflation and with ever-growing
material gains in the bargain. The message
got through that we had "harnessed the
existing economics . . . to the purposes of
prosperity, stability, and growth," and
that as to the role of the tax cut in breaking
old molds of thinking, "nothing succeeds
like success" (Heller). The Economist
unkindly corrected me: "Nothing exceeds
like success."6

To be sure, critics and converts alike
ignored our caveats that the goal of
"prosperity without a price-wage spiral"
had "eluded not only this country but all
of its industrial partners in the free world,"
that "the margin for error diminishes as
the economy reaches the treasured but
treacherous area of full employment. . .
and that "the 'new economics' promises
no money-back guarantees against occasional
slowdowns or even recessions"

(Heller).

All too soon, Vietnam blew the economy
off-course. Economists found that in the
political arena fiscal policy was not a twoway
street and that the much delayed surtax
adopted in mid-1968 was no match for
surging inflation. Nor was the induced recession
of 1969-70. It took a combination
of the 1971 shock therapy of tight wageprice
controls and the stimulus of tax cuts
to subdue inflation and energize expansion.
It is worth noting that economists analyzed
and projected the effects of this
''new economic policy" with exceptional
precision. That the tax cuts, coupled with
controls and devaluation, would generate a
surging expansion at very moderate rates
of inflation in .1972 was widely and accurately
forecast.

But the period from August 1971 to
January 1973 was in the nature of a remission
from the inflationary disease,
clearly not a cure. The 1969-70 recession
brought home the worsening problem of
persistent inflation in the face of slowdown
and recession. It presented new
empirical puzzles for the analysts of the
Phillips curve, wage equations, and expectational
inflation. And it began to prompt
the public mutterings that are being intensified
by the 1974-75 stagflation: "All
right, so you did not promise us a rose
garden without thorns-but the thorns
without the rose garden?"

Keenly aware of these problems, economists
have long been at the drawing

boards on this problem of endemic inflation.
In a close parallel with research on
cancer, economists are working on various
pieces of the inflation puzzle and producI
As gauges of the contrast between prewar and postwar
performance: unemployment averaged 18.8 percent
in the decade of depression (1931-40) in contrast
with 4.8 percent in the twenty-eight years since World
War II; the prewar peak annual rate was 24.9 percent,
the postwar peak was 6.8 percent. Annual real GNP
dropped 30 percent from 1929 to 1933; since the war,
mild declines have occurred only in three years (1949,
1954, and 1970), though 1974-75 may add two more.
Consumer prices in 1940 were 18 percent below 1929;
from 1948 to 1974, they increased 106 percent.
6 Macroeconomists were not alone in their exuberance
in the mid-1960's. On this rostrum a decade ago,
George Stigler, after reviewing the great promises and
early accomplishments of the "Quantitative Revolution
in Economics," was moved to say, "I am convinced
that economics is finally at the threshold of its Golden
Age-nay, we already have one foot through the door.
... Our expanding theoretical and empirical studies
will inevitably and irresistibly enter into the subject of
public policy, and we shall develop a body of knowledge
essential to intelligent policy formulation. And
then, quite frankly, I hope that we become the ornaments
of democratic society whose opinions on economic
policy shall prevail."


### ---Economics-1975-0-20.txt---
ing useful insights and guidance for policy
purposes. But as economists, we would be
the first to underscore that these puzzles
are far from being fitted into an articulated
and holistic theory of inflation. Inflationary
analysis appears as an appendage to
Keynesian and monetarist theories. But
as yet, the Keynesian apparatus cannot
tell us how any given change in aggregate
demand is divided between changes in
real output and changes in prices. Nor has
monetarist theory unlocked the puzzle of
how the effects of monetary changes are
divided between output and price level
changes. And no big breakthrough is in
sight.

Does this mean that the economist has
to stand mute in the meanwhile? Not at
all. He is pushing ahead on the various
pieces of basic research on the cancer of
inflation and isolating and prescribing
effectively for particular forms of the
cancer even without having a complete
explanation of the disease. Let me come
back to the sustained and systematic research
efforts on endemic inflation after
examining the 1973-74 epidemic and the
economist's responses to it. Since the
epidemic is an over-layer on the endemic
base, the distinctions won't be clear-cutbut
they are nonetheless useful for viewing
what the economist is able to contribute
to policy.

The food-fuel price bulge generated over
half of the 1973-74 inflation-and of
economists' woes as well. Yet, it is asking
a lot of economists to expect them to have
foreseen that the oil cartel would quadruple
oil prices, that the world would suffer
widespread and successive crop failures,
that the Peruvian anchovies would go into
hiding, and that the Soviets would "solve
our surplus grain problem" overnight.
Several unpleasant policy surprises also
beset the inflation forecasters. First, just
when a new rash of inflation was breaking
out early in 1973, the reasonably effective
Phase II controls were abruptly dropped
in favor of the weak and ineffective Phase
III. Second, six months later, after inflation
had changed into a commodity-driven
structural phenomenon involving a drastic
readjustment of relative prices, the White
House (to the pained surprise of economists
inside and outside the administration)
prescribed just the wrong medicine,
a new wage-price freeze. A third policy
surprise was that the dollar was allowed
to sink like a stone: At its low point in the
summer of 1973 (just before a substantial
rebound), relative prices of imports had
risen 10 percent in six months. About a
quarter of the 1973 inflation has been attributed
to these policy developments

(see William Nordhaus and John Shoven).
It is worth noting that unexpected
twists and turns of federal policy which
might be termed "internal shocks" in
contrast with the "external shocks" of the
food-fuel price explosion-are a continuing
bane of the forecaster's existence. The
about-face of the Federal Reserve in 1974
is another painful case in point. The sharp
turn from ease to tightness in the first
quarter of the year was a major factor in
transforming prospects of recovery into
recession in the second half of 1974. It is
not quite clear why economists should be
better at anticipating these shocks, especially
the external ones, than society as
a whole, or other professional specialists,
or practical men of the world. Nothing in
statistical methodology or economic science
enables us to predict random shocks.
What can be expected of us is that when
they occur, we will spot them quickly,
identify them, and analyze their significance
for policy.

It is also worth remembering that
democratic governments, by their nature,
are pressure-responders rather than problem-
anticipators. This carries two implications
for political economists. On one
hand, if an idea's time has not yet come,


### ---Economics-1975-0-21.txt---
or if a problem has not yet become a crisis,
the economist's call for action is likely to
go unheeded. On the other, spotting
emergent problems early can perhaps
hasten an idea's time and alert the policy
makers to impending danger.

Economists can more readily be faulted
for being caught by surprise by the shortages
of materials and primary processing
capacity that caused the economy to
bump against its ceiling sooner than expected
and by the worldwide economic

boom that put severe pressure on raw
commodity supplies and prices. On the
first point, we suffered both from information
failure-the official capacity indexes
simply did not reveal how close the economy
was to its output ceilings-and from
analytic limits. While identifying the
causes, economists have been unable to
pinpoint the relative significance of the
shortfall of investment that began in the
late 1960's, of underinvestment caused by
price controls, of delays induced by environmental
policies, and of the surge in

foreign demand touched off by devaluation.
However, I should add that the
shortages problem is meat and drink for
economists, and they are responding (especially
in the energy field) with new

analyses of price elasticities, investment
needs, and the like. All of a sudden, price
theory is back in vogue, and elasticities
have replaced multipliers as the badge of a
policy maker's savoir faire.

Delays in perceiving that the U.S. economic
expansion was part of a worldwide
upsurge can again be laid more to lack of
an adequate information system than to
any inability to understand the underlying
principles. Still, a better sense of history
and of the emerging worldwide imbalance
between growing aspirations and
growing incomes on one hand and inelastic
resource supply and lagging technology
on the other would have made us more
conscious and cautious. We are considerably
less likely to be caught by surprise in
the future in view of the new worldwide
data networks that are being developed by
Project LINK at the University of Pennsylvania
and by Otto Eckstein and his

colleagues at Data Resources Incorporated
(DRJ).

Without absolving economists, one
should apply this operational test: With
proper foresight, would tighter monetary
and budget policy have been able to
damp inflation? It is worth recalling, first,
that the full employment budget was
making a swing of over $10 billion towards
restraint between fiscal 1973 and fiscal
1974 (from a $2 billion deficit to a $10
billion surplus under the old 4 percent unemployment
standard) and that monetary

policy pushed interest rates into the
double-digit region; second, that there was
little that an aggregate demand squeeze
could have done to push world commodity
prices down. So the answer is clear: Even
tougher fiscal and monetary policy would
have had limited scope in holding inflation
down.

This is not to deny that generating a
larger full employment surplus would
have been the prudent course in calendar
1973. But it is worth noting that to offset
the food and fuel price explosions-which
were triggered by forces largely immune to
U.S. fiscal and monetary policy-would
have required a reduction of 3 percent in
all other prices. Such a target implies depression-
inducing doses of fiscal and monetary
restriction, an unthinkable "solution.
"

Looking toward the future, many economists
draw the lesson not that one should
keep the economy's motor idling,' but
rather that one should provide it with
safety devices and heavy-duty shock absorbers,
for example, stock-piling of foodstuffs,
oil, and basic raw materials, careful
tracking of commodity exports, distant
early warning systems to spot shortagesin-the-making,


### ---Economics-1975-0-22.txt---
and conservation and development
measures to limit dependence

on foreign raw materials cartels. In other
words, it is a call for better planning,
better data, and faster conversion of
knowledge into policy.

Another criterion of economists' responses
to inflationary shocks is how

quickly they adapted (read, "disaggregated"
) their macromodels, large and
small, to incorporate new supply and price
parameters that had previously been
judged of second or third order importance
and hence relegated to Marshall's ceteris
paribus pound. Some of the mongrel pups
impounded there turned out to be full
blooded huskies, for example, food prices,
the exchange value of the dollar, oil and
other raw material supplies and prices. At
first most economists were slow and the big
models sluggish in their responses. After
all, for two decades prices had moved in
tandem with wages, with a year-by-year
percentage-point differential of 23+ 1. So
most models relied on wage trends, with
some adjustment for productivity and
capacity behavior, to give them a fix on
price trends. Their eyes were on labor
market indicators rather than commodity
supplies, exchange rates, and the like.
After some initial delays, the model builders
scrambled to disaggregate, to build
microelements into their macromodels.
For example, DRI now has good stage-ofprocessing
models that absorb the impacts
of food and energy explosions. Price elasticities
are being built into the macromodels
to reflect the impact of massive
relative price changes on the macrodimensions
of the economy.

The whole experience reminds us of the
role and limits of econometric forecasting
models. First, the combination of computers,
mathematics, and econometrics

cannot produce the miracles that the uninitiated
may expect of them-there is no
way of replicating reality with its 3 million
equations, all of them non-linear. Second,
their indispensable function is to bring us
closer to reality and help the mind manage
the previously unmanageable-they permit
us to release vastly more animals from
the ceteris paribus pound than we could
manage without these tools. Third, they
have to be constantly adjusted to plug in
common sense, adjust the length of the
lags, and bring in new dimensions of the
problem. Else, they will lock out things
that a more judgmental approach would
include, and will fail to respond quickly to
changes in order of importance.
So the inflation-shock experience has
brought home the need not just to watch
supply but to watch all the pieces lest the
model prevail over the mind, rather than
having the model help the mind prevail
over matter. The macro-stalactites have
to reach toward the micro-stalagmites, and
vice versa. I hope that metaphor is not a
portent of the pace at which the advance
toward macro-micro fusion will proceed.
Economists who use judgmental models
have shown us how to be the master
rather than the slave of the computer. A
case in point was the early analysis (especially
by George Perry) of the macroimpact
of the oil price increases. A year
ago, his work had already brought out the
oil paradox-the inflation of costs and
hence prices, leading to a deflation of aggregate
demand-and had provided some

estimates of both. The insight that some
$15 to $20 billion of consumer purchasing
power would be siphoned off into the
hands of oil producers and royalty collectors
without any early return to the
economy in the form of demand for imports
or investment goods had important
implications for demand-management policy-
implications that were ignored until
severe recession was full upon us.7


### ---Economics-1975-0-23.txt---
These important insights into the
macro-economic policy implications of oil
prices fit into the broader efforts of economists
to disentangle the sources of the
current inflation and identify the appropriate
remedies. They differentiate among
(1) excess demand, which had spent
most of its force by early in 1974, (2) the
price-wage-price spiral, which began to
turn more rapidly in 1974, and (3) external-
shock or special-sector inflation, in
particular, the commodity-price surges
that permeate the present inflation and
account for its special character and
ferocity.

The first responds rather readily to
monetary-fiscal pressure, the second responds
more reluctantly, and the third is
highly resistant to the demand-management
measures of any given country. For
the second and especially the third types,
therefore, high costs in unemployment and
foregone output have to be incurred for
small gains in curbing inflation. So the distinction
is an instructive one for policyeven
when the instructions are ignored.
As we meet here tonight, the economic
lessons that were so long ignored are being
painfully driven home by severe recession
and unemployment coupled with continuing
inflation. A much-belated consensus
that fiscal and monetary stimulus can now
be undertaken with minimal inflationary
risk is rapidly forming.

The economists' three-ply classification
of inflation sources is also useful in driving
home another point: In most U.S. inflations,
consisting of the first two types, one
person's price is another person's income,
so that in spite of some reshuffling, there
is no net loss in real income. Not so in
1973-75. Commodity inflation has transferred
tens of billions of dollars of real income
out of the pockets of urban consumers
and wage earners into the hands of
farmers and foreigners where it is beyond
the reach of the collective bargaining
process. From this, several important
inferences can be drawn:

Point for point, this inflation cum
relative price changes is harsher in its
impact than previous postwar inflations.
In this "no-win" inflation, the wage
earner's loss has not generally been the
employer's gain; hence, if the wage
"catch-up" process succeeds in recouping
the full rise in the cost of living,
much of the wage increase will pass
through to prices and thereby give the
wage-price spiral another self-defeating
turn.

It follows, as various economists
urged throughout 1974, that tax cuts to
bolster the real income of labor, if put in
the context of a social contract, might
well relieve some of the pressure for
higher wages.

In this respect, today's situation contrasts
rather sharply with the 1950-51 inflation
when a similarly rapid run-up in world
commodity prices was accompanied by a
rapid rise in profit margins side-by-side
with vigorous federal policies to boost
capacity. The ensuing combination of
ebbing world market prices and wage increases
that could be granted without

generating higher product prices resulted
in a remarkable four-year period of price
stability from 1952 to 1956.

A closely allied economic insight goes to
the nature of the inflationary process. It
scale formal models. His analysis shows that the purchasing
power loss had reached $37 billion (annual rate)
by the third quarter of 1974 and that the rise in the
deflator attributable to the oil price jump was 3.8 percent.
His analysis embraced not only the real-income
effect (the transfer of real income from consumers to
producers), but also the monetary-policy effect (the
reduction of the real value of the money stock and the
rise in interest rates stemming from the highly inelastic
short-run demand for petroleum products), the automobile-
demand effect (higher saving), and the inducedinflation
effect (the price-wage-price effect) of the oil
price rise on the macroeconomy.


### ---Economics-1975-0-24.txt---
explains in good part why inflation is so
stubborn even in the face of overly restrictive
monetary-fiscal policy and rapidly
mounting unemployment and slack in the
economy. It is the sharp run-up in relative
prices of food, fuel, and imported goodscoupled
with the downward rigidities of
wages and prices that is the key to most
of our stagflationary malaise today.
These downward rigidities are a striking
example of the way in which economic
solutions create their own problems and
move the economist relentlessly from one
new frontier to another. Once macroeconomics
gave governments the know-how

and tools of modern demand-management
to avoid depression, and once the public
caught on that even recessions are essentially
man-made-chiefly by That Man in
the White House, whoever he is, together
with the Congress and the Federal Reserve
Board-it became part of the politics of
survival to hold employment high and
keep recessions in check. Absent the fears
of mass unemployment and prolonged
recession, the risks of not cutting prices
and not accepting lower wages are minimized.
Having put the Great Depression
of the 1930's far behind us, will we therefore
have to live with the Great Inflation
of the 1970's?

Essentially, the economist answers that,
given the ratchet behavior of wages and
prices, the price level can only float upward
to accommodate the massive relative
price increases of oil, grains, certain raw
materials, and imported goods. These
sharp changes in the composition of supply
touch off reverberating price increases
throughout the economy as prices in the
scarce-supply sectors become costs in the
less-scarce ones. The reverberations go on
-in substantial part independent of the
state of aggregate demand and hence of
monetary and fiscal policy-until the
prices of the initiating goods have risen
sufficiently farther than prices in general
to accomplish the necessary realignment
of relative prices. This is the process going
on now. It takes time, but not forever. It
has much to do with double-digit inflation,
but it does not condemn us to Weimar
Republic inflation.

Solow (1975) reminds us that the supply-
shift phenomenon bears a close relationship
to the demand-shift analysis of
the creeping inflation of the mid-1950's.
At that time, the parallel process was
touched off by an investment boom that
put excess demand pressures on capital
goods industries even when there was no
excess aggregate demand in the economy.
Given the downward rigidity and costoriented
nature of wages and prices in

areas of excess market power, the price
level had to float upward to accommodate
those relative price changes (see Schultze
(1959)).

John Dunlop and other economists have
emphasized that there is a closely related
phenomenon on the wage side known as
"scale wages" or "wage relativities" or
even a "just wage" (see Robert Hall and
Michael Piore). If the relative wage scale
is thrown out of kilter by an outsized
wage settlement in one industry, the others
will writhe, twist, and turn until the old
relationships are reestablished. There is
only one way the wage structure can move
to accommodate this process: Up. Again,
the process burns itself out only when a
new equilibrium has been established on a
higher plateau.

The policy implications of the supplyshift,
demand-shift, and wage-shift insights
are reasonably clear. One is the
limited scope of repressive monetary-fiscal
policy in coping with this process. Another
is that the key to a successful wageprice
policy for these circumstances is
to establish and effectuate norms for the
pace-setters and thus thwart the wagewage
and price-price spirals and the interacting
wage-price spiral. Once the process


### ---Economics-1975-0-25.txt---
is launched, the role of a wage-price
watchdog with teeth would be to see to it
that the adjustment process is a limited
and straightforward one, not a leapfrogging
sequence that will prolong the agony
of adjustment. Again, understanding the
economics of the process is the sine qua
non for shaping the right policy to fit the
particular type and phase of inflation that
is beleaguering us.

Let me return now, before closing, to
several of the abiding problems of endemic
inflation that are engaging the attention
and efforts of economists.

An important but' elusive question for
the policy maker concerns the costs of
inflation. Can the economist tell him
anything useful and definitive on this
subject? Useful, perhaps. Definitive, no.
First, the economist would remind him
that people continually blame inflation for
crimes it does not commit. They are sure
that every increase in their pay envelope
is a reward for merit, every increase in
prices an inflationary theft. Especially
pertinent to our present shock-spiral is
the observation that people "blame inflation
for changes in relative prices and in
real incomes that stem from market forces
that have nothing to do with the course of
the general price level" (Edward Foster).
Second, studies show that in a typical
U.S. inflation, the poor have gained more
in jobs and incomes than they have lost in
higher prices. But in the present inflation,
prices have shifted sharply against the
poor, and any initial gains they may have
made in jobs and income in 1973 have been
more than offset by the losses incurred in
the deepening 1974-75 recession induced
to fight inflation.

Third, at the rates of inflation experienced
prior to the 1973-75 explosion, most
economists find it difficult to believe that
the costs of inflation-mostly in redistributional
effects, but with some distortion
in resource allocation-hold a candle
to the welfare losses of substantial add-ons
to unemployment. Fourth, however, when
inflation reaches double-digit levels, the
costs in terms of the social conflicts and
tensions it generates and the uncertainties
and loss of confidence in the dollar yardstick
it may breed are important intangibles
that economists cannot ignore, yet
have not been able to quantify. We need
to understand far more about what unsettles
and upsets people about inflation,
how this affects their economic behavior,
and what economic costs result. Clearly,
in an economy where inflation is endemic,
the balance between its gains and losses
deserves intensive further study.
Another important question is this:
How much of the present run-up in prices
of foodstuffs, oil, and raw materials is a
transitory phenomenon, how much is a
one-time shift to a new plateau, and how
much represents a new upward trend?
Economists have trained the guns of price
theory and price elasticity estimation on
these questions in the case of oil and several
other basic materials. They generally
come up with more optimistic answers
for five to ten years hence than for the
near-term. But much of the answer lies
in geo-political, meteorological, and similar
puzzles-for example, the effectiveness
of oil and other raw material cartels,
the pace of world population increases
and income growth, and the possibility of
a dry, cold phase in world weather-that
lie largely or wholly beyond the reach of
economic analysis.

What we do know is this: The 1950's
and the 1960's were a period of gently declining
or roughly stable world prices for
raw materials or foodstuffs. Now, rising
population, industrialization, income, and
aspirations may put such pressure on the
world's supply capabilities that while we
are not nearing any Club-of-Rome ultimate
limits, we may for some time exceed
the speed limits of stable expansion. If so


### ---Economics-1975-0-26.txt---
we may have passed an inflection point in
the price trends of basic inputs to the economy
(see Walt Rostow). The mild downward
trend of the 1951-71 period facilitated
the rise in real incomes of urban workers
side-by-side with rising profits. If this trend
is reversed, rising income claims will generate
greater strains, and the Phillips
curve tradeoff will take place around a
higher inflation constant. Economic analysis
of long-run supply prices of basic
commodities using alternative assumptions
regarding world political, weather,
and economic trends could be a useful aid
to rational economic planning.
Coming back into the domain of economics
as such, one should take account of
the important new thinking and efforts
now being devoted to the continuing
mysteries of industrial pricing policies and
the role of fixed-rule (generally, mark-up)
pricing as a shield against uncertainty.
Answering the question of how, and how
fast, supply-shifts in the auction markets
or market-oriented sector are transmitted
through the rule-determined sectorwhere
certain relativities seem to be maintained
in the structure of prices (and
wages)-is essential to an understanding
of structural inflation (see Piore).
In turn, this analysis will strongly influence
thinking on government intervention
in private wage-price and perhaps
also supply-demand decisions. If the wageprice
structure is indeed fairly rigid and if
supply- and demand-shifts set off an inflationary
spiral, the "natural market

forces" will not readily make the necessary
supply-demand adjustment in any case.
Wage-price restraint or controls would not
be supplanting some supple and efficient
resource allocation mechanism, yet would
insert a circuit breaker into the inflationary
spiral. This view of the world would
also suggest that government action to
stimulate supply and suppress demand at
certain pressure points in the economy
might well pass the test of economic efficiency.
In pursuing these questions and
hypotheses, the economist will be laying a
firmer conceptual and empirical foundation
for specifying the areas and circumstances
in which intervention may be the
lesser evil.

One should not leave the subject of
economists' contributions to analysis and
prescription on the inflation problem
without mention of the intriguing attempt
of the Brookings Panel on Economic
Activity to bring the best analytical and
empirical efforts of economists to bear directly
on the problems and puzzles that
confront the policymaker. In relation to
inflation, the Panel has focussed much of
its attention on such questions as the
structure of labor markets, the Phillips
curve relationship and wage equations,
the costs of unemployment, price behavior
in specific sectors like foodstuffs and oil,
and the role of fiscal and monetary
policies. Apart from the significant contributions
that have been made to understanding
these problems, and to bringing
academic work into closer contact with
current policy problems, the Brookings
Panel is an interesting and perhaps unique
exercise in "continuing confrontational
econometrics." Responding to the kinds of
criticisms quoted earlier in my remarks,
the Brookings Panel combines rigorous
quantitative testing with continuing surveillance
by one's peers to assure that the
investigator (a) looks beyond mathematics
and makes his assumptions and- relations
conform to common sense, (b) spells
out the implications of his econometrics
and, if they are implausible, tries again,
and (c) constantly keeps asking questions
of the model. With the Panel now going
into its sixth year of thrice-yearly meetings,
previous analyses become not undisturbed
museum pieces, but grist for the
mill of constant retesting under the harsh
light of reality and peer-group criticism.


### ---Economics-1975-0-27.txt---
I have dealt at some length with the
substance of economists' work and findings
on inflation because mere assertions of
progress would hardly suffice to demonstrate
what's right with economics in this
most vulnerable area. The fact that there
are no final or comprehensive answers
has not kept economists from making
significant distinctions, analyses, and measurements
that equip policy makers with

better means of judging the policy tradeoffs
and determining how to improve the
fit of policy-to-problem for the different
types and stages of inflation. When policy
makers fail to heed these lessons, as in
1974, both the economy and the economist
feel the backlash.

Throughout this discourse, I have time
and again been tempted to kick over the
traces I fastened on myself and give voice
to my own criticisms, dissatisfactions, and
admonitions. But since an unholy (and unwitting)
alliance of my colleagues and outside
critics has amply and ably taken care
of this, I felt it best to stay within my constraints
in the interest of doing what I
could do to redress the balance. As economists,
we have many sins, none deadly,
to confess. But these are far outweighed
by the virtues, all quite lively, that we can
legitimately profess.
 ## Economics-1976-0


### ---Economics-1976-0-03.txt---
The title of this paper summarizes the
two-fold theme to which I want to address
myself this evening. First, the mainstream
of economic theory sacrifices far too much
relevance in its insistent pursuit of ever
increasing rigor. And, second, we economists
pay too little attention to the changing
institutional environment that conditions
economic behavior. We do not often
enough reexamine our basic postulates in
light of changes in this environment, and,
perhaps more important, we shy away
from the big questions about how and why
the institutional structure is changingand
where it is taking us.'

I

Economists pride themselves on belonging
to the most "scientific" of the social
sciences. The justification for this contention
lies in the growing resemblance between
the nature of the analytical tools
used in economics and in the natural
sciences-above all, the increasing use of
mathematical tools in theoretical analysis
and the development of sophisticated
mathematical and statistical techniques in
empirical work. Today, mathematically
formulated economic theory, the development
of econometric techniques, and the
sophisticated application of econometric
methods to the "testing of hypotheses" in
a variety of applied fields constitute the
core of the science of economics.2
What is science? One brief definition
runs: "A systematic knowledge of the
physical or material world." Most definitions
emphasize the two elements in this
definition: (1) "systematic knowledge"
about (2) the real world. Without pushing
this definitional question to its metaphysical
limits, I merely want to suggest that if
economics is to be a science, it must not
only develop analytical tools but must also
apply them to a world that is now observable
or that can be made observable
through improved methods of observation
and measurement. Or in the words of the
Hungarian mathematical economist J'anos
Kornai, "In the real sciences, the criterion
is not whether the proposition is logically
true and tautologically deducible from
earlier assumptions. The criterion of 'truth'
is, whether or not the proposition corresponds
to reality" (p. 9, his italics). (Compare
Wassily Leontief 1966, p. 23.)
One of our most distinguished historians
of economic thought, George Stigler, has
stated that: "The dominant influence upon
the working range of economic theorists is
the set of internal values and pressures of


### ---Economics-1976-0-04.txt---
the discipline. The subjects of study are
posed by the unfolding course of scientific
developments" (p. 22). He goes on to add:
"This is not to say that the environment is
without influence...... "But, he continues,
"whether a fact or development is significant
depends primarily on its relevance to
current economic theory" (p. 23). (Compare
Tjalling Koopmans, p. 170.) What a
curious relating of rigor to relevance!
Whether the real world matters depends
presumably on "its relevance to current
economic theory." Many if not most of today'
s economic theorists seem to agree
with this ordering of priorities.3
To what aspects of the observable world
does economics apply its analytical tools?
According to the familiar definition in the
International Encyclopedia of the Social
Sciences, which we owe originally to Lionel
Robbins, economics "is the study of the
allocation of scarce resources among unlimited
and competing uses." To this definition
of microeconomics the Encyclopedia
then rather weakly adds macroeconomics,
which is defined as the study of
money, the general price level, and the
level of output and employment.
Let us consider the microeconomic part
of this definition. Presumably the reference
is to real resources, used to produce observable
goods and services, that are exchanged
and consumed by real people living
in the kind of world we see around us.
Of course, some degree of abstraction is
necessary if useful generalizations are to be
reached. Here the economic theorist quickly
runs up against a dilemma. Shall he seek to
make his analysis ever more rigorous, regardless
of the possibly diminishing relevance
of his conclusions to the observed
world? Or shall he sacrifice elegant refinement
for somewhat cruder analysis that
may lead to testable results? I suppose the
reply can be given that we need to do both
and that we need specialists in each of
these approaches. But certainly those who
construct the analytical apparatus should
pay more attention than many of them
now do to the substantive problems with
which economists are presumably concerned.
4

In speaking of how well current microeconomic
theory combines rigor and relevance,
I should distinguish among the
different but interrelated problem areas
with which this part of economic theory
concerns itself. Micro theory addresses itself
primarily to three related topics: (1)
the conditions necessary for, and the means
of actually achieving, an optimum allocation
of resources within decision-making
units, both firms and households, under
given assumptions as to the criteria of optimization;
(2) again under given assumptions,
the conditions necessary for the
existence of a general (or partial) equilibrium
among all (or some) of these decisionmaking
units, including the determination
of the uniqueness and stability of such an
equilibrium; and (3) the conditions required
for the achievement of an economic
optimum from a broad, social point of
view.

Some success has been achieved in blending
rigor and relevance in the theory of the
single decision-making unit. Certainly, as
this year's Nobel prize in economics attests,
considerable progress has been achieved in
dealing with production planning in the
individual firm or establishment (or government
department) for which mathematical
tools, including programming and
the whole range of activity analysis, have
proved to be useful. To some extent, howI
This has led Leontief to expostulate: "Seldom, in
modern positive science, has so elaborate a theoretical
structure been erected on so narrow and shallow a
factual foundation" (1966, p. 33). Compare also Nicholas
Kaldor, p. 1240, and Martin Shubik (1970).
I For a careful and judicious defense of theoretical
models, see Koopman's second essay. He also adds:
"Precision and rigor in the statement of premises and
proofs can be expected to have a sobering effect on our
beliefs about the reach of the propositions we have developed"
(p. 147).


### ---Economics-1976-0-05.txt---
ever, the success achieved here makes this
part of economics resemble more a branch
of engineering than a social science. And at
the level of the individual firm some progress
continues to be made in empirical
studies of production and cost functions,
the determinants of the demand for inputs,
the transmission of technical change, and
related topics. I do not wish to minimize
the value of this work. But we should not
ignore the extent to which rigorous formulations
of the theory of the firm have had
to be relaxed in order to obtain useful results
in empirical work. Nor, I might add,
should we forget the extent to which conventional
theory ignores how and why

work is organized within the firm and
establishment in the way that it is, what
may be called the "social relations" of the
production process.

Some success in blending rigor and relevance
has also been achieved in the field of
household behavior-ranging from studies
of the determinants of consumers' demand
to recent work on human capital, the behavior
of labor markets, the economics of
crime, and the like. I must confess to some
skepticism, however, about the relevance
of the economic models of household behavior
recently developed by Gary Becker
and his followers-what has been referred
to as "the new home economics." Granted
that much useful work has been done in
this area. Nonetheless there is a lamentable
tendency among scholars in this field to
rely upon a caricature of human beings
who continuously and consciously balance
costs and benefits at the margin, whether
in deciding on another year of schooling,
whether and when to marry or be divorced,
how many children to have and when, or
whether and when to commit a crime. And
after a substantial amount of intensive research,
the human capital approach still
leaves unexplained a significant part of the
differences in personal incomes.
In the second area mentioned, particularly
general equilibrium theory, it seems
to me that relevance has been largely absent
in the recent literature. To find much
relevance at the theoretical level, and I
refer here only to the theoretical literature,
we must go back to the partial-equilibrium
analysis of Alfred Marshall and his followers.
Walras, Pareto, and their successors-
with their assumptions of atomistic
competition, perfectly flexible prices, costless
information, and limitless futures markets-
have contributed little to relevance
in their steadfast pursuit of rigor. Although
a step in the right direction, recent attempts
to develop a pure theory of disequilibrium
are subject to much the same

criticism.

Rigor and relevance have been successfully
blended in input-output analysis,
although largely at the expense of ignoring
the price sensitivity of input-output coefficients.
Here again, the emphasis is on
engineering-type relationships, but in this
case the entire economy is the object of
study.

Micro-economic analysis, as I have noted,
concerns itself with "the allocation of
scarce resources among unlimited and competing
uses." As economists we are concerned
with how resources may be allocated
efficiently, and we are prepared to
provide the layman and the policymaker
with a rigorous definition of efficiency. But
"efficient allocation" for whose benefit? To
me it has always been startling that the
accepted province of micro-economic theory
has little room for the personal distribution
of income-and virtually none

for the personal distribution of wealth. Of
course, we speak of "distribution," but by
this we mean either factor prices or total
factor shares.

Why wealth and labor services, the latter
carrying widely different market-determined
prices, are distributed among different
human beings in the way they actually
are is a question that the non-Marxian


### ---Economics-1976-0-06.txt---
economic theorist seldom asks. An exception
should be made for the relevant literature
in the field of labor economics. Similarly,
some exception should be made for
the recent literature on human capital, but
at best it is only a partial exception.5 The
mathematically inclined general theorist
continues to show little interest in the determinants
of the personal distribution of
income and wealth.6 It has been said by
George Stigler that: "The problem of personal
income distribution will eventually
receive much theoretical attention, since
it is a problem of all economies and all
times" (p. 22). (Compare Becker, p. 135.)
But if the problem is so important, why
only "eventually"? It is not surprising
that many younger economists are seeking
a radical alternative for the neoclassical
framework, more or less along Marxian
lines. And it is also this concern with the
unequal personal distribution of opportunity
and income which has led to the
development of models of a dual labor market.
These models, with their emphasis on
institutional barriers to labor mobility,
offer some valuable insights into the operation
of contemporary labor markets, and
they also raise important questions about
some of the assumptions implicit in neoclassical
theory.7

From the point of view of human welfare-
a concept that will not go away no
matter how uncomfortable it makes the
economic theorist-can we ignore the personal
distribution of income? Which is
more relevant: a rigorous demonstration as
to how resources can be most efficiently
allocated under ideal conditions that have
never existed, or a much cruder exploration
of how wealth and income came to be distributed
as they in fact are and what might
be done to affect the distribution of income
in one way or another? As Alice Rivlin put
it in her Ely lecture last year (p. 2), economists
"worth their pay" ought to be able
to explain the shape of the income distribution
and why it is changing or not changing.
By this criterion very few of us are
worth our pay.8 To go further, why do
we have so little to say about the intergenerational
movement among occupational

and income classes, about the determinants
of the distribution of income

relative to those affecting the distribution
of wealth, and about the ethnic, social, political,
and regional factors affecting the
distribution of both wealth and income?
Am I suggesting that economic theory
become much more normative than it now
is? Of course I am not. "Relevant" and
"normative" are not synonyms, and what
I am alleging here is that neoclassical economics
has failed to be relevant in its
refusal to deal with the personal distribution
of income and wealth. This refusal
stems, I presume, fron the fact that, with
the analytical tools at hand, the problem
has seemed too difficult.

I should add in this connection that neoclassical
economics has always had a normative
slant. As others have suggested,
I In the second edition of Human Capital, pp. 94-144,
Becker republishes his earlier attempt to utilize the
human capital approach to sketch out a theory of per-
sonal income distributioin. See also Jacob Mincer.
6 On the various approaches to the study of the distribution
of income, see the useful volumes by Martin
Bronfenbrenner and Jan Pen. Interestingly, some
three-quarters or more of Bronfenbrenner's book is de-
voted to the functional distribution of income; only one
chapter is devoted entirely to "Topics in Personal Income
Distribution." Yet one reviewer criticized him, as
well as Pen, for devoting too much space "to the distributional
influence of institutional forces and to the
personal, as against functional, distribution of income"
(Charles E. Ferguson, p. 440). Pen's volume contains
a useful summary of the more important recent literature
on the determinants of the personal distribution
of income; and, interestingly, his longest chapter is
concerned with "norms and policies" for income redistribution.


7 Compare Arthur Okun (1973, pp. 237-46) and
Michael Wachter and the references there cited. Cairnes
and Marshall, of course, did talk about noncompeting
groups, but little of this has carried over into the main
body of current economic theory.
8 One who has recently earned his salary is Okun
(1975); see also James E. Meade and Alan Blinder. An
earlier example of an economist who "bucked the
trend" to discuss the personal distribution of income and
wealth was Hugh Dalton.


### ---Economics-1976-0-07.txt---
conventional micro-economic theory as it
has developed, particularly in the last
seventy-five years or so, takes a normative
stance by default.9 Indeed, it takes a
normative stance by more than default. It
says outright that the primary question to
which economists should address themselves
is the "optimum" allocation of resources,
and it insists on providing a precise
definition of optimum. (Compare
Ward, pp. 52-54, 90.) Since, as Koopmans
points out, competitive equilibrium theory
ignores the welfare implications of the personal
distribution of income that results,
"the term 'optimum' [is] a misnomer"
(p. 49). (See also Ward, pp. 197-98.)
My remarks thus far have been directed
toward microeconomics. But macro-economic
analysis also must face the problem
of how optimally to combine rigor and
relevance. What we today call macroeconomics
grew out of the catastrophe of
the Great Depression,'0 and in the early
development of macro-economic analysis
relevance took precedence over rigor. Today,
rigor competes with relevance in
macro-economic and monetary theory, and
in some lines of development macro and
monetary theorists, like many of their colleagues
in micro theory, seem to consider
relevance to be more or less irrelevant. A
good example has been the elaboration of
so-called growth theory Ahich now seems
to be losing some of its earlier popularity.
Apparently, there has come to be a growing
recognition that the considerable efforts
spent on growth models have not significantly
advanced knowledge beyond the

contributions of the original Harrod-Domar
and the Solow-Swan neoclassical models.
And other intriguing topics have arisen to
stimulate the pursuit of rigor at the expense
of relevance.

In both micro- and macroeconomics, efforts
are sometimes made to extract a drop
or two of relevance from exercises in analytical
rigor; and conclusions are drawn
about the functioning of some aspect of
the real world, or policy recommendations
are made, on the basis of theoretical exercises
which rest on assumptions that fly in
the face of the facts. One example is the
frequent fitting of the neoclassical investment
function, with its built-in assumptions
of constant returns and perfect competition,
to industries in which these assumptions
obviously do not hold. Another
example is a good deal of the recent literature
seeking to reformulate the microeconomic
foundations of macro-economic

theory and policy-what Edmund Phelps
(1969, 1972) has termed "The New Microeconomics
in Inflation and Employment

Theory." The theoretical analysis in much
of this literature rests on assumptions that
also fly in the face of the facts. To cite a
few examples: all unemployment is a voluntary
activity as part of a search procedure
in which workers are continuously
equating costs and prospective benefits at
the margin; the labor supply is typically
taken to be homogeneous with perfect mobility
among labor submarkets; so-called
structural unemployment is ignored as are
the striking differences in unemployment
rates among different age, sex, ethnic, and
occupational groups; and downward wage
flexibility is generally assumed, although
some recent attempts have been made to
relax this assumption. Another related recent
development in which theory proceeds
with impeccable logic from unrealistic
assumptions to conclusions that contradict
the historical record, is the recent
work on rational expectations. (Compare
Robert J. Gordon.) And, as a final illustration
I might cite much of the recent literature
on capital theory.

II

I turn now from the first part of my title
to the second from rigor and relevance to


### ---Economics-1976-0-08.txt---
the fact that we live in a world that is continually
changing. And here I want to pose
two questions: First, to what extent does
the changing institutional environment affect
the relevance of the analytical tools
that we use and the assumptions that we
make about the determinants of individual
and group behavior? (Here, of course, I am
still raising the question of relevance.) And,
second, why do we ask so few questions
about why and how the institutional environment
has changed in the way that it
has, and what are its internal dynamics
that will lead it to change in particular
ways in the future-not only in the United
States but in other countries? A few economists
have addressed themselves to this
range of questions, notably Karl Marx and
Joseph Schumpeter. Among living economists,
three who at least raise this range
of issues are John Kenneth Galbraith,
Gunnar Myrdal, and, in the Marxist
tradition, Paul Sweezy."1

Certainly the outstanding example of
the failure of economic theory to adapt its
analytical tools to the changing institutional
environment must be the stubborn
adherence to the assumption of perfect
competition, a concept which has been described
as being "as pervasive and fundamental
as any in the whole structure of
classical and neoclassical economic theory"
(Stigler, p. 234).12 Indeed for a century or
so, economists have toiled to make more
precise the notion of a perfectly competitive
market. Over this same period, of
course, the character of actual markets has
been changing in many ways. While improvements
in transportation and communication
have tended to promote competition
in expanding markets, the growth
of large firms and the spread of industrial
concentration have made oligopoly a much
more relevant model for industrial markets
than the perfectly competitive model
which today's theorists insist on using. It
is true that sporadic efforts have been
made to develop a theory of oligopoly, and
for a while high hopes were held for what
might be learned from game theory; but
no generally accepted theory of oligopoly
has yet emerged. At the same time, the
emphasis on general equilibrium theory
has tended to turn attention away from
this egregious departure from perfect competition.
13

It is true, of course, that in the 1930's,
under the stimulus of the pioneering works
by Edward Chamberlin and Joan Robinson,
we developed a theory of monopolistic
or imperfect competition, centering on the
notion of product differentiation. And at
the same time increased attention began
to be paid to the determinants of oligopolistic
behavior. At the applied level, the
field of industrial organization was born.
But while this applied field has continued
to thrive, general micro-economic theory
and the applied work in this area have
largely parted company. General equilibrium
theory (and not only this branch of
theory) has returned to the assumption of
perfect competition. The notion of a sloping
demand curve for the individual firm
seemingly did not add much to the general
theorist's tool box, and the new mathematical
economics found it more exciting
to pick up the challenge of Walras and
Pareto and to turn to general equilibrium
analysis and to setting forth the conditions
of Pareto optimality. And for this the assumption
of perfect competition was convenient
if not essential. As William Baumol
11 There are, of course, a fair number of economists
who consider themselves to be in the American institutionalist
tradition. They are likely to belong to the
Association for Evolutionary Economics, which has its
own journal, but thus far they have had little influence
on the main trends in theoretical and applied work in
economics.

12 See also Schumpeter (1954, pp. 972 ff).
13 See, for example, the papers by Paul Joskow and
Martin Shubik in last year's Papers and Proceedings of
the American Economic Association. See also Shubik
(1970, p. 415), who bluntly declares: "There is no
oligopoly theory."


### ---Economics-1976-0-09.txt---
has noted, "The case of product differentiation
has proved particularly resistant to
general equilibrium analysis" (p. 45).14
And so, as power blocs multiplied in a
pluralistic world, as firms grew larger and
as conglomerates were added to vertical
and horizontal combinations, as advertising
expenditures mounted to influence
spending out of rising discretionary income,
as the problem of externalities became
ever more important, and as the role
of government in the functioning of markets
steadily increased, micro-economic
theory largely averted its eyes and became
ever more enamored of hypothetical systems
of general equilibrium under conditions
of perfect competition.'5

In the meantime, instructors in undergraduate
micro theory have resolutely continued
to teach their students the essentials
of the Chamberlinian partial equilibrium
analysis, with downward sloping demand
curves facing the individual firm,
and laying down the conditions for shortrun
and long-run equilibrium on an industry
basis. That much, at least, undergraduates
seem able to absorb, and some

factual counterparts can be found for the
theoretical analysis. But at the graduate
level in many universities, and the more so
the more advanced the level of instruction,
Walras and Pareto and their successors
take over, and the analysis proceeds on the
basis of the conditions associated with atomistic
competition.

A further example, reflecting another dimension
of the growth of large-scale business,
involves the conditions under which
decisions are made in the large firm.'6 Involved
here are a number of issues which
have been the subject of some theoretical
analysis and a good deal of empirical work
in the postwar years, but very little of the
results of this work has yet found its way
into main corpus of micro-economic theory.
I have in mind such questions as the relative
roles of profit maximizing and of other
criteria in the decision rules used in the
large firm, the effects of the separation of
ownership and management on these optimization
criteria, the ways in which the
bureaucratization of decision making affects
the manner in which business firms
respond to market stimuli, and the effect
of the improvements in the gathering and
processing of information that have come
not only from the revolution in data
processing but also from the development
of a more scientific approach to decision
making.'7 What has been the effect of all
this on the pricing decisions of large firms,
the manner in which they participate in
wage negotiations, or how their investment
planning responds to current and prospective
developments in product and financial
markets? And there are other examples
of the same sort. Early work not only in


### ---Economics-1976-0-10.txt---
economics but also in the other social
sciences led to the development of what is
now called "organization theory," and this
is now a field in the better business schools;
but not much of this has seeped into the
mainstream of economic theory.
Some economic theorists today are beginning
to pay serious attention to the beginnings
of a theory of information, and
increasing attention is being paid to problems
of decision making under uncertainty.
But here again too little attention tends
to be paid to the changing conditions under
which information is collected and processed,
and the manner in which institutional
arrangements affect the way in
which the future is viewed and attitudes
toward uncertainty change.

There is another and indeed startling
respect in which pure micro-economic theory
tends to ignore the changing institutional
environment. This has to do with
the steadily increasing role of government
in the functioning of markets. In neoclassical
general equilibrium theory, there
is no place for any kind of public authority.
Of course, we have the field of public
finance, which on the theoretical side and
at the micro level does borrow from neoclassical
theory; and, as Walter Heller reminded
us last year, cost-benefit analysis
and tax-incidence theory have helped
economists to develop a rationale for many
types of government decision making.
There is also a growing literature on the
economics of government decision making
in a range of problem areas-from pollution
to military spending to housing and
education. But the pure micro theorist
finds no role for a public authority in his
analysis of the determinants of general
equilibrium. (See Bent Hansen, p. 92.)
Here we encounter an important difference
between micro and macro theory.
Much of macro theory (but by no means
all) tends to be policy oriented and to have
a strong normative orientation. It is concerned
both with policy variables and with
variables that can be, directly or indirectly,
influenced by policy. While there is a good
deal of macro theory in which the possible
role of government is ignored, much of
theoretical macroeconomics does, either
explicitly or by implication, leave an important
role for government.

Although macro-economic theory does
not ignore the role of government, we can
still find plenty of examples of the failure
of macro theory to reflect the changing institutional
environment. One of the most

striking examples undoubtedly involves
what little we have in the way of a theory
of inflation. The problem is most acute
with respect to accounting for inflationary
trends since World War II.

A number of explanations are currently
circulating regarding the tendency toward
accelerating inflation in the last decade or
more. The purely monetarist one is the
simplest and makes no reference to a
changing institutional structure. It tends
to ignore the sources and nature of the
pressures, operating through government,
which lead to changes in the supply of
money. An increasing number of economists,
monetarists and Keynesians, emphasize
the existence of a natural rate of
unemployment, with the implication that
government policies to expand aggregate
demand, by pushing unemployment to or
below the natural rate, lead to accelerating
inflation. But advocates of the natural rate
hypothesis have little to say about why the
natural rate, if it exists, is what it is today,
and how and why it has changed over the
years. Much of the work in this area tends
to be done in an institutional vacuum, including
the recent work on the formation
of price expectations.

I think it is fair to say that we still lack
a general theory with a significant time
dimension of the nature of the inflationary
process and how this process is affected by
the changing institutional setting. Gunnar


### ---Economics-1976-0-11.txt---
Myrdal is one of those to emphasize that
"in modern society the tendency of inflation
to become cumulative and to accelerate
is rooted in a wide and complex
institutional reality" (p. 24), and to urge
a broader, institutionally oriented analysis
of underlying causes and possible remedies.
Another is Peter Wiles in a provocative
recent essay in the Economic Journal.
Among these institutional changes making
for more rapid and apparently accelerating
inflation are the postwar commitment in
all advanced countries to a high level of
employment (stronger in Europe than in
the United States), the reluctance fully to
cover increasing public expenditures by
taxation, the growing tendency to link
wage and other payments to the consumer
price index, the increasing aggressiveness
of trade unions (emphasized by Wiles)
and other organized groups of income receivers,
an apparent weakening of the

willingness of employers to resist wage demands
(which is closely related to the government'
s commitment to high employment)
, and the intensification of inflationary
expectations engendered by past
inflation. (See Myrdal, pp. 23 ff.) To all of
which we can add an international monetary
system that permitted huge dollar
outflows which became expanded monetary
reserves for other countries. This is a
familiar list. But how do these different
but related inflationary pressures interact?
Why have they become stronger? How do
we account for the particular rate of acceleration
that has occurred? Why does

the rate of acceleration seem to be different,
and because of what differences in the
institutional environment, in different
countries? And how have the significant
changes in the international system of
trade and finance affected these inflationary
trends? (See R. J. Gordon.)

I shall merely mention one other example
of the failure of economists today to take
adequate account in setting up their
models of the changing institutional environment.
This has to do with the determinants
of household behavior, with

respect to both the demand for goods and
services and the supply of labor services.
What has been the effect on household behavior,
for example, of advancing levels of
education, changes in the ways that news
is disseminated, recent trends toward urbanization
and suburbanization, or the

massive change in the impact of advertising
made possible by television?'8 To what
extent have these and other developments
made households behave more or less in
the way that economic theory assumes?
What bearing do these and other institutional
developments have on the behavior
of personal saving in recent years, in this
and other countries, and on the way that
households respond to recent and prospective
inflationary trends?

III

I turn now to my second question about
the changing institutional environment.
Why does the central body of economic
analysis show so little interest in why and
how the institutional setting for economic
behavior has changed in the past and is
likely to change in the future? The past, of
course, is the domain of the historian, but
I am not aware that the vast amount of
historical research in the past century or
more has yet given us an acceptable model
of socioeconomic development in today's
advanced economies. As I put it more than
a decade ago, "Contemporary economics
does not yet have the tools required for a
comprehensive and evolutionary theory of
economic behavior that would take appropriate
account of the main lines of institutional
change" (p. 146).19 Schumpeter
(1947) came closer in this century to providing


### ---Economics-1976-0-12.txt---
such a theory than has any other
economist in the more-or-less orthodox
tradition. Outside that tradition, we can
turn to the Marxian literature.
Here, of course, I am harking back to a
major plea of the early institutionalists.
Veblen asked long ago, in the title of one
of his papers, "Why Is Economics Not
an Evolutionary Science?," and Wesley
Mitchell urged the need for a comprehensive
theory of economic behavior that
takes the cumulative change of institutions
as its chief concern. The institutionalists
themselves did not develop such a comprehensive
and evolutionary theory. While
the classical writers did have the elements
of a dynamic system-what Baumol
once referred to as the "magnificent
dynamics" of the classical school-they
theorized on the assumption of a fixed
set of social and economic institutions. As
Schumpeter put it, "the classics reasoned
in terms of a particular historical situation
which they uncritically idealized and from
which they uncritically generalized" (1947,
p. 75).

As for contemporary economists in the
neoclassical tradition, they, like their predecessors,
seem to be afraid to ask the

really big questions about the economic
aspects of society questions which, because
they are big, must be concerned with
the changing institutional fabric. Some exception
to this generalization should be
made for the considerable effort that has
gone into the study of the underdeveloped
parts of the world. Here economists have
not been able to ignore the interaction of
the institutional environment and economic
behavior, and increasing attention
has come to be paid to the conditions necessary
for one or another kind of change
in that environment. I might add here that
I continue to be impressed by the fact that
in general economists in the advanced
countries seem to be prepared to be more
institutional in dealing with other parts of
the world than they are in studying the
particular societies in which they live and
do most of their work.20

Might the following conceptual framework
provide a basis for a more systematic
study of the dynamic interaction of economic
behavior and the institutional
framework? At the most basic level, a society
is composed of individual human
beings. These individuals are members of
households. The larger number of them
sell the factor services they control to producing
units ("firms" for short); and those
who sell labor services must physically
participate in the production process. A
flow of newly produced goods and services
results. The distribution of these goods and
services among potential claimants depends
on much more than the operation
of "impersonal market forces." It reflects a
complex of institutional arrangements,
which include, among other things, the distribution
of power among different groups
to influence particular commodity and factor
markets, both directly and through
government, how the ownership of wealth
is distributed and for whose benefit it is
used, the tax structure and network of
government regulations that emerge from
the political process, and the total and distribution
of net claims by the rest of the
world against domestic output.
Individuals not only are members of
households and suppliers of input services
to producing units (which may be governmental
as well as private), but they are also
part of a political process which, while
20 Lance Davis and Douglass North have recently set
forth a tentative and limited "theory of institutional
change" and have applied the resulting modcl to show
how a range of economic needs and opportunities have
led to new institutional arrangements in the American
economy. They are frank in setting forth some of the
limitations of their model, and Lhere is a good deal in
their approach with which I should quarrel; but their
work is clearly a step in the right direction. Incidentally,
this particular approach to economic history tends to
run counter to the recent quantitative emphasis in that
field. Clearly we need to have both types of historical
work.


### ---Economics-1976-0-13.txt---
partly local and regional, culminates in the
powers of a national government.
Thus we begin with the households and
producing units of conventional economic
theory but immediately add government
as a third basic unit. Households, firms,
and government interact within a set of
evolving economic institutions. These economic
institutions include a hierarchy of
markets for current output and for the
services of labor, capital, and natural resources,
and an array of supporting institutions-
from commercial banks to labor
mediators to government agencies-that
also make a contribution to total output.
Households and firms interact not only in
response to the standard market stimuli
but also by organizing pressure groups to
influence both government and particular
markets.

These pressure groups-not only labor
unions and trade associations but a wide
variety of other formal and informal
groups-operate within and are conditioned
by an evolving set of political and
legal institutions that lay down the ground
rules as to the way households, firms, and
government interact with each other. A
major aim of the pressure groups is to influence
government and markets not only
directly but also by altering legal and political
institutional arrangements. And as a
result of these pressures, the conditions
under which households and firms carry
out their economic functions change with
the passage of time-as do the ways in
which households and firms, through a
variety of forms of organization, seek to
change these conditions still further in
favor of their particular interests.21
We may speak of households, firms, and
government as the primary economic
agents which carry on their activities within
the framework of a set of evolving economic
institutions. But these agents and
economic institutions also interact with an
external environment which can be classified
in a variety of ways. One simple classification
might be: (1) the framework of
legal and political institutions, to which I
have already referred; (2) the complex of
social institutions that make up what may
loosely be referred to as the social environment;
(3) the evolving body of scientific
and technical knowledge (and the institutions
through which such knowledge is developed
and transmitted); (4) the physical
environment; and (5) the complex of political
and economic arrangements that tie
a nation to the rest of the world.
Against this background, let us now
come back to my final question about the
evolving institutional environment, which
bluntly put was: How did we get to where
we are, and where are we going? Or, if you
will, what is the future of capitalism and
of the kind of market economy to which we
are accustomed and which is changing before
our eyes? Lacking a dynamic, politicoeconomic,
and institutionally oriented

miodel, the neoclassical economist averts
his eyes-or possibly, and reluctantly, refers
the questioner to the still growing
Marxist literature. Has not the time come
for "orthodox" economists-both defenders
and critics of neoclassical theory-to repair
this glaring deficiency? Let us borrow
what seems appropriate from Marx and
his followers as well as from others, although
what Marx had to say fitted nineteenth
century England much better than
it does late twentieth century Western
Europe or the United States. But at least
let us try to construct a model of the sort
I suggest that will have something to say
about the evolution and future of the kind
of economy and society in which we live.


### ---Economics-1976-0-14.txt---
Sketchy as it is, and it is certainly
sketchy, I think my suggested conceptual
framework or something similar offers a
possible starting point. And there are
many intermediate questions to be raised
along the way. We have been witnessing a
significant extension of government control
of the market mechanism in all of the advanced
economies, more so in some than in
others. This intervention ranges from conventional
forms of regulation of particular
industries, to sporadic attempts to impose
one or another kind of incomes policy, to
large-scale programs to redistribute incomes,
to widening experiments in worker
participation, to outright nationalization
of particular firms or industries. What
combinations of pressures have caused this
extension of government intervention;
what forces will extend it further; what
forms will such intervention take; and
what are likely to be the effects on the
allocation of resources, the distribution of
income and wealth, and the rates of inflation
and of growth in total output-not to
mention the possible effects on the various
dimensions of the institutional environment,
including the institution of private
property?

There are many other elements, besides
the few mentioned here, that would have
to be incorporated into a full-fledged, institutionally
oriented theory of economic

development for the advanced economies
of the nonsocialist world. And we need
also to fit in the underdeveloped countries.
Here clearly we have to be political economists,
and not just economists in the neoclassical
sense. We are currently witnessing
powerful political forces at work aimed at
improving the terms of trade of the third
world with the advanced countries. What
will determine the eventual outcome? Mention
of the third world raises a host of other
issues, political as well as economic. These
include the seemingly inexorable advance
of socialism in many of these countries ;22
the effect of autocratic forms of government
on the pace of economic growth, the
allocation of resources, and the distribution
of income; and the ability of these
countries to deal with the population
growth resulting from high birth rates and
declining death rates.

And finally, to repeat, there is for economists
the basic question to which not only
Marx and his followers but also Schumpeter
addressed themselves: What is the future
of capitalism in the advanced economies,
given the growing size and bureaucratization
of business firms, the increasing
strength of organized pressure groups, and
the momentum from the increasing government
intervention that has already occurred?
It seems to me that capitalism as
we know it in this coulntry or even in
Western Europe has little future in the
third world. What is its future in the advanced
economies?

IV

And so, on this somber note, I end. I
have scolded economists for what I think
are the sins that too many of them commit,
and I have tried to point the way to at
least partial redemption. This road to
salvation will not be an easy one for those
who have been seduced by the siren of
mathematical elegance or those who all too
often seek to test unrealistic models without
much regard for the quality or relevance
of the data they feed into their equations.
But let us all continue to worship at
the altar of science. I ask only that our
credo be: "relevance with as much rigor as
possible," and not "rigor regardless of
relevance." And let us not be afraid to ask
-and to try to answer the really big
questions.

22 Which leads at least this observer to wonder how
compatible are democracy and capitalism in an underdeveloped
country whose aspirations are rising much
faster than its means to satisfy them.


### ---Economics-1976-0-15.txt---

 ## Economics-1977-0


### ---Economics-1977-0-03.txt---
In recent years and especially since the onset
of the current depression, the economics profession
and the lay public have heard a great deal
about the sharp conflict between "monetarists
and Keynesians" or between "monetarists and
fiscalists.". The difference between the two
"schools" is generally held to center on whether
the money supply or fiscal variables are the
major determinants of aggregate economic activity,
and hence the most appropriate tool of
stabilization policies.

My central theme is that this view is quite far
from the truth, and that the issues involved are of
far greater practical import. There are in reality
no serious analytical disagreements between
leading monetarists and leading nonmonetarists.
Milton Friedman was once quoted as saying,
"We are all Keynesians, now," and I am quite
prepared to reciprocate that "we are all monetarists'
'-if by morietarism is meant assigning to
the stock of money a major role in determining
output and prices. Indeed, the list of those who
have long been monetarists in this sense is quite
extensive, including among other John Maynard
Keynes as well as myself, as is attested by
my 1944 and 1963 articles.

In reality the distinguishing feature of the
monetarist school and the real issues of disagreement
with nonmonetarists is not monetarism, but
rather the role that should probably be assigned
to stabilization policies. Nonmonetarists accept
what I regard to be the fundamental practical
message of The General Theory: that a private
enterprise economy using an intangible money
needs to be stabilized, can be stabilized, and
therefore should be stabilized by appropriate
monetary and fiscal policies. Monetarists by
contrast take the view that there is no serious
need to stabilize the economy; that even if there
were a need, it could not be done, for stabilization
policies would be more likely to increase
than to decrease instability; and, at least some
monetarists would, I believe, go so far as to hold
that, even in the unlikely event that stabilization
policies could on balance prove beneficial, the
government should not be trusted with the necessary
power.

What has led me to address this controversy
is the recent spread of monetarism, both in a
simplistic, superficial form and in the form of
growing influence on the practical conduct of
economic policy, which influence, I shall argue
presently, has played at least some role in the
economic upheavals of the last three years.
In what follows then, I propose first to review
the main arguments bearing on the need for
stabilization policies, that is, on the likely extent
of instability in the absence of such policies, and
then to examine the issue of the supposed destabilizing
effect of pursuing stabilization policies.
My main concern will be with instability generated
by the traditional type of disturbances-demand
shocks. But before I am through, I will
give some consideration to the difficult problems
raised by the newer type of disturbance-supply
shocks.

I. The Keynesian Case for Stabilization Policies
A. The General Theory

Keynes' novel conclusion about the need for


### ---Economics-1977-0-04.txt---
stabilization policies, as was brought out by the
early interpreters of The General Theory (for
example, John Hicks, the author, 1944), resulted
from the interaction of a basic contribution
to traditional monetary theory-liquidity
preference-and an unorthodox hypothesis
about the working of the labor market-complete
downward rigidity of wages.

Because of liquidity preference, a change in
aggregate demand, which may be broadly defined
as any event that results in a change in the
market clearing or equilibrium rate of interest,
will produce a corresponding change in the real
demand for money or velocity of circulation, and
hence in the real stock of money needed at full
employment. As long as wages are perfectly
flexible, even with a constant nominal supply,
full employment could and would be maintained
by a change of wages and prices as needed to
produce the required change in the real money
supply-though even in this case, stability of the
price level would require a countercyclical monetary
policy. But, under the Keynesian wage
assumption the classical adjustment through
prices can occur only in the case of an increased
demand. In the case of a decline, instead, wage
rigidity prevents the necessary increase in the
real money supply and the concomitant required
fall in interest rates. Hence, if the nominal
money supply is constant, the initial equilibrium
must give way to a new stable one, characterized
by lower output and by an involuntary reduction
in employment, so labeled because it does not
result from a shift in notional demand and supply
schedules in terms of real wages, but only from
an insufficient real money supply. The nature of
this equilibrium is elegantly captured by the
Hicksian IS-LM paradigm, which to our generation
of economists has become almost as familiar
as the demand-supply paradigm was to
earlier ones.

This analysis implied that a fixed money supply
far from insuring approximate stability of
prices and output, as held by the traditional
view, would result in a rather unstable economy,
alternating between periods of protracted unemployment
and stagnation, and bursts of inflation.
The extent of downward instability would depend
in part on the size of the exogenous shocks
to demand and in part on the strength of what
may be called the Hicksian mechanism. By this
I mean the extent to which a shift in IS, through
its interaction with LM, results in some decline
in interest rates and thus in a change in income
which is smaller than the original shift. The stabilizing
power of this mechanism is controlled
by various parameters of the system. In particular,
the economy will be more unstable
the greater the interest elasticity of demand for
money, and the smaller the interest responsiveness
of aggregate demand. Finally, a large
multiplier is also destabilizing in that it implies
a larger shift in IS for a given shock.
However, the instability could be readily
counteracted by appropriate stabilization policies.
Monetary policy could change the nominal
supply of money so as to accommodate the
change in real demand resulting from shocks in
aggregate demand. Fiscal policy, through expenditure
and taxes, could offset these shocks,
making full employment consistent with the
initial nominal money stock. In general, both
monetary and fiscal policies could be used in
combination. But because of a perceived uncertainty
in the response of demand to changes in
interest rates, and because changes in interest
rates through monetary policy could meet difficulties
and substantial delays related to expectations
(so-called liquidity traps), fiscal policy
was regarded as having some advantages.
B. The Early Keynesians

The early disciples of the new Keynesian
gospel, still haunted by memories of the Great
Depression, frequently tended to outdo Keynes'
pessimism about potential instability. Concern
with liquidity traps fostered the view that the demand
for money was highly interest elastic;
failure to distinguish between the short- and
long-run marginal propensity to save led to overestimating
the long-run saving rate, thereby
fostering concern with stagnation, and to underestimating
the short-run propensity, thereby
exaggerating the short-run multiplier. Interest


### ---Economics-1977-0-05.txt---
rates were supposed to affect, at best, the demand
for long-lived fixed investments, and the
interest elasticity was deemed to be low. Thus,
shocks were believed to produce a large response.
Finally, investment demand was seen as
capriciously controlled by "animal spirits,"
thus providing an important source of shocks.
All this justified calling for very active stabilization
policies. Furthermore, since the very
circumstances which produce a large response to
demand shocks also produce a large response to
fiscal and a small response to monetary actions,
there was a tendency to focus on fiscal policy as
the main tool to keep the economy at near full
employment.

C. The Phillips Curve

In the two decades following The General
Theory, there were a number of developments of
the Keynesian system including dynamization of
the model, the stress on taxes versus expenditures
and the balanced budget multiplier, and the
first attempts at estimating the critical parameters
through econometric techniques and models.
But for present purposes, the most important
one was the uncovering of a "stable" statistical
relation between the rate of change of wages and
the rate of unemployment, which has since come
to be known as the Phillips curve. This relation,
and its generalization by Richard Lipsey to allow
for the effect of recent inflation, won wide acceptance
even before an analytical underpinning
could be provided for it, in part because it could
account for the "puzzling" experience of 1954
and 1958, when wages kept rising despite the
substantial rise in unemployment. It also served
to dispose of the rather sterile "cost push"-
"demand pull" controversy.

In the following years, a good deal of attention
went into developing theoretical foundations for
the Phillips curve, in particular along the lines of
search models (for example, Edmund Phelps et
al.). This approach served to shed a new light on
the nature of unemployment by tracing it in the
first place to labor turnover and search time
rather than to lack of jobs as such: in a sense
unemployment is all frictional-at least in developed
countries. At the same time it clarified
how the availability of more jobs tends to reduce
unemployment by increasing vacancies and thus
reducing search time.

Acceptance of the Phillips curve relation
implied some significant changes in the Keynesian
framework which partly escaped notice until
the subsequent monetarists' attacks. Since the
rate of change of wages decreased smoothly with
the rate of unemployment, there was no longer a
unique Full Employment but rather a whole
family of possible equilibrium rates, each associated
with a different rate of inflation (and requiring,
presumably, a different long-run
growth of money). It also impaired the notion of
a stable underemployment equilibrium. A fall in
demand could still cause an initial rise in unemployment
but this rise, by reducing the growth of
wages, would eventually raise the real money
supply, tending to return unemployment to the
equilibrium rate consistent with the given longrun
growth of money.

But at the practical level it did not lessen the
case for counteracting lasting demand disturbances
through stabilization policies rather than
by relying on the slow process of wage adjustment
to do the job, at the cost of protracted unemployment
and instability of prices. Indeed,
the realm of stabilization policies appeared to expand
in the sense that the stabilization authority
had the power of choosing the unemployment
rate around which employment was to be stabilized,
though it then had to accept the associated
inflation. Finally, the dependence of wage
changes also on past inflation forced recognition
of a distinction between the short- and the longrun
Phillips curve, the latter exhibiting the longrun
equilibrium rate of inflation implied by a
maintained unemployment rate. The fact that the
long-run tradeoff between unemployment and
inflation was necessarily less favorable than the
short-run one, opened up new vistas of "enjoyit-
now, pay-later" policies, and even resulted in
an entertaining literature on the political business
cycle and how to stay in the saddle by riding
the Phillips curve (see for example, Ray Fair,
William Nordhaus).


### ---Economics-1977-0-06.txt---
II. The Monetarists' Attack

A. The Stabilizing Power of the
Hicksian Mechanism

The monetarists' attack on Keynesianism was
directed from the very beginning not at the
Keynesian framework as such, but at whether it
really implied a need for stabilization. It rested
on a radically different empirical assessment of
the value of the parameters controlling the stabilizing
power of the Hicksian mechanism and of
the magnitude and duration of response to
shocks, given a stable money supply. And this
different assessment in turn was felt to justify a
radical downgrading of the practical relevance
of the Keynesian framework as distinguished
from its analytical validity.

Liquidity preference was a fine contribution
to monetary theory but in practice the responsiveness
of the demand for money, and hence of
velocity, to interest rates, far from being unmanageably
large, was so small that according to a
well-known paper by Milton Friedman (1969),
it could not even be detected empirically. On the
other hand, the effect of interest rates on aggregate
demand was large and by no means limited
to the traditional fixed investments but quite
pervasive. The difficulty of detecting it empirically
resulted from focusing on a narrow range
of measured market rates and from the fact that
while the aggregate could be counted on to respond,
the response of individual components
might not be stable. Finally, Friedman's celebrated
contribution to the theory of the consumption
function (1957) (and my own work on the
life cycle hypothesis with Richard Brumberg and
others, reviewed by the author, 1975) implied a
very high short-run marginal propensity to save
in response to transient disturbances to income
and hence a small short-run multiplier.
All this justified the conclusion that (i) though
demand shocks might qualitatively work along
the lines described by Keynes, quantitatively the
Hicks mechanism is so strong that their impact
would be small and transient, provided the
stock of money was kept on a steady growth
path; (ii) fiscal policy actions, like other demand
shocks, would have minor and transitory effects
on demand, while changes in money would
produce large and permanent effects on money
income; and, therefore, (iii) the observed instability
of the economy, which was anyway
proving moderate as the postwar period unfolded,
was most likely the result of the unstable
growth of money, be it due to misguided endeavors
to stabilize income or to the pursuit of
other targets, which were either irrelevant or, in
the case of balance of payments goals, should
have been made irrelevant by abandoning fixed
exchanges.

B. The Demise of Wage Rigidity and the
Vertical Phillips Curve

But the most serious challenge came in
Friedman's 1968 Presidential Address, building
on ideas independently put forth also by Phelps
(1968). Its basic message was that, despite appearances,
wages were in reality perfectly flexible
and there was accordingly no involuntary
unemployment. The evidence to the contrary,
including the Phillips curve, was but a statistical
illusion resulting from failure to differentiate between
price changes and unexpected price
changes.

Friedman starts out by reviving the Keynesian
notion that, at any point of time, there exists a
unique full-employment rate which he labels the
"'natural rate." An unanticipated fall in demand
in Friedman's competitive world leads firms to
reduce prices and also output and employment
along the short-run marginal cost curve unless
the nominal wage declines together with prices.
But workers, failing to judge correctly the current
and prospective fall in prices, misinterpret
the reduction of nominal wages as a cut in real
wages. Hence, assuming a positively sloped
supply function, they reduce the supply of labor.
As a result, the effective real wage rises to the
point where the resulting decline in the demand
for labor matches the reduced supply. Thus, output
falls not because of the decline in demand,
but because of the entirely voluntary reduction in
the supply of labor, in response to erroneous
perceptions. Furthermore, the fall in employment


### ---Economics-1977-0-07.txt---
can only be temporary, as expectations
must soon catch up with the facts, at least in the
absence of new shocks. The very same mechanism
works in the case of an increase in demand,
so that the responsiveness of wages and
prices is the same on either side of the natural
rate.

The upshot is that Friedman's model also
implies a Phillips-type relation between inflation,
employment or unemployment, and past
inflation,-provided the latter variable is interpreted
as a reasonable proxy for expected inflation.
But it turns the standard explanation on its
head: instead of (excess) employment causing
inflation, it is (the unexpected component of)
the rate of inflation that causes excess
employment.

One very basic implication of Friedman's
model is that the coefficient of price expectations
should be precisely unity. This specification
implies that whatever the shape of the short-run
Phillips curve a shape determined by the relation
between expected and actual price changes,
and by the elasticity of labor supply with respect
to the perceived real wage the long-run curve
must be vertical.

Friedman's novel twist provided a fresh prop
for the claim that stabilization policies are not
really needed, for, with wages flexible, except
possibly for transient distortions, the Hicksian
mechanism receives powerful reinforcement
from changes in the real money supply. Similarly,
the fact that full employment was a razor
edge provided new support for the claim that
stabilization policies were bound to prove destabilizing.


C. The Macro Rational Expectations Revolution
But the death blow to the already badly battered
Keynesian position was to come only
shortly thereafter by incorporating into Friedman'
s model the so-called rational expectation
hypothesis, or REH. Put very roughly, this hypothesis,
originally due to John Muth, states that
rational economic agents will endeavor to form
expectations of relevant future variables by
making the most efficient use of all information
provided by past history. It is a fundamental and
fruitful contribution that has already found many
important applications, for example, in connection
with speculative markets, and as a basis for
some thoughtful criticism by Robert Lucas
( 1976) of certain features of econometric models.
What I am concerned with here is only its
application to macro-economics, or MREH,
associated with such authors as Lucas (1972),
Thomas Sargent (1976), and Sargent and Neil
Wallace (1976).

The basic ingredient of MREH is the postulate
that the workers of Friedman's model hold rational
expectations, which turns out to have a number
of remarkable implications: (i) errors of
price expectations, which are the only source of
departure from the natural state, cannot be
avoided but they can only be short-lived and
random. In particular, there cannot be persistent
unemployment above the natural rate for this
would imply high serial correlation between the
successive errors of expectation, which is inconsistent
with rational expectations; (ii) any attempts
to stabilize the economy by means of
stated monetary or fiscal rules are bound to be
totally ineffective because their effect will be
fully discounted in rational expectations; (iii)
nor can the government successfully pursue ad
hoc measures to offset shocks. The private sector
is already taking care of any anticipated
shock; therefore government policy could conceivably
help only if the government information
was better than that of the public, which is
impossible, by the very definition of rational
expectations. Under these conditions, ad hoc
stabilization policies are most likely to produce
instead further destabilizing shocks.
These are clearly remarkable conclusions, and
a major rediscovery for it had all been said 40
years ago by Keynes in a well-known passage of
The General Theory:

If, indeed, labour were always in a position
to take action (and were to do so),
whenever there was less than full employment,
to reduce its money demands by
concerted action to whatever point was
required to make money so abundant relatively


### ---Economics-1977-0-08.txt---
to the wage-unit that the rate of
interest would fall to a level compatible
with full employment, we should, in effect,
have monetary management by the
Trade Unions, aimed at full employment,
instead of by the banking systems.
[p. 2671

The only novelty is that MREH replaces Keynes'
opening "if" with a "since."

If one accepts this little amendment, the case
against stabilization policies is complete. The
economy is inherently pretty stable-except
possibly for the effect of government messing
around. And to the extent that there is a small
residual instability, it is beyond the power of
human beings, let alone the government, to
alleviate it.

III. How Valid Is the Monetarist Case?
A. The Monetarist Model of Wage
Price Behavior

In setting out the counterattack it is convenient
to start with the monetarists' model of price and
wage behavior. Here one must distinguish between
the model as such and a specific implication
of that model, namely that the long-run
Phillips curve is vertical, or, in substance, that,
in the long run, money is neutral. That conclusion,
by now, does not meet serious objection
from nonmonetarists, at least as a first
approximation.

But the proposition that other things equal,
and given time enough, the economy will eventually
adjust to any indefinitely maintained stock
of money, or nth derivative thereof, can be derived
from a variety of models and, in any event,
is of very little practical relevance, as I will argue
below. What is unacceptable, because inconsistent
with both micro and macro evidence, is
the specific monetarist model set out above and
its implication that all unemployment is a voluntary,
fleeting response to transitory misperceptions.


One may usefully begin with a criticism of the
Macro Rational Expectations model and why
Keynes' "if" should not be replaced by
"since." At the logical level, Benjamin Friedman
has called attention to the omission from
MREH of an explicit learning model, and has
suggested that, as a result, it can only be interpreted
as a description not of short-run but of
long-run equilibrium in which no agent would
wish to recontract. But then the implications of
MREH are clearly far from startling, and their
policy relevance is almost nil. At the institutional
level, Stanley Fischer has shown that the
mere recognition of long-term contracts is sufficient
to generate wage rigidity and a substantial
scope for stabilization policies. But the most
glaring flaw of MREH is its inconsistency with
the evidence: if it were valid, deviations of unemployment
from the natural rate would be
small and transitory in which case The General
Theory would not have been written and
neither would this paper. Sargent (1976) has
attempted to remedy this fatal flaw by hypothesizing
that the persistent and large fluctuations
in unemployment reflect merely corresponding
swings in the natural rate itself. In other words,
what happened to the United States in the 1930's
was a severe attack of contagious laziness! I can
only say that, despite Sargent's ingenuity,
neither I nor, I expect, most others at least of
the nonmonetarists' persuasion are quite ready
yet to turn over the field of economic fluctuations
to the social psychologist!

Equally serious objections apply to Friedman'
s modeling of the commodity market as a
perfectly competitive one-so that the real wage
rate is continuously equated to the short-run
marginal product of labor-and to his treatment
of labor as a homogenous commodity traded in
an auction market, so that, at the going wage,
there never is any excess demand by firms or
excess supply by workers. The inadequacies of
this model as a useful formalization of present
day Western economies are so numerous that
only a few of the major ones can be mentioned
here.

Friedman's view of unemployment as a voluntary
reduction in labor supply could at best
provide an explanation of variations in labor
force-and then only under the questionable
assumption that the supply function has a significantly


### ---Economics-1977-0-09.txt---
positive slope-but cannot readily
account for changes in unemployment. Furthermore,
it cannot be reconciled with the wellknown
fact that rising unemployment is
accompanied by a fall, not by a rise in quits, nor
with the role played by temporary layoffs to
which Martin Feldstein has recently called
attention. Again, his competitive model of
the commodity market, accepted also in The
General Theorv, implies that changes in real
wages. adjusted for long-run productivity trend,
should be significantly negatively correlated
with cyclical changes in employment and output
and with changes in money wages. But as early
as 1938, John Dunlop showed that this conclusion
was rejected by some eighty years of British
experience and his results have received some
support in more recent tests of Ronald Bodkin
for the United States and Canada. Similar tests
of my own, using quarterly data, provide striking
confirmation that for the last two decades
from the end of the Korean War until 1973, the
association of trend adjusted real compensations
of the private nonfarm sector with either
employment or the change in nominal compensation
is prevailingly positive and very significantly
so.'

This evidence can, instead, be accounted for
by the oligopolistic pricing model-according
to which price is determined by long-run minimum
average cost up to a mark-up reflecting
entry-preventing considerations (see the author,
1958) coupled with some lags in the adjustment
of prices to costs. This model implies that
firms respond to a change in demand by endeavoring
to adjust output and employment, without
significant changes in prices relative to wages;
and the resulting changes in available jobs have
their initial impact not on wages but rather on
unemployment by way of layoffs and recalls and
through changes in the level of vacancies, and
hence on the length of average search time.
If, in the process, vacancies rise above a critical
level, or "natural rate," firms will endeavor
to reduce them by outbidding each other, thereby
raising the rate of change of wages. Thus, as
long as jobs and vacancies remain above, and
unemployment remains below, some critical
level which might be labeled the "noninflationary
rate" (see the author and Lucas Papademos,
1975), wages and prices will tend to accelerate.
If, on the other hand, jobs fall below, and unemployment
rises above, the noninflationary rate,
firms finding that vacancies are less than optimal
-in the limit the unemployed queuing outside
the gate will fill them instantly will have an
incentive to reduce their relative wage offer. But
in this case, in which too much labor is looking
for too few jobs, the trend toward a sustained
decline in the rate of growth of wages is likely
to be even weaker than the corresponding acceleration
when too many jobs are bidding for too
few people. The main reason is the nonhomogeneity
of labor. By far the largest and more
valuable source of labor supply to a firm consists
of those already employed who are not readily
interchangeable with the unemployed and, in
contrast with them, are concerned with protecting
their earnings and not with reestablishing
full employment. For these reasons, and because
the first to quit are likely to be the best workers, a
reduction of the labor force can, within limits, be
accomplished more economically, not by reducing
wages to generate enough quits, but by
firing or, when possible, by layoffs which insure
access to a trained labor force when demand
recovers. More generally, the inducement to
'Thus, in a logarithmic regression of private nonfarm
hourly compensation deflated by the private nonfarimi
deflator on output per man-hour, time, and private nonfarm
employment, after correcting for first-order serial correlation,
the latter variable has a coefficient of . 17 and a t-ratio
of 5. Similar though less significant results were found for
manufacturing. If employment is replaced by the change in
nominal compensation, its coefficient is .40 with a t-ratio of
6.5. Finally, if the change in compensation is replaced by
the change in price, despite the negative bias from error
of measurement of price, the coefficient of this variable is
only - .09 with an entirely insignificant t-ratio of .7. The
period after 1 973 has been omitted from the tests as irrelevant
for our purposes, since the inflation was driven primnarily by
an exogenous price shock rather than by excess demand. As a
result of the shock, prices, and to some extent wages, rose
rapidly while employment and real wages fell. Thus, the
addition of the last two years tends to increase spuriously the
positive association between real wages and employment,
and to decrease that between real wages and the change in
inominal wages or prices.


### ---Economics-1977-0-10.txt---
reduce relative wages to eliminate the excess
supply is moderated by the effect that such a
reduction would have on quits and costly turnover,
even when the resulting vacancies can be
readily filled from the ranks of the unemployed.
Equally relevant are the consequences in terms
of loss of morale and good will, in part for reasons
which have been elaborated by the literature
on implicit contracts (see Robert Gordon).
Thus, while there will be some tendency for the
rate of change of wages to fall, the more so the
larger the unemployment-at least in an economy
like the United States where there are no
overpowering centralized unions-that tendency
is severely damped.

And whether, given an unemployment rate
significantly and persistently above the noninflationary
level, the rate of change of wages would,
eventually, tend to turn negative and decline
without bound or whether it would tend to an
asymptote is a question that I doubt the empirical
evidence will ever answer. The one experiment
we have had the Great Depression suggests
the answer is negative, and while I admit that,
for a variety of reasons, that evidence is muddied,
I hope that we will never have the opportunity
for a second, clean experiment.
In any event, what is really important for
practical purposes is not the long-run equilibrium
relation as such, but the speed with which it
is approached. Both the model sketched out and
the empirical evidence suggest that the process
of acceleration or deceleration of wages when
unemployment differs from the noninflationary
rate will have more nearly the character of a
crawl than of a gallop. It will suffice to recall in
this connection that there was excess demand
pressure in the United States at least from 1965
to mid- 1970, and during that period the growth
of inflation was from some 1.5 to only about 5.5
percent per year. And the response to the excess
supply pressure from mid-1970 to early 1973,
and from late 1974 to date was equally sluggish.
B. The Power of Self-Stabilizing Mechanisms:
The Evidence from Econometric Models
There remains to consider the monetarists'
initial criticism of Keynesianism, to wit, that
even without high wage flexibility, the system's
response to demand shocks is small and shortlived,
thanks to the power of the Hicksian mechanism.
Here it must be acknowledged that every
one of the monetarists' criticisms of early,
simpleminded Keynesianism has proved in
considerable measure correct.

With regard to the interest elasticity of demand
for money, post-Keynesian developments
in the theory of money, and in particular, the
theoretical contributions of William Baumol,
James Tobin, Merton Miller, and Daniel Orr,
point to a modest value of around one-half to
one-third, and empirical studies (see for example,
Stephen Goldfeld) are largely consistent
with this prediction (at least until 1975!). Similarly,
the dependence of consumption on longrun,
or life cycle, income and on wealth,
together with the high marginal tax rates of the
postwar period, especially the corporate tax,
and leakages through imports, lead to a rather
low estimate of the multiplier.
Last but not least, both theoretical and empirical
work, reflected in part in econometric models,
have largely vindicated the monetarist
contention that interest effects on demand are
pervasive and substantial. Thus, in the construction
and estimation of the MIT-Penn-Social
Science Research Council (MPS) econometric
model of the United States, we found evidence
of effects, at least modest, on nearly every component
of aggregate demand. One response to
money supply changes that is especially important
in the MPS, if somewhat controversial, is
via interest rates on the market value of all assets
and thus on consumption.

There is, therefore, substantial agreement that
in the United States the Hicksian mechanism is
fairly effective in limiting the effect of shocks,
and that the response of wages and prices to
excess demand or supply will also work gradually
toward eliminating largely, if not totally,
any effect on employment. But in the view of
nonmonetarists, the evidence overwhelmingly
supports the conclusion that the interim response
is still of significant magnitude and of considerable
duration, basically because the wheels of
the offsetting mechanism grind slowly. To be
sure, the first link of the mechanism, the rise in
short-term rates, gets promptly into play and


### ---Economics-1977-0-11.txt---
heftily, given the low money demand elasticity;
but most expenditures depend on long-term
rates, which generally respond but gradually,
and the demand response is generally also gradual.
Furthermore, while this response is building
up, multiplier and accelerator mechanisms work
toward amplifying the shock. Finally, the classical
mechanism the change in real money
supply through prices has an even longer lag
because of the sluggish response of wages to
excess demand.

These interferences are supported by simulations
with econometric models like the MPS.
Isolating, first, the working of the Hicksian
mechanism by holding prices constant, we find
that a 1 percent demand shock, say a rise in real
exports, produces an impact effect on aggregate
output which is barely more than 1 percent, rises
to a peak of only about 2 percent a year later, and
then declines slowly toward a level somewhat
over 1.5 percent.

Taking into account the wage price mechanism
hardly changes the picture for the first
year because of its inertia. Thereafter, however,
it becomes increasingly effective so that a year
later the real response is back at the impact level,
and by the end of the third year the shock has
been fully offset (thereafter output oscillates
around zero in a damped fashion). Money income,
on the other hand, reaches a peak of over
2.5, and then only by the middle of the second
year. It declines thereafter, and tends eventually
to oscillate around a positive value because normally,
a demand shock requires eventually a
change in interest rates and hence in velocity
and money income.

These results, which are broadly confirmed by
other econometric models, certainly do not support
the view of a highly unstable economy in
which fiscal policy has powerful and everlasting
effects. But neither do they support the monetarist
view of a highly stable economy in which
shocks hardly make a ripple and the effects of
fiscal policy are puny and fast vanishing.
C. The Monetarist Evidence and the
St. Louis Quandary

Monetarists, however, have generally been
inclined to question this evidence. They countered
at first with tests bearing on the stability of
velocity and the insignificance of the multiplier,
which, however, as indicated in my criticism
with Albert Ando (1965), must be regarded as
close to worthless. More recently, several authors
at the Federal Reserve Bank of St. Louis
(Leonall Andersen, Keith Carlson, Jerry Lee
Jordan) have suggested that instead of deriving
multipliers from the analytical or numerical
solution of an econometric model involving a
large number of equations, any one of which
may be questioned, they should be estimated
directly through "reduced form" equations by
relating the change in income to current and
lagged changes in some appropriate measure of
the money supply and of fiscal impulses.
The results of the original test, using the
current and but four lagged values of M1 and of
high Employment Federal Expenditure as measures
of monetary and fiscal impulses, turned out
to be such as to fill a monetarist's heart with joy.
The contribution of money, not only current but
also lagged, was large and the coefficients implied
a not unreasonable effect of the order of
magnitude of the velocity of circulation, though
somewhat higher. On the other hand, the estimated
coefficients of the fiscal variables seemed
to support fully the monetarists' claim that their
impact was both small and fleeting: the effect
peaked in but two quarters and was only around
one, and disappeared totally by the fourth quarter
following the change.

These results were immediately attacked on
the ground that the authors had used the wrong
measure of monetary and fiscal actions, and it
was shown that the outcome was somewhat
sensitive to alternative measures; however, the
basic nature of the results did not change, at least
qualitatively. In particular, the outcome does not
differ materially, at least for the original period
up to 1969, if one replaces high employment outlays
with a variable that might be deemed more
suitable, like government expenditure on goods
and services, plus exports.

These results must be acknowledged as disturbing
for nonmonetarists, for there is little
question that movements in government purchases
and exports are a major source of demand
disturbances; if econometric model estimates of


### ---Economics-1977-0-12.txt---
the response to demand disturbances are roughly
valid, how can they be so grossly inconsistent
with the reduced form estimates?
Attempts at reconciling the two have taken
several directions, which are reviewed in an
article coauthored with Ando (1976). Our main
conclusion, based on simulation techniques, is
that when income is subject to substantial shocks
from many sources other than monetary and
fiscal, so that these variables account for only a
moderate portion of the variations in income (in
the United States, it has been of the order of onehalf
to two-thirds), then the St. Louis reduced
form method yields highly unstable and unreliable
estimates of the true structure of the system
generating the data.

The crucial role of unreliability and instability
has since been confirmed in more recent work of
Daniel O'Neill in his forthcoming thesis. He
shows in the first place that different methods
of estimation yield widely different estimates,
including many which clearly overstate the
expenditure and understate the money multipliers.
He further points out that, given the
unreliability of the estimates resulting from
multicollinearity and large residual variance,
the relevant question to ask is not whether
these estimates differ from those obtained by
structural estimation, but whether the difference
is statistically significant; that is, larger than
could be reasonably accounted for by sampling
fluctuations.

I have carried out this standard statistical test
using as true response coefficients those generated
by the MPS model quoted earlier.2 I find
that, at least when the test is based on the largest
possible sample-the entire post-Korean period
up to the last two very disturbed years-the
difference is totally insignificant when estimation
is in level form (F is less than one) and is
still not significant at the 5 percent level, when in
first differences.

This test resolves the puzzle by showing that
there really is no puzzle: the two alternative estimates
of the expenditure multipliers are not
inconsistent, given the margin of error of the
estimates. It implies that one should accept
whichever of the two estimates is produced by a
more reliable and stable method, and is generally
more sensible. To me, those criteria call, without
question, for adopting the econometric
model estimates. But should there be still some
lingering doubt about this choice, I am happy to
be able to report the results of one final test
which I believe should dispose of the reduced
form estimates-at least for a while. Suppose
the St. Louis estimates of the expenditure multiplier
are closer to God's truth than the estimates
derived through econometric models. Then it
should be the case that if one uses their coefficients
to forecast income beyond the period of
fit, these forecasts should be appreciably better
than those obtained from a forecasting equation
in which the coefficients of the expenditure variable
are set equal to those obtained from econometric
models.

I have carried out this test, comparing a reduced
form equation fitted to the period originally
used at St. Louis, terminating in 1969 (but
reestimated with the lastest revised data) with an
equation in which the coefficients of government
expenditure plus exports were constrained to be
those estimated from the MPS, used in the above
F-test. The results are clear cut: the errors using
the reduced form coefficient are not smaller but
on the average substantially larger than those
using MPS multipliers. For the first four years,
terminating at the end of 1973, the St. Louis
equation produces errors which are distinctly
larger in eight quarters, and smaller in but three,
and its squared error is one-third larger. For the
last two years of turmoil, both equations perform
miserably, though even here the MPS coefficients
perform just a bit better. I have repeated
this test with equations estimated through the
first half of the postwar period, and the results
are, if anything, even more one-sided.
The moral of the story is pretty clear. First,


### ---Economics-1977-0-13.txt---
reduced form equations relying on just two exogenous
variables are very unreliable for the purpose
of estimating structure, nor are they
particularly accurate for forecasting, though per
dollar of research expenditure they are surprisingly
good. Second, if the St. Louis people want
to go on using this method and wish to secure the
best possible forecast, then they should ask the
MPS or any other large econometric model what
coefficients they should use for government
expenditure, rather than trying to estimate them
by their unreliable method.

From the theory and evidence reviewed, we
must then conclude that opting for a constant rate
of growth of the nominal money supply can
result in a stable economy only in the absence of
significant exogenous shocks. But obviously the
economy has been and will continue to be exposed
to many significant shocks, coming from
such things as war and peace, and other large
changes in government expenditure, foreign
trade, agriculture, technological progress, population
shifts, and what not. The clearest evidence
on the importance of such shocks is provided by
our postwar record with its six recessions.
IV. The Record of Stabilization Policies:
Stabilizing or Destabilizing

A. Was Postwar Instability Due to Unstable
Money Growth?

At this point, of course, monetarists will object
that, over the postwar period, we have not
had a constant money growth policy and will hint
that the observed instability can largely be traced
to the instability of money. The only way of
meeting this objection squarely would be, of
course, to rerun history with a good computer
capable of calculating 3 percent at the helm of
the Fed.

A more feasible, if less conclusive approach
might be to look for some extended periods in
which the money supply grew fairly smoothly
and see how the economy fared. Combing
through our post-Korean War history, I have
been able to find just two stretches of several
years in which the growth of the money stock
was relatively stable, whether one chooses to
measure stability in terms of percentage deviations
from a constant growth or of dispersion of
four-quarter changes. It may surprise some that
one such stretch occurred quite recently and
consists of the period of nearly four years beginning
in the first quarter of 1971 (see the author
and Papademos, 1976). During this period, the
average growth was quite large, some 7 percent,
but it was relatively smooth, generally well
within the 6 to 8 percent band. The average
deviation from the mean is about .75 percent.
The other such period lasted from the beginning
of 1953 to the first half of 1957, again a stretch
of roughly four years. In sharp contrast to the
most recent period, the average growth here is
quite modest, only about 2 percent; but again,
most four-quarter changes fell well within a
band of two percentage points, and the average
deviation is again .7. By contrast, during the
remaining 13-year stretch from mid-1957 to the
end of 1970, the variability of money growth
was roughly twice as large if measured by the
average deviation of four quarter changes, and
some five times larger if measured by the percentage
deviation of the money stock from a
constant growth trend.

How did the economy fare in the two periods
of relatively stable money growth? It is common
knowledge that the period from 1971 to 1974, or
from 1972 to 1975 if we want to allow a one-year
lag for money to do its trick, was distinctly the
most unstable in our recent history, marked by
sharp fluctuations in output and wild gyrations
of the rate of change of prices. As a result, the
average deviation of the four-quarter changes in
output was 3.3 percent, more than twice as large
as in the period of less stable money growth. But
the first stretch was also marked by well above
average instability, with the contraction of 1954,
the sharp recovery of 1955, and the new contraction
in 1958, the sharpest in postwar history
except for the present one. The variability of output
is again 50 percent larger than in the middle
period.

To be sure, in the recent episode serious exogenous
shocks played a major role in the development
of prices and possibly output, although the


### ---Economics-1977-0-14.txt---
same is not so readily apparent for the period
1953 to 1958. But, in any event, such extenuating
circumstances are quite irrelevant to my
point; for I am not suggesting that the stability of
money was the major cause of economic instability-
or at any rate, not yet! All I am arguing is
that (i) there is no basis for the monetarists' suggestion
that our postwar instability can be traced
to monetary instability-our most unstable
periods have coincided with periods of relative
monetary stability; and (ii) stability of the
money supply is not enough to give us a stable
economy, precisely because there are exogenous
disturbances.

Finally, let me mention that I have actually
made an attempt at rerunning history to see
whether a stable money supply would stabilize
the economy, though in a way that I readily
acknowledge is much inferior to the real thing,
namely through a simulation with the MPS. The
experiment, carried out in cooperation with
Papademos, covered the relatively quiet period
from the beginning of 1959 to the introduction of
price-wage controls in the middle of 1971. If
one eliminates all major sources of shocks, for
example, by smoothing federal government
expenditures, we found, as did Otto Eckstein in
an earlier experiment, that a stable money
growth of 3 percent per year does stabilize the
economy, as expected. But when we allowed for
all the historical shocks, the result was that with
a constant money growth the economy was far
from stable-in fact, it was distinctly less stable
than actual experience, by a factor of 50 percent.
B. The Overall Effectiveness of Postwar
Stabilization Policies

But even granted that a smooth money supply
will not produce a very stable world and that
there is therefore room for stabilization policies,
monetarists will still argue that we should nonetheless
eschew such policies. They claim, first,
that allowing for unpredictably variable lags and
unforseeable future shocks, we do not know
enough to successfully design stabilization
policies, and second, that the government would
surely be incapable of choosing the appropriate
policies or be politically willing to provide
timely enforcement. Thus, in practice, stabilization
policies will result in destabilizing the
economy much of the time.

This view is supported by two arguments, one
logical and one empirical. The logical argument
is the one developed in Friedman's Presidential
Address (1968). An attempt at stabilizing the
economy at full employment is bound to be destabilizing
because the full employment or
natural rate is not known with certainty and is
subject to shifts in time; and if we aim for the
incorrect rate, the result must perforce be explosive
inflation or deflation. By contrast, with a
constant money supply policy, the economy will
automatically hunt for, and eventually discover,
that shifty natural rate, wherever it may be
hiding.

This argument, I submit, is nothing but a
debating ploy. It rests on the preposterous assumption
that the only alternative to a constant
money growth is the pursuit of a very precise
unemployment target which will be adhered to
indefinitely no matter what, and that if the target
is off in the second decimal place, galloping
inflation is around the corner. In reality, all that
is necessary to pursue stabilization policies is a
rough target range that includes the warranted
rate, itself a range and not a razor edge; and, of
course, responsible supporters of stabilization
policies have long been aware of the fact that the
target range needs to be adjusted in time on the
basis of forseeable shifts in the warranted range,
as well as in the light of emerging evidence that
the current target is not consistent with price
stability. It is precisely for this reason that I, as
well as many other nonmonetarists, would side
with monetarists in strenuous opposition to
recent proposals for a target unemployment rate
rigidly fixed by statute (although there is nothing
wrong with Congress committing itself and the
country to work toward the eventual achievement
of some target unemployment rate through
structural changes rather than aggregate demand
policies).

Clearly, even the continuous updating of
targets cannot guarantee that errors can be


### ---Economics-1977-0-15.txt---
avoided altogether or even that they will be
promptly recognized; and while errors persist,
they will result in some inflationary (or deflationary)
pressures. But the growing inflation to
which Friedman refers is, to repeat, a crawl not a
gallop. One may usefully recall in this connection
the experience of 1965-70 referred to earlier,
with the further remark that the existence of
excess employment was quite generally recognized
at the time, and failure to eliminate it
resulted overwhelmingly from political considerations
and not from a wrong diagnosis.3
There remains then only the empirical issue:
have stabilization policies worked in the past and
will they work in the future? Monetarists think
the answer is negative and suggest, as we have
seen, that misguided attempts at stabilization,
especially through monetary policies, are responsible
for much of the observed instability.
The main piece of evidence in support of this
contention is the Great Depression, an episode
well documented through the painstaking work
of Friedman and Anna Schwartz, although still
the object of dispute (see, for example, Peter
Temin). But in any event, that episode while it
may attest to the power of money, is irrelevant
for present purposes since the contraction of the
money supply was certainly not part of a comprehensive
stabilization program in the postKeynesian
sense.

When we come to the relevant postwar period,
the problem of establishing the success or failure
of stabilization policies is an extremely taxing
one. Many attempts have been made at developing
precise objective tests, but in my view,
none of these is of much value, even though I am
guilty of having contributed to them in one of my
worst papers (1964). Even the most ingenious
test, that suggested by Victor Argy, and relying
on a comparison of the variability of income with
that of the velocity of circulation, turns out to
be valid only under highly unrealistic restrictive
assumptions.

Dennis Starleaf and Richard Floyd have proposed
testing the effectiveness of stabilization by
comparing the stability of money growth with
that of income growth, much as I have done
above for the United States, except that they
apply their test to a cross section of industrialized
countries. They found that for a sample of
13 countries, the association was distinctly
positive. But this test is again of little value. For
while a negative association for a given country,
such as suggested by my U.S. test, does provide
some weak indication that monetary activism
helped rather than hindered, the finding of a
positive association across countries proves
absolutely nothing. It can be readily shown, in
fact, that, to the extent that differential variability
of income reflects differences in the character
of the shocks-a most likely circumstance for
their sample-successful stabilization also
implies a positive correlation between the variability
of income and that of money.

But though the search for unambiguous quantitative
tests has so far yielded a meager crop,
there exists a different kind of evidence in favor
of Keynesian stabilization policies which is
impressive, even if hard to quantify. To quote
one of the founding fathers of business cycle
analysis, Arthur Burns, writing in 1959, "Since
1937 we have had five recessions, the longest of
which lasted only 13 months. There is no parallel
for such a sequence of mild-or such a sequence
of brief-contractions, at least during the past
hundred years in our country" (p. 2). By
now we can add to that list the recessions of
1961 and 1970.

There is, furthermore, evidence that very
similar conclusions hold for other industrialized
countries which have made use of stabilization
policies; at any rate that was the prevailing view
among participants to an international conference
held in 1967 on the subject, "Is the busi-
3Friedman's logical argument against stabilization policies
and in favor of a constant money growth rule is, I
submit, much like arguing to a man from St. Paul wishing to
go to New Orleans on important business that he would be a
fool to drive and should instead get himself a tub and drift
down the Mississippi: that way he can be pretty sure that the
current will eventually get him to his destination; whereas, if
he drives, he might make a wrong turn and, before he notices
he will be going further and further away from his destination
and pretty soon he may end up in Alaska, where he will
surely catch pneumonia and he may never get to New
Orleans!


### ---Economics-1977-0-16.txt---
ness cycle obsolete?" (see Martin Bronfenbrenner,
editor). No one seemed to question the
greater postwar stability of all Western economies-
nor is this surprising when one recalls
that around that time business cycle specialists
felt so threatened by the new-found stability that
they were arguing for redefining business cycles
as fluctuations in the rate of growth rather than
in the level of output.

It was recognized that the reduced severity of
fluctuations might in part reflect structural
changes in the economy and the effect of stronger
built-in stabilizers, inspired, of course, by
the Keynesian analysis. Furthermore, the greater
stability in the United States, and in other industrialized
countries, are obviously not independent
events. Still, at least as of the time of that
conference, there seemed to be little question
and some evidence that part of the credit for the
greater stability should go to the conscious and
on balance, successful endeavor at stabilizing
the economy.

V. The Case of Supply Shocks and the
1974-76 Episode

A. Was the 1974 Depression Due to Errors of
Commission or Omission?

In pointing out our relative postwar stability
and the qualified success of stabilization policies,
I have carefully defined the postwar period
as ending somewhere in 1973. What has happened
since that has so tarnished the reputation
of economists? In facing this problem, the first
question that needs to be raised is whether the
recent combination of unprecedented rates of
inflation as well as unemployment must be
traced to crimes of commission or omission. Did
our monetary and fiscal stabilization policies
misfire, or did we instead fail to use them?
We may begin by establishing one point that
has been blurred by monetarists' blanket indictments
of recent monetary policy: the virulent
explosion that raised the four-quarter rate of
inflation from about 4 percent in 1972 to 6.5
percent by the third quarter of 1973, to 11.5
percent in 1974 with a peak quarterly rate of
13.5, can in no way be traced to an excessive, or
to a disorderly, growth of the money supply. As
already mentioned, the average rate of money
growth from the beginning of 1970 to the second
half of 1974 was close to 7 percent. To be sure,
this was a high rate and could be expected sooner
or later to generate an undesirably high inflation
-but how high? Under any reasonable assumption
one cannot arrive at a figure much above 6
percent. This might explain what happened up to
the fall of 1973, but not from the third quarter
of 1973 to the end of 1974, which is the really
troublesome period. Similarly, as was indicated
above, the growth of money was reasonably
smooth over this period, smoother than at any
other time in the postwar period, staying within
a 2 percent band. Hence, the debacle of 1974
can just not be traced to an erratic behavior of
money resulting from a misguided attempt at
stabilization.

Should one then conclude that the catastrophe
resulted from too slavish an adherence to a stable
growth rate, forsaking the opportunity to use
monetary policy to stabilize the economy? In
one sense, the answer to this question must in
my view be in the affirmative. There is ample
ground for holding that the rapid contraction that
set in toward the end of 1974, on the heels of a
slow decline in the previous three quarters, and
which drove unemployment to its 9 percent
peak, was largely the result of the astronomic
rise in interest rates around the middle of the
year. That rise in turn was the unavoidable result
of the Fed's stubborn refusal to accommodate,
to an adequate extent, the exogenous inflationary
shock due to oil, by letting the money supply
growth exceed the 6 percent rate announced at
the beginning of the year. And this despite repeated
warnings about that unavoidable result
(see, for example, the author 1974).
Monetarists have suggested that the sharp
recession was not the result of too slow a monetary
growth throughout the year, but instead of
the deceleration that took place in the last half of
1974, and early 1975. But this explanation just
does not stand up to the facts. The fall in the
quarterly growth of money in the third and fourth
quarters was puny, especially on the basis of


### ---Economics-1977-0-17.txt---
revised figures now available: from 5.7 percent
in the second to 4.3 and 4.1-hardly much
larger than the error of estimate for quarterly
rates! To be sure, in the first quarter of 1975 the
growth fell to .6 percent. But, by then, the violent
contraction was well on its way-between
September 1974 and February 1975, industrial
production fell at an annual rate of 25 percent.
Furthermore, by the next quarter, monetary
growth had resumed heftily. There is thus no
way the monetarist proposition can square with
these facts unless their long and variable lags
are so variable that they sometimes turn into
substantial leads. But even then, by anybody's
model, a one-quarter dip in the growth of money
could not have had a perceptible effect.
B. What Macro Stabilization Policies
Can Accomplish, and How

But recognizing that the adherence to a stable
money growth path through much of 1974 bears
a major responsibility for the sharp contraction
does not per se establish that the policy was
mistaken. The reason is that the shock that hit
the system in 1973-74 was not the usual type of
demand shock which we have gradually learned
to cope with, more or less adequately. It was,
instead, a supply or price shock, coming from
a cumulation of causes, largely external. This
poses an altogether different stabilization problem.
In particular, in the case of demand shocks,
there exists in principle an ideal policy which
avoids all social costs, namely to offset completely
the shock thus, at the same time, stabilizing
employment and the price level. There
may be disagreement as to whether this target
can be achieved and how, but not about the
target itself.

But in the case of supply shocks, there is no
miracle cure-there is no macro policy which
can both maintain a stable price level and keep
employment at its natural rate. To maintain
stable prices in the face of the exogenous price
shock, say a rise in import prices, would require
a fall in all domestic output prices; but we know
of no macro policy by which domestic prices can
be made to fall except by creating enough slack,
thus putting downward pressure on wages. And
the amount of slack would have to be substantial
in view of the sluggishness of wages in the face
of unemployment. If we do not offset the exogenous
shock completely, then the initial burst,
even if activated by an entirely transient rise in
some prices, such as a once and for all deterioration
in the terms of trade, will give rise to further
increases, as nominal wages rise in a vain attempt
at preserving real wages; this secondary
reaction too can only be cut short by creating
slack. In short, once a price shock hits, there is
no way of returning to the initial equilibrium
except after a painful period of both above equilibrium
unemployment and inflation.

There are, of course, in principle, policies
other than aggregate demand management to
which we might turn, and which are enticing
in view of the unpleasant alternatives offered by
demand management. But so far such policies,
at least those of the wage-price control variety,
have proved disappointing. The design of better
alternatives is probably the greatest challenge
presently confronting those interested in stabilization.
However, these policies fall outside my
present concern. Within the realm of aggregate
demand management, the only choice open to
society is the cruel one between alternative feasible
paths of inflation and associated paths of
unemployment, and the best the macroeconomist
can offer is policies designed to approximate
the chosen path.

In light of the above, we may ask: is it conceivable
that a constant rate of growth of the
money supply will provide a satisfactory response
to price shocks in the sense of giving
rise to an unemployment-inflation path to which
the country would object least?
C. The Monetarist Prescription: Or, Constant
Money Growth Once More

The monetarists are inclined to answer this
question affirmatively, if not in terms of the
country's preferences, at least in terms of the
preferences they think it should have. This is
evidenced by their staunch support of a continuation
of the 6 percent or so rate of growth through


### ---Economics-1977-0-18.txt---
1974, 1975, and 1976.

Their reasoning seems to go along the following
lines. The natural rate hypothesis implies
that the rate of inflation can change only when
employment deviates from the natural rate. Now
suppose we start from the natural rate and some
corresponding steady rate of inflation, which
without loss of generality can be assumed as
zero. Let there be an exogenous shock which
initially lifts the rate of inflation, say, to 10 percent.
If the Central Bank, by accommodating
this price rise, keeps employment at the natural
rate, the new rate of 10 percent will also be maintained
and will in fact continue forever, as long
as the money supply accommodates it. The only
way to eliminate inflation is to increase unemployment
enough, above the natural rate and for
a long enough time, so that the cumulated reduction
of inflation takes us back to zero. There
will of course be many possible unemployment
paths that will accomplish this. So the next
question is: Which is the least undesirable?
The monetarist answer seems to be-and
here I confess that attribution becomes difficult
-that it does not make much difference because,
to a first approximation, the cumulated
amount of unemployment needed to unwind
inflation is independent of the path. If we take
more unemployment early, we need to take less
later, and conversely. But then it follows immediately
that the specific path of unemployment
that would be generated by a constant money
growth is, if not better, at least as good as any
other. Corollary: a constant growth of money is
a satisfactory answer to supply shocks just as it
is to demand shocks-as well as, one may suspect,
to any other conceivable illness, indisposition,
or disorder.

D. Why Constant Money Growth Cannot
Be the Answer

This reasoning is admirably simple and elegant,
but it suffers from several flaws. The first
one is a confusion between the price level and
its rate of change. With an unchanged constant
growth of the nominal money stock, the system
will settle back into equilibrium not when the
rate of inflation is back to zero but only when,
in addition, the price level itself is back to its
initial level. This means that when inflation has
finally returned back to the desired original rate,
unemployment cannot also be back to the original
level but will instead remain above it as long
as is necessary to generate enough deflation to
offset the earlier cumulated inflation. I doubt that
this solution would find many supporters and for
a good reason; it amounts to requiring that none
of the burden of the price shock should fall on
the holder of long-term money fixed contractssuch
as debts-and that all other sectors of society
should shoulder entirely whatever cost is
necessary to insure this result. But if, as seems
to be fairly universally agreed, the social target
is instead to return the system to the original rate
of inflation-zero in our example-then the
growth of the money supply cannot be kept constant.
Between the time the shock hits and the
time inflation has returned to the long-run level,
there must be an additional increase in money
supply by as much as the price level or by the
cumulant of inflation over the path.
A second problem with the monetarists' argument
is that it implies a rather special preference
function that depends only on cumulated unemployment.
And, last but not least, it requires the
heroic assumption that the Phillips curve be not
only vertical in the long run but also linear in the
short run, an assumption that does not seem
consistent with empirically estimated curves.
Dropping this last assumption has the effect that,
for any given social preference, there will be in
general a unique optimal path. Clearly, for this
path to be precisely that generated by a constant
money growth, would require a miracle-or
some sleight of the invisible hand!
Actually, there are grounds for holding that
the unemployment path generated by a constant
money growth, even if temporarily raised to take
care of the first flaw, could not possibly be close
to an optimal. This conclusion is based on an
analysis of optimal paths, relying on the type of
linear welfare function that appears to underlie
the monetarists' argument, and which is also a
straightforward generalization of Okun's famous


### ---Economics-1977-0-19.txt---
"economic discomfort index." That index
(which according to Michael Lovell appears to
have some empirical support) is the sum of unemployment
and inflation. The index used in my
analysis is a weighted average of the cumulated
unemployment and cumulated inflation over the
path. The weights express the relative social
concern for inflation versus unemployment.
Using this index, it has been shown in a forthcoming
thesis of Papademos that, in general, the
optimum policy calls for raising unemployment
at once to a certain critical level and keeping it
there until inflation has substantially abated. The
critical level depends on the nature of the Phillips
curve and the relative weights, but does not
depend significantly on the initial shock-as
long as it is appreciable. To provide an idea of
the order of magnitudes involved, if one relies on
the estimate of the Phillips curve reported in
my joint paper with Papademos (1975), which
is fairly close to vertical and uses Okun's
weights, one finds that (i) at the present time, the
noninflationary rate of unemployment corresponding
to a 2 percent rate of inflation can be
estimated at 5.6 percent, and (ii) the optimal response
to a large exogenous price shock consists
in increasing unemployment from 5.6 to only
about 7 percent. That level is to be maintained
until inflation falls somewhat below 4 percent;
it should then be reduced slowly until inflation
gets to 2.5 (which is estimated to take a couple of
years), and rapidly thereafter. If, on the other
hand, society were to rate inflation twice as
costly as unemployment, the initial unemployment
rate becomes just over 8 percent, though
the path to final equilibrium is then shorter.
These results seem intuitively sensible and quantitatively
reasonable, providing further justification
for the assumed welfare function, with its
appealing property of summarizing preferences
into a single readily understandable number.
One important implication of the nature of the
optimum path described above is that a constant
money growth could not possibly be optimal
while inflation is being squeezed out of the system,
regardless of the relative weights attached
to unemployment and inflation. It would tend
to be prevailingly too small for some initial
period and too large thereafter.
One must thus conclude that the case for a
constant money growth is no more tenable in the
case of supply shocks than it is in the case of
demand shocks.

VI. Conclusion

To summarize, the monetarists have made a
valid and most valuable contribution in establishing
that our economy is far less unstable than
the early Keynesians pictured it and in rehabilitating
the role of money as a determinant of
aggregate demand. They are wrong, however,
in going as far as asserting that the economy is
sufficiently shockproof that stabilization policies
are not needed. They have also made an
important contribution in pointing out that such
policies might in fact prove destabilizing. This
criticism has had a salutary effect on reassessing
what stabilization policies can and should do,
and on trimming down fine-tuning ambitions.
But their contention that postwar fluctuations
resulted from an unstable money growth or that
stabilization policies decreased rather than increased
stability just does not stand up to an
impartial examination of the postwar record of
the United States and other industrialized countries.
Up to 1974, these policies have helped to
keep the economy reasonable stable by historical
standards, even though one can certainly point
to some occasional failures.

The serious deterioration in economic stability
since 1973 must be attributed in the first place
to the novel nature of the shocks that hit us,
namely, supply shocks. Even the best possible
aggregate demand management cannot offset
such shocks without a lot of unemployment
together with a lot of inflation. But, in addition,
demand management was far from the best. This
failure must be attributed in good measure to the
fact that we had little experience or even an
adequate conceptual framework to deal with
such shocks; but at least from my reading of
the record, it was also the result of failure to
use stabilization policies, including too slavish
adherence to the monetarists' constant money


### ---Economics-1977-0-20.txt---
growth presciption.

We must, therefore, categorically reject the
monetarist appeal to turn back the clock forty
years by discarding the basic message of The
General Theorxv. We should instead concentrate
our efforts in an endeavor to make stabilization
policies even more effective in the future than
they have been in the past.
 ## Economics-1978-0


### ---Economics-1978-0-01.txt---
I. The Meaning of Supply and Demand
in a Macroeconomic Context

It is worth considering whether a new
basic model should guide our thinking
about performance of the economy as a
whole. It is not that the macro models of
the past twenty-five years or so have failed
to serve us well. When we consider the state
of our knowledge about the analytics of the
economy at the end of World War II and
the apprehensiveness with which we approached
the modern era of expansion, it
should be evident that we have come a long
way professionally. Yet the economic problems
of today seem to be intractable when
studied through the medium of simplified
macro models. The new system should
combine the Keynesian model of final demand
and income determination with the
Leontief model of interindustrial flows.
This is the motivation for my focusing attention
on the supply side of the economy.
It is frequently said, in almost an offhand
manner, that the theories of aggregate employment
and output determination are demand
models, that economic policy for
overall direction of the economy is a policy
of demand management. I would generally
agree with these remarks, but not in every
last detail, once the meaning of demand in
these contexts is carefully pulled apart and
analyzed. The demand aspects are possibly
overstated.

It is, of course, true that demand for the
GNP built up as the sum of demands by
consumers, businesses, government, and
foreigners (consumption, investment, public
spending, and net exports) covers total demand
in the economy and is composed of
demands by the constituent parts. But demand
by firms, and, in many cases by government,
are not ends in themselves. Business
demand is largely for goods to produce
goods. The capital formation that results
from business demand goes into the increment
of capital stock, after allowance for
capital consumption, and the capital stock
becomes a factor input in the production
function. The accumulation of capital contributes
to the supply of goods and services.
Indeed, investment demand now for new
capital facilitates the implementation of the
production process with the supply of factors
of ever-increasing powers of productivity,
thus making it possible to supply increasing
amounts of goods and services

with inputs that are increasing at a somewhat
slower rate.

By focusing attention excessively on the
"short run," in which the capital stock is
timelessly held fixed by assumption only
and not in reality, we have ignored the supply-
side characteristics of investment demand.
Students of today's business cycle
commonly cite investment demand as the
promising potential route to higher productivity
in the relatively near future, thereby
lessening inflationary pressure. In this respect,
economic theoreticians have been
myopic relative to the applied economic
analysts in the world of affairs. Nevertheless,
as we shall see, there is much more
to the supply side than the transformation
of investment into productive capital, and
the basic characterization of contemporary
macroeconomics as demand analysis has a
point. A strong indication of the demand
side orientation is given by the elaboration
of the standard macro model. In place of
aggregate consumption, the more elaborate
model gives separate treatment to consumer
expenditures on durables, nondurables, and
services. This is the first stage. At a higher
stage, there is further disaggregation into
types of durables, nondurables, and services
such as food, cars, medical services, etc.
The detail that is introduced for consumption


### ---Economics-1978-0-02.txt---
is repeated in business investment demand,
housing demand, public expenditures,
exports, and imports. Elaboration
essentially means taking a closer look at demand
side components by types of demand.
The mainstream model of macro economic
thought has thus become a detailed
system of demand analysis, but if it is to be
a closed system, it will also have to include
corresponding detail on the national income
side of the social accounts. If this is done
fully, there will have to be analyses of factor
rewards, factor use, and pricing. The development
of factor demand goes beyond capital
formation, which appears as a demand
for final goods in the GNP, and takes up
an explanation of wage income. An adequate
explanation of wage income cannot
avoid the explicit treatment of physical
production involving labor input as well as
capital input. The demand for labor, like
the demand for capital, is supply side analysis.
While the demand for capital enters directly
as a component of total demand, the
demand for labor, together with wage formation,
enters national income, and only
after expenditure does it enter final demand
for GNP. To the extent that labor productivity
affects wage determination and also
price formation, we find supply-side factors
influencing inflation and consequently the
overall performance of the economy. Labor
demand can also be associated with training.
The training component is, in fact,
investment in human rather than fixed
capital. Looked at in this way, factor demand
for labor and factor demand for fixed
capital are simply different, but related,
aspects of total investment.

Behind the IS-LM diagram or other simplified
renditions of the aggregate demand
model lie many supply-side relationships.
Not only is the supply side in the background,
but it also plays a more essential
role once it is recognized that the simplified
model is actually incomplete. If we were to
assume the existence of money illusion, it
would be possible to consider the IS-LM
system as a closed system of relationships
depending on nominal income and nominal
interest rates. I find this approach theoretically
unsatisfactory. That simple system
exists only as an aggregative approximation
for a given price level. If we assume no
money illusion and, more properly, I believe,
the need to determine the aggregate
price level, then the IS-LM diagram does
not provide a closed system analysis, and
we must extend the system to include the
whole supply-side apparatus of production
relationships, factor demand, and factor
supply.

It is well known that Keynes included
the aggregate supply function in the General
Theory, but it was introduced in his
chapter on "The Principle of Effective Demand.
" That part of his analysis dealing
with supply has been largely played down
by the profession at large-not by all students
of macroeconomics.' Also, by way of
side comment, Keynes probably confused
the issues by making labor supply dependent
on the nominal wage rate, assuming
the existence of money illusion, and by not
treating the stock of capital as an explicit
variable.

If the demand relationships explaining
the components of the GNP are disaggregated
into a highly detailed set, it does not
necessarily mean that the supply side must
be equally disaggregated to a similar extent,
as long as the total flow of income and
purchasing power to be directed towards
the expenditure flow can be generated. The
detailed expenditure flow will, however, involve
price relatives. That is a consequence
of disaggregation. An aggregate supply-side
explanation that generates only an average
price level for output as a whole can be
adequate, provided separate prices, needed
for the price-relatives, can be explained in
terms of a relationship to the overall price
or wage level. This is much like the use of a
term structure relationship in credit market
analysis to explain the spectrum of interest
rates, given one strategic rate.
It is, however, more satisfactory, and
more revealing, to explain the whole set of
prices, one by one, on the basis of costs
in individual sectors. These sector prices, on
the side of production, are then combined
with input weights into the several final deISee
Sidney Weintraub (1956, 1957).


### ---Economics-1978-0-03.txt---
mand prices needed to account for variation
in components of final expenditure. This
brings us to a fundamental set of new considerations
on the supply side.

II. The Task of Modeling Supply
If sector prices by line of production are
to be explained in a fundamental way by
sector costs, there will have to be an accompanying
explanation of sector outputs and
inputs. This brings us directly to the supply
side of things. While the supply side is
represented in the macro production function
from an aggregative point of view, once
we disaggregate the supply side by sector of
production, we encounter a new dimension.
The aggregate production function, in the
spirit of Paul Douglas and Charles Cobb,
expresses value-added as a function of primary
factor inputs, namely, labor and capital.
They were able to compress the technology
as they did, because at a full macro
level, one sector's output is someone else's
input, and for the economy as a whole, only
value-added is left in the output aggregation.
Intermediate inputs or outputs may be
neglected in the interests of avoiding double
counting. This way of looking at things is
strictly correct only for a closed economy.
In an open system, intermediate imports
must be treated like primary factor inputs.
At the sector level, however, there is no
question about the need to consider intermediate
inputs. Sector output (gross) is
properly a function of intermediate inputs,
labor input, and capital input-all sector
designated. The presently fashionable way
of summarizing this idea is to use the
KLEM production function, whose inputs
consist of capital, labor, energy, and materials.


The KLEM production function concept
is useful in partial studies of separate industries
or sectors, and has long been anticipated
in aggregative production function
studies. It has been routine in production
function studies in agriculture to use feed,
seed, fertilizer, and other intermediate inputs
as explanatory variables. The dependent
variable is generally a measure of gross
output gallons of milk, bushels of grain,
or bales of cotton. In manufacturing, one
of the earliest studies was by Ragnar Frisch.
He expressed isoquants for the output of
the Freia chocolate factory as a function of
fat content and molding-cooling input. One
of these is a pure material input and the
other stands for some capital, labor, and
general running cost input. In my own investigations
of U.S. railroad production

functions, I included fuel consumption (in
coal equivalents) as one of the factor inputs
together with labor and capital. The gross
output concept consisted of a log-linear
combination of ton miles and passenger
miles.2

These individual industry production
functions with a small number of intermediate
inputs are hardly substitutes for a detailed
input-output analysis on a general
system level. The role of input-output
analysis is to explain intermediate flows in
the economic system. The full system is
needed in order to provide an adequate supply
analysis because

(i) There is much more to economic activity
than can be summarized by the

system of final goods production.
(ii) The explanation of types of final
prices depends on highly specific
types of intermediate, as well as final,
goods/services prices.

The occurrence of bottlenecks potential
or realized as in the oil embargo of
1973--74 or the diversion of large amounts
of agricultural output to export markets as
in 1973 and 1975 are striking examples of
cases where there was a great deal of economic
activity going on outside final GNP
sectors. An economic understanding of
those activities and an estimate of their
macro impacts on the GNP could not be
readily derived from demand analysis without
consulting the table of intermediate
flows in I-0 analysis. These are only striking
examples. Many more have arisen in
the past, and more are bound to occur in
the future; therefore, the concern of this
presentation is not with singular events.
An adequate explanation of the price system,
especially on the cost side, cannot stop
2See the author.


### ---Economics-1978-0-04.txt---
at the KLEM level with separate consideration
of energy, materials, wage, and capital
costs. It must take account of prices of
grains, ferrous metals, nonferrous metals,
coal, crude oil, machinery, textiles, and the
other component prices in an input-output
system. The appropriate amount of detail
is not a fixed matter. It depends on human
capabilities of analysis, machine facilities,
data bases, and other practical considerations,
but it is, in any case, an order of
magnitude greater than contemplated by
mainstream macro model analysis.
From an analytical point of view, what is
being suggested is a full combination of two
systems of thought. the Leontief model and
the Keynesian model. That these two svstems
can be put back-to-back into a single
consistent model, with full feedback between
each part, is now well known, having
been implemented first with the Brookings
Model and later with various generations of
Wharton Models, and more recently by
Dale Jorgenson in a translog mode. A principal
feature of such combined systems is
that they are not based on restrictive assumptions
of the fixed coefficient inputoutput
model, but are generalized to allow
the coefVlcients of production to vary, according
to the variation of relative prices.
The above expression, "full feedback,"
means that the macro model of final demand
and national income generation cannot
be solved, by itself, without also solving
the input-output system for generatin- sector
production flows. Moreover sector
prices cannot be solved without also solving
the macro model simultaneously.
Price formation in individual sectors is
specified in terms of mark-up relations over
unit labor costs. Thus, sector outputs and
labor inputs are needed in order to explain
sector prices. These prices are needed, in
turn, in order to explain final demand
prices. Similarly, sector investment depends
on sector output as well as sector price.
It is for these and similar reasons that final
demand cannot be generated without making
use of the input-output system in order
to generate sector outputs.

At the same time, the input-output system
is driven by final demand; therefore,
the conventional macro demand model
must be used in order to solve the inputoutput
system. These are the specific senses
in which full feedback is used in order to
obtain simultaneous and consistent integration
of the entire supply and demand sides
of the economy.

In terms of the history of economic
thought, the above approach means thinking
in terms of the empirical implementation
of the Walrasian system. Essentially,
Tinbergen implemented the Keynesian system
and Leontief implemented a part of the
Walrasian system. By putting the two together,
with due allowance to Kuznets for
making the data bases of final demand and
national income available, a complete synthesis
of supply and demand in the economy
as a whole can be put together. This
gives the antecedents of what is meant by
modeling supply, taking into account what
is needed from demand models at the same
time.

III. Why Model Supply?

At the time of the Keynesian Revolution,
there was a pervasive deficiency of demand
throughout most of the world. The Keynesian
policy development, building on
that model, did, in my opinion, much good
for the economy of the Western world, enabling
us to come through an expansive era
of more than twenty-five years without a recurrence
of a Great Depression. That does
not mean that this system of thought and
policy formation did its work for all time
in putting the world economy on a stable
footing. It carried the situation only so far,
and undoubtedly underestimated inflation
potentials, leaving us now at the point
where new systems of thought, drawing
more on the supply side, are needed in
order to develop policies that will be able
to deal with the world's contemporary economic
problems; hopefully, policies that
will have as much longevity as the demand
management policies of the last two to three
decades. That should bring us nicely into
the twenty-first century, which is about as
far ahead as we might attempt to look at the
present time.


### ---Economics-1978-0-05.txt---
The limits of demand management policies
have become clearly visible in recent
years. Let us look at the issues through the
medium of specific problems, say the joint
problems of too much unemployment and
too much inflation. Policies of demand
management alone have appeared to be
adequate to deal with one or the other, but
not both together. If demand is stimulated
enough to bring down the unemployment
rate to a full-employment minimum, there
is danger of generating undue inflationary
pressure as a side effect. Conversely, antiinflationary
policies of demand restriction
run the danger of generating excessive unemployment
while holding down the inflation
rate.

How might supply-side policies be introduced
to lower both the inflation and unemployment
rates at the same time'? It is
conventionally thought that policies of aggregative
demand stimulus through traditional
fiscal and monetary policies might be
able to bring down the U.S. unemployment
rate to about 5.5 percent. This is not a firm
point estimate, and is subject to error of at
least one-half point above or below that
figure, but it is not, in any case, a full-employment
target figure.

One way, but not the only way, of getting
to full employnment without generating fresh
inflationary pressure is to design a jobs program
for about 1.0 million long-term, hard
core unemployed. This jobs program cannot
be described in full detail in the context
of this presentation, but it is not to be
viewed as an ordinary public jobs program.
It is viewed as a job training program
aimed at people who show signs of receptivity
to training and enlisting the participation
of employers who provide really productive
jobs with potential for upward
mobility. The 1.0 million target, spread over
three years, is not purely indicative. It is
meant to be plausible and necessary if full
employment is to be reached by 1980-82 in
the United States.

Apart from the fact that some public
funds are to be spent on this program, it is
not a typical demand management policy.
It is aimed at increasing the supply of
goods, at raising labor productivity, at sectors
of the economy where job training can
be accomodated or needed, and at sectors
of the labor force. It is basically a supplyside
policy and needs for its implementation/
assessment a full scale analysis through
the medium of a Leontief-Keynes system.
In first approximations, such assessments
have been made with the appropriate version
of the Wharton Model.

In anticipation of criticism of this policy
approach from the side of those who are
strongly wedded to emphasis on demand
management, I want to stress that a jobs
program aimed at increasing productivity
and reducing hard core unemployment is
not a futile exercise in pushing some subsidized
workers into the ranks of the employed
while pushing others out. The program
is intended to have balance; i.e., to be
part of a larger program with corresponding
support from the demand side. Such
support could not be justified from the
point of view of inflation potential unless
steps are being taken to complement the
effect with a jobs program and eventual
lifting of productivity. Undue preoccupation
with demand policies is not going to be
adequate to meet the problems of the day,
nor is pure emphasis on supply. Both sides
of the economy must be coordinated in
policy formation.

It should also be emphasized that demand
policies of federal expenditures for
public service employment appear to be inferior
to private sector jobs programs of
the type being mentioned here. In the
former case, there is no long-term opportunity
for those taken into the program and
there is no contribution to national productivity.
As long as job expenditures are going
to be made, they ought, preferably, to be directed
to an effort that promises to have
some lasting benefit.

This example of the jobs program is one
that fits the contemporary American economic
scene and has been investigated with
a U.S. model and data. The underlying
idea, however, is meant to be much more
general. It is that the whole industrial world
is faced with a series of new supply-side
economic problems. The problems of cyclical
stabilization and reaching full employment


### ---Economics-1978-0-06.txt---
without inflation will have to be
dealt with as before, and the latter will require
some degree of supply-side analysis in
other economies as in the U.S. case, but a
whole new range of economic issues looms
on the horizon. These are development of
new, greater energy supplies, protection of
the environment, controlling the exhaustion
of resources, enhancing agricultural supplies,
balancing population development,
and others of like nature. The juggling of
public budgets, the setting of tax rates, and
the giving of a tone to money market conditions
are not going to deal effectively with
this new class of problems, from the viewpoints
of influencing them in a favorable
direction. Similarly, the demand oriented
model is not going to provide much understanding
of them.

The coming problems of the industrial
economy are not going to be wholly dealt
with or analyzed on the basis of the general
purpose Leontief-Keynes system that is
being advocated here. In many cases, the
unforeseen problems that are bound to arise
are going to be more specialized than can
be conveniently anticipated. In such cases,
the analysis must extend into partial system
analysis giving more detailed and explicit
treatment on the supply side. In terms of
model building that means construction of
many "satellite" systems on the supply side,
as the need arises. At the present time,
many energy satellite systems are being developed
to deal with new fuel processes,
large energy using sectors, and large energy
delivery sectors. These satellite systems are
then all linked, in a technical and consistent
way, with the input-output cum macro
model system. In any event, the intent is to
move the discussion of macroeconomics
and policy formation to a new plane of discourse.


The discussion, thus far, has focused on
the modeling and related policy problems
for the modern industrial economy. The
analysis of the supply side, however, is not a
new issue for the developing economy. A
deficiency of demand analyzed within the
framework of the Keynesian Model has not
generally been thought to be the issue or
approach for dealing with the problems of
economic development. That is not to say
that demand relations are nonexistent or
unimportant for the developing economy. It
is primarily a matter of emphasis. Availability
of fixed capital treated as a limiting
factor in production is central to understanding
the problems facing many developing
economies.

Energy problems of production and use
are already apparent, as are population
control and agricultural production. Where
problems of environmental protection and
resource exhaustion have not yet arisen,
they are bound to occur in significant instances;
therefore, it is wise for the development
economist to be forearmed with a full
model for analysis of both supply and demand
sides.

The centrally planned economies are for
the most part industrial economies and
have the same needs for supply-side analysis.
In their cases, the supply side has perhaps
been excessively developed with inadequate
attention paid to the demand side,
not from the viewpoint of deficient demand
but from the viewpoint of chronic excess
demand, with latent inflationary pressure.
The present analysis attempts to look at a
particular facet of the modern economy,
namely the supply side. That does not imply,
by any means, that monetary analysis
and policy are unimportant. Most of the
supply-side problems have monetary implications
or aspects; therefore, monetary
policy must be appropriate to insure the
smooth working of the supply side of the
economy.

In terms of the analytical apparatus
needed to combine monetary analysis with
the kind of supply-demand model that I
have outlined above, it is a matter of integrating
the flow-of-funds system together
with the input-output and final demand national
income system. It would also be in a
full feedback mode. To complement the
supply-side detail underlying the IS curve,
we would turn to the complete flow-offunds
model to provide background for the
LM curve.

It is my feeling that overall monetary and
fiscal policies have been overworked, with
expectations of results that are not j ustified.


### ---Economics-1978-0-07.txt---
Without downgrading their very important
role, the present message simply says that
a full supply-side analysis must be developed
into which an elaborated IS-LM system
of thought can be fully integrated.
 ## Economics-1979-0


### ---Economics-1979-0-03.txt---
The title of my address implicitly assumes
that economics is itself one of the sciences.
I believe that to be so, and intend as I go on
to indicate more fully in what sense I hold
that view. However, my principal aim in
choosing my topic is not that of claiming
any particular status for economic analysis.
Rather, I want to share with you some observations
I have made over the last six

years as a result of involvement in various
interdisciplinary studies, through reading
the reports of other such studies, or discussing
them with colleagues in various
fields of science.

With increasing frequency natural and
social scientists are indeed finding themselves
thrown together in the study of new
problems that are of great practical importance
for society, and essentially interdisciplinary
in character. Prominent among

these are problems of environmental policy,
such as the protection of air and water
quality. Another class of problems concerns
a desirable long-range mix of technologies
of energy supply, conversion and use. These
two classes of problems overlap, for instance,
with respect to the disposal of nuclear
wastes, heat rejection to the environment,
and in the case of fossil fuels the
as yet poorly understood global and regional
effects of sustained large releases of
carbon dioxide into the atmosphere.
Assembled in pursuit of such studies, our
interdisciplinary group soon finds that its
diverse participants ask different questions;
use different concepts; use different terms
for the same concept and the same term
with different meanings; explicitly or implicitly
make different assumptions; and
perceive different opportunities for empirical
verification which may lead them
to apply different methods to that end. The
result can be politely concealed bewilderment,
possibly a suppressed surge of "weand-
they" feeling, in the worst case a growing
mistrust that only time and sustained
interaction can overcome.

I shall try to illustrate the difficulties of
such interaction by a few examples from recent
studies involving, besides economics,
mostly the natural sciences and engineering.
Limitations of experience, background, and
time have compelled me to omit examples
involving a strong participation from the
other social sciences. Had my guide, mentor,
and dear friend Jacob Marschak lived
to give this address, and had he chosen a
similar topic, the social sciences would have
received an emphasis reflecting their importance
to the problems of contemporary society.
The writings Marschak left us, and
the program and the Proceedings of last
year's meeting of the American Economic
Association, stand together as a monument
to his awareness and vision of the
actual and potential contributions of the
social and behavioral sciences.
To prepare for the task I have set myself,
I have requested and obtained interviews
with a somewhat casually selected sample
of natural scientists and engineers, and with
a few colleagues in economics. Their responses
have been drawn on in the preparation
of this address, without attribution by
name. I here express my, and indeed our,
indebtedness for the help we have been
given. Later on, I will cite some statements
verbatim.

Table 1 can serve as a two-dimensional
table of contents for my discussion. Three
topics of study are listed on the left. On
each of the three topics a recent study has


### ---Economics-1979-0-04.txt---
TABLE 1-ISSUES AND METHODS IN ESTIMATING BENEFITS AND COSTS
Estimation From

Prod ucMeasures

of Value tion

Process

Health and other Market

and Technical Behav- Dis- UncerIllustrative
GNP Life Energy Data ior counting tainty
Decisioni Problems (a) (b) (c) (d) (e) (f) (g)
(I) Helium v V v

Conservation

(2) Technology Mix -X v V v

ol Future Energy

Supply and Use

(3) Automobile v

En ission

Control

Note: Check marks designate issues or methods discussed or mentioned for each illustrative
problem.

been made by or for the National Research
Council. I will draw mostly on the first two
studies and briefly mention the third.
My principal intent is not that of criticizing
these studies or of evaluating their findings.
I want merely to identify some of the
issues that arise in their formulation, contrast
responses to these issues in different
professions, and comment on the methods
that have been or might have been proposed
or applied by the respective collaborating
professions. Those issues and

methods that I shall have time to refer to
are set out along the top of the table.
Each check mark in a cell of the table indicates
that a reference is made to that issue
or method in my discussion of that topic of
study.

1. The Case for Helium Conservation
This study is described in the preface to
the report of the Helium Study Committee
as "a task that had to be undertaken
quickly and completed with great speed."'
Likewise, on the concluding page (40), it is
called a "preliminary analysis.'
The principal current source of helium is
as an optional by-product of the production
of natural gas, in which it may occur in concentrations
ranging (by volume) from 10

percent on down with increasing costs of
separation. Present demand for various industrial
and space uses falls below present
supply, and a program of storage in the partially
depleted Cliffside gas field near
Amarillo, Texas, is in operation. The study
is motivated by the anticipation of a substantially
higher future demand.

The report of the Helium Study Committee
lists, on pages 35-36, five steps that
can be taken for the purpose of increasing
the rate of storage. I paraphrase:
Step i: Stop the current venting of
helium which has been separated from
natural gas allocated as a feedstock to
petro-chemical industries. Store the
helium instead.

Step ii: Designate helium currently
stored in Cliffside a "national strategic
reserve" for possible major technical
changes that may greatly expand future
demand.

Step iii: Reactivate presently idle
separation plants to reduce the release of


### ---Economics-1979-0-05.txt---
helium resulting from productive combustion
of the host gas, and store the
helium instead.

Step iv: Build new helium separation
plants on helium-rich gas streams. Store
the helium.

Step v: Delay the use of helium-rich
gas fields, undeveloped, and already producing.


Ultimate Step: Extract helium from the
atmosphere.

The ultimate step is not included in the
report of the Committee, but is mentioned
in the transcript, page 135, of the Public
Forum held as part of the study. It involves
a process that by present technology costs a
large multiple of the cost of extraction from
natural gas containing .3 to .5 percent
helium.

Steps i, iii, iv, and the "ultimate step"
consist of successive technical process
choices. Taken in that order, they correspond
to the economist's notion of a
long-run supply curve, indicating how the
cost of each additional unit of supply is a
rising step function of the cumulative supply
up to that point assuming a constant
state of the technology of extraction. These
steps need to be carried out only according
as the expectation of demand growth becomes
larger and firmer. Steps ii and v are
steps whose timing should depend on additional
factors besides the separation cost
sequence already mentioned.

The expectation of a much larger demand
well into the twenty-first century is documented,
in the report and in the Forum, by
a fascinating enumeration of anticipated
future technologies. Many of these are based
on or may utilize superconductivity, so far
attained best by cryogenic techniques for
which helium is the working fluid. The
superconductivity is in turn expected to be
applicable to a number of uses, such as
power transmission with low energy loss,
energy storage, and a number of applications
of strong magnetic fields. Among the
latter are several "technologies that either
do not now exist or are in early stages of
development" (pp. 13-18), such as magnetic
containment for nuclear fusion reactors.
Another possible application is magnetohydrodynamic
(MHD) power generation

that converts some of the energy contained
in a high temperature gas stream from either
a coal-fired burner or a nuclear fission reactor
directly into electricity, instead of routing
all the energy through a conventional
steam cycle. The MHD development is
further along in the Soviet Union. There
also is a development furthest along in
West Germany and Japan-of magnetically
levitated low-noise high-speed trains.
It should be added that research is in
progress on the use of aluminum and possibly
other materials reaching low resistivity
at temperatures of 20-30?K, a range reachable
using liquid hydrogen as a coolant.
(See L. A. Hall, National Bureau of Standards
Report, and E. B. Forsyth et al.)
From the economic point of view, the
case for the helium storage program is not
convincingly made either in the report or
in the Forum. I have not found either cost
or benefit estimates for the program. Actually,
because of the importance of energy
supply processes among the increased uses
of helium listed above, the benefits cannot
be estimated without comparable cost and
fuel availability estimates for alternative
energy supply and use technologies which
have low or zero helium requirements. In
other words, to assess the helium storage
program, one also needs a long-run model
of the energy sector of the economy that
addresses the second decision problem of
Table 1. One will, of course, also need to
consider other important helium uses that
are not directly energy related.
Before turning to problem (2), I draw attention
to a few passages in the Helium Report
that will provide background for Sections
III and IV below, where I shall discuss
the choice of measures of value in which to
express benefits and costs. The report
(p. 23) contains an important piece of information
bearing on cost comparisons, to
the effect that the energy requirements for


### ---Economics-1979-0-06.txt---
extracting helium from the atmosphere are
about 1,000 times those for extracting it
from natural gas containing .3 to .5 percent
of helium. Large as that figure is, I shall describe
later the economist's case (Section
III) for including in the calculation inputs
other than energy, such as that for plant
and equipment, and (Section IV) for taking
into account that the costs of all kinds in
any required future extraction from the atmosphere
will not be incurred until much
later.

In fact, one statement in the report reads
as a rejection of the idea that the time at
which capital cost is incurred is at all pertinent.
In a description of the possible role
of the government in implementing the five
steps, the report says: "The burden of the
discount rate as a criterion of performance
could be eliminated and the present debt to
the U.S. Treasury written off' (p. 38). I
shall explain in Section IV why I think not
many economists will support the proposal
to eliminate the criterion of the discount
rate. Meanwhile, the statement leads one to
infer that the capital cost component is not
negligible as a factor in the decision.
II. Technology Mixes of Future
Energy Supply and Use

My second illustration is a study that was
carried out as an input to the deliberations
of the Committee on Nuclear and Alternative
Energy Systems (CONAES, in short),
and of its Synthesis Panel. It is entitled
"Energy Modeling for an Uncertain Future.
" As explained in the preface, it is a
"supporting paper" published without having
gone through the customary report review
procedure of the Academy. While for
the other illustrations I have not named
authors or committee chairpersons, I
should not conceal that I was the chairman
of the group, called the Modeling Resource
Group (MRG), which collectively did the
work described in its Report. The group
consisted mostly of economists and operations
researchers, two somewhat like-minded
professions. My comments on interdisciplinary
interactions about the ideas and
findings of the group will therefore draw
on discussions with members of other professions
within and outside CONA ES.

For that purpose it may suffice to give
only the briefest description of the questions
addressed, the assumptions made, and
the methods used. One important question
arises from the fact that several competing
objectives enter into the choice of a longrun
energy technology mix. The net economic
effect (economic benefit minus cost)
of the development of a given technology
mix can be estimated in a crude way, as suggested
in Table 1, column (a), by its effect
on the Gross National Product (GNP). In
addition, one will also want to register risks
of adverse effects such as mining accidents,
air pollution, acid rain, oil spills, possible
leakage from nuclear waste disposal, or diversion
or proliferation of weapons-grade
nuclear materials. For brevity, all such effects
will be called "environmental" effects.
The place in Table 1 for these impacts is
column (b), tersely dubbed "health and
life."

The Risk/Impact Panel of CONAES decided
not to try to estimate money equivalents
for such adverse impacts of various
magnitudes. Were such estimates possible
and available, then one could also define
and find a balance between desired benefits
and adverse "environmental" impacts that
remain after scrubbers, inspectors, Civex
and the like have done their jobs. Not having
such estimates, the MRG turned the
question around: Assume that tentatively
chosen upper bounds are imposed on the
use of technologies that have such impacts.
Estimate the loss in GNP associated with
these bounds. Then that number also places
a price tag on the reduction in "environmental"
impacts achieved by those bounds.
Thus, even if an a priori valuation of the
reduction in impacts is not available, then
such a valuation is still implicit in any
decision actually taken. It may help the
decision makers to know these implicit
price tags.

I will list only the principal assumptions
made for this purpose. Numerical values
were assigned to three sets of variables. As


### ---Economics-1979-0-07.txt---
principal exogenous, also called "realization,
" variables we chose:

R1. The growth rate of real GNP, out to
2010, in the absence of new environmental
bounds on energy technologies.


R2. Capital cost levels of present and potential
future energy technologies.

R3. Availabilities of oil, gas, uranium, at
various costs of extraction.

R4. Long-run price and income elasticities
of demand for end-use energy

forms.

The "policy" variables represent the hypothetical
bounds already described,

P1. Moratoria on new nuclear construction.


P2. Limits placed on output of coal and
shale oil.

Forming a third category, the "blend" variables
have traits of both realization and
policy variables,

Bi. Discount rates applied to future
benefits and costs.

B2. Oil import price or quantity ceilings.
The method applied was to compare the
already specified projection of a rising future
GNP in which no bounds have been imposed
on the use of energy technologies (the
base case), with other projections in which
such a bound or bounds were imposed. This
procedure was carried out for each of three
long-run models of the U.S. energy sector.
The numerical inputs into the three models
were the same for almost all realization and
blend variables, except for the price and income
elasticities of demand, which were
specific to each model. Two ideas central to
current economic analysis entered into this
procedure. One is the use of an optimization
algorithm to simulate the behavior of a
competitive market economy, in any one
year, and through time. The other is the use
of long-run elasticities of demand. For demand
by end-use consumers, these are to be
based on econometric analysis of timeseries
and/or cross sections of income,
prices, and quantities consumed. For industrial
demand, a process analysis of alternative
industrial energy-using processes
may add valuable information.

The principal finding was the proportionally
small effect on GNP of sizable cuts
in energy use below its base case growth
path. In interpreting this finding, note that
the optimization procedure implies an assumption
whereby the economy responds to
anticipated changes by minimizing the cost
of adaptation. The principal means of adaptation
are changes in the type and composition
of the capital equipment for the extraction,
conversion, transport, and use of
energy-at the regular time for replacement
or earlier.

Table 2 shows the numerical results. For
two models, with price elasticities of -.25
and -.4, respectively, policies entailing percentage
cuts in energy use out of the base
case that gradually increase to between 10
and 20 percent in the year 2010, were found
to cut not more than 2 percent out of the
discounted sum of annual real GNP, 1975-
2010, and a comparably small percentage
out of GNP for the single year 2010. The
instruments of the curtailment of growth in
energy use were, in row (1) of the table, the
placement of bounds on specific energy supply
technologies described above. In row
(2), a zero-energy-growth path is simulated
by the imposition of a hypothetical "conservation
tax" on primary energy flows. The
rate of this fictitious tax must increase as
the GNP continues to grow in spite of the
downward pull from the zero-energy-growth
path.

Another finding, reported in row (2), was
that for the effects of the larger cuts in
energy use, the value of the long-run price
elasticity of demand for energy becomes
crucial. In a sensitivity analysis made with
one model, a zero-energy-growth policy
from 1975 on, leading to a 60 percent cut in
energy use out of the base case in 2010,
was found to induce only a 2 percent cut in
cumulative discounted GNP if that price
elasticity is -.5, but a 30 percent cut if it is


### ---Economics-1979-0-08.txt---
TABLE 2-ESTIMATED FEEDBACK FROM
CURTAILED GROWTH OF ENERGY USE TO GNP,
1975-2010, UNITED STATES

Reduction out of

the Base Case in

Discounteda

Price

Energy Sum of Elasticity

Use in GNP of Demand

Policies 2010 1975-2010 for Energy
(1) Bounds on

Specific Up to 20% 1 to 2% -.25
Tech- or -.4

nologies

(2) Zero

Energy 600 { to 2% -.5

Growth ' up to 30% -.25

through

Conservation

Tax

Source: MRG report, Tables 111.22 and 111.23.
aDiscount Rate: 6 percent per annum.
-.25. I shall come back.later to the estimation
of the elasticity parameters found to
have been very important.

111. Interdisciplinary Differences in Outlook
We have now assembled enough reference
material for us to make a start with our
main topic-the way in which differences in
outlook between the disciplines affect the
conduct and evaluation of joint studies.
The most significant difference between
economics and the natural sciences lies in
the opportunities for testing and verification
of hypotheses. Jacob Marschak used to say
that economists carry the combined burdens
of meteorologists and engineers. Like
the meteorologists, they are expected to predict
the future course of important variables
in their field of study. Just as engineers
design more and more efficient
machines, economists are also expected to
improve the design of society where it affects
good use of resources. But, like the
meteorologist, the economist has traditionally
been confined to drawing inferences
from passive observations, records of data
generated by the turbulence of the atmosphere
or the fluctuations and trends of
economic life. Finally-a very important
difference-meteorologists and engineers
have all the laws and measurements established
by physics and chemistry available
to them, fully documented by experimental
tests and results.

Traditionally, economists have not
searched for similar inputs from experimental
or observational research of a
psychological or sociological nature. In the
1950's and early 1960's they have engaged
in some experimentation of their own on behavior
under uncertainty and in bargaining
and gaming situations. However, the findings
of this work have not been put to use
as premises for modeling an entire economy.
For that purpose, over a few articulate protests,
many economists have been satisfied
to postulate simple rules of behavior by
consumers and business firms. The terms
"introspection" and "casual empiricism"
have been used to describe the cognitive
sources of these premises. In the version of
the currently dominant "neoclassical"
school of thought, these premises express
optimizing responses of demand and supply
to a uniform price system: satisfaction
maximizing by consumers, profit maximizing
by firms.

These premises have a certain intuitive
plausibility about them. Undoubtedly, their
widespread adoption has also been aided by
the richness of the body of inferences one
can draw from them. In fact, the premises
form the logical foundation for the paradigm
of neoclassical economics: the concept of an
equilibrium of prices and quantities that in
some way ties together the economic decisions
taken by all seemingly independent
agents. Conceptually the prices and the
quantity responses may describe a stationary
state over an extended period of
time. More realistically, they may be dated
variables and thus also link decisions that
vary over successive periods, to sustain a
moving intertemporal equilibrium.
Parenthetically, use of the term equilibrium
does not imply an assumption that


### ---Economics-1979-0-09.txt---
the real economy actually is at any time "in
equilibrium." Rather, the notion of equilibrium
is a first approximation, a reference
point or path, like the cycles of Ptolemy
without the epicycles and the eccentricity.
If the market were to extend to all pertinent
economic decisions over the entire
period considered, the result of an intertemporal
equilibrium would be an "efficient"
path of the economy in the limited
sense that no one can be made better off at
any time without someone being made
worse off at some time. Where market
power interferes with competition, or where
important economic decisions are made at
government levels, the instinct of the neoclassical
economist is to recommend that
legislation, regulation, the use of suitable
incentives, or direct government decision
either restore or mimic the operation of the
competitive market.

In the present context, an important trait
of the neoclassical model is that it does not
postulate one sole primary resource, be it
labor, energy or any other, whose scarcity
controls that of all other goods, and which
thereby becomes a natural unit of value for
all other goods. The model of production is
such that-not by logical necessity, but as
an empirical fact--any primary input to
production can be substituted to some extent
for any other. If such substitution does
not take place within one-and-the-same
production process, then it can still come
about through suitable changes in the levels
of several processes and in the inputs to
these. In this view "the energy problem" is
not one of just "saving energy," regardless
of the cost in other resources. It is rather
one of seeing to it that the increasing real
cost of domestic energy extraction and supply,
and the increased market power of
OPEC, are-over time-reflected in the
real prices of primary energy forms relative
to other primary inputs, and thereby in different
degrees in the prices of all other
goods and services. In the projections described
above, the energy prices are calculated
so as to be in balance with an efficient
path of the technology mix into the
future, and thereby to induce the right
amount of energy saving. In particular if, as
projected by MRG, real prices of primary
energy rise in this path, then energy use is
projected to grow less than proportionally
to GNP.

The contrary doctrine-that regardless of
prices there is a persistent relationship between
energy use and GNP-has frequently
been expressed in the engineering literature.
In line with this observation, the MRG finding
of a possible small impact on GNP of
incisive bounds on specific energy technologies
led to lively correspondence and
discussions with members of CONA ES, and
of its Supply/Delivery Panel, an engineering-
oriented group. I should add that the
MRG study was not the first modeling study
to cast doubt on the doctrine referred to.
By my knowledge the first was a study by
Edward Hudson and Dale Jorgenson.
IV. Discounting Future Benefits and Costs
We are now ready for a closer look at the
discounting of future benefits and costs.
This practice reflects a simple technological
fact combined with the paradigm of equilibrium
over time. The simple fact is thatshort
of capital saturation-society can
temporarily curtail the production of current
consumption goods by transferring
some factors of production to the formation
of additional suitable capital goods,
in such a way as to return a multiple (> 1) of
the same unit bundle of consumption goods
in the future. Efficient intertemporal equilibrium
then demands that the present

value of the goods returned to consumption
be equal to that of the goods not now
consumed. The quantity of the future
bundle being larger, its per-unit present
value must be correspondingly lower. In a
projection that gives to one unit of the future
bundle a future real market price numerically
equal to its current price, a discount
factor d < 1 must be applied to the
future market price to obtain the present
value, per unit of the future bundle. Given
competitive markets for capital, present
goods and future goods, and ignoring differences
in risk, different investments bearing


### ---Economics-1979-0-10.txt---
fruit in the same future year t will tend
to give rise to the same discount factor
I t

dt + )

where rt is the annual discount rate applicable
to the period from year zero to year
t. The usual practice in cost-benefit analyses
is to assume also that rt is independent of
t, rt = r, say.

This reasoning simply registers the economic
accounting implications of assumed
intertemporal efficiency with capital nonsaturation.
To many highly educated

people, there is something ethically offensive
about it.

A difficult practical problem on which
economists still differ among themselves is
how to read a good estimate for r from
capital market and other data. Different tax
rates on corporate and individual incomes
complicate the problem. Considering this
and various market imperfections, the precluded
alternative use of funds drawn upon
for a public project also enters into the
choice of r. I will not venture into these
questions here.

Coming back to helium storage, the discounting
criterion would lead most economists
to recommend that those steps of the
storage program be implemented for which
the rate of return on the total investment
(not that on energy alone) exceeds or equals
the discount rate appropriate to the problem.
Step i, storing helium currently vented,
is likely to meet the test. The problem is to
estimate which of the four or six steps
would.

Two final remarks, the first added as an
afterthought since August 30.

The two issues we have discussedwhether
to count only energy costs or all
costs, and whether or not to discount future
benefits and costs-are logically distinct
implications of the notion of intertemporal
equilibrium. However, psychologically they
are related. If one counts only energy costs,
everything is expressible in equivalent
Btu's, and to the physicist, steeped in the
law of conservation of energy, Btu's are the
same everywhere and at all times. To discount
future Btu's therefore seems not just
strange but outright wrong. So it is. But the
economist does not discount quantities of
any kind. He discounts only real values,
that is, quantities (including energy) multiplied
by real prices that reflect the expected
balance of cost and preference as of a
specified future time and beyond. It is to
these prices that the discount factor is applied.
I am hoping that this simple distinction
may help to reduce misunderstanding
between the professions.

Secondly, our reasoning has proceeded
blandly as if there were no uncertainty about
the outcome of the development of processes
expected to be substantial users of helium.
If there is considerable uncertainty, economists
may want to add an allowance for risk
to the discount rate. They may also wish to
experiment with models in which judgmental
probabilities are attached to these
uncertain outcomes. This device may produce
insights even if the conclusions depend
on admittedly uncertain premises. A
study of this kind is included as chapter IV
in the MRG report.

V. Attaching Values to Health and Life
The question of estimating the value of
health and the value of life arises mostly in
contexts where either public decisions, or
public monitoring of private decisions, can
be shaped so as to improve health and prolong
life. One example is the investment of
public funds to diminish physical risks to
traffic by the design of roads, bridges, turnouts,
and crossings. Another is regular expenditures
for traffic police, building inspectors,
and other law enforcers who

restrain some people from killing or hurting
themselves or others by recklessness or
neglect.

A common trait of these decisions is that
from good experience records one may be
able to estimate the years of lives saved,
perhaps also of health and limbs preserved,
per dollar spent on efficiently run projects
or activities of this kind. Such calculations
make it possible to spot discrepancies between
different projects in regard to "health


### ---Economics-1979-0-11.txt---
and life benefits" bought per dollar spent.
The ideal of equilibrium then suggests redistributing
expenditures, if needed, in order
to maximize total benefits from the
given expenditure for protection. Valuations
of health or life that have a modicum
of public approval could result from such
redistribution. Note that these valuations,
also called shadowv prices, are in efTect set by
the budgetary decision makers, whether
they are aware of it or not.

After such redistribution if called for, the
calculation of money values of health and
life registers what in good practice we consistently
spend to save a life. The process
recognizes that, disturbing as it is to our
sensibilities, society is being compelled by
the facts of technology and behavior to set
up equivalences between lives of unidentified
people and bundles of goods and services
implicitly of the same market or
shadow value - --thus bracketing contemporary
lives together with current goods
and services in the same category of exchangeables.


The examples given so far concern small
to moderate risks affecting small to moderate
numbers of people, less than 100 at a
time, say. Moreover, the time intervals between
the decision to commit funds for the
reduction of risks, the actual expenditure of
these funds, and the reaping of benefits
therefrom are moderate, less than twenty
years, say. Finally, the problems are mostly
local or national, not international in scope.
The long-run choices between energy
technology mixes are different in these respects.
By a gradual shift from oil and gas
to coal, fossil fuels can remain a principal
source of energy for countries with abundant
resources of coal, especially the United
States, the USSR, and China, for a long
time to come. Intensive current discussion
with regard to this option concerns the possible
climatic effects of the increase in the
atmospheric concentration of carbon dioxide
caused by continued large-scale combustion
of fossil fuels or their derivatives,
alongside with world-wide deforestation.
Among the large-scale effects held possible
are an increase in average global
temperature, entailing dislocation of agriculture
depending on how each region is
affected, and an increase in the level of the
oceans due to the melting of polar ice not
previously floating. The present state of
knowledge is not such as to be ready for an
assessment of these risks. New hypotheses
and observations appear regularly in the
pages of Science and other journals. So I
would describe this problem as involving an
unknown risk to a large number of people.
If current estimates of the capital cost of
central station solar power are realistic, the
principal alternative to fossil fuels for bulk
power generation is nuclear power. I am
not qualified to even comment on the reactor
safety and waste disposal problems. I
assume, however, that the developers of
these technologies would classify these
problems in terms of very small risks to
substantial numbers of people. Perhaps this
leaves as the principal concern the difficulty
of keeping industrial and weapons use of
nuclear materials apart. Since on this one
we are all groping in the dark, I feel I should
describe this aspect of nuclear technology
as an unknown risk to a very large number
of people.

I cannot see my way through to a calculus
of the value of human lives in large
numbers, that would help clarify issues of
the scope of those just discussed although
estimates of numbers of lives at risk are
and will remain important. These are
basically problems for judgment, even
though the need for making these judgments
will weigh hard on the people called
upon to make them. But supposing I should
be wrong, let me point to one apparent
paradox to be faced in any attempt to
bring a calculus of the shadow price of
human life to bear on problems with a long
time span.

Suppose one accepts as an ethical principle
that, in balancing risks to human life
in the present and in the future, equal numbers
of lives should receive equal weight.
This would make the present value of the
future human life independent of the time
at which it is lived. However, we have seen
that as long as capital saturation is not attained,


### ---Economics-1979-0-12.txt---
the present value of a standard bundle
of goods in the future decreases as that
future time recedes. Hence the present value
of future life relative to that of future goods
will be much higher than the value of present
life in relation to present goods. It should
not be inferred from this that future decision
makers are assumed or advised to devote
greater resources to safety and health
than the present decision makers, although
the future ones may well want to do this for
reasons of their own. The inference is rather,
I submit, that the present values I have described
reflect a curious mixture of three
ingredients: one intertemporal ethical rule,
present preferences between consumption
and protection, and an assumption about
savings behavior of all generations within
the next fifty years, say. Under these assumptions,
sets of "present values" formed
at successive points in time need not, and
generally will not, be consistent with each
other.

VI. The Empirical Basis of

Quantitative Economics

I now go on to a discussion of the empirical
basis for some of the quantitative
statements that economics contributes to
interdisciplinary studies. I will again illustrate
this question with reference to the
few studies I have chosen as examples. At
the same time I will emphasize the role that
the premises underlying the concept of
equilibrium play in this process.
The premise of profit maximization implies
a subpremise of cost minimization.
I regard that subpremise as fitting reality
more closely than the entire premise. It
underlies the supply side of the MRG study
of future energy technology mixes I have
described.

The premise of maximization of satisfaction
by the consumer can be made more
plausible and more applicable by a further
specification. Applied to energy, it says that
successive equal additions to a consumer's
annual energy end-use budget are worth
less and less to him. Operationally, how
much each successive addition is worth to
him can be measured, for instance, by that
increase in the expenditure for the rest of
his consumption that he would have regarded
as equivalent to each next addition
to his energy consumption.

This specification implies the existence of
a household demand function for energy, in
which per capita demand for energy decreases
as its price increases, and increases
as per capita real income increases. The
MRG extended this concept to the sum of
direct and indirect demand for energy, the
latter being the energy used as input to the
production of all nonenergy goods, including
capital goods as well. Another extension
distinguishes demand for individual
fuels, where the demand for one fuel increases
if the price of another competing
fuel goes up.

These functions are then estimated from
empirical data. In the procedure followed
in the model with the estimated long-run
price elasticity of -.4 mentioned above, a
parametric form of these functions was
fitted to cross-section and time-series data
for seven OECD countries, including the
United States, for the period 1955-72. In
the model with price elasticity -.25 the
estimation procedure was not stated with
comparable explicitness. In both models the
estimated long-run demand functions,
written with price as a function of quantity,
were then integrated to estimate the benefit
from the consumption of energy in all
forms.

By comparison, the empirical basis for
the production side is more direct. Each of
the various competing energy producing,
converting, and using processes is represented
by constant ratios of inputs to outputs,
reflecting operating experience where
available, or based on estimates of such
ratios and of future availability dates for
processes not yet developed. For instance,
process estimates for the years 1985 and
2000 were drawn upon in estimating the
elasticity of substitution between electric
and nonelectric energy in the second of the
two models just discussed. This did constrain
but not by itself imply numerical
estimates of the elasticities of demand for


### ---Economics-1979-0-13.txt---
energy, whether in toto (given as -.25), or
for the two components.

This completes my description of the empirical
basis for the MRG procedures and
the premises on which they rest. I want,
in passing, to draw attention at this point
to the econometric aspects of another
study, designed to estimate perceived benefits
of air quality improvements from residential
property values in areas with different
air quality. The study is entitled "The
Costs and Benefits of Automobile Emission
Controls." It draws on a body of
econometric work in which property values
are related to various characteristics of the
site and the neighborhood, including air
quality and other environmental amenities,
and the income of the household.
The foregoing examples lead me to some
broader remarks on the empirical basis of
quantitative economic knowledge in general,
not limited to the type of studies we
are here mostly concerned with.
In all formal procedures involving statistical
testing or estimation, there are explicitly
stated but untested hypotheses,
often called "maintained" hypotheses by
statisticians. In the econometric studies we
have here considered, the "premises" already
discussed play that role. More in
general, any statement resulting from such
studies retains the form of an "if ... then . . ."
statement. The set of "ifs," sometimes called
"the model," is crucial to the meaning of
the "thens," usually but somewhat inaccurately
called the "findings." For instance,
in fitting demand relations, the principal
maintained hypotheses specify the variables
entering into these relations, and possibly
other variables with which these variables
are in turn linked in other pertinent relations.


The "if ... then ..." statements are similar
to those in the formal sciences. They
read like logical or mathematical reasoning
in the case of economic theory, and like applications
of statistical methods in the case
of econometric estimation or testing. The
heart of substantive economics is what can
be learned about the validity of the "ifs"
themselves, including the "premises" discussed
above. "Thens" contradicted by observation
call, as time goes on, for modification
of the list of "ifs" used. Absence of
such contradiction gradually conveys survivor
status to the "ifs" in question. So I do
think a certain record of noncontradiction
gradually becomes one of tentative confirmation.
But the process of confirmation
is slow and diffuse.

For some purposes, and at considerable
expense, short cuts can be made to diminish
the dependence on untested "ifs." I
am speaking of systematic experiments such
as the so-called negative income tax experiment
conducted in New Jersey over the
period 1968-72, and followed by similar income
maintenance experiments in other
areas of the United States. If one wants to
know whether income maintenance payments
to families near the poverty line have
a disincentive effect, or no effect, or even a
positive incentive effect on labor supply,
one does not need to have a pretested theory
as to what, if anything, the family is maximizing.
Instead, one can make such payments
to a sample of families and compare
its behavior with that of an unaided
control group. This is what the New Jersey
experiment did. In one category of families
where the numbers spoke rather clearlywhite
husband-and-wife whole familiesthe
effect on labor supply was found to be
negative, moderate but statistically significant,
and with the effect on the husband's
labor supply smaller than that on the wife's.
In addition, much was learned about the design,
conduct, and evaluation of such experiments
for use in later studies.

There have not been many such experiments
on a scale needed to obtain statistically
significant outcomes. Moreover,
they have been limited to questions of
great and urgent policy importance. Meanwhile,
we do need to find ways in which
verification of the premises of economics,
through cumulative econometric analyses
and through experiments that find a sponsor,
can be pursued.

I have not found in the literature a persuasive
account of how such confirmation
of premises can be perceived and documented.


### ---Economics-1979-0-14.txt---
How do we keep track of the contradictions
and confirmations? How do we

keep the score of surviving hypotheses?
And what are we doing in those directions?
The same questions have been raised before,
among others by my predecessors,
Wassily Leontief and Robert Aaron Gordon,
and good and bad examples of concern
and unconcern were referred to by
both of them. Meanwhile, unresolved issues,
sometimes important ones from the policy
point of view, and mostly quantitative ones,
drag on and remain unresolved. Do they
have to?

With one exception I am aware of, even
our best college-level introductory texts of
economics do not press these questions.
They teach good reasoning, and describe
the views of leading minds and schools of
thought, present and past, in the field.
Texts in econometrics teach with great care
how to test assumptions and to estimate
parameters, duly emphasizing the crucial
role of the models. What is also needed is
to teach the tested and confirmed statements.


VII. Aphorisms on Interactions
After all I have said about the need for
empirical validation, I owe you a brief report
on my own casual-empirical sample
study of the difficulties of interaction between
scientists, engineers, and economists,
as seen by participants in joint studies.
Rather than classifying and tabulating the
views expressed, I shall let the respondents
speak for themselves. The following is a
selection (by me) of statements, drawn from
my notes, that carried the most punch.
A physical scientist: "Economists are
technological radicals. They assume everything
can be done."

A geologist: "Economists have been
too enthusiastic about deep sea mining.
They think there is more than there is, that
it is easier to get up than it is, and easier to
process than it is."

A development economist: "Scientists
think big. Economists are marginalists.
Scientists don't think in terms of opportunity
cost."

An engineer: "Economics is not dismal
but incomplete. The things missed are very
important."'

A life scientist: "Market imperfection
is more widespread than economists care to
admit."

An economist: "Where economists see
the invisible hand guiding the market place
to produce pretty good outcomes, scientists
see only chaos."

An engineer: "The economic motive is
overrated."

A psychologist: "All the conclusions
that are drawn from the assumption of rationality
can also be drawn from assumptions
of adaptive behavior."

A life scientist: "Economists have great
skill in handling data. However, they tend
to ask only for data, not for concepts and
ideas. Drawing up a model is an interdisciplinary
task."

An engineer: "Economists often use
smooth production functions even when
engineers might be reluctant to do so."
A life scientist: "Many scientists do not
understand discounting."

An engineer: "Economics is the Thermodynarnics
of the Social Sciences. Everything
is deduced from a few simple postulates
without the necessity for knowing
detailed mechanisms."

VIII. Final Remarks

After this instructive intermezzo, allow
me a few final words. I will not be able to
match the brevity and incisiveness we just
savored. However, I do look on the collaboration
of the diverse professions involved
in the newly discovered joint problems
as an important development. To
economists it is a new challenge and a new
frontier. Among the problems themselves
are some of great importance, nationally
and internationally. They deserve the best
effort and talent that can be brought to
bear, within and across the disciplines.
An important talent requiring cultivation
is skill in communication between dislThe
reference is to the need to fit environmental
protection into economic analysis.


### ---Economics-1979-0-15.txt---
ciplines. We should begin with the defusing
of jargon. Perhaps some terms should be
explained at first use. To the physicist who
has used calculus on problems going back
to Isaac Newton, it is unexpected to learn
that everything called "marginal" is a first
derivative of something. It appears natural
to him, however, to learn that an "elasticity"
is the dimensionless slope of a curve
plotted on double-log paper. There is more
trouble lying in wait with "externalities,"
an institutional concept presupposing private
property, or at least an accountability
for private or public production or household
decisions that is dispersed over individuals
and organizations. If we will be
more forthcoming with explanations of our
cherished terms, our science colleagues may
be more inclined to help us out with "entropy,
" which to me is a more difficult concept
than anything economics has to offer.
A more serious problem is that, while our
universities are the principal training ground
for future scientists of all kinds, they do not
seem to be the best place for gaining experience
in interdisciplinary interaction. I
believe that the root of the difficulty lies in
the procedures for academic appointment
and promotion. The initiative, the decisive
first step, is usually taken in the department
of one's own discipline. Young faculty
members must prove their worth first to
their senior colleagues in the field they are
identified with. A joint appointment holds
somewhat less promise as a stepping stone
to tenure. Even our graduate students are
already aware of these factors.
The increasing demand for the contributions
of interdisciplinarians may gradually
break the barriers down. Progress will be
slow unless university faculties and administrations
perceive the problem. Once they
do, the irrepressible curiosity and venturesomeness
of our undergraduates will provide
a point at which to start and from
which to build up.
 ## Economics-1980-0


### ---Economics-1980-0-01.txt---
There is a long-standing tension in economics
between belief in the advantages of
the market mechanism and awareness of its
imperfections. 'Ever since Adam Smith,
economists have been distinguished from
lesser mortals by their understanding of and
-I think one has to say-their admiration
for the efficiency, anonymity, and subtlety
of decentralized competitive markets as an
instrument for the allocation of resources
and the imputation of incomes. I think we
all know this; for confirmation one can look
at the results of a paper (James Kearl et al.)
presented at the last annual meeting, reporting
the responses of professional economists
to a sort of survey of technical opinion. The
propositions which generated the greatest
degree of consensus were those asserting the
advantages of free trade and flexible exchange
rates, favoring cash transfers over
those in kind, and noting the disadvantages
of rent controls, interest rate ceilings, and
minimum wage laws.

Views on these policy issues did not seem
to represent mere conservative ideology:
half of the respondents agreed and another
30 percent agreed "with provisions" that
redistribution of income (presumably toward
the poorest) is a legitimate function
of government policy. The profession's reservations
about rent control, interest rate
ceilings, and minimum wage laws do not
appear to reflect a rejection of the goals of
those measures, but rather a feeling that
nonprofessionals simply do not understand
fully the consequences, often unexpected
and undesired, of messing around with the
market mechanism. Most of us are conscious
of a conflict that arises in our minds
and consciences because, while we think it is
usually a mistake to fiddle the price system
to achieve distributional goals, we realize
that the public and the political process are
perversely more willing to do that than to
make the direct transfers we would prefer. If
we oppose all distorting transfers, we end up
opposing transfers altogether. Some of us
seem to welcome the excuse, but most of us
feel uncomfortable. I don't think there is
any very good way to resolve that conflict in
practice.

Simultaneously, however, there is an important
current in economics that focuses
on the flaws in the price system, the ways
that real markets fail because they lack
some of the characteristics that make idealized
markets so attractive. I think that
outsiders, who tend to see economists as
simple-minded marketeers, would be
astonished to learn how much of the history
of modern economic analysis can be written
in terms of the study of the sources of
market failure. The catalog runs from natural
and artificial monopoly, to monopolistic
competition, to the importance of public
goods and externalities of many other kinds,
to-most recently-a variety of problems
connected with the inadequate, imperfect,
or asymmetric transmission of information
and with the likelihood that there will simply
be no markets for some of the relevant
goods and services.

Even the vocabulary can be revealing.
Market "imperfection" suggests- a minor
blemish of the sort that can make the
purchase of "irregular" socks a bargain.
Market "failure" sounds like something
more serious. To take a more subtle example,
I mentioned that one kind of flaw in the
system can be the absence of certain
markets. The common generic term for the
reason why markets are missing is "transaction
costs." That sounds rather minor,
the sort of thing that might go away in
due course as accounting and information


### ---Economics-1980-0-02.txt---
processing get cheaper. But some of the
cases of missing markets really go much
deeper. The fact that distant future generations
can not participate directly in the
markets for nonrenewable resources will not
be remedied by improvements in communication.
Nor are the residents of densely
populated areas ever likely to be able to
dicker effectively with the dozens or hundreds
of sources of barely traceable pollutants
whose health effects, if any, cumulate
over many years.

There is a large element of Rohrschach
test in the way each of us responds to this
tension. Some of us see the Smithian virtues
as a needle in a haystack, as an island of
measure zero in a sea of imperfections.
Others see all the potential sources of
market failure as so many fleas on the thick
hide of an ox, requiring only an occasional
flick of the tail to be brushed away. A
hopeless eclectic without any strength of
character, like me, has a terrible time of it.
If I may invoke the names of two of my
most awesome predecessors as President of
this Association, I need only listen to Milton
Friedman talk for a minute and my mind
floods with thoughts of increasing returns to
scale, oligopolistic interdependence, consumer
ignorance, environmental pollution,
intergenerational inequity, and on and on.
There is almost no cure for it, except to
listen for a minute to John Kenneth
Galbraith, in which case all I can think of
are the discipline of competition, the large
number of substitutes for any commodity,
the stupidities of regulation, the Pareto optimality
of Walrasian equilibrium, the importance
of decentralizing decision making to
where the knowledge is, and on and on.
Sometimes I think it is only my weakness of
character that keeps me from making obvious
errors.

The critics of the mainstream tradition
are mistaken when they attribute to it a
built-in Panglossian attitude toward the
capitalist economy. The tradition has provided
both the foundations for a belief in
the efficiency of market allocations and the
tools for a powerful critique. Economic
analysis by itself has no way of choosing
between them; and the immediate prospects
for an empirically based model of a whole
economy, capable of measuring our actual
"distance" from the contract curve, are
mighty slim. The missing link has to be a
matter of judgment-the Rohrschach test
I spoke of a minute ago. For every Dr.
Pangloss who makes the ink blot out to be
of surpassing beauty, give or take a few
minor deviations-the second-best of all
possible worlds, you might say-there is a
Candide to whom it looks a lot like an ink
blot. Maybe there are more Panglosses than
Candides. But that was true in Voltaire's
time too-just before the French Revolution,
by the way-and has more to do with
the state of society than with the nature of
economics.

The tension between market efficiency
and market failure is especially pointed in
discussions of the working of the labor
market, for obvious reasons. The labor
market connects quickly with everything
else in the economy and its performance
matters more directly for most people than
that of any other market. Moreover, the
labor market's own special pathology, unemployment,
is particularly visible, particularly
unsettling, and particularly frustrating.
The fuse leading from theory to policy in
this field is short, and has been known to
produce both heat and light throughout
much of the history of economics.
Contemporary macro-economic theory,
though apparently full of technical novelties,
has revived many of the old questions
in only slightly different form. One of the
points I want to make is that underneath the
theoretical innovations-some of which are
interesting and important the basic controversial
issues that come to the surface are
the same ones that occupied earlier literature.
The most important among them is
really the old tension between market
efficiency and market failure. Should one
think of the labor market as mostly clearing,
or at worst in the process of quick return to
market-clearing equilibrium? Or should one
think of it as mostly in disequilibrium, with
transactions habitually taking place at nonmarket-
clearing wages? In that case presumably
the wage structure is either not
receiving any strong signals to make it


### ---Economics-1980-0-03.txt---
change in the right direction or is not responding
to the signals it receives. My own
belief in this case lies with the marketfailure
side. That is to say, I believe that
what looks like involuntary unemployment
is involuntary unemployment.

Of course that conclusion only leads to
another question. If the labor market often
fails to clear, we had better figure out why.
There is no shortage of candidate hypotheses.
Here I think it is worthwhile to insist on
a commonplace: although it is natural for
academic people to seek a single weighty
Answer to a weighty Question, if only because
it is so satisfying to find one, it is
quite likely that many of the candidate hypotheses
are true, each contributing a little
to the explanation of labor-market failure.
Now the second general point I want to
make is one that I am surprised to hear
myself making. While I find several of the
candidate hypotheses entirely believable, I
am inclined to emphasize some that might
be described as noneconomic. More precisely,
I suspect that the labor market is a
little different from other markets, in the
sense that the objectives of the participants
are not always the ones we normally impute
to economic agents, and some of the constraints
by which they feel themselves

bound are not always the conventional constraints.
In other words, I think that among
the reasons why market-clearing wage rates
do not establish themselves easily and adjust
quickly to changing conditions are some
that could be described as social conventions,
or principles of appropriate behavior,
whose source is not entirely individualistic.
I said that I am a little surprised at myself.
That is because I am generally stodgy
about assumptions, and like to stay as close
to the mainstream framework as the problem
at hand will allow. In any case, I think
that the unconventional elements in what I
have to say are only part of the story. And I
assure you that I am not about to peddle
amateur sociology to a captive audience. All
I do mean to suggest is that we may predispose
ourselves to misunderstand important
aspects of unemployment if we insist on
modelling the buying and selling of labor
within a set of background assumptions
whose main merit is that they are very well
adapted to models of the buying and selling
of cloth. Far from advocating that we all
practice sociology, I am pleasantly impressed
at how much mileage you can get
from the methods of conventional economic
analysis if only you are willing to broaden
the assumptions a little.

I

It might be interesting to have a history of
the evolution of economic ideas about unemployment,
and their relation both to the
internal logic of the subject and to the
parallel evolution of the institutions of the
labor market. I am not sufficiently well read
to provide that kind of survey. To make my
point about the persistence of the marketefficiency
market-failure tension, I took a
short cut. I went back to reread Pigou's
Lapses from Full Employment, a little book I
remember having been assigned to read as a
student just after the war. And that in turn
sent me back to its parent book, Pigou's
Theory of Unemployment. The Preface to
The Theory of Unemployment is dated April
1933, after a decade of poor performance
and relatively high unemployment in Great
Britain, well into the Great Depression, and
before the publication of the General Theory.
The Preface to Lapses from Full Employment
(another example of a revealing
vocabulary) is dated November 1944, after
five years of the war that put an end to the
depression, and well after the appearance of
the General Theory. That seemed like an
interesting approach to the historical question,
because current controversies in
macro-economic theory are often described
as a debate between "Keynesians" and
others- "monetarists," "Classicals," or
" equilibrium theorists" - and because
Pigou, besides being a great economist, was
in particular the embodiment of the
Marshallian tradition, the leading figure in
the "classical economics" that the Keynesian
revolution was explicitly intended to
overthrow.

Lapses makes interesting rereading. It emphasizes
the money wage, whereas its predecessor


### ---Economics-1980-0-04.txt---
was written almost entirely in terms
of the real wage. The general macro-theoretic
framework, in which the discussion of
the labor market is embedded, clearly has
an eye on Keynes. The underlying model
could be IS-LM without doing much violence
to the argument. There are little
anachronisms: Pigou tends to think of the
interest rate as being determined in the
goods market (by Savings = Investment) and
nominal income as being determined by
the demand for money. Today we take
simultaneity seriously, but the General Theory
more or less speaks as if real output is
determined in the goods market and the
interest rate by liquidity preference. After
what is to me a confusing description
of a Keynesian low-level liquidity-trap
equilibrium, Pigou invokes the Pigou effect
to explain why the low level might not be as
low as all that and then, characteristically,
remarks that none of it is very important in
practice anyway. All this is relevant here
only as background for the treatment of the
labor market.

Pigou says the obvious thing first, and I
agree that it is the first thing to say: if there
is "thorough-going competition" among
workers, then the only possible equilibrium
position is at full employment. That is little
more than a definition of equilibrium. He is
aware that he is taking a lot of dynamics for
granted. Expectations of falling wages could
perversely reduce the demand for labor; and
he discusses the possibility that under some
conditions, with the interest rate at its practical
floor, nominal wage rates and prices
may chase each other down and thus prevent
the real-wage adjustment needed for an
increase in employment. (This is where the
Pigou effect makes its appearance, of
course.)

It is what comes next that interests me. It
is obvious to Pigou, writing in 1944, that the
labor market does not behave as if workers
were engaged in thorough-going competition
for jobs. With the common sense that
seems somehow to have escaped his modem
day successors, he wonders why it does not.
And he discusses three or four of the institutional
factors that a reasonable person
would mention even now as obstacles to the
classical functioning of the labor market.
First of all, he realizes that the labor
market is segmented. Not everyone in it is
in competition with everyone else. I am not
referring here to the obvious fact that abilities,
experience, and skills differ, so that
unemployed laborers can not compete for
the jobs held by craftsmen. That fact of life
merely reminds us that "labor" is not a
well-defined homogeneous factor of production.
Even within skill categories or occupational
groups, however, workers have ties to
localities, to industries, to special job classifications,
even to individual employers.

These ties can be broken, but not easily. It
is interesting to me that even the Theory of
Unemployment of 1933 devotes a lot of space
to the analysis of a labor market in
which there are many "centers of employment"
-to use the neutral term chosen
by Pigou to describe segmentation of the
labor market-between which mobility is
absent or slow. Of course he observes that
even in a completely segmented labor
market, if there is thorough-going competition
within segments, full employment will
be the rule, although there may be wage
differentials between centers of employment
for otherwise identical workers. I think that
the fact of segmentation is very important,
not only because it limits the scope of competition
but because its pervasiveness suggests-
though it can not prove-that habit
and custom play a large role in labor market
behavior. From the prominence that he
gives it, I gather that Pigou might have
agreed.

A second factor, which has been more
often discussed, is trade unionism. Pigou
does not have very much to say about collective
bargaining, but what he says makes
sense.

Of course, these agencies in their decisions
have regard to the general state
of the demand for labour; they will
have no wish to set wage rates so high
that half the people of the country are
thrown out of work. Nevertheless,
there is reason to believe that they do
not have regard to demand conditions
in such degree as would be necessary
to secure, as thorough-going competition
would do, the establishment of
full employment. [ 1945, p. 26]


### ---Economics-1980-0-05.txt---
Later on in the book, Pigou makes an observation
that is not explicitly connected
with collective bargaining. He does connect
it with "actual life" however, and it fits
organized workers very well, and perhaps
others besides:

In periods of expansion employers
might be willing to agree to substantial
advances in wage rates if they
were confident that, when prosperity
ended, they would be able to cancel
them. They know, however, that in
fact this will not be easy, that elaborate
processes will have to be gone
through, and that their work-people
will put up a strong rear-guard action.
.. . In periods of depression

wage-earners, for precisely similar reasons,
hold out against wage reductions,
which they might be ready to

concede if it were not for the difficulty
that they foresee in getting them
cancelled when times improve.... A
widespread desire for 'safety first'
helps to make wage rates sticky.
[1945, p. 48]

These casual remarks raise more questions
than they answer about the determination
of nominal wages by collective bargaining.
The first excerpt can be taken as a redefinition
of full employment when the labor
market is not competitive; the second, however,
advances an account of wage stickiness
and is therefore on a different footing.
It would help to explain the failure of the
labor market to clear on any reasonable
definition, and thus provide a connection
between nominal demand and real output.
The third institutional factor mentioned
by Pigou has also been the subject of much
analysis, past and present: the provision of
unemployment insurance. There are several
channels by which the availability of unemployment
compensation can add to the recorded
amount of unemployment. The

prolongation of search is only the most
obvious. My own impression is that this is
currently a significant factor. As an indication
of the complexity of the issues, let me
just mention here that some recent research
by my colleagues Peter Diamond and Eric
Maskin suggests the possibility that in some
environments search activity conveys a positive
externality. So the optimal search
strategy for the individual might provide
less than the socially optimal amount of
search, and unemployment compensation
could be regarded as a corrective subsidy.
This is a neat twist on the theme of the
counterpoint between market efficiency and
market failure. In any case, it can hardly be
doubted that the unemployment compensation
system is an important determinant of
behavior on both sides of the labor market,
and complicates even the definition of full
employment.

The last comment of Pigou's that I want
to cite is especially intriguing because it is so
unlike the sort of thing that his present day
successors keep saying. Already in the 1933
Theory of Unemployment he wrote: "... . public
opinion in a modem civilized State
builds up for itself a rough estimate of what
constitutes a reasonable living wage. This is
derived half-consciously from a knowledge
of the actual standards enjoyed by more
or less 'average' workers.... Public opinion
then enforces its view, failing success
through social pressure, by the machinery
of... legislation" (p. 255). A similar remark
appears in Lapses. Such feelings about equity
and fairness are obviously relevant to
the setting of statutory minimum wages, and
Pigou uses them that way. I think they also
come into play as a deterrent to wage cutting
in a slack labor market. Unemployed
workers rarely try to displace their employed
counterparts by offering to work for
less; and it is even more surprising, as I
have had occasion to point out in the past,
that employers so rarely try to elicit wage
cutting on the part of their laid-off employees,
even in a buyer's market for labor.
Several forces can be at work, but I think
Occam's razor and common observation
both suggest that a code of good behavior
enforced by social pressure is one of them.
Wouldn't you be surprised if you learned
that someone of roughly your status in the
profession, but teaching in a less desirable
department, had written to your department
chairman offering to teach your courses for
less money? The fact that nominal wage
rates did fall sharply during the early stages
of the depression of the 1930's, and the fact
that the Chrysler Corporation has been able


### ---Economics-1980-0-06.txt---
to negotiate concessions from the UAW certainly
show that wage rates are not completely
rigid. But those very instances seem
to me only to confirm the importance of
social convention in less extreme circumstances.
After all, people have been known
to try to claw their way into a lifeboat who
would never dream of cheating on a lift-line.
I think I have made the case that the most
eminent representative of orthodox economics
in the 1940's was fully aware of the many
obstacles to "thorough-going competition"
among workers, that is, of the many ways in
which the labor market may "fail." In particular,
one cannot under those circumstances
expect the labor market always to
clear. Pigou certainly drew that conclusion.
He says, in the Preface to Lapses: "Professor
Dennis Robertson... has warned me
that the form of the book may suggest that
I am in favour of attacking the problem
of unemployment by manipulating wages
rather than by manipulating demand. I
wish, therefore, to say clearly that this is not
so" (P. v).

Pigou clearly felt the tension between
market efficiency and market failure. Nevertheless,
he did not come down on the side of
market failure, even after the 1930's. The
very title of Lapses from Full Employment
tells us that much. Evidently he concluded
that the tendency of the capitalist economy
to seek (and find) its full-employment
equilibrium was strong enough so that departures
from full employment could be regarded
as mere episodes. Is that surprising?
Well, to begin with, there is no accounting
for Rohrschach tests. One person's ink blot
is another person's work of art. But I think
there is also something more systematic to
be said.

In the Theory of Unemployment, Pigou
gives an elaborate analysis of the short-run
elasticity of demand for labor. He is very
careful: he allows for the elasticity of supply
of complementary raw materials; he allows
for the (presumably very high) price elasticity
of demand for exports; he discusses the
effects of discounting future returns to
labor. It is a masterly attempt to get a grip
on orders of magnitude. It is all based on
the presumption that the only possible starting
point is the elasticity of the marginalproduct-
of-labor curve. Let me remind you
that in the old standby, two-factor CobbDouglas
case, the elasticity of demand for
labor with respect to the real wage is the
reciprocal of the share of capital. Everybody'
s back-of-the-envelope puts the capital
share at 1/4 and the elasticity of demand
for labor at 4. This is not exactly the way
Pigou proceeds, but he reaches the same
conclusion: the initial estimate of the elasticity
is "certain to be (numerically) much
larger than - 1 and may well amount to - 5
or more." There follow some modifications,
but the conclusion remains that in times of
depression, the aggregate elasticity of demand
for labor with respect to the real wage
"cannot, on the least favourable assumption
here suggested, be numerically less than - 3
and may well be larger than -4" except
perhaps in the very shortest run.
For practical purposes, one would want
to know the elasticity of demand with respect
to the nominal wage, taking account
of the likelihood that prices will follow
wages down, at least partially. (Obviously if
product prices fall equiproportionally with
wage rates, as Keynes thought might
happen in unlucky circumstances, the real
wage doesn't move at all and employment
will not improve.)' The details of Pigou's
calculations do not concern us, but his conclusion
does: "... we may... not unreasonably
put the elasticity of the money demand
for labour in times of deep depression at not
less numerically than - 1.5."

If I could believe that, I too could believe
that the labor market generally clears. To
reduce the unemployment rate by 6 percentage
points is to increase employment by
about 6 percent, if we ignore for this purpose
the side effects that go to make up
Okun's Law. If that could be accomplished
by a real-wage reduction of 2 percent, or
even less, that is, by foregoing one year's
normal productivity increase, than I could
imagine that the labor market might easily
'Neither Pigou nor Keynes invoked Kaldor's notion
that prices can be expected to fall faster than wages in
a recession with the resulting rise in real wages providing
the force for recovery from the demand side,
through a distributional shift toward wage incomes
which generate more spending per dollar than other
incomes do.


### ---Economics-1980-0-07.txt---
learn to adjust smoothly to fluctuations in
aggregate demand. I could even imagine
that workers might accept the necessary 4
percent reduction in nominal wages, in the
expectation that half of it would be offset by
lower prices. The trouble is that Pigou's
demand elasticities are way too high. A recent
econometric study by Kim Clark and
Richard Freeman, based on quarterly data
for U.S. manufacturing. 1950-76, puts the
real-wage elasticity of demand for labor at
about one-half, a whole order of magnitude
smaller than Pigou's guess.2 And the ClarkFreeman
work is presented as revisionist, a
counterweight to other estimates that are
typically lower, averaging out at about 0.15
according to a survey by Daniel Hamermesh.
To my mind, smooth wage adjustment
seems intrinsically unlikely in a world
with such a small demand elasticity and
institutions like those sketched earlier.
Nothing I read in the newspapers suggests
to me that 6 percent of nonfrictional unemployment
produces a threat adequate to set
off a quick 12-15 percent fall in the real
wage, or a drop in nominal wage rates twice
as large. Sellers facing inelastic demands
usually try to discourage price cutting; why
should workers be different?

The modern classical school seems curiously
remote from all this. When they try to
explain how the equilibrium volume of employment
can fluctuate as widely as actual
employment does in business cycles, their
only substitute for Pigou's high elasticity of
demand is a high elasticity of supply (of
labor) in the face of a perceived temporary
opportunity for unusual gains, which in this
case reflects wages that differ from average
expected (discounted) future wages. In other
words, People who give the vague impression
of being unemployed are actually engaged
in voluntary leisure. They are taking
it now, planning to substitute extra work
later, because they think, rightly or wrongly,
that current real wages are unusually low
compared with the present value of what the
labor market will offer in the future. They
may be responding to changes in real wages
or to changes in the real interest rate.
It is astonishing that believers have made
essentially no effort to verify this central
hypothesis. I know of no convincing evidence
in its favor,3 and I am not sure why it
has any claim to be taken seriously. It is
hardly plausible on its face. Even if the
workers in question have misread the future,
they are merely mistaken, not confused or
mystified about their own motives. It is thus
legitimate to wonder why the unemployed
do not feel themselves to be engaged in
voluntary intertemporal substitution, and
why they queue up in such numbers when
legitimate jobs of their usual kind are
offered during a recession.4

When they face the market-clearing issue
at all, Pigou's successors take a rather abstract
line. They regard it as inherently incredible
that unexploited opportunities for
beneficial trade should be anything but
ephemeral-which means merely that they
ignore all those human and institutional
facts of which Pigou was aware. Or else they
argue that one cannot believe in the failure
of markets to clear without having an
acceptable theory to explain why that
happens. That is a remarkable precept when
you think about it. I remember reading once
that it is still not understood how the giraffe
manages to pump an adequate blood supply
all the way up to its head; but it is hard to
imagine that anyone would therefore conclude
that giraffes do not have long necks.
At least not anyone who had ever been to a
zoo. Besides, I think perfectly acceptable
2The Clark-Freeman estimates are based on quarterly
data for aggregate U.S. manufacturing. Their
difference from other work appears to rest on allowing
wage changes to operate with a lag different from other
factor prices. According to their results the lag of
employment behind wage changes is quite short; it is
complete in about two quarters.
3Just after writing those words, I received a working
paper by Robert Hall which (a) concludes that the
elasticity of supply of labor required to make the intertemporal-
substitution hypothesis work is actually in the
ballpark suggested by other facts, but (b) rejects the
whole theory on other empirical grounds. I have done
some further experimentation on Halls data (with the
help of Mr. Sunil Sanghvi) with results that cast doubt
on the reliability of even the first conclusion. On reflection,
I stand by the words in the text.
4I have tried to phrase that carefully. For some
direct evidence, see "Jobs and Want Ads: A Look
Behind the Evidence," Fortune, Nov. 20, 1978.


### ---Economics-1980-0-08.txt---
theories can indeed by constructed, as soon
as one gets away from foolishly restrictive
and inappropriate assumptions.
II

That brings me to the second and last
general point I had hoped to make. Suppose
one chooses to accept the apparent evidence
of one's senses and takes it for granted that
the wage does not move flexibly to clear the
labor market. By the way, my own inclination
is to go further and claim that commodity
prices are sticky too, at least downward.
But it is the persistence of disequilibrium in
the labor market that I want to emphasize.
How can we account for it?

There is, as I mentioned at the beginning,
a whole catalog of possible models of the
labor market that will produce the right
qualitative properties. Since I have surveyed
this literature elsewhere, I will just list a
half-dozen possibilities now, with the reminder
that they are not mutually exclusive
alternatives.

(1) There is Keynes's idea that caseby-
case resistance to wage reductions is the
only way that workers can defend traditional
wage differentials in a decentralized
labor market. The net result is to preserve
the general wage level or its trend, but that
is an unintended artifact.

(2) There is a complementary hypothesis
about the behavior of employers that I
have proposed myself: if employers know
that aggressive wage cutting in a buyer's
market may antagonize the remaining work
force, hurt current productivity, and make it
harder to recruit high-quality workers when
the labor market tightens, they will be less
inclined to push their short-run advantage.
(3) Pigou realized that widely held notions
of fairness, enforced by social pressure
or by legislation, might have to be part of
any serious account of wage determination.
George Akerlof has pursued this trail further,
documented the prescription of codes
of good behavior in manuals of personnel
practice, and showed formally that such
codes of behavior can be self-enforcing if
people value their reputations in the community.
Obviously there are no Emily Post
manuals to consult as regards the behavior
of laid-off workers, but you would certainly
not be astonished to learn that self-esteem
and the folkways discourage laid-off workers
from undercutting the wages of their
still-employed colleagues in an effort to displace
them from jobs. Reservation wages
presumably fall as the duration of unemployment
lengthens; but my casual reading
suggests that this pattern shows up more in
a willingness to accept lower-paid sorts of
jobs than in "thorough-going competition"
for the standard job. The cost to the worker
of this sort of behavior is diminished by the
availability of unemployment insurance. It
is worth remembering that the acceptance of
lower-grade jobs is itself a form of unemployment.


(4) I need only touch on the AzariadisBaily-
Gordon implicit-contract theory, because
it has been much discussed in the
literature. Here wage stability is a vehicle by
which less-risk-averse firms provide income
insurance for more-risk-averse workers, presumably
in exchange for a lower average
wage.5 It is now understood that the theory
works well only when workers have some
source of income other than wages, unemployment
compensation for instance. This is
not really a disadvantage in a world with
well-developed unemployment insurance
systems. In any case such implicit contracts
do not themselves account for unemployment.
Their effect is to reduce the average
amount of unemployment below the level
that would occur in a simple spot market.
The theory belongs in my list because I
suspect it does help to account for the habit
of wage inertia and therefore the vulnerability
of employment to unexpected fluctuations
in aggregate demand.

(5) Wherever there is collective bargaining
in our economy, the standard pattern,


### ---Economics-1980-0-09.txt---
with few exceptions, is that wage rates are
specified in the contract, and the employer
chooses the amount of employment. This is
not exactly simple monopoly, because the
union cannot set the wage schedule unilaterally.
To the extent that it can, another
source of wage stickiness can be identified.
Under a reasonable assumption about what
the union maximizes, it turns out that the
only aspect of the demand for labor that has
any effect on the monopoly wage is its elasticity.
So if the demand curve for labor
shifts down nearly isoelastically in a recession,
the contractual wage will change little
or not at all, and the full effect of the fall in
demand will bear on employment. The
amount of unemployment compensation
available plays a role here too. (There is
much more to be said along these lines,
and Ian McDonald of the University of
Melbourne and I hope to say it on another
occasion.)

(6) As a last example, I recall Pigou's
observation that wage changes may be
seen by the parties as hard to reverse without
a struggle whose duration and outcome
cannot be foreseen. The resulting uncertainty
causes employers to drag their feet
when demand increases temporarily and
workers to reciprocate when demand falls.
The result is wage stickiness in the face of
fluctuating employment.

Only what Veblen called trained incapacity
could prevent anyone from seeing that
some or all of these mechanisms do indeed
capture real aspects of the modern capitalist
economy. Assessing their combined significance
quantitatively would be a very difficult
task, and I do not pretend to be able
to do that. We are all interpreting this ink
blot together. Obviously I would not be
giving this particular talk if I did not think
that wage stickiness is a first-order factor in
a reasonable theory of unemployment.
To make my position plausible, I want to
try to summarize the sort of general characteristics
that the labor market should have
if the particular mechanisms that I have
enumerated are to be important. By the
way, I have no reason to believe that my list
is anything like exhaustive; you may think
of others. Simply to narrow the field, I have
deliberately left out of account factors relating
specifically to age, sex, race, and other
characteristics that normally form the basis
for discussions of structural unemployment
as distinct from cyclical unemployment.
The sort of labor market I have in mind is
segmented. It often makes sense to think of
an employer or definable group of employers
as facing its own labor pool. Some
members of the labor pool may be unemployed,
but still belong to it. Although
transportation, information, and transaction
costs are possible sources of segmentation,
they need not be among the most important.
The buildup of firm-specific or industryspecific
human capital may be more fundamental,
and equally a kind of mutual knowing-
what-to-expect that gives both parties in
the labor market a stake, a rent, in the
durability of the relationship. This point is
close to the distinction between auction
markets and customer markets made by
Arthur Okun in a different context. The
labor market, at least the "primary" labor
market, is a customer market; this may be
one of the important facts that differentiates
the primary from the secondary labor
market.

A second general characteristic is the
availability of some nontrivial source of
nonemployment income. The obvious one is
unemployment compensation, but I imagine
that fringe activity ranging from hustling to
home maintenance can function in much
the same way. I suppose in some societies
the possibility of returning temporarily to
farming is now as important as it once was
here. The presence of a second earner in the
family can make an obvious difference. One
consequence is that it becomes easier to
maintain a labor pool in the presence of
fluctuating employment. In addition, as I
mentioned a few moments ago, several of
the specific sticky-wage mechanisms in my
catalog depend for their operation on this
characteristic.

Third, the stability of the labor pool
makes it possible for social conventions to
assume some importance. There is a difference
between a long-term relationship
and a one-night stand, and acceptable behavior
in one context may be unacceptable
in the other. Presumably most conventions
are adaptive, not arbitrary, but adaptiveness


### ---Economics-1980-0-10.txt---
may have to be interpreted broadly, so as to
include pecuniary advantage but not be
limited by it. Critics who deride the notion
of "economic man" have a point, but usually
the wrong point. Economic man is a
social, not a psychological, category. There
are activities in our culture in which it is
socially acceptable and expected that individual
pecuniary self-interest will be the
overriding decision criterion: choosing a
portfolio of securities, for example.6 There
are others in which it is not: choosing a
mate, for example.7 The labor market is
more complicated than either, of course,
and contains elements of both. Perhaps in
nineteenth-century Manchester labor was
bought and sold by "thorough-going competition"
but I think that is unlikely to be a
good approximation to contemporary wage
setting. In particular, as I have emphasized,
there is nothing in the data or in common
observation to make you believe that moderate
excess supply will evoke aggressive
wage cutting on either side of the labor
market.

In

I draw two conclusions from this whole
train of thought, one about economics and
the other about the economy.

About economics: it -need not follow
that we old dogs have to learn a lot of new
tricks. It still seems reasonable to presume
that agents do the best they can, subject to
whatever constraints they perceive. But in
some contexts the traditional formulations
of the objective function and constraints
may be inappropriate. In the labor market,
the participants are firms and groups of
firms on one side, and individual workers,
organized trade unions, and informally
organized labor pools on the other. Grant
me that all feel constrained, to some nontrivial
degree, by social customs that have to
do with the wage and wage-setting procedures.
The result is that factor prices turn
up in our equations in unfamiliar ways. Let
me just mention a few examples from my
earlier list of hypotheses. If Keynes was
right about the conventional significance of
relative wages, then ratios of wage rates
appear in the objective functions on the
labor side. If the current or future performance
of workers depends on their feelings
that wage levels are fair, then wage rates
appear in the production functions constraining
firms. If the individual worker's
utility function depends quite conventionally
on current income, then the collective
objective function of a labor pool of identical
workers might reasonably be a weighted
average of the utility of the wage and the
utility achievable when unemployed, with
weights equal to the employment and unemployment
fractions. This objective function
contains both wage and volume of employment
as arguments; and it has the interesting
property that the marginal rate of
substitution between wage rate and employment
can depend very sensitively on the size
of the unemployment insurance benefit.
Constrained maximization and partial or
complete reconciliation in the market can
still be the bread and butter of the macro
theorist. Spread with more palatable behavior
assumptions, they may make a tastier
sandwich, and stick to the ribs.
About the economy: if the labor market
is often not in equilibrium, if wages are
often sticky, if they respond to nontraditional
signals, then there is a role for macro
policy and a good chance that it will be
effective. Equilibrium theories that conclude
the opposite may conceivably turn out to
have the right answer, but they simply
assume what they purport to prove. It is not
my argument that standard textbook policy
prescriptions are bound to be right. That
has to be worked out case by case. All I do
claim is that a reasonable theory of economic
policy ought to be based on a reasonable
theory of economic life.
 ## Economics-1981-0


### ---Economics-1981-0-01.txt---
The early debates over the role of government
in economic life, at least during the
era of industrialization, took the form of a
contest between laissez-faire and thoroughgoing
socialism. In Western Europe and
North America, however, the movement
away from individualism followed a much
less radical course, which John Maynard
Keynes was one of the first to define. His
famous lectures in the mid-1920's on The
End of Laissez-Faire carried the following
passage:

... a time may be coming when we
shall get clearer than we are at present
as to when we are talking about
Capitalism as an efficient or inefficient
technique, and when we are talking
about it as desirable or objectionable
in itself. For my part, I think that
Capitalism, wisely managed, can probably
be made more efficient for attaining
economic ends than any alternative
yet in sight, but that in itself it
is in many ways extremely objectionable.
Our problem is to work out a

social organization which shall be as
efficient as possible without offending
our notions of a satisfactory way of
life. [p. 53, emphasis added]

Keynes, as we can now see, was among
the first writers to form a definite vision of
the kind of system under which we have
come to live during the last half century, the
system we now call the Mixed Economy or
Welfare Capitalism or the Middle Way. Like
the much more individualistic, much less
guided, system that preceded it, the Mixed
Economy developed with the support of a
broad consensus of opinion. That consensus,
however, has now weakened. The
economic role of government is again the
subject of debate, attack, and resistance far
more intense than we have known for decades.
The attack ranges over a wide spectrum.
It questions the scope of government,
the particular measures and policies through
which government exercises its functions,
and the political institutions which shape
the measures and policies employed. A few
voices call on us to move on to a more encompassing
socialism, including the ownership
of industry. Many more call for a drastic
revival of market rule.

We all, I think, sense that we have come
to a very difficult juncture in the development
of our Mixed Economy. How we shall
emerge is still in dim prospect. As in other
illnesses, social crises often are surmounted
and are followed by periods of renewed
stable development. But sometimes not. We,
therefore, ought to think where we are and
what the nature of our troubles is.
I

There is no single, simple way to gather
together all the threads of our present discontent,
and I shall not try. One useful
opening, however, is to consider the pronounced
and worrisome retardation of productivity
growth from which we now suffer.
Productivity growth, I need hardly say, is
the main source of measured per capita
output growth. And per capita output, in
turn, is a central component of economic
welfare as we economists conceive it, many
would say the central component. It is elementary,
however, that per capita output
growth and welfare growth are not the same
thing. National product is not even an adequate
long-term measure of net output relevant
to welfare. It makes inadequate allowance


### ---Economics-1981-0-04.txt---
for the quality and variety of goods.
It excludes the household and treats all
government expenditure as final product. It
neglects the externalities of production and
consumption and the costs of growth proper,
for example, the dislocation of people. It
makes dubious assumptions about people's
ability to appraise and guard against the
dangers carried by jobs and products. And
there is much more to economic welfare
than can be captured in any long-term measure
of output: job stability; income security,
a fair distribution of opportunities and
rewards.

The economic role of government expanded
during the last half century and
more in large part in order to pursue the
social objectives that are not comprehended
in measured net national product. The result
is the mixed economy or welfare state in
which we now live and which is now the
object of attack.

Productivity growth is a useful focus of
discussion in relation to the current discontents
and the accompanying reappraisal of
our mixed economy for a combination of
two reasons.

To begin with, productivity, viewed as a
source of private earnings, exists in a state
of uneasy tension with the other welfare
objectives, which we pursue largely through
the government. The causes of the tension
need to be underscored.

First-an obvious point-the more income
that is diverted to social uses, the less
of any given aggregate remains under the
private control of income earners for their
own personal use.

Next, the size of the diversion and the
way it is made and used affects the level of
output and productivity, present and future.
That is partly because a host of government
activities are supportive of current output
and productivity, and many activities, including
some, like education, that are undertaken
for generalized social objectives,
are in the nature of capital formation.' In a
still more basic sense, moreover, and one
much neglected in current debates, the pace
of growth in a country depends not only on
its access to new technology, but on its
ability to make and absorb the social adjustments
required to exploit new products and
processes. Simply to recall the familiar, the
process includes the displacement and redistribution
of populations among regions and
from farm to city. It demands the abandonment
of old industries and occupations, and
the qualification of workers for new, more
skilled occupations. The extension of education,
with all its implications for shifts in
social status, in aspiration, and in political
power, is a requisite. Along the technological
path which we have followed, growth
also demands very large-scale enterprise
which establishes new types of market power
and alters the relations of workers and employers.
Viewed from another angle, the dependent
employment status of workers and
the mobility of industry and people imply a
great change in the structure of families and
in their roles in caring for children, the sick,
and the old. Because the required adaptations
can and do alter the positions, prospects,
and power of established groups,
conflict and resistance are intrinsic to the
growth process. To resolve such conflict and
resistance in a way which preserves a large
consensus for growth, yet does not impose a
cost which retards growth unduly, a mechanism
of conflict resolution is needed. The
national sovereign state necessarily becomes
the arbiter of group conflict and the mitigator
of those negative effects of economic
change which would otherwise induce resistance
to growth.2

The enlargement of the government's economic
role, including its support of income
minima, health care, social insurance, and
the other elements of the welfare state, was,
therefore-at least up to a point-not just a
question of compassionate regard for the
unfortunate, and not just a question of reducing
inequalities of outcome and opportunity,
though that is how people usually
think of it. It was, and is-up to a point-a


### ---Economics-1981-0-05.txt---
part of the productivity growth process itself.


And yet, manifestly, there is another side
to the story, the side that is so much to the
fore today. The government's roles as referee
and as mitigator of the costs of growth- as
well as instrument for pursuing welfare goals
supplementary to measured productivitymust
be paid for. But it is essentially impossible
to design a tax system that places no
marginal burden on the rewards for productive
effort, or a regulatory system that has
no cost in measured output. Similarly, we
can hardly design a transfer system whichup
to a point-necessarily divorces income
from work, but which yet does not qualify
economic incentives. There is a presumption,
therefore, that the tax-transfer-regulatory
system, whatever its essential, longterm,
indirect, supportive role, operates more
immediately and directly to constrict work,
saving, investment, and mobility-just how
much is, of course, a question.
There is, therefore, an uneasy manyfaceted
tension between measured productivity
growth and the private earnings it
generates on the one side, and the pursuit of
other welfare goals through government on
the other side. The tension implies a difficult
and delicate problem of choice and
balance. A balance-certainly a wide acceptance
of the pace and nature of our joint
pursuit of different welfare goals- seemed
to exist during the first two postwar decades
when productivity growth was relatively
rapid. That balance, if it was a balance,
has, however, now been upset by the protracted
retardation of productivity growth
during the last dozen or more years. That is
the second reason why productivity growth
is a useful focuis for examining the current
dissatisfaction with our mixed economy.
I shall deal briefly with three matters:
1) What were the developments which
were antecedent to (which stand in the
background of) our present troubles and its
accompanying discontent?

2) What can we now say about the
causes of the current productivity retardation?
In particular, to what extent is the
retardation connected with the enlarged role
of government and its pursuit of alternative
social goals?

3) What is the outlook for productivity
growth, and what are the implications of
that outlook for the further development of
our mixed economic system?

II

In the early part of the postwar period,
economic growth, in the aggregate and per
capita, established itself as a premier goal of
economic policy-co-equal with "full" employment,
perhaps of even higher priority.
Besides the standard reason, that per capita
growth raises average levels of consumption,
there were special reasons. Growth was seen
as the best way to overcome poverty without
the social conflicts accompanying redistribution.
It would create a favorable environment
in which to open opportunities for
blacks and other minorities. It would provide
the resources for meeting still other
social goals, for example, extended education
and health care. Growth was also sought
to maintain defense, to compete politically
with a fast-growing Soviet Union and to
assert continued leadership in our rapidly
progressing alliance. Growth would enable
us to help not only the poor in our own
country, it would permit us to help the still
more impoverished people of the lessdeveloped
world. Productivity growth was a
goal distinguishable from full employment,
but it was also seen-not necessarily correctly-
as a condition of full employment.
Unless we could hold our own in international
trade, our foreign accounts would impose
demand restraints on policy and make
for chronic underemployment.

This growth, so ardently desired, was in
fact achieved. For two decades, income per
capita grew faster than ever before and output
per hour much faster. At the same time,
there was a rapid development of government
in pursuit of other welfare objectives,
and this was also eagerly sought. The Social
Security system established in the 1930's
was enlarged; education was rapidly extended;
science was fostered; there were
large programs for hospital building and
housing. The proportion of the population
living below defined poverty levels was


### ---Economics-1981-0-06.txt---
reduced- the joint result of rising average
incomes, and extended insurance and welfare
provision. Partly because government
was bigger, partly because the scope of progressive
taxation was wider, partly because
of old age and unemployment insurance and
other forms of income maintenance, we
enjoyed the benefits of a system of "built-in
stabilizers." Recessions were milder and
growth more steady than they had ever been
before in American experience as an
industrialized country.

The main point, however, is that in this
period, productivity growth paid easily for
the pursuit of other welfare goals. Although
government grew faster than GNP, fast
growth of productivity supported fast growth
of per capita disposable income, of real
spendable earnings of workers, and of average
family incomes.3 Productivity growth
was, therefore, the substantial basis on which
the consensus of opinion supporting the
development of the mixed economy rested.
III

Frank Knight liked to say that progress is
not a question of happiness; it is a question
of what people are unhappy about. Not
surprisingly, therefore, the progress of the
first two postwar decades was followed by a
certain recoil from growth-a reordering, if
not reversal, of priorities. This took several
forms:

1) Whereas in the 1950's, measured growth
was regarded as the main instrument for
overcoming poverty, as the 1960's wore on
the view took hold, with much justification,
that future growth alone could not deal adequately
with the poverty which past growth
had left behind. Although technical progress,
capital accumulation, and general education
would continue to be important in
the future, an increasing proportion of the
"residual poor" had special handicaps. They
had to be helped directly, principally by a
fight against discrimination, by special education
and training programs, and by new
and expanded schemes for social insurance,
income support, health care programs, and
other transfers in kind. The impulse to fight
poverty directly was fed by new information
about the size and composition of the remaining
poor population, by the indignation
of social reformers and, most of all, by
rising racial tensions.4 "We cannot," said
the Council of Economic Advisors, "leave
the further wearing away of poverty solely
to the general progress of the economy"
(1964, p. 60).5

2) As individual income levels rose, people
generally became more sensitive to their
immediate surroundings. They found hospital
and educational facilities inadequate
and the urban physical plant shabby. Yet
the demand for improvement had to be met
in difficult circumstances which continue to
plague and torment local government to this
day. The relative price of public, like that of
private, services was rising. Higher incomes
and automobiles were transporting upwardly
mobile families to the suburbs, carrying
their tax base with them. The cities,
increasingly abandoned to the poor, unable
to tap the suburban affluence about them,
could barely cope. Congestion on the highways
and streets, noise, air and water pollution,
all fed by growth itself, swelled, moved
to the countryside and everywhere became
more objectionable to otherwise more affluent
people.

3) People discovered the terrors of technology-
products, working conditions, and
environmental changes that carried risks.
The dangers feared were often invisible, they
operated at a distance and cumulated over
time, carrying both real and imaginary
threats to health and life now and in generations
to come. Technological progress, which
for decades had been seen as the process by
which problems and dangers might be overcome,
was now increasingly feared as a
major source of our troubles.

These shifts in outlook had two important
practical consequences. One was the very


### ---Economics-1981-0-07.txt---
rapid expansion of government social welfare
and civil rights programs which began
in the mid-1960's and which developed and
matured in the 1970's. Expenditures for "social
welfare," which were 9 percent of GNP
in 1950 and only 10 percent in 1960, rose to
15 percent in 1970 and to 20 percent in
1977.6 The other was "explosion" of public
regulatory legislation and administration directed
to the protection of the environment,
and to the safety of workers and consumers.7
The new legislation became the basis for
strong, privately organized campaigns to
limit growth and the application of new
technology.

IV

The maturing of the Great Society programs
in the spheres of welfare and civil
rights, and the implementation and expansion
of the social regulatory laws, brought
our mixed economy to a new stage of development.
There was a new distribution of
emphasis among the different dimensions of
economic welfare, and correspondingly a
new distribution of economic power between
the private and public spheres. The
new development of the mixed economy,
however, is now confronted by a changed
and less-favorable growth environment.
Looking back, we can now see that a
slower rate of productivity growth accompanied
the institution and the maturing of the
Great Society programs. To what extent the
two developments were associated as effect
and cause, however, is still an open question.
So is a related matter; that is, the
responsibility of transient as distinct from
durable factors for bringing about the
slowdown we observe. It would be wrong to
pretend that there are now definite answers
to these questions. The factual position,
however, deserves description because it
bears on the origins of our present discontents.


Beginning in the late 1960's, private-sector
productivity growth fell back from the high
speed it had reached in the years preceding.
The retardation before 1973 was moderate.
The new pace approximated that during the
somewhat slack later 1950's. After 1973,
however, the slowdown became much more
serious. The upshot is that average productivity
growth for the fourteen years between
1965 and 1979 ran at only one-half the pace
of the years from 1948 to 1965; since 1973,
it has risen at less than one-fifth that earlier
pace.8 The extent of the slowdown between
the two rough halves of the postwar period,
before and after 1965-to say nothing of
the post-1973 period by itself-may be
judged from the fact that the post-1965 productivity
slowdown has been more severe

than any of the retardations measured across
major depressions going back to the 1890's.
That includes the retardation from the 1920's
to the 1930's.9 Yet, up to 1979 we had had
no major depression.

In my judgment, the productivity retardation,
at least since 1973, has been accompanied
by a slower rate of improvement in
material conditions of well-being. In some
respects, and by some measures, there have
even been significant declines. It is true that,
because the labor force was rising rapidly in
relation to population, the growth rate of
real disposable income per capita was well
maintained-at least if we depend on the
deflator for "personal consumption expenditures"
; not if we use the CPI. As perceived
by many people, however, the welfare sig-
6See U.S. Social Secuxity Administration. Social
welfare expenditures cover social insurance, public
assistance, health and medical care, veterans' programs,
education, housing and "other." At present, exhaustive
expenditures accdunt for nearly half and transfer
programs for somewhat more than half of total welfare
expenditures. (See Sheldon Danziger, Robert
Haveman, and Robert Plotnick, pp. 6-8.) The major
reasons for the accelerated growth since 1965 appear to
lie in the initiation and expansion of new programs,
such as Medicare, and in the generous increase of
benefit schedules in old programs like Social Security
(see Plotnick, pp. 277-78).

7The Federal Register, which records new regulations,
contained 10,000 pages in 1953, but 65,000 pages
in 1977. The federal budget to administer regulatory
activities was $5 billion in 1978, having doubled since
1974. Compare Arthur Bums, p. 4.
8I depend for these comparisons on the easily accessible
Bureau of Labor Statistics figures for "output per
hour of all persons" in the private business sector. See
Economic Report of the President (1980, Table B-37).
9See Appendix Table 1.


### ---Economics-1981-0-08.txt---
nificance of even the more favorable measure
is qualified. That is partly because the
demographic changes that supported laborforce
growth also made for a faster increase
of households than of population, so to some
degree expenses per head increased with income
per head.' It is qualified also to the
extent that women felt forced to take paid
work to offset the slower rise or actual decline
of their husbands' real earnings; to the
extent that the proportion of persons living
in pretransfer poverty has been tending to
rise since 1968; to the extent that transfer
incomes became a more important part of
aggregate disposable income- to the disadvantage
of income earners; and to the extent
that the rise of noncash compensation
reduced worker's discretionary take-home
pay. The upshot is that in recent years, the
average real cash incomes of workers have,
depending on the measure, almost ceased to
rise or begun to fall. The same is true of the
average real total income of families, supported
as that has been by transfer incomes
and by the entry of second workers. The
presumption is that the real earned income
of representative single worker families, still
more their cash income, has definitely
declined."

The slowdowns in the growth rates of
productivity, annual wages, and household
incomes are, moreover, not the only disturbing
elements in our economic situation. They
are accompanied by rapid and volatile inflation
which redistributes income and wealth
in arbitrary and confusing ways. Taken together,
these developments have disappointed
peoples' expectations; they have
robbed many people of the fruits of earlier
work and saving, and made almost everyone
unsure or fearful about their future.
These developments stand in the background
of the current discontent with the
operation of our mixed economy. They have
led to a blacklash against the earlier recoil
from productivity growth. This blacklashperhaps
justifiably, perhaps not-raises
sharply the issue of maintaining a steady
balance between the productivity growth
that supports the rise of earned incomes and
the pursuit of other vitally important social
goals.

V

Our attitudes towards that issue would be
clearer if we could know to what extent the
current productivity retardation is actually
due to the workings of our mixed economy
or to its past and current attempts to raise
social welfare through government actions.
Many believe that the welfare and regulatory
programs are heavily implicated both
in direct ways and because of their arguably
plausible connection with the onset and persistence
of an erratic and accelerating inflation.
There is a concomitant fear that the
welfare and regulatory programs may be a
serious drag on future productivity growth.
Opposition to these programs, is, therefore,
rising. True, if future productivity growth is
slow for whatever reasons, people will be
less willing than they might otherwise be to
bear the cost of pursuing alternative welfare
goals. But if that pursuit were actually a
significant cause of slower growth, the reluctance
would be still stronger, as it then
should be.

The causes of the current retardation,
however, remain cloudy. A portion of the
slowdown is, by general agreement, due to a
virtual cessation of the shift of workers from
small-scale inefficient farming and from
self-employment in petty trade to higher
productivity occupations in larger- scale
urban enterprise. A portion too is assignable
to the massive entry of workers-youth and
women-since the mid-1960's. Finally, a
small part of the retardation is attributable
to the diversion of resources to comply with
environmental regulation and safety requirements
in ways that do not register in
measured output, though, of course, they


### ---Economics-1981-0-09.txt---
should. Serious students, however, offer
widely different estimates of the contributions
of other factors: the quality of schooling,
conventional capital services, R&D, and
the influence of cyclical or other forces affecting
intensity of resource use. The impact
of higher energy prices on the substitution
of labor for capital in the operation of existing
energy-using equipment and on the
post-1973 slowdown of capital deepening is
equally unclear, though possibly very important.
Most analyses leave a substantial
part of the retardation unconnected with
any identified and measured contributory
source, and they disagree about the timewhether
after 1973 or as early as the latter
1960's-when that unspecified residual retardation
made its appearance.'2

In this state of factual uncertainty, it is
not hard to propose estimates of the sources
of retardation which assign substantial responsibility
to factors connected with the

government's welfare and regulatory activities.
We, therefore, find William Fellner asking:
"...whether, directly or indirectly [the
analyses of the retardation] do not suggest
that the weakening of the productivity trend
is attributable in part to changes in the
socio-political environment that are of recent
origin or that have cumulated to a
'critical mass"' (p. 4).

The suggested mode of operation of these
factors is, first, through a decline in the rate
of capital deepening; second, through a decline
of worker effort symptomized by absenteeism
and by a drop in hours worked

relative to hours paid; third, by a disinclination
for risky, innovatory effort, whose
manifestation is the observed slowdown in
the residual measures of total factor productivity
growth; and fourth, through the diversion
of resources to regulatory compliance,
the benefits of which do not register in
measured output even when they should.
These sources of retardation whether great
or small-the "suspects," as Fellner calls
them-are arguably associated with characteristic
features of our mixed economy,
even if they are not exclusively due to them.
The first of those features is the widening
difference between before- and after-tax
marginal rates of return to work, saving,
investment, and risk taking. The magnitude
of the rise in these rates is indicated by the
overall increase of total government expenditures
from 20 percent of GNP in

1947-49 to 28 percent in 1963-65 and again
to over 32 percent in 1977-79.13 The incentive
effects of the tax increases are still imperfectly
understood, but there is little
reason to suppose they are not distinctly
unfavorable.'4 Allied to the effects of rising
tax burdens is the possible effect of the
cumulating "social security wealth" of individuals
on savings and that of other insurance
and income-support programs on
work.'5 Next, there are the effects of burgeoning
regulatory activity. These go beyond
the direct resource costs of compliance
already mentioned. There are also indirect
costs and risks of obtaining administrative
and judicial clearance for new projects, the
diversion of R&D expenditure to meet environmental
and safety standards, and the

hazards of possible future changes in regulatory
requirements. Finally, there are the
manifold effects of erratic and accelerating
inflation.

12Some representative references which illustrate the
variety and uncertainty of the results obtained by different
investigators are: Edward Denison, especially ch.
9; J. R. Norsworthy, Michael Harper, and Kent Kunze,
pp. 387-421, and the accompanying discussion and
reports by Peter Clark, Martin Baily, Denison, and
Michael Wachter; Laurits Christainsen and Haveman;
Robert Coen and Bert Hickman; Kendrick (1980); M.
Ishaq Nadiri.

13See Economic Report of the President (1980, Table
B-72).

14See James Tobin, Lecture III.
'3The large effect shown in Feldstein's original,
much-noticed time-series analysis (1974) has been
thrown into doubt by the discovery of a flaw in his
computer program. In a forthcoming NBER working
paper, he now finds a smaller but still significant effect.
Such time-series estimates remain uncertain because it
is hard to measure expected Social Security benefits
and hard to separate the effects of Social Security
wealth on saving from those of other variables during
periods of relative stability, as in samples covering the
postwar years alone. The conclusion that Social Security
benefits work to reduce saving, however, is sup-
ported by other studies, based on samples of individual
households and on cross-country evidence, to which
Feldstein refers in his new working paper.


### ---Economics-1981-0-10.txt---
Inflation belongs in this litany because
our pursuit of alternative welfare goals has
thus far also involved a tolerance, indeed a
pressure, for chronic budgetary deficits, and
an understandable political incapacity to
employ monetary and fiscal restraint forcefully
and consistently at the risk of elevated
unemployment. Inflation, in conjunction
with tax rules and accounting practices designed
for a stable price regime, has meant
very high marginal taxes on returns to
capital. In the judgement of some public
finance experts, it has also meant a differential
burden on business investment compared
with that on household borrowing,
spending, and investing.'6 If there are fears
of accelerated inflation in the future, they
carry the prospect of still higher taxes and
lower returns while the erratic nature of
rapid inflation makes the future more difficult
to discern and increases the sense of
risk. And if the same fears give rise to a
vision of price controls, the risks of investment
and innovation are compounded. In
any event, inflation compels- or threatens
to compel-governments to reduce capacity
utilization below its potential. Therefore
inflation acts to diminish one of the inducements
to invest, as the 1980 business contraction
following on financial disorder illustrates.
We should remember, moreover,

that there is an element of vicious circularity
in this aspect of our present conjecture. Inflation
has deleterious effects on productivity
growth-and unexpected declines in
productivity growth exacerbate inflation.
This range of considerations leads some
students to the view that the pursuit of
alternative welfare goals accounts for a very
considerable part of the retardation. Fellner,
whom I mentioned before, suggests that "the
causes of at least 1 percentage point annual
slackening of the trend in output per
worker's hour can be found among the 'suspects"
' (p. 10). That loss is equal to one-half
the observed difference between the privatesector
productivity growth since 1973 and
that during the quarter century between 1948
and 1973.

Such numbers and the argument that leads
to them should be understood to be no more
than what they are-a prima facie indication
that something very substantial may be
involved in the choices we make between
productivity growth and alternative welfare
goals. I would not mention them if I did not
fear that there is much to the problem, if
not as a cause of the recent abrupt retardation,
then as a longer-term secular constraint.
Yet, at the present time the argument
is only speculative, and the estimated
loss still more so. The theoretical and
quantitative issues are unsettled and deserve
our most urgent attention.'7


### ---Economics-1981-0-11.txt---
VI

So much for the past. We must now try to
look ahead. What general view of the future
is it sensible to entertain? And what are its
implications?

Since our understanding of the productivity
retardation of the last dozen years is so
clouded, conjecture about the future must
be still more fuzzy. True, the negative impact
of the recent big influx of inexperienced
young workers is due to be reversed.
In looking ahead, however, more basic questions
need to be addressed. No one, indeed,
ought to doubt the persistence of some
substantial continuity in what Solomon
Fabricant has identified as

the basic factors underlying economic
growth in the United States: the tastes
and preferences of the American people,
the economic opportunities and
alternatives open to them, the social
framework within which they live and
work together, and the relations of the
United States with the rest of the
world. Different assumptions would be
contrary to all experience and could
only lead to wild speculation. [So he
concludes] The trend of national output
per worker-hour will ... continue to
be upward. [p. 1]

I agree; but, as Fabricant also asks, how
fast will the trend line rise? A "substantial
degree of continuity" is not the same as
ironclad fixity, and much of this talk has
already pointed to some change in
Fabricant's basic factors. Within the country,
preferences and goals have changed in
the degree to which concern for income
security, equality of opportunity, environmental
protection, and consumer and worker
safety sways votes and, to some degree, personal
behavior. Corresponding to these shifts
in tastes and concerns, the "social framework
within which we live and work together"
has been recast. The government
has come to play a larger role in shaping the
"economic opportunities and alternatives"
open to us-while imposing burdens on our
growth potential whose weight we can now
suspect but cannot yet clearly assess. Partly
because of higher incomes, partly because
of changes in industrial and labor market
organization, and partly because of government
regulation and income support, there
has been a decline in market flexibility-in
the responsiveness of prices and wages to
the balance of supply and demand, and
of people's own responsiveness to price
changes-the implications of which Tibor
Scitovsky sketched last year.

Our relations with the rest of the world
have also changed in ways which I believe
are dominantly, but not entirely, unfavorable
to U.S. growth prospects. The economic
rise of Europe and Japan has, indeed,
brought those countries to the technological
after 1973, namely, the great increase in the price of
energy and the rapid, accelerating, and erratic inflation.
Our mixed economy is then implicated to the extent
that it works to sustain, if not generate, inflation, and
to the extent that our welfare concerns impede the
formulation and execution of an energy policy consistent
with the maintenance and rapid rise of measured
productivity. Continuing work may well clear up
these questions about the responsibility of public policy
for the current retardation, but, for the time being, we
have to live with uncertainty.
The puzzle is still further confused by the experience
of the continental European countries. Their fiscal
burdens are on the whole heavier than those of the
United States, yet their productivity retardation does
not generally begin before the oil shock of 1973-74
and the aggravated inflationary disorders that followed.
One must, therefore, ask whether the longer persistence
of high European productivity growth rates did not
reflect a difference in "cyclical" experience. Unlike the
United States, they did not generally enjoy a cyclically
induced intensification of resource use in the early
1960's and, therefore, a cyclical acceleration of productivity
growth. They had no occasion, therefore, to suffer
a cyclical retardation in the latter 1960's, as the United
States may have done as our economy approached
capacity utilizatidn. We may also ask whether the
Europeans were more resistant to the incentive effects
of heavy taxes and large transfers because of the special
factors supporting their great postwar growth
booms; or perhaps because their tax and transfer systems
are designed differently than ours; or perhaps
because of still other matters that differentiate their
economies and societies from our own. Or is it the case
that what I have referred to as the "suspect factors"-
other than inflation and monetary disorder itself-have
little to do with the observed retardations of productivity
growth? Clearly, the theoretical and empirical issues
embodied in these questions call for our very urgent
attention.


### ---Economics-1981-0-12.txt---
frontier in many fields. On that account, the
effort and experience on which world technological
advance rests now has a wider

base. The United States, therefore, should
now begin to profit more from other countries'
technical effort even as other countries
borrow from us. It remains to be seen, of
course, whether we shall prove as successful
at borrowing and adapting foreign technology
as some other countries have been.
The advance of other countries, however,
also has a darker side for us. The development
of many industries in which this country
has long been a leader is now threatened
by the competition of other countries. This
changes the prospects for U.S. productivity
growth to our disadvantage. It is harder for
an industry to push forward, or even to keep
up with, the technological frontier when its
rate of expansion slows down, still harder
when it is contracting. It is an old story that,
in the course of aggregate productivity
growth, the rise of new, more rapidly progressing
industries constricts the growth of
the old. That is Schumpeter's "creative destruction,
" and it helps explain why retardation
in the growth of output and productivity
is the normal fate of individual industries
within a country, while the growth rate of
the aggregate remains constant or even
speeds up. The reverse, however, is not necessarily
true, nor even probably true. We
cannot count on new, more progressive sectors
stepping into the breach merely because
the development of our old industries is
constricted by foreign competition. Foreign
success, of course, offers us cheap imports.
Yet the experience of Britain from 1870 to
1913 presents this country with a worrisome
historical question mark. As Britain's basic
industries lost their leadership and markets
to the United States, Germany, and other
countries after 1870, Britain's labor productivity
growth rate was halved compared with
previous decades, and her average total factor
productivity growth during the forty
years after 1870 fell to zero.'8 The question
is: Can we mount a more energetic and
successful response to the challenge of newly
rising foreign competitors after 1970 than
Britain did after 1870?19

The relative decline of U. S. economic and
political power carries with it other disadvantages,
and not for ourselves alone. The
leadership of the United States in the liberalization
and stabilization of international
economic relations was one of the bases for
rapid world-wide productivity growth in the
postwar years. We were able to assert that
leadership because superabundant economic
strength permitted us to propose arrangements
beneficial to ourselves but generous
to other countries, and because dominant
political power persuaded sometimes recalcitrant
partners to cooperate. Today, with
U.S. influence reduced and U.S. as well as
European industries under pressure, the
world economy is threatened by a resurgence
of protectionism, in which this country
is itself taking part. The world-wide price
discipline, which a relatively stable U. S
monetary policy imposed through the dollar-
exchange standard, has, for the time
being, been lost. And with U.S. influence
diminished, effective international cooperation
in the petroleum market and in other
aspects of relations between industrialized
and developing countries has been beyond
our reach.

In these circumstances, it is just as difficult
to maintain a vision of an unbroken 3


### ---Economics-1981-0-13.txt---
percent trend rate of private-sector productivity
growth as it is to discard a vision of a
trend rate which continues to be significantly
positive. It should, therefore, be no
surprise that official and other responsible
projections foresee productivity growth rates
that lie above zero, but significantly below
the average postwar rate.20

The uncertainty surrounding any such
forecasts can hardly be overstated. The
progress of science and the enlargement of
the knowledge bases of technology go on
apace. Our problem is to overcome or mitigate
the forces that are checking our ability
to give our growing knowledge practical application
and to exploit its benefits fully.
There are both physical and monetary sides
to our present condition which make our
prospects particularly perplexing. On the
physical side is the new energy question.
Quite apart from the policies we pursuewhich
may themselves be of crucial importance-
we do not now know on what terms
supplies will be available, even so far as they
depend only on physical and technological
considerations. We are uncertain about the
elasticity of substitution between energy and
other resources, and we do not know how
much technological progress will itself be
impeded as we try to move along a lessenergy-
intensive path than we have followed
in the past. The spread of industrialization
from Europe and North America to Asia
and Latin America also raises questions
about the supplies of other primary materials.
As for money, so long as we prove
incapable of overcoming our present disposition
to inflation, we shall not be able to
reach and exploit what would otherwise be
the growth potentials of our economy. But if
we ever do regain a substantial degree of
price stability, we may be happily surprised,
even as the Stagnationists of the 1930's were
astonished by our growth performance in
the postwar period.

VII

In spite of these uncertainties and whatever
pleasant or gloomy surprises they may
hold, we can hardly avoid the present presumption
that our policy choices in the
calculable future will need to be made in a
less favorable growth environment than that
of the generation just past. Our problem of
choice will be all the more aggravated if, as
now seems likely, the burden of defense
expenditures must increase.

That means, first, that our further pursuit
of social welfare goals will have to be paid
for out of smaller increments of output and
income. So, there will be a more difficult
problem of choice even if our growth rate
itself were not affected by what we choose.
It means, second, that the impact of our
choices on the measured growth rate itself
becomes a more pressing concern and may
go far to determine whether the projections
now entertained are, indeed, ratified by history
or belied. The new, more confined
growth environment means, third, that the
role of government as a contributor to measured
productivity will also be more vitally
important, not merely insofar as the government
may act to minimize its regulatory or
fiscal impact on private performance, but
also in the support it gives to research, education,
information, labor mobility, and to
human capital formation generally.
As we think about these questions, we
should not be trapped in the grooves of
popular debate. As already said, the alternative
paths to economic progress do not present
us with clear-cut choices between
welfare through government production
guidance and income redistribution on the
one side, and welfare through private productivity
growth on the other. Even if we
cared for little except the private use of
private earnings, we could not ignore the
costs and conflicts arising from the economic
and social displacements which
accompany growth. We could not, for
20For example, in its 1979 Economic Report of the
President, the Council of Economic Advisors estimated
the current trend rate of advance of labor productivity
in the national economy at 1.5 percent a year, corresponding
to 1.75 percent in the private sector, which
is little more than half the postwar pace. In its 1980
Report, moreover, the Council writes: "Since the average
rate of increase during the past 6 years has been
below that figure [of 1.5 percent], the trend rate of
increase [in the national economy] may very well be
still lower, perhaps 1 percent" (p. 88). For further
discussion and other projections, see Fabricant (pp.
63 ff.).


### ---Economics-1981-0-14.txt---
example, disregard problems which the
changing structure and role of the family
bring in their train. The state of our cities
with all their problems of poverty, crime
and deteriorating education, and all their
exposure to the pressures of racial concentration
and frustration, should be a sufficient
reminder. All are bound up with the
productivity growth process itself. They are
sources of antagonism, conflict, and decline
of personal quality which will work to constrain
growth unless moderated.

VIII

In the new, less-favorable growth environment,
the tensions between productivity and
other welfare goals are screwed several
notches tighter. The success of our mixed
economy and pluralistic society in the next
generation will depend heavily on how those
tensions are managed. In present circumstance,
therefore, economic progress turns
very largely on the policies we pursue, on
what we do through government, and how
we do it. As things now stand, however, we
can hardly be said to be adopting policies so
much as floundering among them, recoiling
from growth and backlashing against the
recoil, for lack of knowledge and for lack of
proper political institutions to use such
knowledge as we have.

The gaps in our knowledge define the job
for economics. Virtually every facet of the
way productivity depends on policy involves
matters of fact still to be established. What
is the elasticity of substitution between energy
and other resources, and how much
will it cost us in future output if we forego
the cheapest mode of increasing energy supplies
in order to provide a greater degree of
protection for environment and people?
What are the full benefits and what are the
full costs of other environmental or safety
measures as now legislated and applied? And
how much could we save if we sought similar
levels of protection more efficiently by
making larger use of market incentives as
regulatory devices? What are the effects of
different levels and-just as importantdifferent
types of taxes and transfers on the
supplies of saving, investment, and risky
enterprise, and on the supply of labor and
the quality of people? What is the full range
of our government expenditure which has
the character of capital formation-and
what are the returns to investment in education
and in research and development? What
would our progress in productivity look like
if we tracked it by a system of national
accounts more relevant to long-term change
in economic welfare than our conventional
national product? The questions go on and
on. These are matters to which, for the most
part, economists have only recently turned.
They are now being attacked with vigor,
which is testimony to the fact that the aggravated
tension between measured productivity
growth and other welfare goals is
eliciting a constructive response. There are
promising beginnings of useful analytical
and empirical work, and these will benefit
from future experience and experiment. At
the same time, our knowledge about this
entire range of questions continues to be
uncertain.

The weakness of our knowledge, moreover,
is matched, probably exceeded, by the
weakness of the political institutions and
procedures through which that knowledge
must be brought to bear. The structure of
government and politics, which served us
well enough during a more individualistic
era and before the population movements of
the last fifty years, has not been successfully
adapted to the new scale and complexity of
public functions. Let me just allude to three
political problems.

One concerns federal budget procedure.
In principle, the budget is the place where
the conflicting claims of special interests
should confront, not only one another, but
also the general interest in economy and in
maintaining a balance between private and
public uses of income. It is also the place
where our concern for increasing welfare by


### ---Economics-1981-0-15.txt---
raising measured productivity should be
brought into balance with our interest in
other welfare goals. But our budgetary process,
in spite of improvements in recent
years, remains weak. Tolerance for deficits
is the overt, inflation is the covert, mode by
which competing claims are reconciled. For
lack of a systematic way of facing the future
costs of present acts, three-quarters of the
budget consists of "uncontrollable" items.
Capital investment is not distinguished from
current consumption. We have just begun to
recognize that regulatory acts impose private
costs of compliance, analogous to excise
taxes, which must somehow be brought
within the budgetary ambit of the public
household.

A second matter is what, by pleasant euphemism,
is called our system of local

government. Fractionated geographically
and functionally and poorly coordinated,
operating in a confused relation to the
federal government, plagued by financial
crisis reflecting in part the disjunction between
the populations they serve and the tax
bases on which they rest, our towns, cities,
and districts are fertile generators of external
costs, duplicative and costly regulation,
and chronic neglect. If, as historians generally
agree, Britain could not have carried
through its Industrial Revolution without
the great Victorian reforms of local government,
we ought to be asking whether we can
meet the emerging problems of growth and
welfare in the second half century of our
mixed economy without also facing up to
the need for systematic local government
reform.

The third matter is both basic and diffuse,
and that is the weakness of our party system.
It is a commonplace that our national
parties are no more than fluid, transistory,
and undisciplined coalitions of regional
and economic interest groupings. Their lack
of central organization and authority, reflecting
the size and diversity of the country
and people, and our lack of ideological
commitment, lays us wide open to the distorting
influence of special-interest lobbies
and single issue politics. In our political life,
we are all too vulnerable to particularistic
pressures and all too resistant to the needs
of general interest legislation.
Ix

The rationale supporting the development
of our mixed economy sees it as a pragmatic
compromise between the competing virtues
and defects of decentralized market capitalism
and encompassing socialism. Its goal
is to obtain a measure of distributive justice,
security, and social guidance of economic
life without losing too much of the allocative
efficiency and dynamism of private enterprise
and market organization. And it is a
pragmatic compromise in another sense. It
seeks to retain for most people that measure
of personal protection from the state which
private property and a private job market
confer, while obtaining for the disadvantaged
minority of people through the state
that measure of support without which their
lack of property or personal endowment
would amount to a denial of individual freedom
and capacity to function as full members
of the community.

The viability, to say nothing of the success,
of this compromise demands a rough, threecornered
balance between the degree to

which we look for economic progress
through the development of our powers of
production by private action, the degree to
which we try through government to protect
and promote those aspects of production
which markets do not reach, and the degree
to which we use governments to alter and
cushion the market's income verdicts and to
resolve the social conflicts which are inherent
in growth and change. Until recently, we
have paid inadequate attention to the requirements
of achieving that balance wisely.
We were able to neglect the problem because
we enjoyed the amplitude of a run of
fortunate years, when rapid and steady
growth was the unseen moderator of the
tensions of balance. In the new and less
favorable environment of growth, however,
the tensions between productivity and the
alternative dimensions of welfare are aggravated
and the problems of balance-of
how much to do and how to do it- are
more severe.

In the last analysis, values- feelings,
tastes, and sympathies- control choices. But
those feelings and sympathies should not


### ---Economics-1981-0-16.txt---
have to be deployed with the sad deficiencies
of knowledge which, in so many spheres,
is the case today. Nor should we have
to bring feelings and knowledge to bear
through political institutions and procedures
which are as imperfect as those through
which we -now act.

When Keynes spoke of the potential efficiency
of a "wisely managed" capitalism,
he was assuming that the knowledge necessary
for wise management was either in
hand or would be forthcoming. But he did
not seem to be thinking about the limitations
of the political process in bringing
knowledge to bear. Now that economists
and other social scientists have begun to
work at it, we can be cautiously hopeful that
our knowledge about both the tradeoffs and
the complementarities between productivity
growth and the other dimensions of economic
welfare will gradually improve. For
the calculable future, however, our limited
political capabilities may well prove to be
the most binding constraint on our ability to
work out a social organization which, as
Keynes said, "shall be as efficient as possible
without offending our notions of a
satisfactory way of life."

Contemplating these obdurate realities,
what can one say to conclude this talk on an
upbeat note? The best I can do is a somewhat
inspirational passage from a lecture by
Jacob Viner, who, as we all know, was no
flaming New Dealer, no Great Society man,
and no Keynesian. I am fond of this passage,
not only because of its sturdy determination,
but also because it displays so
well Viner's precise but involuted mind, and
his amiable weakness for the nonstop
sentence. At the close of a long critique of
the American welfare state, which is the
mixed economy I have been talking about,


### ---Economics-1981-0-17.txt---
Viner says:

For all these reasons, ... there is in the
abstract no reason for making an idol
of the welfare state in its American
form or for dedicating ourselves unreservedly
to its continuance as it is today
without qualification or amendment.
Given the... imperfection of the
procedures whereby it deals with problems
which it cannot evade or defer or
with problems which special interests
may press upon it for premature resolution,
it would be only by the dispensation
of a benevolent Providence

that it would ever make precisely the
right decisions or always avoid major
mistakes. It does not have theoretical
superiority over all conceivable alternative
systems.... If ... I nevertheless
conclude that I believe that the welfare
state, like old Siwash, is really worth
fighting for and even dying for as
compared to any rival system, it is
because, despite its imperfections in
theory and in practice, in the aggregate
it provides more promise of preserving
and enlarging human freedoms,

temporal prosperity, the extinction
of mass misery, and the dignity of
man and his moral improvement than
any other social system which has previously
prevailed, which prevails

elsewhere today or which, outside
Utopia, the mind of man has been
able to provide a blueprint for.
[pp. 166-67]

APPENDIX TABLE 2- INDICATORS OF CHANGE IN MATERIAL WELFARE
Compound Growth Rates (percent per year)
1948-65 1965-73 1973-79

Productivity and Per Capita GNP
(1) GNP per employed worker 2.57 1.60 0.25
(2) Workers per capita -0.42 1.02 1.43
(3) GNP per capita 2.14 2.64 1.69
Real Disposable Income per Capita
(4) All income (PCE deflator) 1.90 3.22 1.75
(5) (CPI deflator) 2.21 2.85 0.84
(6) All income less transfers (PCE) 1.74 2.55 1.29
(7) (CPI) 2.05 2.18 0.38

(8) All income less transfers and
other labor incomee (PCE) 1.58 2.27 0.79
(9) (CPI) 1.89 1.90 -0.11

Workers' Earnings

(10) Real compensation per full-time
equivalent employee (PCE) 2.66 2.69 0.84a
(11) (CPI) 2.96 2.32 0.19a

(12) Real wages and salaries per fulltime
equivalent employee (PCE) 2.35 2.20 0.lOa
(13) (CP,I) 2.66 1.83 -0.54a

(14) Real wage and salary income, fulltime
white males (PCE) 3.01 -0.4 b

(15) (CPI) 2.61 -1.03"

Median Real Total Income, Persons 14- Years Old and Over
(16) All males (PCE) 2.44c 2.00 -1.O9a
(17) (CPI) 2.65c 1.64 -1.73a

(18) Year-round full-time male
workers (PCE) 2.6Id 3.03 -0.32a
(19) (CPI) 2.8Id 2.66 -0.95a

Median Real Total Family Income
(20) PCE deflator 2.74 2.99 0.48a
(21) CPI deflator 3.05 2.62 -0. 16a (Continued)


### ---Economics-1981-0-18.txt---

 ## Economics-1982-0


### ---Economics-1982-0-03.txt---
The address of the departing president is
no place for modesty. Nevertheless, I must
resist the temptation to describe the analysis
I will report here as anything like a revolution.
Perhaps terms such as "rebellion" or
"uprising" are rather more apt. But, nevertheless,
I shall seek to convince you that the
work my colleagues, John Panzar and Robert
Willig, and I have carried out and encapsulated
in our new book enables us to look at
industry structure and behavior in a way that
is novel in a number of respects, that it
provides a unifying analytical structure to
the subject area, and that it offers useful
insights for empirical work and for the formulation
of policy.

Before getting into the substance of the
analysis I admit that this presidential address
is most unorthodox in at least one significant
respect-that it is not the work of a single
author. Here it is not even sufficient to refer
to Panzar and Willig, the coauthors of both
the substance and the exposition of the book
in which the analysis is described in full. For
others have made crucial contributions to the
formulation of the theory-most notably
Elizabeth Bailey, Dietrich Fischer, Herman
Quirmbach, and Thijs ten Raa.

But there are many more than these. No
uprising by a tiny band of rebels can hope to
change an established order, and when the
time for rebellion is ripe it seems to break
out simultaneously and independently in a
variety of disconnected centers each offering
its own program for the future. Events here
have been no different. I have recently received
a proposal for a conference on new
developments in the theory of industry structure
formulated by my colleague, Joseph
Stiglitz, which lists some forty participants,
most of them widely known. Among those
working on the subject are persons as well
known as Caves, Dasgupta, Dixit, Friedlaender,
Grossman, Hart, Levin, Ordover,
Rosse, Salop, Schmalensee, Sonnenschein,
Spence, Varian, von Weiszacker, and
Zeckhauser, among many others.' It is, of
course, tempting to me to take the view that
our book is the true gospel of the rebellion
and that the doctrines promulgated by others
must be combatted as heresy. But that could
at best be excused as a manifestation of the
excessive zeal one comes to expect on such
occasions. In truth, the immediate authors of
the work I will report tonight may perhaps
be able to justify a claim to have offered
some systematization and order to the new
doctrines-to have built upon them a more
comprehensive statement of the issues and
the analysis, and to have made a number of
particular contributions. But, in the last
analysis, we must look enthusiastically upon
our fellow rebels as comrades in arms, each
of whom has made a crucial contribution to
the common cause.

Turning now to the substance of the theory,
let me begin by contrasting our results
with those of the standard theory. In offering
this contrast, let me emphasize that much of
the analysis rests on work that appeared
considerably earlier in a variety of forms.


### ---Economics-1982-0-04.txt---
We, no less than other writers, owe a heavy
debt to predecessors from Bertrand to Bain,
from Cournot to Demsetz. Nevertheless, it
must surely be acknowledged that the following
characterization of the general tenor of
the literature as it appeared until fairly recently
is essentially accurate.

First, in the received analysis perfect competition
serves as the one standard of welfare-
maximizing structure and behavior.
There is no similar form corresponding to
industries in which efficiency calls for a very
limited number of firms (though the earlier
writings on workable competition did move
in that direction in a manner less formal
than ours).

Our analysis, in contrast, provides a generalization
of the concept of the perfectly competitive
market, one which we call a "perfectly
contestable market." It is, generally,
characterized by optimal behavior and yet
applies to the full range of industry structures
including even monopoly and oligopoly.
In saying this, it must be made clear that
perfectly contestable markets do not populate
the world of reality any more than perfectly
competitive markets do, though there
are a number of industries which undoubtedly
approximate contestability even if
they are far from perfectly competitive. In
our analysis, perfect contestability, then,
serves not primarily as a description of reality,
but as a benchmark for desirable
industrial organization which is far more
flexible and is applicable far more widely
than the one that was available to us before.
Second, in the standard analysis (including
that of many of our fellow rebels), the properties
of oligopoly models are heavily dependent
on the assumed expectations and reaction
patterns characterizing the firms that
are involved. When there is a change in the
assumed nature of these expectations or reactions,
the implied behavior of the oligopolistic
industry may change drastically.
In our analysis, in the limiting case of
perfect contestability, oligopolistic structure
and behavior are freed entirely from their
previous dependence on the conjectural variations
of incumbents and, instead, these are
generally determined uniquely and, in a
manner that is tractable analytically, by the
pressures of potential competition to which
Bain directed our attention so tellingly.
Third, the standard analysis leaves us with
the impression that there is a rough continuum,
in terms of desirability of industry
performance, ranging from unregulated pure
monopoly as the pessimal arrangement to
perfect competition as the ideal, with relative
efficiency in resource allocation increasing
monotonically as the number of firms expands.


I will show that, in contrast, in perfectly
contestable markets behavior is sharply
discontinuous in its welfare attributes. A
contestable monopoly offers us some presumption,
but no guarantee, of behavior consistent
with a second best optimum, subject
to the constraint that the firm be viable
financially despite the presence of scale
economies which render marginal cost pricing
financially infeasible. That is, a contestable
monopoly has some reason to adopt the
Ramsey optimal price-output vector, but it
may have other choices open to it. (For the
analysis of contestable monopoly, see my
article with Elizabeth Bailey and Willig, Panzar
and Willig's article, and my book with
Panzar and Willig, chs. 7 and 8.)
But once each product obtains a second
producer, that is, once we enter the domain
of duopoly or oligopoly for each and every
good, such choice disappears. The contestable
oligopoly which achieves an equilibrium
that immunizes it from the incursions of
entrants has only one pricing option-it must
set its price exactly equal to marginal cost
and do all of the things required for a first
best optimum! In short, once we leave the
world of pure or partial monopoly, any contestable
market must behave ideally in every
respect. Optimality is not approached gradually
as the number of firms supplying a
commodity grows. As has long been suggested
in Chicago, two firms can be enough
to guarantee optimality (see, for example,
Eugene Fama and Arthur Laffer).
Thus, the analysis extends enormously the
domain in which the invisible hand holds
sway. In a perfectly contestable world, it
seems to rule almost everywhere. Lest this


### ---Economics-1982-0-05.txt---
seem to be too Panglossian a view of reality,
let me offer two observations which make it
clear that we emphatically do not believe
that all need be for the best in this best of all
possible worlds.

First, let me recall the observation that
real markets are rarely, if ever, perfectly contestable.
Contestability is merely a broader
ideal, a benchmark of wider applicability
than is perfect competition. To say that contestable
oligopolies behave ideally and that
contestable monopolies have some incentives
for doing so is not to imply that this is even
nearly true of all oligopolies or of unregulated
monopolies in reality.

Second, while the theory extends the domain
of the invisible hand in some directions,
it unexpectedly restricts it in others.
This brings me to the penultimate contrast I
wish to offer here between the earlier views
and those that emerge from our analysis.
The older theoretical analysis seems to have
considered the invisible hand to be a rather
weak intratemporal allocator of resources, as
we have seen. The mere presence of unregulated
monopoly or oligopoly was taken to be
sufficient per se to imply that resources are
likely to be misallocated within a given time
period. But where the market structure is such
as to yield a satisfactory allocation of resources
within the period, it may have seemed that it
can, at least in theory, do a good job of
intertemporal resource allocation. In the absence
of any externalities, persistent and
asymmetric information gaps, and of interference
with the workings of capital markets,
the amounts that will be invested for the
future may appear to be consistent with
Pareto optimality and efficiency in the supply
of outputs to current and future generations.
However, our analysis shows that where
there are economies of scale in the production
of durable capital, intertemporal contestable
monopoly, which may perform relatively
well in the single period, cannot be
depended upon to perform ideally as time
passes. In particular, we will see that the
least costly producer is in the long run
vulnerable to entry or replacement by rivals
whose appearance is inefficient because it
wastes valuable social resources.
There is one last contrast between the
newer analyses and the older theory which I
am most anxious to emphasize. In the older
theory, the nature of the industry structure
was not normally explained by the analysis.
It was, in effect, taken to be given exogenously,
with the fates determining, apparently
capriciously, that one industry will
be organized as an oligopoly, another as a
monopoly and a third as a set of monopolistic
competitors. Assuming that this destiny
had somehow been revealed, the older
analyses proceeded to investigate the consequences
of the exogenously given industry
structure for pricing, outputs, and other decisions.
2

The new analyses are radically different in
this respect. In our analysis, among others,
an industry's structure is determined explicitly,
endogenously, and simultaneously
with the pricing, output, advertising, and
other decisions of the firms of which it is
constituted. This, perhaps, is one of the prime
contributions of the new theoretical analyses.
I. Characteristics of Contestable Markets
Perhaps a misplaced instinct for melodrama
has led me to say so much about
contestable markets without even hinting
what makes a market contestable. But I can
postpone the definition no longer. A contestable
market is one into which entry is
absolutely free, and exit is absolutely costless.
We use "freedom of entry" in Stigler's sense,
not to mean that it is costless or easy, but
that the entrant suffers no disadvantage in
terms of production technique or perceived
product quality relative to the incumbent,
20f course, any analysis which considered the role of
entry, whether it dealt with perfect competition or monopolistic
competition, must implicitly have considered
the determination of industry structure by the market.
But in writings before the 1970's, such analyses usually
did not consider how this process determined whether
the industry would or would not turn out to be, for
example, an oligopoly. The entry conditions were studied
only to show how the assumed market structure could
constitute an equilibrium state. Many recent writings
have gone more explicitly into the determination of
industry structure, though their approaches generally
differ from ours.


### ---Economics-1982-0-06.txt---
and that potential entrants find it appropriate
to evaluate the profitability of entry
in terms of the incumbent firms' pre-entry
prices. In short, it is a requirement of contestability
that there be no cost discrimination
against entrants. Absolute freedom of
exit, to us, is one way to guarantee freedom
of entry. By this we mean that any firm can
leave without impediment, and in the process
of departure can recoup any costs incurred
in the entry process. If all capital is salable
or reusable without loss other than that corresponding
to normal user cost and depreciation,
then any risk of entry is eliminated.
Thus, contestable markets may share at
most one attribute with perfect competition.
Their firms need not be small or numerous
or independent in their decision making or
produce homogeneous products. In short, a
perfectly competitive market is necessarily
perfectly contestable, but not vice versa.
The crucial feature of a contestable market
is its vulnerability to hit-and-run entry. Even
a very transient profit opportunity need not
be neglected by a potential entrant, for he
can go in, and, before prices change, collect
his gains and then depart without cost, should
the climate grow hostile.

Shortage of time forces me to deal rather
briefly with two of the most important properties
of contestable markets-their welfare
attributes and the way in which they determine
industry structure. I deal with these
briefly because an intuitive view of the logic
of these parts of the analysis is not difficult
to provide. Then I can devote a bit more
time to some details of the oligopoly and the
intertemporal models.

A. Perfect Contestability and Welfare
The welfare properties of contestable
markets follow almost directly from their
definition and their vulnerability to hit-andrun
incursions. Let me list some of these
properties and discuss them succinctly.
First, a contestable market never offers
more than a normal rate of profit-its economic
profits must be zero or negative, even
if it is oligopolistic or monopolistic. The
reason is simple. Any positive profit means
that a transient entrant can set up business,
replicate a profit-making incumbent's output
at the same cost as his, undercut the incumbent'
s prices slightly and still earn a
profit. That is, continuity and the opportunity
for costless entry and exit guarantee that
an entrant who is content to accept a slightly
lower economic profit can do so by selecting
prices a bit lower than the incumbent's.
In sum, in a perfectly contestable market
any economic profit earned by an incumbent
automatically constitutes an earnings opportunity
for an entrant who will hit and, if
necessary, run (counting his temporary but
supernormal profits on the way to the bank).
Consequently, in contestable markets, zero
profits must characterize any equilibrium,
even under monopoly and oligopoly.
The second welfare characteristic of a contestable
market follows from the same argument
as the first. This second attribute of
any contestable market is the absence of any
sort of inefficiency in production in industry
equilibrium. This is true alike of inefficiency
of allocation of inputs, X-inefficiency, inefficient
operation of the firm, or inefficient
organization of the industry. For any unnecessary
cost, like any abnormal profit, constitutes
an invitation to entry. Of course, in
the short run, as is true under perfect competition,
both profits and waste may be present.
But in the long run, these simply cannot
withstand the threat brandished by potential
entrants who have nothing to lose by grabbing
at any opportunity for profit, however
transient it may be.

A third welfare attribute of any long-run
equilibrium in a contestable market is that
no product can be sold at a price, p, that is
less than its marginal cost. For if some firm
sells y units of output at such a price and
makes a profit in the process, then it is
possible for an entrant to offer to sell a
slightly smaller quantity, y - E, at a price a
shade lower than the incumbent's, and still
make a profit. That is, if the price p is less
than MC, then the sale of y - E units at price
p must yield a total profit X + A7r which is
greater than the profit, va, that can be earned
by selling only y units of output at that price.
Therefore, there must exist a price just
slightly lower than p which enables the entrant
to undercut the incumbent and yet to


### ---Economics-1982-0-07.txt---
earn at least as much as the incumbent, by
eliminating the unprofitable marginal unit.
This last attribute of contestable equilibria
-the fact that price must always at least
equal marginal cost-is important for the
economics of antitrust and regulation. For it
means that in a perfectly contestable market,
no cross subsidy is possible, that is, no
predatory pricing can be used as a weapon of
unfair competition. But we will see it also
has implications which are more profound
theoretically and which are more germarte to
our purposes. For it constitutes half of the
argument which shows that when there are
two or more suppliers of any product, its
price must, in equilibrium, be exactly equal
to marginal cost, and so resource allocation
must satisfy all the requirements of first best
optimality.

Indeed, the argument here is similar to the
one which has just been described. But there
is a complication which is what introduces
the two-firm requirement into this proposition.
p < MC constitutes an opportunity for
profit to an entrant who drops the unprofitable
marginal unit of output, as we have just
seen. It would seem, symmetrically, that p >
MC also automatically constitutes an opportunity
for profitable entry. Instead of selling
the y-unit output of a profitable incumbent,
the entrant can now offer to sell
the slightly larger output, y + e, using the
profits generated by the marginal unit at a
price greater than marginal cost to permit a
reduction in price below the incumbent's.
But on this side of the incumbent's output,
there is a catch in the argument. Suppose the
incumbent is a monopolist. Then output and
price are constrained by the elasticity of
demand. An attempt by an entrant to sell
y + e rather than y may conceivably cause a
sharp reduction in price which eliminates the
apparent profits of entry. In the extreme case
where demand is perfectly inelastic, there
will be no positive price at which the market
will absorb the quantity y +? . This means
that the profit opportunity represented by
p > MC can crumble into dust as soon as
anyone seeks to take advantage of it.
But all this changes when the market contains
two or more sellers. Now p > MC does
always constitute a real opportunity for profitable
entry. The entrant who wishes to sell a
bit more than some one of the profitable
incumbents, call him incumbent A, need not
press against the industry's total demand
curve for the product. Rather, he can undercut
A, steal away all of his customers, at least
temporarily, and, in addition, steal away e
units of demand from any other incumbent,
B. Thus, if A and B together sell Ya + Yb > Ya'
then an entrant can lure away Ya + -> Ya
customers, for - sufficiently small, and earn
on this the incremental profit e( p - MC)>O.
This means that the entrant who sells Ya + e
can afford to undercut the prevailing prices
somewhat and still make more profit than an
incumbent who sells Ya at price p.
In sum, where a product is sold by two or
more firms, any p > MC constitutes an irresistible
entry opportunity for hit-and-run entry
in a perfectly contestable market, for it
promises the entrant supernormal profits
even if they accrue for a very short period of
time.

Consequently, when a perfectly contestable
market contains two or more sellers,
neither p < MC nor p > MC is compatible
with equilibrium. Thus we have our third
and perhaps most crucial welfare attribute of
such perfectly contestable markets- their
prices, in equilibrium, must be equal to
marginal costs, as is required for Pareto optimality
of the "first best" variety. This, along
with the conclusion that such markets permit
no economic profits and no inefficiency in
long-run equilibrium, constitutes their critical
properties from the viewpoint of economic
welfare. Certainly, since they do enjoy
those three properties, the optimality of perfectly
contestable equilibria (with the reservations
already expressed about the case of
pure monopoly) fully justifies our conclusion
that perfect contestability constitutes a
proper generalization of the concept of perfect
competition so far as welfare implications
are concerned.

B. On the Determination of Industry
Structure

I shall be briefer and even less rigorous in
describing how industry structure is determined
endogenously by contestability


### ---Economics-1982-0-08.txt---
analysis. Though this area encompasses one
of its most crucial accomplishments, there is
no way I can do justice to the details of the
analysis in an oral presentation and within
my allotted span of time. However, an intuitive
view of the matter is not difficult.
The key to the analysis lies in the second
welfare property of contestable equilibriatheir
incompatibility with inefficiency of any
sort. In particular, they are incompatible with
inefficiency in the organization of an industry.
That is, suppose we consider whether
a particular output quantity of an industry
will be produced by two firms or by a thousand.
Suppose it turns out that the two-firm
arrangement can produce the given output at
a cost 20 percent lower than it can be done
by the 1,000 firms. Then one implication of
our analysis is that the industry cannot be in
long-run equilibrium if it encompasses 1,000
producers. Thus we already have some hint
about the equilibrium industry structure of a
contestable market.

We can go further with this example. Suppose
that, with the given output vector for
the industry, it turns out that no number of
firms other than two can produce at as low a
total cost as is possible under a two-firm
arrangement. That is, suppose two firms can
produce the output vector at a total cost
lower than it can be done by one firm or
three firms or sixty or six thousand. Then we
say that for the given output vector the industry
is a natural duopoly.

This now tells us how the industry's structure
can be determined. We proceed, conceptually,
in two steps. First we determine what
structure happens to be most efficient for the
production of a given output vector by a
given industry. Next, we investigate when
market pressures will lead the industry toward
such an efficient structure in equilibrium.


Now, the first step, though it has many
intriguing analytic attributes, is essentially a
pure matter of computation. Given the cost
function for a typical firm, it is ultimately a
matter of calculation to determine how many
firms will produce a given output most efficiently.
For example, if economies of scale
hold throughout the relevant range and there
are sufficient complementarities in the production
of the different commodities supplied
by the firm, then it is an old and
well-known conclusion that single firm production
will be most economical-that we
are dealing with a natural monopoly.
Similarly, in the single product case suppose
the average cost curve is U shaped and
attains its minimum point at an output of
10,000 units per year. Then it is obvious that
if the industry happens to sell 50,000 units
per year, this output can be produced most
cheaply if it is composed of exactly five
firms, each producing 10,000 units at its
point of minimum average cost.
Things become far more complex and more
interesting when the firm and the industry
produce a multiplicity of commodities, as
they always do in reality. But the logic is
always the same. When the industry output
vector is small compared to the output vectors
the firm can produce at relatively low
cost, then the efficient industry structure will
be characterized by very few firms. The opposite
will be true when the industry's output
vector is relatively far from the origin. In the
multiproduct case, since average cost cannot
be defined, two complications beset the characterization
of the output vectors which the
firm can produce relatively efficiently. First,
since here average cost cannot be defined, we
cannot simply look for the point of minimum
average costs. But we overcome this
problem by dealing with output bundles having
fixed proportions among commodity
quantities-by moving along a ray in output
space. Along any such ray the behavior of
average cost is definable, and the point of
minimum ray average cost (RA C) is our
criterion of relatively efficient scale for the
firm. Thus, in Figure 1 we have a ray average
cost curve for the production of boots and
shoes when they are produced in the proportion
given by ray OR. We see that for such
bundles ytm is the point of minimum RA C. A
second problem affecting the determination
of the output vectors the firm can produce
efficiently is the choice of output proportions
-the location of the ray along which the
firm will operate. This depends on the degree
of complementarity in production of the
goods, and it also lends itself to formal analysis.


We note also that the most efficient number
of firms will vary with the location of the


### ---Economics-1982-0-09.txt---
RAY

AVERAGE

COST

RAC

I I ~~~~~R

0 SHOES

FIGURE 1

industry's output vector. The industry may
be a natural monopoly with one output vector,
a natural duopoly with another, and
efficiency may require seventy-three firms
when some third output vector is provided
by the industry.

This, then, completes the first of the two
basic steps in the endogenous determination
of industry structure. Here we have examined
what industry structure is least costly
for each given output vector of a given industry,
and have found how the result depends
on the magnitudes of the elements of
that output vector and the shape of the cost
function of the typical firm. So far the discussion
may perhaps be considered normative
rather than behavioral. It tells us what
structure is most efficient under the circumstances,
not which industry structure will
emerge under the pressures of the market
mechanism.

The transition toward the second, behavioral,
stage of the analysis is provided by the
observation that the optimal structure of an
industry depends on its output vector, while
that output vector in turn depends on the
prices charged by its firms. But, since pricing
depends on industry structure, we are
brought full circle to the conclusion that
pricing behavior and industry structure must,
ultimately, be determined simultaneously and
endogenously.

We are in no position to go much further
than this for a market whose properties are
unspecified. But, for a perfectly contestable
market, we can go much further. Indeed, the
properties of perfect contestability cut
through every difficulty and tell us the equilibrium
prices, outputs, and industry structure,
all at once.

Where more than one firm supplies a
product, we have already characterized these
prices precisely. For we have concluded that
each equilibrium price will equal the associated
marginal cost. Then, given the industry's
cost and demand relationships, this yields
the industry's output quantities simultaneously
with its prices, in the usual manner.
Here there is absolutely nothing new in the
analysis.

But what is new is the format of the
analysis of the determination of industry
structure. As I have already pointed out,
structure is determined by the efficiency requirement
of equilibrium in any contestable
market. Since no such equilibrium is compatible
with failure to minimize industry costs,
it follows that the market forces under perfect
contestability will bring us results consistent
with those of our normative analysis.
Whatever industry structures minimize total
costs for the equilibrium output vector must
turn out to be the only structures consistent
with industry equilibrium in the long run.
Thus, for contestable markets, but for contestable
markets only, the second stage of the
analysis of industry structure turns out to be
a sham. Whatever industry structure was
shown by the first, normative, portion of the
analysis to be least costly must also emerge
as the industry structure selected by market
behavior. No additional calculations are required
by the behavioral analysis. It will all
have been done in the normative costminimization
analysis and the behavioral

analysis is pure bonus.

Thus, as I promised, I have indicated how
contestability theory departs from the older
theory which implicitly took industry structure
to be determined exogenously in a
manner totally unspecified and, instead,
along with other recent writings, embraces
the determination of industry structure as an
integral part of the theory to be dealt with
simultaneously with the determination of
prices and outputs.

At this point I can only conjecture about
the determination of industry structure once
we leave the limiting case of perfect contestability.
But my guess is that there are no


### ---Economics-1982-0-10.txt---
sharp discontinuities here, and that while the
industry structures which emerge in reality
are not always those which minimize costs,
they will constitute reasonable approximations
to the efficient structures. If this is not
so it is difficult to account for the similarities
in the patterns of industry structure that one
observes in different countries. Why else do
we not see agriculture organized as an
oligopoly in any free market economy, or
automobiles produced by 10,000 firms?
Market pressures must surely make any very
inefficient market structure vulnerable to entry,
to displacement of incumbents by foreign
competition, or to undermining in other
ways. If that is so, the market structure that
is called for by contestability theory may not
prove to be too bad an approximation to
what we encounter in reality.

II. On Oligopoly Equilibrium

I should like now to examine oligopoly
equilibrium somewhat more extensively. We
have seen that, except where a multiproduct
oligopoly firm happens to sell some of its
products in markets in which it has no competitors,
an important partial monopoly case
which I will ignore in what follows, all prices
must equal the corresponding marginal costs
in long-run equilibrium. But in an oligopoly
market, this is a troublesome concept. Unless
the industry output vector happens to fall at
a point where the cost function is characterized
by locally constant returns to scale,
we know that zero profits are incompatible
with marginal cost pricing. Particularly if
there are scale economies at that point, so
that marginal cost pricing precludes financial
viability, we can hardly expect such a solution
to constitute an equilibrium. Besides, we
have seen that long-run equilibrium requires
profit to be precisely zero. We would thus
appear to have run into a major snag by
concluding that perfect contestability always
leads to marginal cost pricing under oligopoly.


This is particularly so if the (ray) average
curve is U shaped, with its minimum occurring
at a single point, y n. For in this case
that minimum point is the only output of the
firm consistent with constant returns to scale
and with zero profits under marginal cost
pricing. Thus, dealing with the single product
case to make the point, it would appear, say,
that if the A C-minimizing output is 1,000, in
a contestable market, equilibrium is possible
if quantity demanded from the industry happens
to be exactly 2,000 units (so two firms
can produce 1,000 units each) or exactly
3,000 units or exactly 4,000 units, etc. But
suppose the demand curve happens to intersect
the industry AC curve, say, at 4,030
units. That is, then, the only industry output
satisfying the equilibrium requirement that
price equals zero profit. But then, at least
one of the four or five firms in the industry
must produce either more or less than 1,000
units of output, and so the slope of its AC
curve will not be zero at that point, precluding
either MC pricing or zero profits and,
consequently, violating one or the other of
the requirements of equilibrium in a perfectly
contestable market.

It would appear that equilibrium will be
impossible in this perfectly contestable
market unless by a great piece of luck the
industry demand curve happens to intersect
its AC curve at 2,000 or 3,000 units or some
other integer multiple of 1,000 units of output.


There are a variety of ways in which one
can grapple with this difficulty. In his dissertation
at New York University, Thijs ten
Raa has explored the issue with some care
and has shown that the presence of entry
costs of sufficient magnitude, that is, irreversible
costs which must be borne by an
entrant but not by an incumbent, can
eliminate the existence problem. The minimum
size of the entry cost required to permit
an equilibrium will depend on the size of the
deviation from zero profits under marginal
cost pricing and ten Raa has given us rules
for its determination. He has shown also that
the existence problem, as measured by the
required minimum size of entry cost,
decreases rapidly as the equilibrium number
of firms of the industry increases, typically
attaining negligible proportions as that number
reaches, say, ten enterprises. For, as is
well known, when the firm's average cost
curve is U shaped the industry's average cost
curve will approach a horizontal line as the


### ---Economics-1982-0-11.txt---
INDUSTRY

AVERAGE

COST

AC(ym) -_-

I I I I__ _ _ _ _ _ _

O yr 2ym 3ym 4ym y

FIGURE 2

size of industry output increases. This is
shown in Figure 2 which is a standard diagram
giving the firm's and the industry's AC
curves when the former is U shaped. As a
result, the deviations between average cost
and marginal cost will decline as industry
output increases and so the minimum size of
the entry cost required to preserve equilibrium
declines correspondingly.

However, here I want to describe another
approach offered in our book to the problem
of existence which I have just described- the
difficulty of satisfying simultaneously the
zero-profit requirement and the requirement
of marginal cost pricing. This second avenue
relies on the apparently unanimous conclusion
of empirical investigators of the cost
function of the firm, that AC curves are not,
in fact, characterized by a unique minimum
point as they would be if they had a smooth
U shape. Rather, these investigators tell us,
the A C curve of reality has a flat bottom -an
interval along which it is horizontal. That is,
average costs do tend to fall at first with size
of output, then they reach a minimum and
continue at that level for some range of
outputs, after which they may begin to rise
once more. An A C curve of this variety is
shown in Figure 3. Obviously, such a flat
segment of the A C curves does help matters
because there is now a range of outputs over
which MC pricing yields zero profits. Moreover,
the longer the flat-bottomed segment
the better matters are for existence of equilibrium.
Indeed, it is easy to show that if the
left-hand end of the flat segment occurs at
output y' and the right-hand end occurs at
kym, then if k is greater than or equal to 2 the
existence problem disappears altogether, because
the industry's AC curves will be horizontal
for any output greater than Ym, That
FIRM'S

AVERAGE

COST

.. I I .

O ym kym y

FIGURE 3

is, in any contestable market in which two or
more firms operate the industry AC curve
will be horizontal and MC pricing will always
yield zero profits. To confirm that this
is so, note that if, for example, the flat
segment for the firm extends from y = 1,000
to y 2,000, then any industry output of,
say, 9,000 + Ay where 0< A y < 9,000 can be
produced by nine firms, each of them turning
out more than 1,000 but less than 2,000
units. Hence, each of them will operate along
the horizontal portion of its AC curve, as
equilibrium requires.

Thus, if the horizontal interval (yi, kym)
happens to satisfy k , 2, there is no longer
any problem for existence of equilibrium in a
contestable market with two or more firms.
But fate may not always be so kind. What if
that horizontal interval is quite short, that is,
k is quite close to unity? Such a case is
shown in our diagram where for illustration I
have taken k =4/3.

I should like to take advantage of your
patience by dealing here not with the simplest
case-that of the single product industry-
but with the multiproduct problem.
I do this partly to offer you some feeling of
the way in which the multiproduct analysis,
which is one of the hallmarks of our study,
works out in practice.

Because, as we have seen, there is no way
one can measure average cost for all output
combinations in the multiproduct case, I will
deal exclusively with the total cost function.
Figure 4 shows such a total cost function for
the single firm, which is taken to manufacture
two products, boots and shoes.
Let us pause briefly to examine its shape.
Along any ray such as OR, which keeps


### ---Economics-1982-0-12.txt---
output proportions constant, we have an
ordinary total cost curve, OST. With one
exception, which I will note soon, I have
drawn it to have the usual sort of shape, with
marginal costs falling near the origin and
rising at points much further from the origin.
On the other hand, the trans ray cut above
AB yields a cross section C'TC which is more
or less U shaped. This means that it is relatively
cheaper to produce boots and shoes
together (point U) than to produce them in
isolation (point A or point B). That is, this
convex trans ray shape is enough to offer us
the complementarity which leads firms and
industries to turn out a multiplicity of products
rather than specializing in the production
of a single good.

Now what, in such a case, corresponds to
the flat bottom of an AC curve in a single
product case? The answer is that the cost
function in the neighborhood of the corresponding
output must be linearly homogeneous.
In Figure 5 such a region, a/3yS, is
depicted. It is linearly homogeneous because
it is generated by a set of rays such as L, M,
and N. For simplicity in the discussion that
follows, I have given this region a very regular
shape-it is, approximately, a rectangle
which has been moved into three-dimensional
space and given a U-shaped cross section.
Now Figure 6 combines the two preceding
diagrams and we see that they have been
drawn to mesh together, so that the linearly
homogeneous region constitutes a portion of
the firm's total cost surface. We see then that
the firm's total cost does have a region in
which constant returns to scale occur, and
which corresponds to the flat-bottomed segment
of the A C curve.



Moreover, as before, I have deliberately
kept this segment quite narrow. Indeed, I
have repeated the previous proportions, letting
the segment extend from a distance ym
from the origin to the distance I 4ym along
any ray on the floor of the diagram.
Let us now see what happens in these
circumstances when we turn to the total cost
surface for the industry. This is depicted in
Figure 7 which shows a relationship that
may at first seem surprising. In Figure 7 I
depict only the linearly homogeneous portions
of the industry's cost surface. There we
see that while for the firm linear homogeneity
prevailed only in the interval from y' to
13 Imy in the case of industry output linear
homogeneity also holds in that same interval
but, in addition, it holds for the interval 2ym
to 22 , and in the region extending from
3ym to infinity. That is, everywhere beyonc
3y' the industry's total cost function is linearly
homogeneous. In this case, then, we
have three regions of local linear homogeneINDUSTRY


### ---Economics-1982-0-13.txt---
TOTAL

COST

0~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~0
FIGURE 7

FIGURE 7

ity in the industry's cost function, a,/yS,
which is identical with that of the individual
firm, the larger region abcd, and the infinite
region aleph beth....

Before showing why this is so we must
pause to note the implications of the exercise.
For it means that even a relatively
small region of flatness in the A C curve of
the individual firm, that is, of linear homogeneity
in its total cost function, eliminates
the bulk of the existence problem for oligopoly
equilibrium in a contestable market. The
problem does not arise for outputs nearer to
the origin than Ym because such outputs are
supplied most efficiently by a monopoly
which is not required to price at marginal
cost in a contestable market equilibrium. The
problem also does not arise for any industry
output greater than 3ym in this case, because
everywhere beyond that marginal cost pricing
yields zero profits. There are two relatively
narrow regions in which no equilibrium
is, indeed, possible, but here we may
conjecture that the vicissitudes of disequilibrium
will cause shifts in the demand relationships
as changing prices and changing
consumption patterns affect tastes, and so
the industry will ultimately happen upon an
equilibrium position and remain there until
exogenous disturbances move it away. Thus
we end up with an oligopoly equilibrium
whose prices, profits, and other attributes are
determined without benefit of the conjectural
variation, reaction functions, and the
other paraphernalia of standard oligopoly
analysis.

To complete this discussion of oligopoly
equilibrium in a contestable market, it only
remains for me to explain why the regions of
linear homogeneity in the industry's cost
function are as depicted in Figure 7. The
answer is straightforward. Let C(y) be the
firm's total cost function for which we have
assumed for expository simplicity that in the
interval from ym to I lym along each and
every ray, total cost grows exactly proportionately
with output. Then two firms can
produce 2ym at the same unit cost, and three
firms can produce 3ym at that same unit cost
for the given output bundle, etc. But by
exactly the same argument, the two firms
together, each producing no more than 1 ' '


### ---Economics-1982-0-14.txt---
can turn out anything up to 2'ym without
affecting unit costs, and three firms can produce
as much as 3 ym, that is, as much as
4ym. In sum, the intervals of linear homogeneity
for the industry are the following:
Interval 1: from ym to I ym

Interval 2: from 2ym to 22ym

Interval 3: from 3ym to 4ym

Interval 4: from 4ym to 54ym

Interval 5: from Sytm to 62y'

That is, each interval begins at an integer
multiple of ym and extends 1/3 ym further
than its predecessor. Thus, beyond 3ym
successive intervals begin to touch or overlap
and that is why linear homogeneity extends
everywhere beyond 3ym as I claimed.3
There is one complication in the multiproduct
case which I have deliberately slid
over, feeling the discussion was already complicated
enough. The preceding argument assumes
implicitly that the firms producing the
industry output all employ the same output
proportions as those in the industry output
vector. For otherwise, it is not legitimate to
move outward along a single ray as the number
of firms is increased. But suppose increased
industry output were to permit savings
through increased specialization. Might
there not be constant returns with fixed output
proportions and yet economies of scale
for the industry overall? This problem is
avoided by our complementarity assumption
used to account for the industry's multiproduct
operation-our U-shaped trans-ray cross
section. This, in effect, rules out such savings
from specialization in the regions where linear
homogeneity also rules out savings from
increased scale.

This, then, completes my discussion of
oligopoly equilibrium in perfectly contestable
markets, which we have seen, yields a
determinate set of prices and outputs that is
not dependent upon assumptions about the
nature of incumbent firm's expectations relating
to entrants' behavior and offers us a
concrete and favorable conclusion on the
welfare implications of contestable oligopoly.
III. Intertemporal Vulnerability to
Inefficient Entry

Having so far directed attention to areas
in which the invisible hand manifests unexpected
strength, I should like to end my
story by dealing with an issue in relation to
which it is weaker than some of us might
have expected. As I indicated before, this is
the issue of intertemporal production involving
durable capital goods.

The analysis is far more general than the
following story suggests, but even the case I
describe is sufficiently general to make the
point. We deal with an industry in which a
product is offered by a single firm that provides
it period after period. The equilibrium
quantity of the commodity that is demanded
grows steadily with the passage of time in a
manner that is foreseen without uncertainty.
Because of economies of scale in the production
of capacity the firm deliberately builds
some excess capacity to take care of anticipated
growth in sales volume. But there is
some point, let us say, z =45 years in the
future, such that it would be uneconomic to
take further growth in sales volume into
account in the initial choice of capacity. This
is so because the opportunity (interest) cost
of the capacity that remains idle for 45 or
more years exceeds the savings made possible
by the economies of scale of construction.
Thus, after 45 years it will pay the firm
to undertake a second construction project
to build the added capacity needed to produce
the goods demanded of it.

Suppose that in every particular period
our producer is a natural monopolist, that is,
he produces the industry's supply of its one
commodity at a cost lower than it can be
done by any two or more enterprises. Then
considering that same product in different
periods to be formally equivalent to different
goods we may take our supplier to be an
intertemporal natural monopolist in a multiproduct
industry. That is, no combination of


### ---Economics-1982-0-15.txt---
two or more firms can produce the industry's
intertemporal output vector as cheaply as he.
I will prove now under a set of remarkably
unrestrictive assumptions that despite its cost
advantages, there exists no intertemporal
price vector consistent with equilibrium for
this firm. That is, whatever his price vector,
his market will at some time be vulnerable to
partial or complete takeover by an entrant
who has neither superior skills nor technological
superiority and whose entrance
increases the quantities of resources used up
in production. In other words, here the
invisible hand proves incapable of protecting
the most efficient producing arrangement and
leaves the incumbent producer vulnerable to
displacement by an aggressive entrant. I leave
to your imaginations what, if anything, this
says about the successive displacements
on the world market of the Dutch by the
English, the English by the Germans and the
Americans, and the Americans, perhaps, by
the Japanese.

The proof of our proposition on the intertemporal
vulnerability of incumbents to entry
that is premature from the viewpoint of
cost minimization does require just a little
bit of algebra. To keep our analysis simple, I
will divide time into two periods, each lasting
z =45 years so that capacity in the first
period is, optimally, just sufficient to satisfy
all demand, but in the second, it requires the
construction of added capacity to meet demand
growth because, by assumption, anticipatory
construction to meet growth more
than z years in the future simply is too
costly. Also for simplicity, I will assume that
there are no costs other than cost of construction.
Of course, neither this nor the use
of only two periods really affects the argument
in any way. My only three substantive
assumptions are that demand is growing with
time, that there are economies of scale, that
is, declining average costs in construction,
and that there exists some length of time, z,
so great that it does not pay in the initial
construction to build capacity sufficient for
the growth in quantity demanded that will
occur beyond that date.

The argument, like the notation, is now
straightforward. Let y, be output in period t,
P, be price in period t, and K(y) be the cost
of construction of capacity sufficient to produce
(a maximum of) y units per period.
Here, both p, and K(y) are expressed in
discounted present value.4

Then, by assumption, our firm will construct
at the beginning of the first period
capacity just sufficient to produce output y,
at cost K(y,) and at the beginning of the
second period it will produce the rest of the
capacity it needs, Y2- yI >0, at the cost
K(y2 - y ).

The first requirement for the prices in
question to be consistent with equilibrium is
that they permit the incumbent to cover his
costs, that is, that

(1) PIYI+P2Y2 2 K(y1)+K(y2-y1).
Second, for these prices to constitute an
equilibrium they must protect the incumbent
against any and all possible incursions by
entrants. That is, suppose an entrant were to
consider the possibility of constructing
capacity y, and not expanding in the future,
and, by undercutting the incumbent, selling
the same output, y,, in each period. Entry on
these terms will in fact be profitable unless
the prices are such that the sale of y, in each
period does not bring in revenues sufficient
to cover the cost, K(y,), of the entrant's
once-and-for-all construction. That is, entry
will be profitable unless

(2) plyl + P2Y1 - K(y1).

Thus, the prices in question cannot constitute
an equilibrium unless (2) as well as (1)
are satisfied.

Now, subtracting (2) from (1) we obtain
immediately

P2(Y2-y) K(y2- yi)

or

(3) p2 > K(Y2-Y1Y)/(AY2-Yj),

4That is, if pI, p*, represent the undiscounted prices,
PI = P, P2= p*/(l + r), where r is the rate of interest,
etc.


### ---Economics-1982-0-16.txt---
but, by the assumption that average construction
cost is declining, since Yi >0,
(4) K(y2-y1)/(y2-y1)> K(y2)/y2.
Substituting this into (3) we have at once
P2> K(Y2)/Y2

or

(5) P2Y2 > K(y2).

Inequality (5) is our result. For it proves
that any prices which satisfy equilibrium
requirements (1) and (2) must permit a second-
period entrant using the same techniques
to build capacity Y2 from the ground
up, at cost K(y2), to price slightly below
anything the incumbent can charge and yet
recover his costs; and that in doing so, the
entrant can earn a profit.

Thus, our intertemporal natural monopolist
cannot quote, at time zero, any prices
capable of preventing the takeover of some
or all of his market. Moreover, this is so
despite the waste, in the form of replication
of the incumbent's plant, that this entails.
That, then, is the end of the formal argument,
the proof that here the invisible hand
manifests weakness that is, perhaps, unexpected.


You will all undoubtedly recognize that
the story as told here in its barest outlines
omits all sorts of nuances, such as entrants'
fear of responsive pricing, the role of bankruptcy,
depreciation of capital, and the like.
This is not the place to go into these matters
for it is neither possible nor appropriate here
for me to go beyond illustration of the logic
of the new analysis.

IV. Concluding Comments

Before closing let me add a word on policy
implications, whose details must also be left
to another place. In spirit, the policy conclusions
are consistent with many of those
economists have long been espousing. At
least in the intratemporal analysis, the heroes
are the (unidentified) potential entrants who
exercise discipline over the incumbent, and
who do so most effectively when entry is
free. In the limit, when entry and exit are
completely free, efficient incumbent monopolists
and oligopolists may in fact be able to
prevent entry. But they can do so only by
behaving virtuously, that is, by offering to
consumers the benefits which competition
would otherwise bring. For every deviation
from good behavior instantly makes them
vulnerable to hit-and-run entry.
This immediately offers what may be a
new insight on antitrust policy. It tells us
that a history of absence of entry in an
industry and a high concentration index may
be signs of virtue, not of vice. This will be
true when entry costs in our sense are negligible.
And, then, efforts to change market
structure must be regarded as mischievous
and antisocial in their effects.
A second and more obvious conclusion is
the questionable desirability of artificial
impediments to entry, such as regulators were
long inclined to impose. The new analysis
merely reinforces the view that any proposed
regulatory barrier to entry must start off
with a heavy presumption against its adoption.
Perhaps a bit newer is the emphasis on
the importance of freedom of exit which is as
crucial a requirement of contestability as is
freedom of entry. Thus we must reject as
perverse the propensity of regulators to resist
the closing down of unprofitable lines of
activity. This has even gone so far as a
Congressional proposal (apparently supported
by Ralph Nader) to require any plant
with yearly sales exceeding $250,000 to provide
fifty-two weeks of severance pay and to
pay three years of taxes, before it will be
permitted to close, and that only after giving
two years notice!

There is much more to the policy implications
of the new theory, but I will stop here,
also leaving its results relating to empirical
research for discussion elsewhere.
Let me only say in closing that I hope I
have adequately justified my characterization
of the new theory as a rebellion or an uprising.
I believe it offers a host of new analytical
methods, new tasks for empirical
research, and new results. It permits reexamination
of the domain of the invisible
hand, yields contributions to the theory of


### ---Economics-1982-0-17.txt---
oligopoly, provides a standard for policy that
is far broader and more widely applicable
than that of perfect competition, and leads
to a theory that analyzes the determination
of industry structure endogenously and
simultaneously with the analysis of the other
variables more traditionally treated in the
theory of the firm and the industry. It aspires
to provide no less than a unifying theory as a
foundation for the analysis of industrial
organization. I will perhaps be excused for
feeling that this was an ambitious undertaking.
 ## Economics-1983-0


### ---Economics-1983-0-03.txt---
When I began to study economics, in the
1930's, macroeconomics certainly existed, in
the works of such luminaries as Wicksell,
Fisher, Robertson, and the early Keynes. But
it was surely not recognized as a branch of
economics in which one might specialize; nor
was it regarded as necessary for the education
of an economist. Thus, like almost everyone
else, I began professional life as a
price-theorist. The principal alternative was
to become an institutionalist; and that didn't
particularly attract me. Only later, and somewhat
by accident, did I become a macroeconomist.


Ever since the emergence of macroeconomics
as a distinct and (almost) respectable
branch of analysis, there has been a conscious
tension between macroeconomics and
microeconomics; much of this tension relates
to the roles and the behavior of prices and of
the price level. Surely, it cannot be said that
macroeconomics ignores prices and price
changes, as is sometimes suggested. Milton
Friedman and his monetarist associates and
followers-who are macroeconomists of the
first water-surely do not ignore prices. And
the patron saint of my kind of macroeconomics,
John Maynard Keynes, also certainly
paid a great deal of attention to both relative
prices and the price level. My concern with
prices here, however, is not with inflation,
but rather with the price-theoretical foundations
of macroeconomics. Essentially, I will
be discussing some of the roles that microeconomists
and macroeconomists see for

prices, and particularly for price changes.
Price theory has moved a long way since I
deserted it, and I no longer claim any expertise
in this area. But I have the impression
that many current problems both in microand
macroeconomics tend to be the same
problems-looked at from opposite sides of
the borderline between them. Thus I propose
to lead us on a stroll along some sectors of
that border, moving back and forth across it
from time to time, for there is no fence. And
I intend only a meander, not a mapping. The
spirit in which the journey is undertaken is
that this is really all one country, and strollers
should be welcome.

But it is not an imaginary country that I
propose we visit. We will see no Walrasian
auctioneers, although we will see many
markets that seem to work pretty well without
them. On the other hand, we will see very
few wage rates being frequently revised; and
many prices will look as though they were
being revised only to maintain fairly stable
markups over unit costs. In general, we will
observe that the population of this country is
neither very much brighter-nor much more
stupid-than you and I are, in our own
economic decisions.

Some aspects of the problems that I will
discuss are particularly important in an age
of inflation, and to the theory of inflation.
But I prefer to conduct most of my discussion
without explicit reference to changes in
the general price level. For almost all of the
matters that I will discuss primarily involve
relative prices; and it is simpler to deal with
them on the assumption of a constant price
level. That way, I do not need to keep repeating
"real price."

I start with the prices and inventories of
standardized commodities.

I. Commodity Prices and Inventories
Economists often find it useful to think of
the quantities of a commodity supplied to a
competitive market as consisting of a supply
from current production plus a possible
supply from inventory; and the market demand,


### ---Economics-1983-0-04.txt---
similarly, as consisting of a demand
for current use or consumption plus a possible
demand for additions to inventory. Any
market-participant's demand for additions to
inventory clearly should be based upon an
expectation that the price will increase sufficiently
over some future period, at least to
cover costs of storage plus interest for that
period. Any market-participant's willingness
to supply goods from inventory should be
based on an expectation that the price will
rise insufficiently over any future period to
cover costs of storage and interest for that
period. Since expectations of future prices
are never certain, the expected prices described
should include discounts to reflect
risk aversion, or premiums to reflect the
pleasures of risk assumption.

Given the conditions that permit or require
the holding of inventories, inventories
may have either a stabilizing or a destabilizing
effect on the commodity's price, production,
and consumption. Traditionally,
however, economists have assumed that the
effect of inventories is to stabilize each of
these. We think of the wheat market, for
instance, where an entire year's new supply
(at least in either Hemisphere) becomes
available in a relatively short period of time.
Were the holding of inventories of wheat (or
wheat products) impossible, the market price
would have to fall low enough during the
harvest period to assure immediate consumption
of the entire crop. The price would then
rise high enough during the remainder of the
year to cut off all consumption-or else high
enough to make profitable the production
of wheat in greenhouses, and to reduce its
off-season consumption to the quantity so
produced. However, given the possibility of
storage of wheat or wheat products-by producers,
dealers, or intermediate or final consumers-
the price would normally fall only
enough during the harvest period to induce
the appropriate amount of inventory accumulation.
The price would then rise, during
the remainder of the year, as inventories
were drawn down, sufficiently to cover at all
times the costs of storage, interest, and the
assumption of risk from holding inventories.
Such seasonal price and inventory movements
often are approximately observable.
The recognition that not only seasonal but
also year-to-year variations may occur in the
harvest as a result of weather conditions or
civil disturbance, and that year-to-year variations
may also occur in the demand for
wheat, reflecting, say, the "business cycle,"
will induce some or many market participants
to carry over inventories of wheat
(or of its products) from one crop year to the
next (or to reduce stocks previously carried
over). Stocks held will increase or decrease as
the current market price varies relative to
what is believed to be the current "normal"
or "long-run equilibrium" price: the price
that would exactly balance normal annual
new supply and normal annual consumption.
Through this process, not only seasonal
fluctuations but also year-to-year fluctuations
of market price and of consumption are
reduced, even in the presence of substantial
year-to-year changes in production or in final
demand. Through changes in inventories,
supply or demand in any particular year, in
effect, may thus borrow from past and/or
from future years' supply or demand.
As, and to the extent, that market participants
learn of new "events" affecting or
likely to affect either future normal production
or future normal demand, the rate of
inventory accumulation or decumulation will
be varied so as to produce a new time path
of price and current consumption, associated
with an expected new equilibrium of prices
and quantities. Needless to add, this behavior
is stabilizing with respect both to prices
and quantities: at least as compared to what
would have happened if inventories could
not exist, or if storage costs-or the interest
rate-were higher. And this activity clearly
increases economic welfare.

These activities of course reflect the behavior
now described as based upon "rational
expectations." Market participants have in
their heads a "model" of how supply and
demand affect market price over time; on
the basis of that model, and of all available
information-including all new information-
about present and future production
and demand and the size of existing
inventories, agents buy for or sell from inventories
in ways that cause the market price
to move continuously toward its current normal,


### ---Economics-1983-0-05.txt---
long-term, market-clearing level, taking
account of storage costs and interest, and
appropriately discounted for uncertainty. Indeed,
it was precisely in the context of reasoning
about markets for storable individual
commodities (and agricultural commodities
in particular) that John Muth first presented
the concept that he called "rational expectations"
. ' In this particular microeconomic
context there can surely be no valid objection
to rational expectations. But we must
not forget that the primary way in which
expectations- rational or otherwise-affect
current prices is precisely through affecting
behavior with respect to inventories, along
with sales or purchases for future delivery
(i.e., negative inventories) or through affecting
some other form of intertemporal substitution
between, for example, labor and
leisure, now and in the future.2
Markets for commodities, of course, vary
widely in the extent to which inventories are
able to stabilize price or consumption. At the
one extreme is the traditional market for
fresh flowers. Holding inventories is, by definition,
impossible. Random fluctuations in
the production of or the demand for fresh
flowers are thus fully reflected in daily prices.
And because the immediate response of
amounts supplied to price changes is zero, by
definition, and the response of amounts demanded
to price is relatively small, price
fluctuations in fresh flowers may be substantial.
Inventories cannot supplement

below-normal current production; nor can
inventories be accumulated to profit from a
below-normal current price or from an
expected above-normal future price.
At the opposite extreme are the markets
for the durable products of agriculture and
mining-for example, for wheat or tin. Here
storage costs are relatively low, and inventories
may easily reach the equivalent of one or
even several years' production. Under such
circumstances, it is not implausible to argue
that rational expectations and inventory
adjustments tend to stabilize prices and consumption
in the presence of random fluctuations
in supply or demand.3 On the other
hand, durable or " permanent" changes in
supply or demand alter the equilibrium price,
and, almost at once, the actual market price
begins to reflect that change.
II. Price Speculation

However, the existence of large inventories
also introduces the possibility of price instability
arising from "speculation." Consider,
for example, the extreme case of gold, when
it is only a commodity, and not used as
money. Gold is obviously very durable. And
its consumption and its production in any
year, or even in any decade, are both very
small relative to the size of the total existing
stock. The costs of storage are relatively low.
It is also nearly indestructible: its services as
ornament or store of value are consumed
with little or no disappearance or dissipation.
Most of the gold that was ever mined presumably
still exists, and is potentially available
as an addition to the market supply
from new production-at prices deemed sufficiently
above equilibrium. Gold is currently
produced; and its rate of production does
' Muth's 1961 article is, of course, now a classic.
However, an excellent much-earlier statement of the
way in which information and expectations affect the
wheat price appears (of course!) in Alfred Marshall. The
accompanying marginal note reads "Nearly all dealings
in commodities that are not very perishable, are affected
by calculation of the future" (pp. 337-38). Marshall
perhaps failed to close his argument by stating explicitly
that expected prices enter into today's production decisions,
although that is clearly implied.
21t was in precisely the context of inventories-and
by explicit analogy with the wheat market-that I once
explained Keynes' theory of the "speculative demand"
for money (or bonds). In my Macroeconomic Theory (pp.
175-76), I argued that, in the bond market-as in the
wheat market-speculation on future bond prices, in
the presence of large outstanding stocks of bonds and of
money, by agents who form expectations of "normal"
market-clearing prices, tends to stabilize bond prices at
or close to their normal level-thereby preventing interest
rate fluctuations from stabilizing aggregate demand
for goods and services in the short run against exogenous
variations in saving or investment propensities.
Although the term rational expectations had not yet
been invented, I was arguing that rational expectations
explained an interest-elastic (or bond-price-elastic) demand
for money.

31t is therefore somewhat anomalous that, although
Muth thus demolished the previous theory of the "cornhog
cycle," it is my impression that traces of such
price-quantity behavior still persist.


### ---Economics-1983-0-06.txt---
respond to its relative price (and to the expected
price that so strongly influences its
current price). But a year's production, even
at an historically enormous real price, adds
very little to the current market supply, compared
to that which is available from inventories.
Changes in the market price may affect,
at least modestly, the quantities of gold
demanded: by dentists, by manufacturers of
jewelry and objets d'art, or in industry (as a
conductor or chemical). But such changes in
quantities demanded are likewise small relative
to the size of the stock; and even the
manufacture of gold objects does not really
remove them from the inventory of gold.
Indeed, unmined gold (and the shares of
gold mining companies) are in effect part of
the inventory, too.

Although the price of gold has some effect
on its rates of discovery and production, as
well as on its consumption (in the sense of
its complete and permanent removal from
potential market supply), these responses of
quantities produced and consumed are truly
minimal in comparison with the size of outstanding
inventories. Thus, the market price
of gold depends mainly on peoples' (or, in
the past, on governments') willingness at that
price to hold the immense existing inventory
of gold. And this means that the current
price of gold depends mainly on expectations
of its future price. As noted earlier, the current
price of a standardized commodity can
diverge from any expected future price only
by the relatively small costs of storage plus
interest (and the cost of assuming the risk of
an incorrect expectation). But, in the case of
gold, a changed expectation of future price is
not soon erased by a changed rate of gold
production, nor by an altered level of any
consumption that subtracts permanently
from the stock. There may always be some
normal, long-run equilibrium price of gold
that would exactly equate current production
and current consumption (in the sense of
permanent disappearance). But that normal
price does not discipline the actual market
price of gold in the way that the normal,
long-run equilibrium price of wheat disciplines
its current market price. Rather-in
the absence of a fixed monetary price of
gold-its current price depends overwhelmingly
on the current expectation of what its
future price will be. And that future price
will depend on the expectation then of its
subsequent price.

Such a market is best characterized by
borrowing Keynes' parable (pp. 154-160) of
the beauty contest, which he used to explain
share prices. The price level of shares, he
suggested, depends on each market participant'
s calculation of what the other participants
are likely to expect it to be. It is
analogous to the contest to choose a beauty
queen, he said, in which the prizes go to
those who select the candidate for queen
thought most beautiful by the largest number
of other contestants. Instead of selecting
the candidate who is the most beautiful, each
contestant tries to calculate which candidate
is most likely to be chosen by other contestants.
But once each realizes that others
will be choosing on the same basis, he is
forced to speculate about what the average
opinion will be of what the average opinion
is. And that speculation can be carried to
progressively higher degrees, Keynes suggested.
Given no more solid a basis than this
for valuing shares, said Keynes, the price of
shares can be almost anything-and is likely
at times to be highly unstable.
Why does anyone buy gold at $52 an
ounce-or at $520 an ounce? Why does anyone
sell it at that price? Because the buyer
expects that the price tomorrow will exceed
$52 by more than the cost of carrying an
ounce of gold; and the seller expects that it
may be less than $52 plus carrying costs
tomorrow. Each such expectation in turn
reflects buyers' and sellers' judgments about
what buyers and sellers will be expecting
tomorrow for the day after tomorrow. The
only other rational reason for buying or selling
gold is the pure pleasure of risk assumption-
better known as gambling. And there
is surely much of that in the gold market.
Presumably, pure gambling has little net effect
on the price, although it does raise the
incomes or employment of brokers.4


### ---Economics-1983-0-07.txt---
Now, what precisely is the basic difference
between the wheat and the tin markets, on
the one hand, and the gold and the share
markets, on the other, that lets us assume in
the wheat or tin case that rational expectations
will normally stabilize prices and consumption
in the face either of random or of
predictable fluctuations in amounts supplied
or demanded, while in the gold case, there is
often destabilizing speculation?
The difference obviously has nothing to do
with price flexibility and continuous market
clearing; for these are trademarks not only of
the markets for wheat, tin, and pork bellies,
but also for gold (and for General Motors
shares). Rather, the difference is that in the
wheat and similar cases, the response of
quantities produced and/or quantities consumed
to price changes arising from shifts in
supply or demand can be large enough and
prompt enough to begin to move the price
toward its new equilibrium level within a
relevant time period; and that this response
therefore comes to be anticipated by market
participants. This prompt and substantial
stabilizing response of quantities produced
and consumed does not occur-and therefore
it is not expected to occur-in the case
of gold. As a consequence, in the presence of
large inventories, price expectations dominate
price-level determination not only in the short
term, but even in the moderately long term.
The prices so determined are unlikely to
contribute either to the stability of production
or consumption, or to the equality of
production and consumption. They may thus
produce movements in actual inventories, the
price effects of which, however, have only
modest and long-delayed effects in reversing
such movements.

Clearly, wheat and gold are more-or-less
extreme examples of a spectrum of durable
commodities, capable of storage. At the
wheat end of that spectrum, the responses of
amounts produced and consumed to price
changes are strong and prompt. Market participants
come to expect them to occur, and
take actions with respect to inventories that
tend to stabilize prices around their equilibrium
level, including any expected new
equilibrium level. At the gold end of the
spectrum, the responses of amounts produced
and consumed to price changes are
weak and slow, while inventories are very
large.5 Participants in such markets thus cannot
depend on price movements to be quickly
self-limiting. It is therefore not irrational for
some or many of them to speculate on the
direction of intermediate price movements.
Their speculation may often extend and exaggerate
price movements that had some

origin in changed supply or demand conditions.
Or it may extend and exaggerate price
movements caused by some random exogenous
disturbance, or even by the actions of
a few large traders (for example, the Hunts
in silver).

Most commodity markets lie between these
extremes. In such intermediate markets, price
movements may sometimes be-perhaps ordinarily
are-in the direction of a price that
tends fairly quickly to equate production and
consumption. At other times, price movements
may develop a cumulative momentum
in one direction, which can easily overshoot
the current long-run equilibrium price. Occasionally,
moreover, speculation can affect
even the most stable markets; and speculative
fevers can be transmitted from one
market to another.

The years 1971 through 1973 provide a
clear example of such speculation and of
such transmission.6 Coincidental crop failures
in wheat and coarse grains in several of the
main producing countries of the world-at a
time when world stocks were severely depleted-
caused grain prices to soar; this
helped to attract public attention to the then
popular "Club-of-Rome" fantasies of general
resource scarcity, which in turn helped
to transmit the speculative fever to still other
commodities. In a political environment in
which-partly for accidental reasons-macroeconomic
policies in a number of major

countries were unusually permissive, the
speculative rise in commodity prices soon
came to be reflected in wage rates and in-
'An extreme case, on the side of supply, is that of
nonproducible goods (for example, "old master" paintings
or old postage stamps) where all supply to the
market comes from inventory. An earlier version of this
paper included a considerable section on prices of such
" collectibles."

6For an excellent account of this period, see Barry
Bosworth and Robert Lawrence, especially pp. 24-87,
and other references given therein.


### ---Economics-1983-0-08.txt---
dustrial costs. Given the inertia of inflation,
once it becomes embodied in wage rates and
industrial costs; given the fortuitous response
of the oil-producing countries when
awakened by the international inflation; and
given the perhaps appropriate, but surely
delayed, responses to inflation by monetary
and fiscal policymakers in many of the industrial
countries, it is possible to understand
why the 1970's turned out to be such a
disappointing- even a disastrous- decade.
And commodity price speculation played
more than a small part in that disaster.
III. Rational "Price Bubbles"

The fact that rational expectations may
not preclude the existence for considerable
periods of speculative, self-maintaining price
movements toward zero or infinity has recently
been recognized in a rapidly expanding
theoretical literature of working papers
and a few published articles.7 Indeed, we are
seeing a "bubble" of papers on "rational
price bubbles." Whether rational expectations
are or are not consistent with price
bubbles, however, seems to depend entirely
on what definition one gives to the "rationality"
of rational expectations. One may define
as rational any expectation that, if generally
acted upon, will turn out to be essentially
self-confirming. In that case, price bubbles
generated by the simplest of extrapolative
price expectations may be consistent with
rational behavior. On the other hand, one
may alternatively define as rational only
those expectations that fully incorporate an
understanding of " long-run" "market fundamentals"
: namely, the assumption by each
agent of rationally maximizing behavior by
each other agent, each responding to observed
changes in prices and quantities as
would occur in a market model of "long-run
equilibrium," although observed through a
screen of random, nonserially correlated,
short-run disturbances. Actions based on
such expectations thus tend to confirm the
expectations; and each confirmation reinforces
further reliance up on the long-term
equilibrium model. There seems little room
for price bubbles in that version of rational
expectations.

But whether, and to what extent, market
fundamentals will, in fact, prevail, surely depends,
as I have argued above, on the durability
of the commodity, its storage costs,
and particularly on the degree to which, and
the speed with which, its production and
consumption are affected by expected price.
Given the world as it is, I suggest that the
"fundamentals" do not always and everywhere
prevail. Bubbles do occur, and are
important.

However, most or all of the new literature
on price bubbles appears to deal only with
price movements away from, or back towards,
the equilibrium price that reflects
long-run market fundamentals. It thus implies
that, when prices are, for any period,
approximately stable, they must therefore
be at or close to an equilibrium dictated
by market fundamentals-reflecting, at that
price, an approximate balance between the
expected supply from new production and
the expected demand for final consumption.
Price bubbles, in contrast, involve an extrapolation
of expected price movements (however
originating), causing further movement
in the same direction.

In my view, however, where the impacts of
price on amounts produced and consumed
are small, slow, and uncertain, a speculative
price-(i.e., a nonequilibrium price) is not
always or necessarily a moving price. The
price may rest for a considerable period of
time at-or fluctuate narrowly around-a
level far above (or below) any equilibrium
determined by market fundamentals. This
situation might represent either the extrapolation
of an originally accidental stability, or
a standoff between expectations of further
rise by some participants and the expectation
of fall by others.8


### ---Economics-1983-0-09.txt---
Under circumstances in which uncertainty
is overpowering, as Keynes suggested was
often the case for stock prices, a price may
remain approximately stable for a considerable
time on the basis of a tacit "convention,
" which market participants come to
accept. The convention amounts to the assumption
that

... the existing state of affairs will continue
indefinitely, except in so far as
we have specific reasons to expect a
change... For, if there exist organized
investment markets,... an investor can
legitimately encourage himself with the
idea that the only risk he runs is that of
a genuine change in the news over the
near future, as to the likelihood of which
he can attempt to form his own judgment,
and which is unlikely to be very
large. For, assuming that the convention
holds good, it is only these changes
which can affect the value of his investment,
and he need not lose his sleep
merely because he has not any notion
of what his investment will be worth
ten years hence.

[General Theory, pp. 152-53]

Thus, the price of gold might well, for a
considerable period of time, fluctuate narrowly
around a level far from the equilibrium
price at which amounts produced
and consumed would in the long run be
equal. And so, although to a considerably
lesser extent, could the price of wheat or tin.
Yet such a period of approximate stability
might at any time be upset by a major,
self-maintaining movement in either direction.


IV. Macroeconomic Inventory Theory
Let us now turn to some of the macroeconomic
implications of this microeconomic
theory of commodity prices and inventories.
First, and most important, is to recognize
that there is nothing in standard price theory-
even when we expand it to take account
of speculative demands for inventories, price
bubbles, or of the possible transmission of
speculative fever from one market to another-
that implies that the aggregate stock
of inventories, and the rate of aggregate
inventory accumulation for an entire economy
should exhibit any systematic variation
over time. Random disturbances to supply
and/or demand for particular commodities
or products might lead to periods of net
accumulation or net decumulation of inventories
of those products. But the aggregation
of many time-series, each subject to random,
non-serially-correlated variation, produces
only a time-series with proportionally smaller
variation. Thus, several years of systematic
general accumulation of inventories, followed
by substantial periods of general
decumulation of inventories, finds no basis
in conventional price theory.

Yet one of the most obvious macroeconomic
facts of life is the existence of pronounced
cycles, both in the size of aggregate
inventories and in their rate of accumulation.
Indeed, of all of the conventional subaggregates
of real national product, the one that
shows the greatest decrease (not merely in
percentage but in aggregate dollar amount)
between periods of business expansion and
of business contraction is, almost invariably,
that of the net addition to business inventories.
This total typically goes from a large
positive amount at business cycle peaks to
a large negative amount at business cycle
troughs. This difference exceeds the typical
cyclical decline in business investment in
plant and equipment, in residential construction,
in purchases by consumers or governments,
or in net exports. The existence of
these pronounced cycles in aggregate inventory
accumulation has long challenged macroeconomists
to develop theories that might
explain this phenomenon. The response to
this challenge has produced a second and
macroeconomic literature about inventories,
that is almost completely nonintersecting
with microeconomic price theory.
In this macroeconomic inventory literature,
prices and price expectations are rarea
world of constant change, few if any market participants
may share the same model of equilibrium-or
at least the same and "correct" model. In these situations,
most market participants may come to realize that
there is no price other than the current one that has any
better claim to being considered the equilibrium price.


### ---Economics-1983-0-10.txt---
ly mentioned. This second literature deals
primarily with work-in-process and manufactured
product inventories, including inventories
held by wholesalers and retailersalthough
it may also embrace commodity

inventories held by manufacturers or processors
(but not by producers and dealers). The
earliest versions of this literature described
inventories as being held, increased, or reduced
as the result of production or ordering
decisions that were based upon expectations
of future changes in quantities produced or
demanded-expectations generated mainly
by recently experienced levels of production
or sales. It purported to describe the determinants,
for example, of the desired and actual
stocks of coal and iron ore held by steel
mills; of the desired and actual inventories of
finished steel held by mills, steel warehouses,
or steel-using manufacturers; of the desired
and actual inventories of automobiles held
by producers and dealers; and of the desired
stocks and actual stocks of steel bedsprings
held by department stores. In early versions
of this macroeconomic literature, inventory
decisions were shown to be capable of having
highly destabilizing macroeconomic effects-
particularly when expectations of future
sales were modelled as responsive in
particular ways to experienced levels and
changes of sales, while production responded
to orders and expected sales.9
This literature, of course, belongs to a
universe of discourse in which prices (including
wage rates) do not clear markets continuously,
but rather adjust slowly to evidence of
gaps between amounts supplied and demanded.
Or they may be "administered" on
the basis of working rules of thumb based
mainly on costs or on other prices. In this
universe of discourse, such pricing rules are
not necessarily irrational, given the limited
extent of knowledge, and the experience of
past instability. And, since prices so established
do not typically cause production and
consumption to move quickly into approximate
equality, market participants do not
expect that prices will so move, and therefore
they do not so move. Rather, production and
sales are brought toward equality primarily
by changes-delayed changes-in planned
rates of production or purchase, that reflect
observed or expected changes in quantities
consumed or used in production, and that
are designed to restore or to maintain efficient
levels of inventories. Indeed, microeconomic
theories of rational quantity adjustments
to unplanned differences between
production and sales began to be developed
in the early 1950's, and have increasingly
supplied a formal basis for macroeconomic
inventory theory.'0 These fluctuations in
quantities produced or purchased may lead,
in the face of relatively inflexible prices and
wages, to further self-reinforcing changes in
production, purchases, and employmentself-
reinforcing through income effects on
consumption and "accelerator" effects on investment:
perhaps dampened by interest rate
effects on investment, to the extent that the
supply of money does not accommodate
changes in the demand for it.

Indeed, if such patterns of quantity responses
at the level of the firm to unplanned
changes in inventories become standardized-
because such patterns do, in time, reverse
imbalances between production and
sales-it may become entirely rational to
expect such patterns of quantity movements
to occur at a macroeconomic level. Rational
expectations based on such "theories" could
then lead to inventory, production, and
purchase decisions that help to perpetuate the
macroeconomic instability of production, consumption,
and inventories.

Persistent and massive macroeconomic inventory
cycles deny either the existence of
rational expectations, or, more plausibly, the
ability of such expectations to stabilize output
and employment. As the persistence of
such cycles comes to be expected, and their
amplitude to be accepted, it is not surprising
that full flexibility of prices comes to be seen


### ---Economics-1983-0-11.txt---
by many firms as a useless and unprofitable
course of conduct.

V. The Demand for Commodities and
the Demand for Labor

I referred earlier to the case (exemplified
by fresh flowers) in which the holding of
inventories is by definition impossible, and
in which the entire response to unexpected
shifts in supply or demand must be a price
change-at least, given perfectly competitive
markets. This case may be only a trivial
curiosity when we deal with commodities.
But it is a matter of considerable importance
when we recognize that an inability to store
output is, by definition, the case for services-
whether they are final products, or
are services used in production. The largest
class of services, of course, is the labor
services used for the production of goods. If
the price of labor services was a flexible,
continually market-determined price, the
price of labor services should respond rather
sharply to random shocks to the demand for
goods produced using labor. Indeed, prices
of labor should, in general, be more variable
than prices of commodities, on the reasonable
assumption that the supply of labor is
less price elastic than the supply of cooperating
factors of production."

In fact, of course, the evidence is undeniable
that wage rates normally fluctuate far
less than do prices. This anomaly seems to
require an "institutional" explanation, or,
more fundamentally, a theory that explains
the institutions that produce these results.
Such explanations often run in terms of
" set-up," "transactions," and "information"
costs, that make implicit or explicit contracts
mutually beneficial both to workers and to
employers, despite the fact that such contracts
necessarily imply substantial intermittent
unemployment, as well as occasional
labor shortages.12 An extensive literature has
developed along these lines, greatly advanced
in Arthur Okun's posthumous Prices and
Quantities: A Macroeconomic Analysis. The
conclusion of Okun's analysis, of course, is
that models of the labor market that assume
continuous market clearing- and therefore
continuous full employment-are erroneous.
And, as a result, critiques of government
stabilization policy-as powerless to improve
economic welfare even when properly used
(or unable to damage the national interest
when badly used)-necessarily fail.
VI. A Price-Theoretic Version of
the Business Cycle

One well-known critique of stabilization
policy is that associated with Robert Lucas
and Robert Barro. However, Lucas and Barro
accept the reality of a business cycle, involving
serially-correlated changes in real output,
although they still deny the ability of monetary
or fiscal policy to affect the economy.
The price-theoretic approach of Lucas and
Barro stands, however, at an opposite pole
from that of Okun. For instead of trying to
model the imperfect flexibility of wage rates
in a real world, wages are sometimes assumed
not to exist. In one version (1977) of
Lucas' theory of the business cycle, we have
a world reminiscent of J. B. Say, in which
there are no firms, no hired labor, but only
"yeoman farmers."'3 Thus, there are only
prices and profits-no wage rates. Actually,
this model could also accommodate simple
manufacturers- craftsmen without hired
workers-as did the model world of Say.
Needless to say, competition is perfect, and
information free and nearly complete.
1 " To be sure, the presumed rational expectations
response to fluctuations in the demand for commodities,
through planned additions to inventories or the drawing-
down of inventories, would tend also to stabilize the
demand for labor. However, given the costs of commodity
storage, it can easily be shown that this response is
consistent with the presumption stated in the text: that
labor prices should fluctuate more than goods prices.
12On the other hand, the explicitly or implicitly
contractual nature of the employment relationship not
only contributes to the explanation of wage rate insensitivity,
but also allows producers greater freedom to use
temporary layoffs, along with inventory changes, as a
means of adjustment to temporary fluctuations in the
demand for output in a manner which may be economically
efficient both for workers and for employers. The
relationship between inventory behavior and the demand
for labor has recently been interestingly explored
by Robert Topel.

13As Blinder and Fischer have recently characterized
the producers in this model.


### ---Economics-1983-0-12.txt---
However, this very simple world has a
central bank, or its ruler has a printing press.
When bank loans or printing press money
are used to finance a public purpose (such as
an increment in the Prince's consumption)
goods prices are bid up. However, each yeoman
farmer or craftsman is likely to confuse
the rise in his own selling price as a rise in
his price relative to prices for the products
and services of other yeomen. Such relative
price changes occur for various reasons, and
are familiar to each of them. Each therefore
works harder in the presence of inflation,
and produces more, so as to permit him to
consume more of the output of his fellow
farmers or craftsmen, whose prices he presumes
not to have risen. Of course, the Prince
has already consumed more of their output,
too; and their prices, too, have risen, But
until each yeoman discovers that the rise in
his price is general rather than relative, real
GNP increases. Soon, however, each learns
that there has been no increase in his real
price, and finds only the sour taste of inflation.
The boom ends. It can be repeated as
soon as the short memories of all citizens are
erased. 14

What can this have to do with the business
cycle in a world in which yeoman farmers
and craftsmen are replaced by firms with
hired employees? Very little, I think. For, in
even the simplest possible version of this
more familiar world, there have to be two
kinds of prices, not one: prices for goods,
and prices for labor. Increased production
now requires increased inputs not only from
proprietors, but also from their workers. Under
standard price theory, in order for
employers to wish to hire more labor services,
they must believe that product prices have
risen relative to wages; but in order for existing
workers to respond to inflation by
working longer hours, and other potential
workers to enter the labor force, requires
that they interpret what happens as a rise in
the ratio of wages to prices. It was a nice
enough trick to be able to fool each yeoman
about his own real price; but it is an even
nicer trick for an inflation to fool both employers
and workers- in opposite directions
-about the movement of the real wage
paid by one and received by the other!'5
I suggest that more may be learned about
cycles from Knut Wicksell's discussion of
business fluctuations, seventy-five years earlier.
Wicksell understood that, even in a similarly
simple world, there needed to be not
merely one kind of price level-nor merely
two kinds-but actually three: price levels
for goods, for labor, and for loans. And
whether or not there was a Prince with a
printing press, there were competitive banks,
aggressively lending at a flexible price to
businesses. Given a plausible (although incompletely
specified) lag structure, Wicksell
showed that banks' competition in money
creation might initially reduce the price for
loans; and their continuing money creation
might keep it depressed for a time. Through
this means, investment, financed by bank
credit, could exceed ex ante saving, crowding
out consumption. The result, of course, was
also inflation, unanticipated by all. Once the
limits of money creation were reached, the
boom collapsed.

At roughly the same time, in another part
of Europe, J. A. Schumpeter was describing
a rather different source of instability. His
bourgeois prince- the innovating entrepreneur-
driven by new technological, managerial,
or marketing ideas that promised
abnormal profits, repeatedly upset the general
equilibrium: accommodated, of course,
by an elastic banking system. To be sure, his
entrepreneurial activities-his "creative destruction"
-would soon generate an inflation
that had to subside before the next
wave of innovation might again disturb the
economy.

'41n another version (Lucas, 1975), there are genuine
firms selling products and purchasing labor and capital
inputs; but production is scattered among noncommunicating
islands, using capital that is immobile and
labor that is randomly mobile (between periods), while
increments of money are distributed stochastically
among the separated markets from period to period.
This schema permits changes in money to create fluctuations
of real output (of capital goods); but it is not at all
clear (to me) why or how the postulated source of such
"cycles" bears any relationship to the cycles of the "real
world."


### ---Economics-1983-0-13.txt---
Of course, the disturbances of macroeconomic
equilibrium that were visualized by
Wicksell and Schumpeter, and by Fredrich
von Hayek, R. G. Hawtrey, D. H. Robertson,
the early Keynes (and so many others)
were, in the end, mainly disturbances of the
price level, although they normally left a
permanent legacy of a larger capital stock,
newer technology, and increased human
capital.'6 However, most of these pioneers
recognized that the price level did not automatically,
instantaneously, and proportionately
reflect every change in the stock of
money or the output of goods; for instance,
many recognized that prices did not fall sufficiently
promptly in recessions to avoid nonfrictional
unemployment of labor and/or

machines. And such unemployed resources
then permitted a subsequent expansion of
real output, once the next boom began.
VII. Price-Theoretic Models of
Aggregate Investment

Most business cycle theories-or, more
broadly, most macroeconomic theories of the
"medium run"-from the time of Wicksell
or even earlier, have thus been built on
more-or-less elaborate price-theoretical considerations,
related essentially to investment;
and mainly to fixed investment. For, clearly,
it is investment-not consumption or proprietors'
labor-that is the primary source of
medium-run macroeconomic disturbance and
instability. Although (as noted earlier) sharp
fluctuations in the rate of investment in inventories
contribute more to U.S. business
cycle recessions and to the early stages of
business recoveries than do fluctuations in
plant-and-equipment investment, the latter
typically contribute significantly more to expansions
of GNP from cyclical troughs to
peaks than does inventory accumulation.
Moreover, in the medium and longer run,
fluctuations in inventories are governed by
most of the same factors that govern plant
and equipment expenditures.

There are many price levels-relative price
levels-important for investment in plant,
equipment, and normal inventories. They include:


1) The price level for loans: some particular
rate of interest, or some average of rates,
at which investment can be financed;
2) The demand-price level of new capital
goods, reflecting their physical productivity,
the prices of the goods produced by their
use, and the supply-prices of cooperating
factors of production;

3) The supply-price level for new capital
goods, sometimes taken as a function of their
rate of production;

4) The supply-price and the demand-price
levels of entrepreneurship and innovation;
5) The supply-price levels of risk and/or
uncertainty bearing, by investors and lenders;
6) The level and nature of the prices
imposed by government; taxes and their structure;
and

7) The price level of ownership of existing
enterprises; that is, the level of share
prices.

The question is not which among these
price levels are relevant for investment: all of
them (and others) are doubtless relevant to
some degree. Rather, the question is: which
are "strategically" important? Which are the
price levels, the changes in one or more of
which relative to another or others are primarily
responsible-in the real world-for
macroeconomic stability or instability, real
and nominal?

I can illustrate the wide variety of price
theoretic explanations of investment by reference
to three familiar but quite different
examples. 7

16For effective summaries of these and related theories,
see Gottfried Haberler. Hayek (and others of the
"Austrians") denied that there were any positive legacies
from the boom.

17However, not all macroeconomic investment theories
have been or are price theoretical. Examples of
quantity-theoretic investment theories are the following:
(a) Robert Eisner's adaptation of the accelerator theory,
which, in practice, makes real investment, and thus
employment and income, depend on fluctuations in
current levels of real "permanent" business sales (analogous
to permanent income); (b) The works of W. H. L.
Anderson, James Duesenberry, and others, whose emphases
have mainly been on factors influencing the gross
flow of internal funds for investment financing, including
tax rates seen as a determinant of internal fundsavailability
rather than of profitability; and (c) Keynes
and George Katona, who stressed political, sociological,


### ---Economics-1983-0-14.txt---
I remind you of Martin Feldstein's complex
analyses of the interaction of tax laws
and inflation as affecting the profitability
of investment. Alternatively, there is Dale
Jorgenson's "neoclassical" investment theory,
which carefully models the relationship
among all of the prices and quantities that
enter into dynamic profit-maximizing investment
decisions.'8

As a third example, there is James Tobin's
theory, suggested by Keynes, that focuses on
the relationship between the aggregate market
value of the marketable debt and equity
claims against existing enterprises, and the
aggregate cost of reproducing, at today's
prices, the assets of those enterprises. The
ratio of these two global magnitudes has
come to be designated as "q."

I propose to conclude these comments on
price-theoretical explanations of investment
with a few remarks on the q theory. The
Keynes-Tobin approach makes the volume
of aggregate investment dependent upon
-indeed, ordinarily taken as proportional
to-the excess of q over 1: where q is the
ratio of the market value of enterprises to the
replacement cost of their assets. The market
value of enterprises includes the value both
of equities and debts, as currently priced in
security markets; the replacement cost of
their assets is the reproduction cost of existing
plant, equipment, and inventories at current
price levels."9

However formulated in detail, the q theory
of course employs much of the same information
that is used in other macroeconomic
theories of investment. For example, most
investment theories incorporate a bondmarket
interest yield, presumably to represent
the cost of borrowed long-term funds,
or the alternate return on owned funds. Some
investment theories may use short-term interest
rates, as well, to represent the cost of
(or the alternative return on) funds borrowed
to finance inventories or other working
capital, or to finance the purchase of capital
goods while awaiting more favorable rates in
the long-term market. The q theory incorporates
interest rates as they affect the value of
marketable debt claims against enterprises.
This use of interest rates in the q theory is
thus conventional in investment theory.
What is essentially unique about the q
formulation is its inclusion of the market
value of stocks: the price level of ownership
of corporations. This can be thought of as an
indirect measure of the cost (or opportunity
cost) of equity funds.20 Given the typical
volatility of share prices, medium-run movements
of q are often or perhaps ordinarily
dominated by changes in the prices and thus
the market value of outstanding shares.
With that in mind, let me recall to you my
earlier discussion of the speculative demand
for and supply of storable commodities-
gold was the specific example. You
recall my borrowing Keynes' parable of the
beauty contest to describe such markets. You
also recall that this parable was, in fact,
Keynes' analogy for the price determination
of equities.

In recent years, economists have been paying
greatly increased attention to stock
prices-a subject largely avoided by economists
after the debacle, in and after 1929,
both of share prices and of economists' forecasts
of share prices. The revival of
economists' interest in stocks has involved
the development and application of the
"capital asset pricing model" and associated
"portfolio theories," and, most recently, the
attempted application of rational expectations
theory to stock prices.


### ---Economics-1983-0-15.txt---
However, Robert Shiller's recent paper
"Do Stock Prices Move Much Too Much to
be Justified by Subsequent Changes in Dividends?
" appears to demolish the possibility
that movements of U.S. stock prices can be
explained by the rational expectations of
share holders.2' For over a century, real stock
prices appear to have fluctuated far more
than any plausible change in the rational
expectation of real dividends or earnings.
Shiller does not attempt to characterize the
source of the greatly excessive volatility of
stock prices. But, surely, it is possible that
speculative price bubbles, upward or downward,
based upon the extrapolation of nominal
share-price levels and movements, and
on the effort to profit (or to avoid loss) from
such movements, supply some part of the
explanation. Another piece of the explanation
for recent stock prices may well be
Franco Modigliani's and Richard Cohn's
"inflation-illusion" theory-itself clearly inconsistent
with rational expectations.

The rational expectations theory, at least
as applied to individual commodities, assumes
that there exists, at all times, some
long-run equilibrium price-that price which
would equate amounts produced and consumed.
Market participants can and do have
at least a rough idea of what this price is;
moreover, their individual estimates of this
price are basically similar, for all have essentially
the same information, and use essentially
the same model of how that price is
determined.

Those who attempt to apply the concept
of rational expectations to the stock market
rarely state their understanding of what is
the equilibrium price level for shares-or of a
particular share-that it would be rational
for participants to expect. Is it the level of
share prices at which net new issues of shares
would be zero? Or would it be the level of
share prices at which net private investment
would be zero? The level at which net private
investment would equal net saving at full
employment? The level at which investment
might maintain the economy on a Golden
Rule growth path? Or is there some concept
of a less-fundamental, less-long-run equilibrium
level of share prices which market
participants can be expected to understand
and quantitatively approximate? If there is
no such concept of equilibrium share-price
level, how can there be a unique rational
expectation of share prices?

A simpler course is to admit that we have
no very precise concept of an equilibrium
level of share prices, but to argue that we can
nevertheless predict the direction and rate of
movement over time of that equilibrium,
whatever it may be. The rate of movement
might, for example, be expected to approximate
the rate of growth of profits or of
dividends per share. (This was Shiller's device.
) This may suffice for rough tests of the
ex post rationality of historical share price
movements. But it offers essentially zero
guidance to the purchaser or seller of a
particular stock at a particular time. He must
guess whether the prices of particular stocks
will rise or fall, over some less than infinite
horizon, from where they stand today. Is it
strange that he is more concerned with the
correctness of his guess about what other
buyers and sellers are expecting will happen
to prices of those particular shares, and to
the market averages? Is there a better description
than the "beauty contest" parable?
Actually, Keynes' description of the determination
of stock prices is far more detailed
and substantive than I have here described
it, and I recommend its occasional
rereading. However, there is no reason to
suppose that Keynes in 1935 had the last
word on stock prices. Henry Wallich, an
acute observer, who began his distinguished
career on Wall Street, has recently written
about the "Radical Revisions of the Distant
Future" that occur from time to time in the
stock market, and for which one cannot find
rational explanation, in any narrow sense of
that term. His explanation for at least a part
of the most recent "radical revision" -since
about 1973-runs in terms that I can perhaps
best describe as "sociological":
One might guess that [the reasons for
this massive change in investment attitudes]
have something to do with

21For an important related paper, reaching similar
conclusions by different techniques, see William
Brainard, John Shoven, and Laurence Weiss.


### ---Economics-1983-0-16.txt---
the professionalization of the securities
business. Very likely this tends to homogenize
views, increasing the herd instinct
among bulls and bears, respectively.
The rewards/penalties system

for professionals works in that direction.
It is dangerous to be wrong in
support of an unpopular cause....
Professionals have made the market
efficient, in the narrow sense that there
is nothing of predictive value to be
learned from the past data of the
market. It is far from clear that they
have made it rational. There may be
something to be learned from the history
of mass delusions in the market
after all. [p. 38]

Modigliani and Cohn's suggestion may be
one such systematic mass delusion.
As one close to retirement, and one of
quite a number of us (I suspect) who have
left most of our retirement resources tied up
in CREF, I would of course, be pleased if
stock prices should return closer to their
average past relationship to earnings or dividends.
But I would not, I confess, find it a
confirmation of the rationality of my own
expectations. Personally, I long ago decided
that being an economist was not merely no
advantage, but probably a disadvantage in
the security markets; and I have never personally
participated in them. I have been
fearful, I suppose, of succumbing, myself, to
the herd instincts that I seem to observe
there. The broker who tells his customers
that Joe Granville is stupid, but that they
must pay attention to him because others
will, both assumes and encourages behavior
only one level removed from that of lemmings.


Because stock prices are not fully rational,
either in the large or even in the small,
sharp-eyed members of several generations
of my graduate students learned (not from
me) to support themselves in reasonable
comfort by playing on trivial systematic
anomalies that they had found in share price
movements. They succeeded, presumably, by
acting exactly as others do, but a trifle sooner.
Indeed, the lemming instinct affects participants
in other markets. One current example
is that of the international bankers,
whose loans to particular LDCs were safe
because-but only so long as-other international
bankers recognized exactly the same
source of safety.

Whether one describes investment decisions
in terms of the q formulation or in
some other way, prices in security markets
necessarily affect the volume of aggregate
investment. And, because such prices are
clearly not fully rational, investment is a
potential and actual source of exogenous disturbance
of macroeconomic equilibrium; and
successful government stabilization policies
are not, by definition, precluded.
Our stroll along the border between microand
macroeconomics comes to an end. What
can we conclude? I conclude that the rational
expectations model of economic behavior
adequately describes an important range
of economic activities, where prices adjust
smoothly and efficiently to clear markets and
to stabilize production and consumption over
time. But rational expectations do not adequately
explain other kinds of markets, where
speculative prices may systematically tend to
overshoot changing equilibrium levels: nor
yet another kind-including most labor
markets and many "customers markets"
-where price changes tend systematically to
undershoot changing equilibrium levels,
whether because of an inability to develop
efficient long-term contracts, the existence of
bilateral monopoly, or merely because of the
rapid pace of unpredictable technological,
institutional, or other exogenous change. All
of the resulting aberrant forms of microeconomic
behavior may in some sense be individually
"rational"; yet their macroeconomic
effects are often perverse.

Nevertheless, we economists do our best
to understand our world, and to discover
those dependable regularities of behavior
-whether or not fully rational-that provide
the basis for economics theories, which
we can then use to prescribe policies, whether
these policies are of laissez-faire or of selective
intervention. All forms of dependably
regular behavior that we seek to discover and
to describe-and not merely those that are
fully rational-are equally important parts
of the social science of Economics, that
Marshall once defined simply as "a study of


### ---Economics-1983-0-17.txt---
mankind in the ordinary business of life;
... that part of individual and social action
which is most closely connected with the
attainment and use of the material requisites
of wellbeing" (p. 1, emphasis added).
 ## Economics-1985-0


### ---Economics-1985-0-03.txt---
. .. [T]he world may have its reasons for
being non-Walrasian.

Robert Solow

The vast majority of our profession share
a common view on most microeconomic
policy issues. But we are widely split over
macroeconomic theory and policy. Our concensus
on micro issues arises from a shared
model of how markets work in the long run.
Our division on macro issues stems from a
number of reasons, the emphasis on which
has shifted over the years. In recent times the
main disagreement has centered on how
markets perform in the short run. In particular,
we cannot agree on why nominal wages
are sticky on the face of aggregate demand
shocks. The traditional view argues that
wages are structurally sticky. The new classical
macroeconomists argue that wages are
fundamentally flexible. But the rational expectations
of economic agents, grounded on
past experience with attempts at employment-
supporting monetary policy, have produced
the observed wage stickiness. Introduction
of a changed policy regime, based on
a stable growth path for the money supply or
some similar rule, would-so the argument
goes-eventually change the pattern of expectations
and eliminate the stickiness.

A large and rapidly expanding body of
recent research on implicit contracts, principal-
agent relationships, and related subjects
has begun to flesh out our knowledge of
why wages are sticky. Almost universally the
implicit contract and related literature concludes
that optimal behavior implies a good
deal less wage flexibility in the face of changes
in the marginal revenue product of labor
than would occur in the spot auction markets
of the Walrasian model. But this literature
deals with the stickiness of real and relative
wages in response to shocks of various kinds.
It seems to have little to say about the macroeconomic
stickiness of nominal wages. A
micro theory of real wage stickiness may
help explain the difficulty of adjusting to
sudden large changes in aggregate supply
conditions like the two oil shocks of the
1970's. But the more familiar problems facing
macroeconomic theory and policy have
to do with the ability of the economy to
adjust to aggregate demand shocks where
nominal wage stickiness is a major barrier to
successful adjustment. And here the important
question is what, if anything, does
the new research imply for the behavior of
nominal wages? It is to this subject I want to
give my attention.

A road map will be helpful. After summarizing
the existing literature, the first
section concentrates on relative wage adjustments,
and argues that under optimal

arrangements for the determination of wages,
relative wages while not rigid will be sticky.
They will adjust only gradually to relative
changes in the conditions facing individual
firms. The paper then argues that contrary to
general opinion, relative wage stickiness necessarily
produces aggregate nominal wage
stickiness; if wages move sluggishly in response
to the relative conditions facing individual
firms, they will move sluggishly in
response to aggregate nominal shocks. Several
mechanisms that might produce a flexible
wage response to nominal shocks, given sticky
relative wages, are considered and rejected
-various forms of indexing and rational
expectations. Finally the "external" nature
of the gains from nominal wage flexibility
is invoked against the criticism that attributing
cyclical unemployment to nominal wage
stickiness implies nonrational behavior on


### ---Economics-1985-0-04.txt---
the part of employers and workers. Macroeconomic
shocks will gradually overcome the
relative wage stickiness and move the aggregate
nominal wage level in the desired direction,
but the transitional costs are large.
I. Some Relevant Features of the Implicit
Contract Literature

By now the literature on implicit contracts
is so large and so diverse that I cannot do it
justice in a brief summary. But it is necessary
to sketch out a few elements of that research,
paying particular attention to several key
features that bear on the relationship between
wage behavior and aggregate demand
shocks.

The flexible-price, market-clearing model,
in which economic agents are price takers
and prompt quantity adjusters, would be a
useful paradigm in a very particular kind of
world. In this world labor and product
markets would be characterized by a great
deal of homogeneity, and so individual transactions
would be carried on in very "thick"
markets. There would be no reason for preserving
a continuity of relationships between
workers and firms, customers and suppliers,
lenders and borrowers. Workers would be
interchangeable; the marginal revenue product
of a particular class of workers would be
the same regardless of the firm to which the
worker was attached. Either labor effort could
be easily monitored or it would be completely
proportional to labor hours paid for.
And, wherever commitments had to be made
for the future, expectations about the important
variables entering into the decision
could be drawn from fixed stochastic distributions,
knowledge of which, in turn, could
be derived from the recurrent nature of past
events. As well as being unbiased, forecasts
could significantly improve on coin-flips.
The world that we are trying to model,
however, is in fact different in several fundamental
ways. Most importantly, in a

substantial part of the economy there are
large returns to maintaining the continuity of
association between workers and firms.
Workers acquire nontransferable, firmspecific
skills; and some of the cost of the
acquisition may be paid by the firm. They
acquire knowledge about the nonwage attributes
of a job through experience on thal
job. Continuity of association also provides
firms with hard-to-come-by knowledge about
the reliability and productivity of specific
workers. Additional transition, search, and
moving costs are incurred by workers and
firms when the association is broken. Substantial
bilateral monopoly rents to continuity
are thus generated among firms and their
experienced work force.

Risk aversion introduces another reason
for continuity of association. For at least
some range of possible variations in the economic
environment facing a firm, it is likely
to be less risk averse than its workers. But
the relevant insurance contracts cannot be
traded separately from employment contracts,
and so continuity of association becomes
a joint product with insurance. Finally,
in the real world, labor time is not
synonymous with performance. Monitoring
worker performance or effort is costly, but
the payment of higher than the "going wage"
will bind workers to the firms with an incentive
to avoid shirking. And the particular
workers who "survive" the monitoring are
those who have found the premium sufficient
to avoid monitorable shirking, a piece of
valuable information to the firm flowing from
continuity of association.

Realizing the benefits from long-term
worker-firm relationships requires, of course,
some sort of explicit or implicit agreement
between employers and workers on the terms
of the relationship, especially with respect to
wages and employment opportunities. Several
problems in modeling these long-term worker-
firm agreements have dominated the recent
literature. First, when the marginal revenue
product of labor changes, how are wages
and employment to be adjusted while still
preserving contractual relationships? Second,
since firms are much better able than workers
to observe the marginal product of labor and
since contingent contracts directly tied to the
various states of nature cannot practically be
designed, how can contract terms allow for
some flexibility in meeting changing conditions
without giving employers incentives
to provide false information about labor's
marginal product? This is the problem of


### ---Economics-1985-0-05.txt---
asymmetric information and has given rise to
the modeling of incentive-compatible contracts.
Third, since explicit written contracts
between unions and employers cover only a
small part of the workers involved in longterm
relationships, what keeps either workers
or firms from violating the implicit agreement
or exploiting their half of bilateral monopoly
situations when conditions are favorable
to do so? Analysis of this issue has
given rise to the concept of a firm's labor
market "reputation," or brand-name capital,
fear of losing which provides an enforcement
constraint. This is the problem of enforcement.


Three major strands of the literature on
implicit contracts can be discerned, each of
which emphasizes one of the rationales for
the continuity of association (without necessarily
denying the existence of other aspects),
and each of which deals somewhat differently
with the problems cited in the prior
paragraph.' The earliest version of implicit
contracts emphasized the role of risk aversion,
firms being either risk neutral or less
risk averse than workers to fluctuations in
their income.2 Firms thus offer risk-sharing
contracts that improve social welfare relative
to spot auction markets. Another body of
research stresses transactions costs and asset
specificity -that is, the acquisition of firmspecific
skills by workers, and specific and
valuable knowledge about each other by both
firms and workers.3 This approach also emphasizes
the asymmetry of knowledge between
firms and workers about the marginal
revenue product of labor, and its influence
on the nature of the contract. Still a different
emphasis is given in the efficiency wage4
literature to an assumed positive correlation
between the level (or the career profile) of
wages on the one hand and the "effort" or
productivity of workers on the other.5
While there are substantial differences
among these various approaches, they all
conclude that, under optimal contracts, real
wages will be smoothed in the face of changes
in the marginal revenue product of labor
relative to what would be predicted by an
auction market. And all of them provide a
rationale for the existence of ex post Pareto
inefficiency and involuntary unemployment.
II. Some Extensions to Implicit Contract Theory
We can distinguish several categories and
subcategories of changes in economic circumstances,
the wage response to which must
be accommodated by social conventions and
informal understandings that we call implicit
contracts. Let us consider first some of the
implications of contract theory as it deals
with the response of wages to changes in the
relative conditions facing individual firms or
labor markets. For that purpose I define
relative changes to be those which occur on
the assumption that the perceived general
level of opportunities facing workers-call
it W-remains unchanged in real and nominal
terms. (The relevant W is, of course, in
real terms. But since we are here abstracting
from any aggregate disturbances, real and
nominal W are the same.) In turn there are
two kinds of relative changes in economic
conditions that implicit contracts must allow
for; changes which are realizations of a probability
distribution known at the time the
contract is entered, and those stemming from
developments to which no basis could be
found for assigning probabilities.


### ---Economics-1985-0-06.txt---
To deal with implicit labor contracts
covering the long tenures that are typical in
U.S. industry, it is necessary to make the
currently out-of-fashion distinction between
risk and uncertainty. Most of the mathematical
modeling of implicit contracts has assumed
that workers and firms base their
agreements on a known probability distribution
(presumably commonly held) of the relevant
variables, most importantly the marginal
revenue product of labor or some
related variable. The distribution of the mean
expected bilateral monopoly rents from continuity
is decided at the beginning of the
contract on the basis of the known distribution
of possible economic environments that
will be faced over the life of the contract,
and the contract then specifies the behavior
of wages (and in some cases employment) as
the realization of that distribution occurs.
The wage is either rigid or changes sluggishly
in the face of these changing realizations.
But recent research on the surprisingly long
lengths of job tenure in the United States
casts doubt on the usefulness and sufficiency
of this assumption. Robert Hall (1980) has
estimated that, in 1973, half of all work in
America was done on jobs whose completed
tenures were fifteen years or more. And for
men alone the relevant completed job tenure
was twenty-five years! Douglas Wolf and
Frank Levy (1984) reached very similar conclusions
on the basis of a 1979 survey. The
contracts, rules-of-behavior, and social conventions
that make possible such long associations
must be such as to allow wages to
adjust appropriately in response to changes
in circumstances whose probability of occurrence
could not be determined in advance. In
other words, the informal agreements which
make possible long-term association between
workers and employers must take into
account Knightian uncertainty about future
possible outcomes. With respect to many of
possible states of the world, over such a long
period of time, there is no basis in past
statistical regularities for knowing the distribution.
As William Nordhaus (1976) points
out, contracts must take account of changes
in the economic climate (i.e., when the
parameters of the distribution shift) as well
as changes in the economic weather (i.e., as
realizations of the known distribution).
Most of the possible long-term changes to
be faced by firms and workers do not result
from the cyclical variance of aggregates, like
national income and output, but from
changes in relative variables potentially responding
to a bewildering permutation of
possibilities. Seen in 1973, what were the
rationally expected probabilities of the 1974
and 1979 oil price increases, the introduction
and growth of personal computers, or the
Chrysler brush with bankruptcy? Compared
to workers, firms may indeed be relatively
risk neutral to temporary changes in income
following some known distribution. But no
firm is so risk neutral and has such unlimited
access to capital as to enter upon or honor
contacts specifying rigid wages or some fixed
function of wages on employment over very
long periods,6 when no rational basis exists
for specifying the distribution of outcomes.
Some of the modern wage literature (for
example, Hall and David Lilien, 1979; Sanford
Grossman and Oliver Hart, 1981) models
a contract with a "lump-sum" distribution
of the rents-that is, an amount to be
paid the workers regardless of unemployment
status-and a marginal compensation,
paid when the worker is employed and itself
an agreed upon increasing function of the
level of labor input. Employers then determine
employment by maximizing profits
in the light of the marginal compensation
schedule. In these approaches, the climatic
changes to which I refer would be an occasion
for changing the basic lump sum distribution
of the rents.

A workable distinction can thus be made
between those contract provisions which deal
with wages in the face of moderate and
temporary changes in the marginal product
of labor that are perceived as the realization
of a known probability distribution and those
provisions which deal with climatic and permanent
changes, the probability of whose
6Indeed, for contracts covering long periods of time,
workers are probably less risk averse than firms. Under
some range of unfavorable outcomes, rigid wages could
mean bankruptcy for the firm. Against relative changes
in fortunes the worker loses only his "rent" from continuity
plus search costs. And, however well stockholders
may be able to diversify against bankruptcies,
the managers of the firm find it much more difficult.


### ---Economics-1985-0-07.txt---
occurrence cannot be estimated before the
fact. With respect to changes in labor demand
that are perceived to be consistent
with a previously known distribution, implicit
contract theory predicts either rigid
wages or-in the models like those of Hall
and Lilien and Grossman and Hart-wages
which move less than spot auction markets
would predict but are positively correlated
with the level of employment. But the informal
agreements or generally accepted conventions
that we call implicit contracts must
also provide for changes in circumstances
whose probability cannot rationally be estimated
at the time workers enter into the
contract. These climatic changes in economic
circumstances can be of several broad kinds.
They might involve a relatively long-term
change, because of shifts in demand or costs,
in the rents available to be shared by a firm
and its workers. Long-term changes in the
relative demand and supply of particular
occupations or in particular local labor
markets will also call for changes in relative
wages around a given W.

If the present value of the stream of benefits
to workers (wages and employment probabilities)
flowing from continuing with the
firm begins to decline relative to the alternative
combination of initial investment costs
and future rents that can be generated at
other firms, it is appropriate that contract
terms signal the information to workers, so
that they can make the relevant comparisons,
and decide whether to quit the firm. Similarly,
if a substantial and permanent decline
occurs in the relative demand for labor by
occupation or locality, wages ought to signal
the change. Conversely, economic circumstances
might increase the rents to be divided
making it optimal for the firm to enlarge its
share of the relevant labor market pool of
experienced workers. In that case, the signal
of higher relative wages ought to be transmitted
to attract from other firms or occupations
workers for whom the lost rents are less
than the improved opportunities.
An efficient determination of wages must
thus cope with two hard-to-reconcile sets of
facts. Because long continuity of association
between firms and specific workers confers
substantial benefits on both parties, rentsharing
arrangements that offer protection
against exploitation and promote such continuity
are profitable. But the period of association
is so long that the probability distribution
of circumstances requiring changes
in wages is largely unknowable. And so the
rules and conventions governing wage determination
must be flexible enough to allow
response. Seen in this light, the terms "implicit
contract" may be misleading. It is, I
think, useful to think of wages as being
set and periodically revised by firms within
limits imposed by a set of social conventions,
concepts of equity and fairness, and
informal understandings. These conventions,
concepts, and understandings provide substantial,
if imperfect, protection against exploitation
but allow flexibility to deal with
Knightian uncertainty.

While very lengthy association requires
relative wage adjustments in response to
" unforeseeable" changes in circumstances,
other characteristics of the labor market suggest
that those adjustments will tend to be
gradual and delayed. It would be hard to
account for the long job tenures that we
observe unless the combination of positive
returns to association and the cost of transition
were quite sizeable. The quantitative
evidence on the magnitude of explicit turnover
costs suggests that they are large. Daniel
Mitchell and Larry Kimbell (1982), for example,
report a recent estimate, based on a
survey of Los Angeles firms, that firms' own
costs of turnover (exit costs plus replacement
costs) averaged $3,600, $2,300, and $10,400
for production, clerical, and salary-exempt
workers, respectively.

Under these circumstances, erroneous signals
can have asymmetrical results. Changes
in wages undertaken on the mistaken identification
of a temporary change in circumstances
as a permanent change can cause
a substantial loss to workers and firms from
the unnecessary scrapping of "investment"
and the incurring of other turnover costs.
Entire rent streams are wiped out to the
extent that workers having transferred to
other firms cannot return when the mistake
is discovered. Moreover, once experienced
workers do change jobs, it is likely to take
several tries before they find another longtenure
job. Hall (1982), for example, shows
that in 1978 a worker aged 45-49 who was in


### ---Economics-1985-0-08.txt---
a new job had only a 20 percent probability
of holding that job as long as five years.
While the large magnitude of rents for
experienced workers tends to provide some
room for errors, workers are presumably
arranged along a spectrum with respect to
their own evaluation of potential opportunities
elsewhere. As a consequence, the losses
from the errors will be a continuous positive
function of the size of the errors. Errors of
the opposite type-failing to introduce a
contract change to meet a permanent alteration
in the climate-can be reversed at a
much smaller cost. Some workers will have
remained too long with a particular firm,
while a smaller group will have lost income
from the higher layoffs associated with this
type of error. But these losses are likely to be
much smaller than the loss of the rent stream
itself, unless the error persists for a long
time.

Thus, given Knightian uncertainty about
economic conditions over the long length of
job tenures, implicit contracts must allow for
the possibility of relative wage changes in the
face of climatic changes in external conditions.
But the interest of both firms and
workers dictates that those changes occur
only after enough information has been
accumulated to warrant a high probability
forecast that the change is permanent. To
justify a substantial wage adjustment, it is
not sufficient that the firm act on unbiased
forecasts-they must also acquire some confidence
in the accuracy of the forecasts.
One might object to this line of reasoning
on grounds that failure to adjust wages
quickly enough while the evidence is accumulating
that the change in circumstances is
permanent, is itself likely to send out wrong
signals. If, for example, wages are slow to
adjust downward when the demand for labor
falls, employment will decline. Why will
workers not take this as a signal that the
future stream of benefits from staying in the
current job have declined relative to earlier
anticipation? There are a number of reasons
why this objection is not valid. First, a change
in wages is known immediately, while it takes
some time to begin to realize that an actual
change in employment reflects a shift in the
long-term distribution of employment probabilities.
Second, the common practice of
layoffs subject to recall is a way for a firm to
signal that the lower employment is expected
to be temporary. Third, significant downward
changes in relative wages are not lightly
made. Given the inability of workers easily to
assess the magnitude of the available rents,
firms are deterred from exploiting the bilateral
monopoly relationship through the
damage they might do to their "reputations"
and the future increases in employment costs
thereby imposed. Since reputation is a fragile
asset, and since workers are naturally less
likely than firms to interpret current facts as
warranting a relative wage cut, firms must
wait until a substantial body of evidence
points in the required direction. The efficiency
wage literature emphasizes the unfavorable
productivity consequences of relative wage
reductions that turn out to be unwarranted.
And, since firms know that it is difficult to
reduce wages once they have been raised,
they do not want to make a mistake in the
upward direction. As a consequence, wage
changes that survive these barriers are much
more clearly interpreted as a signal that the
stream of future rents has changed than are
variations in employment.

Efficient implicit wage contracts, in the
presence of uncertainty, must also have the
characteristic that they minimize "haggling
costs." They should not lead to a constantly
renewed battle over the division of the rents.
Frequent struggles would waste resources,
possibly reduce productivity, and erode
scarce reputation capital, thereby reducing
the probability of long-tenure associations.
These considerations argue that significant
relative wage adjustments to meet perceived
permanent changes in condition be an infrequent
occurrence and not lightly changed
once made.

In sum, modern contract theory concludes
that relative wages will tend to be quite
sticky in comparison to predictions from the
auction market model, in the face of changes
in conditions that are the realizations of
known probability distributions-what I
have loosely called temporary changes. But
the existence of very lengthy job tenures and
the large gains from continuity suggest that
the social conventions and informal agreements


### ---Economics-1985-0-09.txt---
which we call implicit contracts must
provide for adjustments in wages that move
towards market clearing in response to unforeseeable
permanent changes in relative

economic conditions. Finally, however, the
substantial penalties which can be suffered if
firms and workers respond to erroneous signals
by prematurely severing association call
for informal arrangements and agreements
that produce a very slow and cautious adjustment
of wages even to what ultimately
turn out to be permanent changes in relative
conditions.7

It is not surprising, given the large social
returns to continuity of association and the
very great difficulty of distinguishing temporary
from permanent changes in economic
circumstances, that society should have developed
social conventions and informal
agreements that minimize the sending of premature
signals for a reallocation of resources.


III. Response to Aggregate Demand Shocks
So far we have considered a world in
which only relative or local changes are permitted,
a world in which the nominal and
real value of the general wage level, or the
wage "norm," was fixed. Now impose aggregate
demand shocks on such a world,
optimal adjustment to which requires a
change in the path of average prices and
wages. (For simplicity of exposition we will
be considering only aggregate demand
shocks, and exclude aggregate shocks to the
supply curve, like those arising from OPECimposed
oil price increases or from crop
shortages. We can thus assume that the equilibrium
solution will not call for changes in
aggregate real wages.) An efficient system of
implicit contracts must obviously provide for
adjustments in the nominal wages of individual
firms when changes occur in the average
level of nominal wages, W. Under the social
conventions and understandings which govern
wage determination, there is a rebuttable
presumption that-barring the existence of
circumstances which call for changes in real
and relative wages-nominal wages in each
firm will be adjusted in line with observed
changes in prices and wages generally. But
this process of wage determination does not
generate prompt and flexible adjustment of
aggregate nominal wages to nominal shocks.
A change in the average level of wages is the
product of changes in wages by individual
firms. To the extent that individual wage
adjustments must wait on changes in the
average, aggregate nominal wage flexibility
will not be a characteristic of the system.
And, as discussed below, this is also true, but
to a somewhat lesser degree, of the wage
response to price changes. Given the substantial
costs of sticky nominal wages to
society as a whole and to individual firms
and their workers, is there not some other
adjustment mechanism which would generate
a prompt and flexible nominal wage response,
preserving relative wages but producing
the desired nominal flexibility?
Several lines of inquiry suggest themselves.
First, why are not nominal wages explicitly
indexed to some aggregate nominal indicator,
producing the appropriate change in W
in response to aggregate demand shocks?
Second, even with relative wage stickiness,
would not rational firms and their workers
forecast the ultimate equilibrium change in
W and promptly set individual wages accordingly?
And, finally, if that is not feasible,
why do implicit contracts not permit
swifter and larger nominal wage adjustments
by individual firms under the force of aggregate
nominal shocks?


### ---Economics-1985-0-10.txt---
Let us start with the issue of indexing
wages in individual firms to the general price
level. In the United States, wages are not
widely protected against changes in price by
explicit indexing formulae. In 1983, only 58
percent of union workers were covered by
COLAs. On the average, the ones that were,
received protection against only 53 percent
of the changes in the CPI. Jo Anna Gray
(1978) and Stanley Fischer (1977) examined
the conditions under which indexing would
or would not be optimal for individual firms
given the assumption that wages once set are
not renegotiated for some period. The main
conclusion from this research is that, in the
face of real (as opposed to nominal) shocks,
indexing, by freezing real wages, produces
real wage results that are not optimal for the
firm. Given the probability that nominal and
real shocks are both likely to occur, Gray
shows that partial indexing will be an optimal
choice. Moreover, even nominal shocks
are likely, during the transition to a new
equilibrium, to have nonneutral effects; the
firm's product price and the Consumer Price
Index may not move in parallel. As a consequence,
indexing would have unwanted real
effects even in the face of monetary shocks.
While we have seen that some degree of real
wage stickiness is optimal, the absence of full
indexing even in multiyear union contracts
indicates that the opposite extreme- automatic
guarantees of a fixed real wage over
several years-is not a workable approach.
Within-year indexing is virtually nonexistent
in annual union contracts and in the
annual wage adjustment cycle followed by
the vast bulk of nonunion firms. Under implicit
contracts, changes in the path of average
wages and prices in the economy as a
whole or in relevant submarkets are commonly
agreed to constitute a major element
in determining the size of those annual wage
adjustments. But, in the United States, it is
almost universally the practice not to make
the relationship to the price level or to average
wages an explicit and automatic one,
either within the year or over longer periods.
And, as noted earlier, in those multiyear
union contracts where indexing is found, that
indexing is almost always less than complete.
Haggling costs are apparently minimized by
making periodic wage adjustments that simultaneously
take into account nominal, real,
and relative factors, rather than by fixing the
nominal relationship in a formula and separately
adjusting for changes in real and
relative conditions.

In any event, even if wages were fully
indexed to prices, they would not produce
highly flexible nominal wages in response to
aggregate demand shocks. The cumulative
costs of wage indexing, in the face of real
shocks and the transitional nonneutrality of
nominal shocks, would be severely exacerbated
if the indexing were instantaneous.
As a consequence, the indexing we do observe-
except in countries which have developed
extremely rapid and sustained inflation
-typically involves a substantial lag between
observed price changes and wage adjustments.
Such indexing only guarantees a
gradual response of wages to aggregate demand
shocks in proportion as prices themselves
are flexible in the face of constant
wages. Even if prices were competitively determined,
the economy would still have to
work its way through a series of price-wageprice
reactions in each one of which prices
fell, relative to wages, by an amount depending
on the slope of the marginal cost curves.
And prices do not move with such frequency
or flexibility. Arthur Okun (1981) has carefully
elaborated the reasons why, in the
customer markets which predominate in
modern economies, prices are not likely to
move quickly and easily up and down a
marginal cost curve. Robert Gordon (1981)
has elaborated how the highly articulated
input-output relationships of modern economies
tend to slow the price reaction to aggregate
demand shocks. And George Akerlof
and Janet Yellen (1984) have recently shown
that in less than perfectly competitive markets
inertial price-setting behavior in response to
a shock may impose only second-order losses
on the firms who follow such behavior, even
though the macro result may be first-order
losses for the economy.8 Everything else
8Technically the Akerlof-Yellen proposition applies
to situations in which agents' objective functions are
differentiable in their own prices and wages-a condition
that is not met in the competitive model, but is met
in a wide range of other market models.


### ---Economics-1985-0-11.txt---
being equal, indexing wages to prices would
provide some nominal wage flexibility in the
face of nominal shocks. But if wages are
otherwise sticky, indexing them to prices
would still yield a very gradual iterative process
of demand inflation or disinflation.
Granted the obstacles to indexing wages to
prices in implicit contracts, and the insufficiency
of that arrangement-even if

feasible-to produce prompt nominal wage
adjustments, why are not implicit contracts
indexed to some aggregate like nominal GNP
or the money supply? Such an arrangement
might seem to be a way to approximate the
role of the Walrasian auctioneer, automatically
generating W at a level to clear the
aggregate supply-demand balance, while relative
wages continued to be set under implicit
contracts along lines suggested earlier.
In fact, of course, we observe no such
arrangement anywhere in the world, and a
little thought supplies a number of reasons.
For purposes of indexing wage contracts to
nominal GNP, some agreed-upon process
would have to be found for separating "disturbances"
in nominal GNP from the trend

increases consistent with full employment at
a stable inflation rate. Anything that altered
the parallel growth of average and marginal
labor productivity, or the growth of full employment
labor inputs, would change the
trend and require the contracts to be renegotiated.
Robert Gordon (1981, 1983) has
identified a number of reasons, why, even if
the appropriate split could be made between
trend and disturbances, indexing wages to
nominal GNP would not be feasible in implicit
contracts. If prices themselves are not
completely flexible relative to wages, declines
in nominal GNP under an indexed system
would produce long periods with unwarranted
real wage decreases. And, since the
costs and hence the prices of the typical large
firm depend on the costs and prices of a long
and heterogeneous chain of suppliers, indexing
wages on nominal GNP would only index
part of an individual firm's costs.
Workers would rightly be skeptical that such
an indexing system would quickly move
prices down proportionally with wages.
The problems of automatic indexing to
some other nominal aggregate, like the money
supply, are even worse since the relationship
between any other aggregate variable
and the equilibrium full employment wage
level is still more complex and unstable than
it is in the case of nominal GNP. And unless
all firms could somehow agree on a common
translation formula for indexing purposes,
nominal demand shocks would produce a
wide dispersion of unwarranted changes in
relative wages. (This point is elaborated further
in the paragraphs that follow.) More
complex explicit indexing formulas can be
imagined, but are no more feasible than simple
ones. The two parties to a contract would
be subjecting themselves to very great uncertainty
in agreeing to a given information set
and forecasting model as the basis for the
indexing. It takes a very large run of data
to sort systematic error from noise in economic
time-series. And so agents would have
huge space for disagreement about the appropriate
information set and the relevant
model for translating information into forecasts.
Alternative choices could lead to biased
outcomes, whose bias could not be determined
for a very long time. (The appropriate
order in which to enter variables in
a vector autoregression model is hardly the
subject for fruitful labor negotiations.) In
short, feasible state-contingent contracts cannot
be designed to replace the Walrasian
auctioneer as a means of coordinating the
system's response to nominal shocks.9
In the absence of explicit state-contingent
contracts, can nominal wage flexibility be
rescued by a rational expectations model of
wage determination? Why do not individual
firms and their workers rationally forecast
the change in the equilibrium path of average
wages (W) expected to result from a
nominal shock, and promptly change wages
accordingly, recognizing that their actions
involve no decision to change relative wages?


### ---Economics-1985-0-12.txt---
Stochastic errors in forecasting could generate
temporary wage "errors" and departures
of employment from its natural rate. But
nominal wages would fundamentally be
flexible.

A rational expectations approach to the
determination of the aggregate nominal wage
and price level, however, cannot simply be
carried over into a world of sticky relative
wages. There are several reasons why this is
so. In the first place, in the new classical
model it is not the expectational element
that generates aggregate nominal wage flexibility.
Rather, a prompt response of aggregate
nominal wages to nominal shocks is
guaranteed by the perfect ex ante flexibility
of relative wages in auction markets. In these
models a downward nominal demand shock
generates a prompt and "neutral" change in
the path of nominal wages precisely because
of individual workers' presumed willingness
to underbid wages, shading their bids and
offers in the face of excess supplies, to the
point where markets are cleared. In the auction-
market model, workers or groups of
workers are willing to accept lower wages in
the face of excess supply even when errors in
expectations lead them to misperceive the
entire wage cut as a relative one. In the event
of such misperceptions, some labor supply is
withdrawn but nominal wages still fall. And
as soon as the misperception is corrected,
wages quickly fall by the remaining amount
necessary to eliminate excess labor supply
and clear labor markets. In the world of
implicit contracts I have described earlier,
however, the absence of substantial relative
wage flexibility eliminates this basis for
prompt nominal wage flexibility.
In the absence of auction markets with
their relative wage flexibility, the achievement
of aggregate nominal wage flexibility
becomes a prisoner's dilemma problem. But,
since wages are not completely rigid under
implicit contracts, it is not possible that individual
firms and their workers forecast the
ultimate equilibrium response of average
wages to nominal shocks, promptly adjust
their own wages to it and thereby solve the
prisoner's dilemma in favor of nominal wage
flexibility? I think not. Two key features of
implicit contracts interact with one central
feature of rational forecasts to reduce sharply
the feasibility of commonly shared expectations
about the equilibrium W as a surrogate
for Arrow-Debreu contingent claims contracts.
First, firms are wage setters, not wage
takers; second, frequent haggling over wage
changes is very costly to the development of
mutually beneficial long-term relationships
between workers and firms; and third, forecasts
of equilibrium wage and price levels are
subject to substantial stochastic error and
forecast outcomes are likely to be widely
distributed over the population of wage-setting
firms.

In a world of auction markets, the fact
that forecasts of individual agents are widely
distributed around the "true" mean is for
most purposes irrelevant. In his seminal
article (1961), John Muth pointed out that
cross-sectional differences in expectations
posed no problem for the theory because
their aggregate effect would be negligible so
long as deviations from the rational forecast
by individual firms were not strongly correlated
with each other. In auction markets,
specific prices or wages are not determined
by individual forecasts. But, in a world of
implicit contracts with firms as wage setters,
they would be. The dispersion of individual
forecasts concerning the equilibrium nominal
wage W, and therefore the dispersion of individual
wage decisions, would often be a wide
one. Robert Lucas' words about the role of
rational expectations in shaping the actions
of economic agents are relevant in this regard:


Neither will [rational expectations] be
applicable in situations in which one
cannot guess which, if any, observable
frequencies are relevant: situations
which Knight called 'uncertainty'. It
will most likely be useful in situations
in which the probabilities of interest
concern a fairly well defined recurrent
event, situations of 'risk' in Knight's
terminology. [1977, p. 15]

Changes in the path of nominal wages, however,
have not simply, or even primarily,
been driven by recurrent patterns of endogenous


### ---Economics-1985-0-13.txt---
events, at least in recent years. The
Vietnam War, two massive oil shocks, the
introduction of floating exchange rates, and
the institution of a new monetary regime in
the United States in 1979 have dominated
events. Even if the vast majority of firms
held a broadly similar view of the way the
economic world works, the very great macroeconomic
uncertainty and the stochastic
variance of prior forecasts around the actual
outcomes would guarantee a wide dispersion
of individual forecasts whenever large shocks
occurred.

A wide dispersion of the forecasts of individual
economic agents and the experience of
errors in prior forecasts would have a
number of consequences. In the first place,
given the large room for disagreements about
the forecast, we have to rule out on moral
hazard grounds implicit contracts under
which workers, prior to actually observing
a deterioration in employment conditions,
would accept employer forecasts of a declining
equilibrium nominal wage as the basis
for a downward wage adjustment. But even
waiving this difficulty, the large dispersion of
individual forecasts would result in widespread
unintended relative wage changes,
even if all firms and their workers accepted a
wage adjustment based on the firm's equilibrium
forecasts. Because of both risk aversion
and the consequences to firms and
workers that follow from erroneous signals,
these changes could impose significant losses
on the parties. Yet, a high frequency of wage
changes is also costly, so that errors would
tend to persist for some time. Indeed, given
the incompleteness and imperfections of
current information, and the murkiness of
the variable being forecast-the equilibrium
nominal wage level-there would be substantial
room for disagreement among the
parties as to whether or not a prior forecast
had or had not been in error. Finally, to the
extent that "bad" experience with forecasts
caused some firms and workers to reduce the
forward-looking element in wage setting, the
unreliability of forecasts for those who used
them would become greater. It would become
increasingly less rational to base individual
wage decisions on the assumption that
others were forecasting the equilibrium
adjustment and promptly adjusting wages
accordingly. Thus, without the "policing"
mechanism of relative wage flexibility, a system
that relied on rational expectations forecasts
of the equilibrium wage outcome would
be unstable.

In sum, under implicit contracts, widespread
tying of individual wage decisions to
expectations about equilibrium aggregate
wage outcomes is not a feasible way of anticipating
the optimal adjustment to nominal
shocks. The policing mechanism of ex ante
relative wage flexibility is absent, so that
nominal wage flexibility becomes a prisoner's
dilemma problem. And prompt rational
expectations "indexing" to the equilibrium
outcome is no solution to that problem, since
it would increase haggling costs, produce
unwanted relative wage changes, and become
increasingly an irrational action on the part
of individual firms.

In the absence of widespread indexing
to some nominal aggregate, or to the rational
expectation of the equilibrium wage norm
W, wages must find their way to a lower level
as the product of specific decisions among
individual firms and their workers, in a process
constrained by the same social conventions
and informal agreements that dictate
the change in relative wages under implicit
contracts. In a world of price and wage
setters, firms and workers observe demand
shocks principally in the form of changes in
their own physical quantities- sales first and
then output and employment-and in the
context, initially, of an unchanged perceived
level of W. The dynamics of the process by
which firms adjust from one level of the
work force to another can, as described by
Okun and by George Perry (1980), generate
modest wage changes in response to the aggregate
shocks. Beyond this, the change in
external pressures for wage adjustment must
be large enough and last long enough to
satisfy the constraints imposed by long-term
implicit contracts on "permanent" wage
changes. Finally, to the extent that these
changes become substantial and widespread,
enough to yield a long-term change in the
perceived level of W, the whole nominal


### ---Economics-1985-0-14.txt---
wage structure around which individual
adjustments occur will be changed.
There are several points to note about this
process. The major gains, in terms of higher
employment, that came from lowering wages
in the face of downward aggregate demand
shocks do not accrue to particular workers in
particular firms as the result of their own
actions. Rather, the gains accrue through the
effect of generalized lower wages in reducing
prices, raising real money balances and
thereby increasing aggregate demand. Once
the assumption of flexible auction markets
and a competitive bidding down of wages by
individual workers is abandoned, it is no
longer valid to level against sticky-wage theories
the charge that they imply an irrational
failure on the part of economic agents to
pursue unexploited gains from trade. Without
the Walrasian auctioneer, the individual firms
in an economy are, as noted earlier, in a
prisoner's dilemma. The large comparative
statics gain from an aggregate nominal wage
cut does not translate into such a gain seen
from the point of view of individual firms.'0
The potential gain they see is the one that
would accrue from making a relative change.
They will indeed make such changes, but
slowly and cautiously, acting under the constraints
on relative changes spelled out
earlier.11 Only as the perceived long-term
level of W falls will the situation be different.


Since what is important for nominal wage
adjustment is the perceived level of W, expectations
about its future value will, of
course, be relevant. Within the constraints
imposed by implicit contracts, wages in individual
firms have to be adjusted to deal with
changing conditions. Since wage changes are
difficult and impose strains on long-term relationships,
the wage once set has to last for
a while, typically at least a year (and under
union contracts often longer). The expectations
that workers and employers have about
the future course of average wages and prices
will therefore exert an important influence
over the current wage decision. I do not want
here to join the controversy over the extent
to which wage setting is backward or forward
looking. But what is central to my
message is that the relevant forecast does not
assume prompt adjustment to a new equilibrium
wage but rather the more hesitant
and gradual process described above.
The appropriate framework for analyzing
the aggregate behavior of wages, therefore, is
not one in which certain macroeconomic imperfections-
information gaps or misperceptions
about the general wage and price level
-prevent the market-clearing adjustment of
wages, which themselves are perfectly flexible
in the face of relative disturbances. The
essence of the behavior of wages in response
to aggregate demand shocks is just the opposite.
Macroeconomic shocks break through
the short-run stickiness of wages and prices
in the face of relative disturbances to produce
the aggregate adjustments, albeit slow
and gradual ones, that we do in fact observe.
There is a nice paradox in all of this. A
prompt adjustment of nominal wages to aggregate
demand shocks, leaving relative
wages unchanged, can be produced only by a
system of highly flexible relative wages. Conversely,
the existence of sticky relative wages
yields long transitional periods in which firms
are adjusting individually to nominal shocks,
and produces, as a side effect, changes in
relative wage and prices.

IV. Final Reflections

The large costs which accompany disinflation
arise from the fact that society has to
0l1n his 1977 article, Robert Barro decouples the
nominal wage and employment responses to perceived
nominal shocks. He argues that even if nominal wages
are sticky, optimal contracts would call for the maintenance
of employment. Firms would vary prices to achieve
this result despite the stickiness of nominal wages.
Otherwise, says Barro, the parties would be ignoring the
unexploited gains from trade, an irrational act. This
paper is not principally concerned with price behavior.
But it is clear that the same externality argument, set
forth above with respect to wages, holds for prices.
Without auction markets or some surrogate for the
Walrasian auctioneer, the external nature of the gains
from price changes would tend to negate the force of the
Barro argument.

1 Since the macroeconomic effects of their own wage
and price decisions do not enter into economic agents'
objective functions, the Akerlof-Yellen conclusions
about the small size of losses from "near-rational"
behavior would apply to the nonauction markets I am
here describing.


### ---Economics-1985-0-15.txt---
send out the same kind of initial signalschanges
in the volume of sales-when it
wants a reallocation of resources as it does
when it wants a change in the general level of
wages and prices. In the first situation, given
the substantial efficiencies which flow from
long-term associations of suppliers with
customers and firms with workers, very cautious
and sluggish changes in wages and
prices are the optimal response to signals. A
large part of the adjustment is optimally
taken up as temporary variations in slack
adjustments in hours, layoffs and rehires,
inventory building and depletion, and rationing
of various kinds. Indeed, a large part of
economic life is dominated by the social
conventions, institutions, and patterns of behavior
that have evolved to avoid the chaos
and inefficiencies that would result from continuous
market clearing.

The signals which firms initially receive
when aggregate demand shocks occur are the
same as those for a resource transfer, but an
exactly opposite response is wanted-large
changes in wages and prices and small
changes in quantities or slack. Since the bulk
of the disturbances to which individual firms
and workers must adjust are relative or local
in nature, and since over long periods of
time micro efficiency tends to outweigh aggregate
resource utilization as a source of
economic welfare, wage- and price-setting
institutions have developed with a bias toward
the sluggish response called for by considerations
of micro efficiency. The cyclical
behavior of the aggregate wage and price
levels unfolds from the gradual overcoming
of that bias.

In the long run, those features of economic
relationships which make short-run price and
wage stickiness optimal and which rationally
prevent continuous market clearing disappear.
Specific assets are converted to capital.
Attrition and learning change the mix of
skills. Random changes get smaller compared
to systematic changes. The private returns
from specific customer-supplier and
worker-firm attachment shrink relative to the
returns from making appropriate adjustments
to changes in tastes, technology, and
other external developments. The rationally
based barriers to market clearing crumble.
We economists are indeed correct to insist on
the long-run efficacy of markets and the utility
of the market clearing paradigm as a way
of explaining long-term market allocations.
But we need not abandon the premise of the
rational maximizing calculus in order to explain
the structural stickiness of wages and
prices and the failure of markets to clear in
the short run. Both phenomena-long-run
market clearing and short-run stickinessultimately
derive from the same rational

aspects of human behavior.

Some of the consequences of this recognition
are nevertheless very troubling for
economic theory and theoretically informed
empirical research. In the new classical economics,
there is no need for empirical research
to determine how wages and prices
respond to demand shocks, given expectations
about the general price level. Pure
theory-the auction-market model-dictates
how prices and wages behave. Empirical research
is needed principally to tell us something
about the formation of expectations on
the general price level.

While contract theory and related research
has been developing rationally based foundations
for structural wage and price stickiness,
the work to date is essentially in the form of
existence theorems. That is, it tells us why
sticky wages are consistent with the rational
calculus. But it does not give us a theoretical
basis for specifying the two basic components
of macro wage adjustment: What
"laws" do firms follow, under implicit contracts,
in adjusting their wages, assuming the
stability of W, the wage norm? And what
does it take to produce a perceived "permanent"
change in that norm?

A full and complete microeconomic foundation
to wage adjustment with the power of
the auction-market model may never be
forthcoming. If that is so, we may have to
look to regularities derived from macroeconomic
empirical research to infer microeconomic
behavior. But this raises another set of
problems. While forward-looking expectations
play much less of a role in the macro
wage adjustment process I have outlined,
they are not completely absent in forming
perceptions about the wage norm. Hence the
force of the Lucas critique, though weakened,


### ---Economics-1985-0-16.txt---
does not disappear. The new classical economics
simply assumes that structural behavior
is market clearing, and hence claims to be
able to identify the expectational effects of a
particular policy regime. In the absence of
such an a priori assumption, however, how
does one go about separately identifying the
expectational from the structural elements in
wage formation? I do not have the answer.
Conceivably, economics, like physics, is subject
to a fundamental indeterminacy theorem.
 ## Economics-1986-0


### ---Economics-1986-0-03.txt---
When the word of my prospective elevation
to this exalted position first circulated at
MIT at the end of March 1983, I happened
to encounter Peter Temin in the library. He
offered congratulations, and added: "In your
presidential address, skip the methodology.
Tell them a story." This is the technique that
he and Paul David used to great effect in
the session on economic history at Dallas a
year ago. I choose, however, to follow the
lead of another economic historian, Donald
McCloskey, who maintains that economics
should be a conversation (1983).
In a recent paper, unpublished I believe,
George Stigler discussed " the imperialism of
economics," which, he claims, is invading
and colonizing political science-through
public choice theory and the economic theory
of democracy-law, and perhaps especially
sociology, where our soon-to-be president-
elect, Gary Becker (1981), has extended
the reach of economics into questions of the
family, marriage, procreation, crime, and
other subjects usually dealt with by the sociologist.
"Imperialism" suggests super- and
subordination, with economics on top, and
raises the question whether as a profession
we are not flirting with vainglory.
My interest has long been in trade, and I
observe that economics imports from, as well
as exports to, its sister social sciences. In
public choice, we can perhaps explain after
the event whose interest was served by a
particular decision, but we need political science
to be able to forecast which interest is
likely to be served, whether that of the executive,
the legislature, the bureaucracy, some
pressure group-and which pressure group
or, in the odd instance, the voters. Individuals
act in their own interest, let us grant,
but a more general motive of emulation may
be drawn from sociology as Adam Smith was
aware in the Wealth of Nations (1776, p.
717), as well as in The Theory of Moral
Sentiments (1759 (1808), I, p. 113). I want
today to borrow one or two ideas from political
philosophy, and to conduct a conversation
with a new, impressive, and growing
breed of political scientists working on international
economic questions. The discussion
falls into two loosely connected halves- the
first dealing with what economists can, perhaps
should, and to some extent do, import
from political philosophy and sociology; the
second dealing more especially with international
public goods.

That sharp and sometimes angry theorist,
Frank Graham (1948), thought it a mistake
to think of trade between nations. Trade
took place between firms, he insisted. The
fact that they were in different states was
irrelevant so long as economic policy was
appropriately minimal, consisting perhaps of
free trade, annually balanced budgets, and
the gold standard. But states may differentiate
between firms, through such measures as
tariffs, embargos, monetary, fiscal, and exchange
rate policy which affect all firms
within a given space, and this adds a political
dimension (see my 1978 study). The essence
may go deeper. In an early graduate quiz,
I asked for the difference between domestic
and international trade, expecting a Ricardian
answer on factor mobility. One paper,
however, held that domestic trade was
among " us," whereas international trade was
between "us" and "them." The student who


### ---Economics-1986-0-04.txt---
wrote this (now escaped from economics and
teaching international law at a leading university)
had come from Cambridge University
and a course with Harry Johnson. We
go beyond this simple statement today in
saying that nations are groups of people with
common tastes in public goods (Richard
Cooper, 1977). Geography discriminates between
countries, as a hypothetical customs
union between Iceland and New Zealand
would demonstrate, and so do governments.
Behind and alongside of governments, people
discriminate.

Public goods, let me remind you, are that
class of goods like public works where exclusion
of consumers may be impossible, but in
any event consumption of the good by one
consuming unit-short of some level approaching
congestion-does not exhaust its
availability for others. They are typically
underproduced-not, I believe, for the Galbraithian
reason that private goods are advertized
and public goods are not-but because
the consumer who has access to the
good anyhow has little reason to vote the
taxes, or pay his or her appropriate share.
Unless the consumer is a highly moral person,
following the Kantian Categorical Imperative
of acting in ways which can be
generalized, he or she is apt to be a "free
rider." The tendency for public goods to be
underproduced is serious enough within a
nation bound by some sort of social contract,
and directed in public matters by a
government with the power to impose and
collect taxes. It is, I propose to argue in due
course, a more serious problem in international
political and economic relations in the
absence of international government.
Adam Smith's list of public goods was
limited to national defense, law and order,
and public works that it would not pay individuals
to produce for themselves. Most
economists are prepared now to extend the
list to include stabilization, regulation, and
income redistribution (Cooper, 1977), even
nationalism (Albert Breton, 1964), and standards
that reduce transaction costs, including
weights and measures, language, and
money. Public goods were popular a decade
ago. There is something of a tendency today,
at least in political science, to draw back and
claim that such institutions as open world
markets are not public goods because countries
can be excluded from them by discrimination.
One monetarist goes so far as to
maintain that money is not a public good,
arguing, I believe, from the store-of-value
function where possession by one individual
denies possession by others, rather than from
the unit-of-account function in which exclusion
is impossible and exhaustion does not
hold (Roland Vaubel, 1984).

II

Before addressing international public
goods, I want to digress to suggest that there
are other limits to the imperialist claims of
economics. Social goods are not traded in
markets, for example-honor, respect, dignity,
love. In his address to the Columbia
University Bicentennial Assembly, Sir Dennis
Robertson asserted that what economists
economize is love (1955, pp. 5-6). Michael
Walzer (1983, pp. 101-02) has compiled a
list of " things" that contemporary moral
philosophy will not tolerate being bought
and sold: human beings, political power,
criminal justice, freedom of expression, marriage
and procreation rights (pace Becker),
the right to leave the political community,
exemptions from military service and jury
duty, political office, basic services like police
protection, desperate exchanges such as
permission for women and children to
work fourteen hours a day, prizes and honors,
love and friendship, criminally noxious substances
such as heroin. The inclusion of a
number of items on the list is debatable, and
history reveals that most of them have been
traded on occasion in some cultures. The
market, moreover, strikes two lawyers as a
dubious device for making "tragic choices,"
like those in which scarcity confronts humanistic
moral values, for example, allocating
food in famine, children available for adoption,
or organ transplants (Guido Calabrese
and Philip Bobbit, 1978). It is difficult to
dissent from Walzers's conclusion that a
radically laissez-faire economy would be like
a totalitarian state, treating every social good
as if it were a commodity (1983, p. 119).
There is, moreover, a similar remark from a


### ---Economics-1986-0-05.txt---
founder of the Chicago school, Frank Knight,
who said that the extreme economic man,
maximizing every material interest, and the
extreme Christian, loving his neighbor as
himself, were alike in that neither had any
friends.'

To admit social goods, not traded in
markets, into our economic calculus does not
call for altruism. Economists are reluctant to
depend on self-denial to any degree (Kenneth
Arrow, 1975, p. 22), and moral philosophers
are not far behind. To a modern
student of ethics, James Fishkin (1982, ch.
ii), obligations to others fall into three categories:
minimal altruism, where the benefit
to the receiver is substantial and the cost
to the altruist low-the acts of a cheap
Samaritan; acts of heroic sacrifice that are
not called for; and a robust zone of indifference
where one has no cause to be concerned
over the effects of one's acts on others.
This is for positive actions. Acts that harm
others are proscribed by the Golden Rule.
Adam Smith expressed the same viewpoint
forcefully: " Every man is, no doubt, by nature
first and principally recommended to his
own care" (1759 (1808), I, p. 193), but goes
on: "Although the ruin of our neighbour
may affect us less than a very small misfortune
of our own, we must not ruin him to
prevent that small misfortune, or even to
prevent our own ruin" (ibid., p. 194). Does
this prohibit us from playing zero-sum games
or negative non-zero-sum games? In international
trade, must we refrain from levying
the optimum tariff? The optimum tariff works
to self-interest mainly in the absence of retaliation,
and if Adam Smith excludes hurting
our neighbor, he recognizes that "as every
man doth, so shall it be done to him, and
retaliation seems to be the great law of nature"
(ibid., p. 191).

Note parenthetically that today's moral
philosophers cover a wide territory either
side of Fishkin, from Peter Singer (1972) at
one extreme whose criterion of justice requires
successive acts of altruism until the
welfare of the recipient has risen to that of
the giver which has fallen, to Robert Nozick
(1974) at the other who believes that self-interest
rules out altruism almost altogether.
III

Self-interest then is legitimate over a large
zone of indifference provided that justice
is served by our not hurting others. But
the robust zone of indifference applies to
strangers, and not to those with whom we
have a special relationship, sharing collective
goods. It does not apply in the family, the
neighborhood, in clubs, in the tribe, racial or
religious group, or in the nation. There is
some uncertainty whether it applies in regions
within a country-New England, the
West, the South-or to arrangements between
countries short of the world level such
as North America or the European Common
Market. Collective goods involved here are
distributed by mechanisms different from the
market: gifts, grants, unequal exchange,
sharing through a budget according to
need, interest-free loans, inheritance, dowries,
alimony, and the like all have a place.
Membership in these groups is decided in
various ways: by birth, by choice-as in
moving into a certain neighborhood or
migrating between countries, by application
for admission and acceptance. Walzer defends
the right of countries to keep out
would-be immigrants motivated by economic
self-interest, but not those subjected to
persecution: "The primary good that we
distribute to one another is membership in
some community" (1981; 1983, ch. ii, p. 1).
He argues, however, that states lack the right
to keep members from emigrating if there is
some other community ready to take them
in. Clubs discriminate against outsiders.
Neighborhoods are more complex, being presumably
open to anyone able to afford and
find a place to live, but, in sociological reality,
often exhibiting tendencies to attract their
own kind and repel others, including harassment
or unwritten or even legal restrictions
against property ownership. The groupings
are amorphous, but they exist.
The nature of the positive bonds that link
families, neighborhoods, tribes, regions, and


### ---Economics-1986-0-06.txt---
nations is usually taken for granted and left
unexplored, but the consequences are not.
Albert Hirschman (1970), for example, makes
a distinction between voice and exit: voice
-speaking up and trying to persuade-being
the appropriate action when one disagrees
with the course followed by a group to which
one belongs; and exit-resigning or refusal
to buy the good or service-as a response to
what one dislikes in the market. Adam Smith
minimizes the difference between families and
strangers, suggesting that affection is little
more than habitual sympathy produced by
propinquity; despite the greater thickness of
blood than water, he claims that siblings
educated at distances from one another experience
a diminution of affection (1759 (1808)
II, pp. 68-70). In arguing against Walzer's
view that countries owe immigrants the right
to become citizens, Judith Lichtenberg (1981)
echoes Smith's view in saying that the crucial
difference between members and strangers
lies between those with whom one has faceto-
face contact and those with whom one
does not. An accident that kills someone in
one's town or a neighboring community is
likely to be more moving than a catastrophe
at the other end of the world in which
hundreds or thousands die. Adam Smith goes
further, comparing the loss of a little finger
with a catastrophe that swallowed up
China: ". . . if he lost his little finger he could
not sleep, but for China he can snore... provided
he has never seen them" (ibid., I, p.
317).

Some years ago in a book on the brain
drain, Harry Johnson (1968) argued in favor
of a cosmopolitan solution, encouraging
emigration, and Don Patinkin (1968) for a
national one. In discussing the Bhagwati
scheme for taxing professional emigrants
earning more abroad than at home, for the
benefit of the poor sending country-saying
this was akin to paying alimony in a divorce
case for breaking a social taboo-I suggested
(1977) that the Johnson position was equivalent
to saying that a person should go where
he or she could earn the highest return, while
Patinkin said that people should stay where
they belonged. Patinkin chided me privately
for this interpretation, and it is admittedly
oversimplified. But the difference between
the Johnson and the Patinkin positions, both
emanating from Chicago, suggests the line
between market and nonmarket areas in economics
is shadowy.

In writing about the multinational corporation,
I have from time to time suggested
that host countries resist the intrusion of
strangers because "...man in his elemental
state is a peasant with a possessive love of
his own turf; a mercantilist who favors exports
over imports; a Populist who distrusts
banks, especially foreign banks; a monopolist
who abhors competition; a xenophobe
who feels threatened by strangers
and foreigners" (1984, p. 39), usually adding
that it is the task of international economics
to extirpate these primitive instincts and to
teach cosmopolitanism. The fact that some
of these reactions remain at a late stage in
the educational process can be tested by the
device of asking students on examinations,
seriatim, a series of questions:
Do you advocate free trade, or at least is
there a strong presumption in its favor?
Do you advocate the free international
movement of portfolio capital?
... of corporate capital in foreign direct
investment?

... free migration of students and professional
labor?

... immigration of relatives of persons
permanently resident in this country?
... free migration for all?

(It is desirable to feed these questions to the
victims one at a time, without revealing the
whole list before the first answer is given,
and to take up the replies to the first questions
so that there is no chance to go back
and amend early answers.) There will be
sophisticated answers expatiating on the second,
third, and fourth-best if the marginal
conditions for a Pareto optimal solution are
not met, and I would particularly excuse a
James Meade (1955) solution that would limit
immigration from countries that have not
accomplished their Malthusian revolution, on
the ground that their emigrants will be replaced,
so that free immigration will reduce
world income per capita, if not world income
as a whole. Most economists and noneconomists
alike would agree, however, that
goods are less intrusive than money, money


### ---Economics-1986-0-07.txt---
less so than corporations with control over
our economic decisions.2 Intellectuals with
whom we identify are hardly intrusive at all.
Most of us grant that relatives must be permitted
to come together. On the other hand,
free migration of labor in general poses a
threat to the national identity. The Swiss cut
off immigration, despite the appeals of business
for more labor, when immigrants constituted
one-third of the labor force. In
Germany, separate localities felt threatened
and stopped inward migration when immigrants
reached 12 percent of the resident
population. Feelings differed, of course, depending
upon the origin of the migrants and
their appearance, language, and religion.
One early venture of international economics
into this line of investigation was
Robert Mundell's "optimum currency area"
(1961), initiating a discussion of how large
the area for a single currency should be, that
can readily be extended to economics in
general and to other social sciences. Mundell
defined an optimum currency area as one
where labor moved freely within the area,
but not between it and other areas, taking us
back to the Ricardian criterion distinguishing
domestic from foreign trade: factor mobility
within but not between countries. In
neither case is the discontinuity in mobility
explained. Perhaps something is owed to low
transport costs, but additionally, factor mobility
requires a group with such strong social
cohesion that those moving are willing to
shift, and those at the receiving end are
content to receive them.

Ronald McKinnon (1963) offered a different
criterion: an optimum currency area was
one that traded intensively at home, but only
to a limited extent abroad. This implied that
tastes within a country are homogeneous for
traded goods (as well as for public goods),
and that regionally specialized production
had grown up to serve those tastes. The
Mundell and McKinnon criteria do not necessarily
converge: on Mundell's standard,
Canada is too big to be an optimum currency
area, because of limited movement between
Quebec and the English-speaking parts
of Canada, and the comparative isolation of
the Maritimes and Vancouver. On McKinnon'
s criterion, however, it was too small
because so much of its trade is with the
United States.

If one broadens the issue from the optimum
currency area to economics more generally
and to the other social sciences,
anomalies arise from the divergence between
the optimum economic area, which on
efficiency grounds I take to be the world, and
the optimum social unit, one that gives the
individual a sense of belonging and counting
-which is much smaller. In shifting to the
optimum political unit, at least two problems
arise, one related to the nature of the ties, the
other to the ambitions of its members. To
take the second point first, for a nation bent
on glory-led by a Bismarck or a de Gaulle
-bigger is better; whereas if one is merely
trying to get along without trouble, like, say,
Denmark, small is beautiful enough.
On the first issue, political ties vary widely.
There are leagues, alliances, commonwealths,
confederations, federations, provinces, states,
principalities, kingdoms. Some lesser units
are "united" in varying degrees, as in the
United Provinces of the Netherlands, the
United States of America, the United Kingdom
of Great Britain, and Northern Ireland.
The North in the American Civil War was a
union, as the Union of Socialist Soviet Republics
asserts it is. The small amount of
literature I have explored in examining the
differences among these forms is not very
conclusive, but perhaps the main distinction
is between a single state that is centralized,
and federations that are loosely joined, with
greater powers at the local level. Designations
are not always congruent with reality:
the Federal German Republic is highly unified,
despite the efforts of the occupation
powers after World War II to spread political


### ---Economics-1986-0-08.txt---
power widely; the Federal Reserve System
was created as a loose agglomeration of
twelve regional money markets but quickly
fused into a single system in World War I.
Centralization and federalization have reflections
in demography and in finance.

City populations in unified states follow a
Pareto-skewed distribution with a single
dominant city like London, Paris, or Vienna,
and no close rival among the tail of smaller
cities and towns. In federations the distribution
of cities is log normal (Brian Berry,
1961). Parallel to the demographic division is
the financial. Paris has 91.3 percent of French
bank clearings; London 87 percent of those
for Britain. The contrast is with Canada:
Toronto, 37.3 percent; Montreal, 25.5 percent;
Vancouver, 6.5 percent. Between these
extremes lies Japan with Tokyo 51.2 percent
and Osaka 19.7 percent (Jean Labasse, 1974,
pp. 144-45).

One explanation for differences between
centralized and federal states is historical:
where larger states were formed later from
unification of lesser units, administrative and
financial functions were already being discharged
at the local level, reducing the need
for centralized services. This hypothesis faces
the difficult counterexamples of Italy and
Germany, unified out of smaller units in the
second half of the nineteenth century, that
quickly centralized administrative and financial
functions, in Rome and Milan for Italy,
and in Berlin for Germany. Another explanation
runs in terms of size, with larger
states necessarily federal because of the difficulty
of providing administration to local
units over long distances. This fits Canada,
Australia, the United States, perhaps India,
but fails to account for Switzerland, unless
size is a proxy for maintaining a dense network
of communication, and division of valleys
by high mountains produces barriers
equivalent to those of continental states. If
the mathematically minded among you need
an analogue, think of federal states as decomposable
matrices.

The difference between a single state and a
federation may be illustrated with two examples.
Some years ago, Seymour Harris (1952)
wrote a book on New England in which he
claimed that the area got a raw deal from the
rest of the country because it paid more in
taxes to the federal government than it received
in federal expenditure. This thesis implicitly
violated the distinction between a
budget and a market: in a market equal
values are exchanged. A budget, on the other
hand, is a device expressing the cohesion of a
sharing group with monies raised according
to one standard, perhaps ability to pay, and
expenditure distributed according to another,
some combination of efficiency and need.
The other example, equally shocking to an
international trade economist, was the notion
of the juste retour, or fair return, propounded
by France in connection with expenditure
for joint projects in Europe. France
insisted that all monies contributed by her be
spent in France. Tied sales are a third- or
fourth-best device to limit balance-of-payments
deficits for a given contribution to
joint efforts, or to maximize the contribution
for a given deficit. They are inefficient rather
than fair.

IV

But I want to move on to the geopolitical
unit that produces public goods. It is a cliche
that these have increased in size as costs of
transport and communication have declined.
Under the eighteenth-century Poor Law in
England, the parish resisted immigration
from neighboring parishes because of reluctance
to share with outsiders. Fernand
Braudel (1982) and Sir John Hicks (1969)
have each expatiated on the rise of the size of
the economic unit from the city-state to the
nation-state. National and international markets
for goods and money grew slowly, with
entrepot centers that intermediated between
buyers and sellers surviving in money- cheap
to move in space-and largely disappearing
for goods where costs of transport were high
and could be saved by direct selling, rather
than relaying goods through fairs in the Middle
Ages and later through cities such
as Amsterdam, Hamburg, Frankfurt, and
London. The hub-and-spoke system recently
discovered in airplane travel and still in place
for money has long been superceded in goods.
Caroline Isard and Walter Isard's (1945)
point that the most pervasive changes in the


### ---Economics-1986-0-09.txt---
economy came from innovations in transport
and communications remains valid: contemplate
the rudder (in place of the steering
oar), fore-and-aft sails; the turnpike; canal;
railroad (despite Robert Fogel, 1964); the
steamship; iron-clad ship; telegraph; telephone;
refrigerator ship; radio; airplane;
bulk carrier; jet airplane; satellite television.
The numbers of people brought into faceto-
face contact across continents and hemispheres
has increased exponentially. It is true,
to be sure, as was said about a well-known
governor and presidential candidate, that it
was impossible to dislike him until one got to
know him, and increases in mobility and
communications have been accompanied by
separatism: of the Walloons from the Flemish
in Belgium, of Scotland and Wales in the
United Kingdom (to pass over the troubled
Irish question), and of the Quebecois in
Canada.3 But it is easier than in Adam
Smith's day to imagine ourselves in the circumstances
of the Chinese, the inhabitants of
the Sahelian desert in Africa, or the
tornado-struck islands of Bangladesh as we
see them nightly on our television screens via
satellite. Do wider communication and transport
change the production and distribution
of public goods?

Conflicts between economics and political
science abound, and many arise from the
fact that goods, money, corporations, and
people are mobile, whereas the state is fixed.
The increase in mobility produced by innovations
in transport and communication
during and after World War II led some of
us to conclude that the nation-state was in
difficulty. A reaction occurred in the 1970's.
It is significant that Raymond Vernon's influential
book Sovereignty at Bay (1971),
showing the multinational corporation ascendant
over the state, was followed by his
Storm over Multinationals (1977) in which
the position is reversed. Cooper's The Economics
of Interdependence (1968) was followed
by an upsurge of interest in national
autonomy, decoupling, and pluralism among
political scientists, most of whom approve
the nation-state and have as heroes, if they
will forgive me, not Adam Smith and
Woodrow Wilson, but Otto von Bismarck
and perhaps even Charles de Gaulle. The
tension remains, however. Mobility limits the
state's capacity to enforce its writ in taxation,
in foreign policy, in standards on such
matters as antitrust, pure food and drugs,
insider trading in securities, and the like.
Mobility undermines social cohesion through
the easy intrusion of different nationalities,
races, religions, and traditions into the body
politic.

V

I come at long last to international public
goods. The primary one is peace. Economists
are poorly qualified to discuss how, after
war, peace is restored and maintained. Most
of us reject the Marxian view that war grows
directly out of capitalism, and as ordinary
citizens and amateur students of history are
prepared to agree that peace may be provided
by a dominant world power- Pax
Romana or Pax Britannica-or by balanceof-
power maneuvering, although that seems
accident prone. Among the more audacious
economists producing an economic theory or
set of theories on war is Walt Rostow (1960,
pp. 108 ff.). There are views that ascribe war
to population pressure, to ambitious rulers
aggressively seeking power, and to complex
miscalculation. How these are to be avoided
or contained is a question primarily for political
science.

In the economic sphere, various international
public goods have been identified: an
open trading system, including freedom of
the seas, well-defined property rights, standards
of weights and measures that may
include international money, or fixed exchange


### ---Economics-1986-0-10.txt---
rates, and the like. Those that have
interested me especially in a study of the
1929 depression and other financial and economic
crises have been trading systems, international
money, capital flows, consistent
macroeconomic policies in periods of tranquility,
and a source of crisis management
when needed. By the last I mean the maintenance
of open markets in glut and a source
of supplies in acute shortage, plus a lender of
last resort in acute financial crisis (see my
1973 book, revised 1986, forthcoming).
Public goods are produced domestically by
government, unless the governmental agenda
is blocked in stalemate among competing
distributional coalitions as described by
Mancur Olson (1982). Voluntary provision
of public goods is plagued by the free rider.
In the international sphere where there is no
world government, the question remains how
public goods are produced. Ralph Bryant is
one of the few economists who has discussed
the public good element in international cooperation.
His vocabulary is different from
that of the political scientists: their "regimes"
are his "supranational traffic regulations"
(1980, p. 470), and he expects leadership in
cooperation in monetary and fiscal policy
from supranational institutions such as the
International Monetary Fund (p. 481). I find
this doubtful on the basis of the interwar
record of such institutions as the League of
Nations.

Political science in this field has produced
two schools: the realists who hold to a national-
interest theory of international politics,
and the moralists, whom Robert Keohane
prefers to call "institutionalists" (1984, p. 7).
Realists maintain that international public
goods are produced, if at all, by the leading
power, a so-called " hegemon," that is willing
to bear an undue part of the short-run costs
of these goods, either because it regards itself
as gaining in the long run, because it is paid
in a different coin such as prestige, glory,
immortality, or some combination of the two.
Institutionalists recognize that hegemonic
leaders emerge from time to time in the
world economy and typically set in motion
habits of international cooperation, called
"regimes," which consist of "principles,
norms, rules and decision-making procedures
around which the expectations of international
actors converge in given issue areas"
(Stephen Krasner, 1983, p. 1). Under British
hegemony, the regimes of free trade and the
gold standard developed more or less unconsciously.
With subsequent American

hegemony, a more purposeful process of institution
making was undertaken, with agreements
at Bretton Woods, on tariffs and trade,
the Organization for Economic Cooperation
and Development, and the like. Political scientists
recognize that regimes are more readily
maintained than established since marginal
costs are below average costs; as hegemonic
periods come to an end with the waning of
the leading country's economic vitality, new
regimes needed to meet new problems are
difficult to create. Cooper (1985) has written
of the eighty years it took to create and get
functioning the World Health Organization
despite the clear benefits to all countries
from controlling the spread of disease. And
it takes work to maintain regimes; in the
absence of infusions of attention and money,
they tend in the long run to decay.
I originally suggested that the 1929 depression
was allowed to run unchecked because
there was no leading country able and willing
to take responsibility for crisis management,
halting beggar-thy-neighbor policies
from 1930, and especially acting as a lender
of last resort to prevent the serious run on
the Creditanstalt in May 1931 spreading, as
it did, to Germany, Britain, Japan, the United
States, and ultimately to the gold bloc. Britain,
the leading economic power of the nineteenth
century, was unable to halt the run;
the United States, which might have had the
ability, possibly assisted by France, was unwilling.
This view has been rejected by one
economic historian who holds that the troubles
of the interwar period were more
deep-seated, and that what was needed was
more fundamental therapy than maintaining
open markets and providing a lender of last
resort, something, that is, akin to the heroic
public good after World War II, the Marshall
Plan (D. E. Moggridge, 1982). That may
have been true, though there is no way I see
that the issue can be settled. Leadership at
an earlier stage in the 1920's, presumably
furnished by the United States with some


### ---Economics-1986-0-11.txt---
cost in foregone receipts on war-debt account,
might have resolved the war-debt-reparations-
commercial-debt tangle that proved so
destabilizing after the 1929 stock market
crash. I conclude that the existence of an
international lender of last resort made the
financial crises of 1825, 1836, 1847, 1866,
and 1907 more or less ephemeral, like summer
storms, whereas its absence in 1873, 1890,
and 1929 produced deep depressions-shortened
in the 1890 case by the deus ex machina
of gold production from the Rand. Again
there is room for disagreement.
The point of all this is that after about
1971, the United States, like Britain from
about 1890, has shrunk in economic might
relative to the world as a whole, and more
importantly, has lost the appetite for providing
international economic public goodsopen
markets in times of glut, supplies in
times of acute shortage, steady flows of
capital to developing countries, international
money, coordination of macroeconomic policy
and last-resort lending. The contraction
of concern from the world to the nation is
general, and applies to economists as well
as to politicians and the public. In reading
recent books on macroeconomic policy by
leading governmental economists under both
Democratic and Republican administrations,
the late Arthur Okun (1981) and Herbert
Stein (1984), I have been struck by how little
attention the authors paid to international
repercussions. The same observation has been
made by Ralph Bryant (1980, p. xviii) and
by the British economist R. C. 0. Matthews,
reviewing Arjo Klamer's Conversations with
Economists... (1985, p. 621). There has been
a recent upsurge of interest in the international
dimension because of the connections
among the federal deficit, the exchange rate
for the dollar, and the balance-of-payments
deficit, but the focus of this interest is almost
exclusively on what the connections mean
for U.S. interest rates, industrial policy,
growth, and wealth. The international impact
is largely ignored, bearing out the truth
in former German Chancellor Helmut
Schmidt's statement that "the United States
seems completely unconscious of the economic
efforts of its policies on the Alliance"
(1984, p. 27).

Some of the discussion of international
regimes by political scientists verges on what
my teacher, Wesley Clair Mitchell, used to
call "implicit theorizing," that is, convenient
ad hoc theoretical explanations to fit given
facts that lack generality. Charles Lipson
(1985), for example, suggested that the slippage
in U.S. hegemony in the 1970's resulted
in a loss of the international public good of
secure property rights and therefore in the
widespread nationalization of foreign direct
investment. He went on to say that the reason
less developed countries (LDCs) did not
default on their debts to bank syndicates was
that bank lending was "better institutionalized,
" "a smaller group," "better protected
by legal remedies" (pp. 136, 158, 170). He
was surprised that the decline of British
hegemony in the interwar period did not
result in more LDC aggression against foreign
property (p. 191), but failed to observe
the widespread default on foreign bonds in
the 1930's, despite the organization of international
finance. In my judgement Keohane
exaggerates the efficacy and importance of
the international regime in oil that was
formed after the first OPEC oil shock of
1973 (see his ch. 10). The crisis caused by the
Yom Kippur embargo of the Netherlands
was to my mind shockingly mishandled by
governments, and the public good of crisis
management was left to the private multinational
oil companies. The formation of the
International Energy Agency was a classic
operation in locking the barn door after the
horse had been stolen.

Between national self-interest and the provision
of international public goods, there is
an intermediate position: indifference to
both. An interesting contrast has been observed
in the 1930's between Britain which
forced Argentina into a bilateral payments
agreement (the Roca-Runciman Agreement
of 1933) in order to take advantage of its
monopsony position, and the United States
that had a similar opportunity vis-a-vis Brazil
but ignored it (Marcelo de Paiva Abreu,
1984).

It is fairly clear from the historical record
that economic hegemony runs down in decay
-in the British case after 1913 and the
United States about 1971-leading Felix


### ---Economics-1986-0-12.txt---
Rohatyn (1984) to say that the American
century lasted only twenty years. The Nixon
shock of 1973 in cutting off soya bean exports
to Japan-a significant harm to an ally
for a small gain to this country-was the act
of a bad Samaritan. The import surcharge of
the same year may have been required to
move the dollar out from the position of the
nth currency when only n -1 countries are
free to fix their exchange rates, but it would
have been possible to start with the later
attempt at cooperation that resulted in the
Smithsonian agreement. This is especially
true when so much of the case against the
1971 exchange rate was the result of the
easy-money policy of the Federal Reserve
System under Chairman Arthur Burns, at a
time when the Bundesbank was tightening its
money market/go-it-alone policies of both
banks that flooded the world with dollars.
The present U.S. administration claims to
be working for open trade and does fairly
well in resisting appeals for protection. The
positive push for a Reagan round of trade
liberalization in services and agriculture,
however, is in pursuit of a national and not
an international public good. The regime in
capital movements-the World Bank, the regional
development banks and that in-lastresort
lending orchestrated by the IMFseems
to be working, with bridging loans and
an ad hoc purchase of oil from Mexico for
the U.S. stockpile in 1982 when the IMF
finds itself unable to move fast enough. But
there are signs of dissension that may spell
trouble. The June 1985 bridging loan for
Argentina was declined by Germany and
Switzerland on the grounds that Argentina
had not been sufficiently austere and that its
problems were not a threat to the world
financial system (New York Times, June 15,
1985, p. 1). The Japanese contribution,
moreover, was said to have been small, although
no figures were given.

What I worry about mostly is exchange
policy and macroeconomic coordination. The
U.S. Treasury under Donald Regan was
committed to the policy of neglect, presumably
benign, but in any event ideological.
And the commitment to consultative macroeconomic
policies in annual summit meetings
of seven heads of state has become a shadow
play, a dog-and-pony show, a series of photo
opportunities-whatever you choose to call
them-with ceremony substituted for substance.
The 1950's and 1960's, when serious
discussions were held at the lowly level of
Working Party No. 3 of the O.E.C.D., were
superior because the United States and other
countries took them seriously.
I am a realist when it comes to regimes. It
seems to me that the momentum set in motion
by a hegemonic power-if we must use
that expression, I prefer to think of leadership
or responsibility-runs down pretty
quickly unless it is sustained by powerful
commitment. The IMF and World Bank were
agreed at Bretton Woods largely as a result
of the U.S. Treasury: the forms were international,
the substance was dictated by a single
country (Armand van Dormel, 1978). In the
early days of the IMF, Frank Southard told
me, if the United States made no proposal,
nothing happened. Today the same is true of
the European Economic Community: unless
Germany and France see eye to eye, which is
infrequent, nothing happens. Proposals of
great technical appeal from individuals or
small countries are not welcomed as the preparatory
phases of the World Economic

Conference of 1933 demonstrated (see my
1973 book, pp. 210-14). There needs to be
positive leadership, backed by resources and
a readiness to make some sacrifice in the
international interest.

The leadership role is not applauded. When
the United States accused the rest of the
world of being free riders, Andrew Shonfield
countercharged the United States of being a
" hard rider," " hustling and bullying the
Europeans," "kicking over chairs when it did
not get its way" (1976, pp. 86, 88, 102).
Furnishing the dollar to the world as international
money has brought the United States
an accusation of extracting seignorage, although
the facts that the dollar is not a
monopoly currency and that foreign holdings
earn market rates of interest deflect that
criticism in sophisticated quarters.
Neglect can verge on sabotage. When the
European central banks collaborated to hold
the dollar down at the end of February 1985,
the conspicuous failure of the United States
to participate on a significant scale encouraged
speculators not to cover long positions.
A former trader for the Federal Reserve


### ---Economics-1986-0-13.txt---
Bank of New York has expressed concern
that the habits of central bank cooperation
and U.S. official intimacy with the workings
of the foreign-exchange market that have
been built up over thirty years are being
squandered for ideological reasons (Scott
Pardee, 1964, p. 2).

Regimes are clearly more attractive in
political terms than hegemony, or even than
leadership with its overtones of the German
Fuhrerprinzip or of Italy's II Duce, if not
necessarily more so than responsibility. Polycentralism,
pluralism, cooperation, equality,
partnership, decoupling, self-reliance, and
autonomy all have resonance. But it is hard
to accept the view, so appealing to the political
right, that the path to achieve cooperation
is a tit-for-tat strategy, applied in a
repetitive game, that teaches the other player
or players to cooperate (Robert Axelrod,
1984). As Tibor Scitovsky demonstrated years
ago (1937), this path can readily end by
wiping out trade altogether. Hierarchical
arrangements are being examined by economic
theorists studying the organization of
firms, but for less cosmic purposes than
would be served by political and economic
organization of the production of international
public goods (Raj Sah and Joseph
Stiglitz, 1985).

Minding one's own business-operating in
the robust zone of indifference-is a sound
rule on trend when macroeconomic variables
are more or less stable. To the economist it
means reliance on the market to the extent
that the conditions for a Pareto optimum
solution are broadly met. But the fallacy of
composition remains a threat, and one cannot
count on the Categorical Imperative.
Markets work most of the time, as a positivesum
game in which the gain for one does not
imply a loss for another. Experience teaches,
however, that crises may arise. When they
do, the rule changes from government and
public indifference to the production of public
goods by leadership or by a standby
regime.

Leadership or responsibility limited to
crises encounters another problem: how to
keep the machinery for handling crises from
obsolescence. In crisis one needs forceful and
intelligent people, capable of making decisions
with speed under pressure. It is
sometimes said that the Japanese practice of
decision by consensus with ideas coming up
from below, makes it hard for that country
to discharge in timely fashion the responsibilities
of world leadership. In Marcus
Goodrich's Delilah (1941), the amiable practice
of fraternization between a watch officer
and enlisted men on the bridge of the destroyer
proved dangerous in a typhoon since
the men had fallen into the habit of discussing
the officer's orders. The paradox is that
the attributes needed in crisis tend to atrophy
in quiet times; for example in the control
room of a Three Mile Island nuclear power
plant.

Let me conclude by emphasizing once
again my concern that politicians, economists,
and political scientists may come to
believe that the system should be run at
all times by rules, including regimes, not people.
Rules are desirable on trend. In crisis
the need is for decision. I quote once more
the letter of Sir Robert Peel of June 1844
a propos of the Bank Charter Act of that
year:

My Confidence is unshaken that we
have taken all the Precautions which
Legislation can prudently take against
the Recurrence of a pecuniary Crisis. It
may occur in spite of our Precautions;
and if it be necessary to assume a grave
Responsibility, I dare say Men will be
found willing to assume such a Responsibility.


[Parliamentary Papers,

1857, 1969, p. xxix]
 ## Economics-1987-0


### ---Economics-1987-0-01.txt---
I want to use this once-in-a-lifetime opportunity
for pontificating to the profession,
to explore ways of improving the interaction
between what economists do and the political
process. Tension and conflict are, of
course, inherent in political decisions, especially
on economic policy. Nothing can make
such decisions easy. Nevertheless, it is my
contention that economic policymaking in
Washington in the last decade has been more
frustrating, muddled, and confusing than
necessary. Some of the fault lies with
economists and economics; some with politicians
and the political process; some in the
interactions. I want to offer some suggestions
for modest improvements.

Most economists probably share my premise
that economics ultimately ought to be
more than just challenging intellectual gymnastics.
It ought to help us understand how
the economy works and provide a basis for
intelligent political choices among economic
policies. Even those who devote their energies
to resolving purely theoretical issues
imagine that somehow in the end their efforts
will prove socially useful.

The dedicated, idealistic young economist
who aspires to advise a government may well
envision herself someday as the wise and
impartial adviser to the philosopher queen.
In this daydream, the adviser presents the
best forecasts that can be made of the future
course of the economy. She explains the
macroeconomic policy options and what is
likely to happen if each is undertaken. She
elucidates why market solutions are efficient,
when markets are likely to fail, and what can
be done when this occurs. She identifies risks
and uncertainties, which fortunately are not
overwhelming. She represents the best professional
judgment of her fellow economists,
indicating the major respects in which most
economists agree and scrupulously pointing
out that in minor respects the views of some
of her professional colleagues might differ
from her own. She remains above the political
fray, identifying any values or distributional
biases that may creep into her
judgments and eschewing identification with
interest groups or ideological causes.
The queen for her part listens carefully
and intelligently, asks thoughtful questions,
and weighs the options. She may consult
other experts on noneconomic aspects of the
decisions, but these can be assumed not to
be very important. She then makes final
decisions-even very hard ones-and sticks
to them. The decisions are carried out, the
economy prospers, and a grateful nation applauds
the wisdom of the monarch and her
economist and the usefulness of economics.
But in the real world, both economics and
politics are frustratingly unlike this picture.
Both are pluralistic in the extreme and appear
to be getting more so. Economists and
political leaders not only miscQmmunicate,
but each accuses the other of incompetence,
obfuscation, self-serving motives, and antisocial
behavior.

Economists, of course, do not wait for
others to attack them; they do it themselves.
Walter Heller said in his presidential address
that the "chorus of self criticism has risen to
a new crescendo" (1975, p. 1), and the selfdeprecation
has not abated in the intervening
decade. If a golden age of economists'
self-confidence ever occurred, it is long past.
Events of recent years have kept reminding
us that our national economy is diverse and
complex, battered by unpredictable shocks,
and increasingly interconnected with the even


### ---Economics-1987-0-02.txt---
more diverse and complex world outside our
borders. Knowledge of how the domestic
economy works and interacts with the rest of
the world is imperfect. Economists keep
coming up with ingeneous theories, but they
have a hard time testing them. Data are
inadequate and controlled experimentation
nearly impossible. Modeling has greatly
enhanced our understanding of the past,
but shows few visible signs of improving
the reliability of macroeconomic prediction.
Forecasting even for short periods remains
an uncertain art in which neither economists
nor politicians can have much confidence.
Many of the most sophisticated and realistic
members of the profession, conscious of
all these difficulties, have abandoned the attempt
to advise governments on policies
in favor of the more manageable tasks of
adding to the knowledge base. This may be
understandable, but it deprives the economic
policy debate of the input of some very good
minds and runs the risk of leaving the job of
interacting with the political arena disproportionately
to those with strong ideological
views.

I. Fragmentation of the Economic Policy Process
The pluralism of economics pales beside
the pluralism of the political system that
policy-minded economists aspire to assist.
Even if one leaves aside the complexities of
federalism, the process by which national
economic policy evolves in Washington is so
fragmented and complicated that it is almost
impossible to explain to the uninitiated how
it is supposed to work, let along how it does
work.

A well-founded distrust of despots led our
forefathers not only to opt for representative
democracy, but to divide power among
the executive and legislative and judicial
branches, and between the House and the
Senate. On matters of taxing and spending,
they were especially protective of the power
of the people's representatives, making it
clear that while the president could propose
taxing and spending, the ultimate authority
lies with the Congress, subject only to presidential
veto. This divided power creates a
built-in hurdle to making and carrying out
fiscal policy. The hurdle is low when the
president is articulating a policy that has
broad support in the country and in the
Congress. It can lead to erratic shifts of
policy when the president is indecisive, and
to deadlock when the president is leading
in a direction in which the public and its
elected representatives do not wish to go.
Deadlocks are rare, but can be serious. The
failure to reduce the huge structural budget
deficit of the mid-1980's largely reflects the
fact that the president's solution-drastic
reduction of the federal role in the domestic
economy-does not command broad popular
support.

The separation of powers between the
Congress and the president is basic to our
system of government and probably worth
the price of occasional deadlock. The difficulties
of making economic policy, however,
are strongly compounded by the propensity
of our pluralistic society to diffuse
power and decision-making authority both
within the executive branch and within Congress.
With respect to taxing and spending
policy, for example, the simple notion that
the president proposes and the Congress disposes
is greatly complicated by the fragmentation
of power within each branch. Moreover,
periodic efforts to make the policy
process more coherent within each branch,
while often temporarily successful, have
added new power centers without consolidating
the old ones.

In the executive branch, the trend since
early in the century has been to centralize
power in the White House in order to make
it easier for the president to formulate and
articulate taxing and spending policy, and to
utilize the growing skills of the economics
profession to that end. But this worthy goal
has been accomplished in stages, with a new
institution added at each stage. The creation
of what is now called the Office of Management
of Budget (OMB) in the 1920's made it
possible for the president to review and
evaluate spending requests and impose a set
of priorities on his budget proposal to Congress
reflecting his administration's view of
the appropriate size and role of government.
The creation of the Council of Economic
Advisers (CEA) in the 1940's provided a


### ---Economics-1987-0-03.txt---
focal point for bringing the advice of the
economics profession into the service of
presidential decision making and a locus
for creating an official forecast of economic
activity.

The creation of OMB and CEA improved
the president's ability to formulate and
articulate macroeconomic policy. It also left
the president, in addition to his other impossible
duties, with the job of resolving a builtin
tension over responsibility for economic
policy among the CEA, OMB, and the
Treasury, not to mention the White House
staff and the agencies with line responsibility
for implementing various aspects of economic
poclcy.

Presidents have tried various coordination
mechanisms including "troika" arrangements
and an almost infinite variety of
broader councils and committees with varying
membership, responsibilities, and leadership.
The system works tolerably well or
exceedingly creakily, depending on the president'
s personal style and the personalities
involved. But it encourages battling over turf
as well as substance, and is hardly designed
to minimize the amount of presidential energy
needed to evolve a coherent, explainable
policy on taxing and spending. One
might wonder whether it is not time to do
what so many other countries do and give
our president the equivalent of a responsible
finance minister charged with the functions
now diffused to our budget director, Council
of Economic Advisers, and Treasury Secretary.


The fragmentation of power and responsibility
is, of course, even more extreme in the
Congress. The legislative branch also has a
long history of attempts to make taxing and
spending policy in a more coherent fashion
by adding new coordinating institutionsappropriations
committees, a joint economic

committee, budget committees, a congressional
budget office-without eliminating or
consolidating any of the old ones.
The most recent attempt to improve congressional
economic decision making-one

in which I was an active participant-followed
the Budget Reform Act of 1974 which
created the budget committees and the Congressional
Budget Office. These budget reforms
succeeded in their main objective of
focusing the attention of the Congress on
overall budget policy, not just individual taxing
and spending fragments. They have
forced the Congress to fit the pieces together,
to debate and vote on an overall taxing
and spending plan-a budget resolution-to
which specific taxing and spending matters
must conform. No one can say that the
Congress in the last few years has ignored
fiscal policy! The creation of the Congressional
Budget Office, moreover, has given
Congress independent access to forecasts,
projections, and analysis of economic options.


The downside of the budget reforms, however,
was that the budget process was superimposed
on the already complex responsibilities
of authorizing, appropriating, and tax
committees. It has added to the layers and
stages of congressional policymaking without
removing any of them, has made the process
of budget decision making nearly impossible
even for members of Congress to understand,
and increased the workload so much that
decisions are routinely made late and in an
atmosphere of crisis. Moreover, Congress
now frequently has to deal with two sets of
estimates, those of the OMB and those of
the Congressional Budget Office, which may
differ because they are based on different
forecasts of economic activity, or for even
less obvious technical reasons.
Meanwhile, back in the separate world of
the Federal Reserve, monetary policy is being
decided and carried out. It is a curious
paradox that a nation, which feels it needs
many more hands on the tiller of fiscal policy
than most countries regard as workable, is
content to leave monetary policy to a central
bank with fewer visible ties to the rest of the
government than the central banks of most
countries.

There is plenty of informal communication,
of course, especially between the
Federal Reserve and the hydraheaded economic
establishment of the executive branch.
More formal cooperation between the monetary
and fiscal authorities, as in the United
Kingdom, might contribute only marginally
to making monetary and fiscal policy decisions
part of a more coherent strategy for


### ---Economics-1987-0-04.txt---
the economy-and at the cost of depriving
the executive branch of the luxury of blaming
the Federal Reserve when things go
wrong. The love-hate relation between the
Congress and Federal Reserve, however,
warrants more attention. Despite occasional
outbursts of anxiety over escalating interest
rates, Congress has shown little inclination
to control monetary policy, or even to inquire
into the consistency of monetary and
fiscal objectives. The Fed is required to report
monetary growth targets to the banking
committees, as though monetary policy were
a matter of banking system regulation, but
has little genuine interaction with the budget
committees whose job is to debate and propose
fiscal policy.

II. The Process under Stress

This whole complicated economic policy
system has been subjected to enormous strain
in recent years. Political economists like to
harken back to the golden years of the 1950's
and 1960's when economists got respect and
the economic policy machinery functioned
smoothly. The nostalgia is only partly a result
of faulty memories. It's not hard to
be satisfied with economists and policy
processes when the economy is growing, productivity
marches steadily upward, and even
the national debt is obligingly declining in
relative importance. It's much harder when
productivity growth plummets for reasons
that no one honestly purports fully to understand,
expectations of public and private
consumers have to be cut back to fit with
slower income growth, and inflation and interest
rates are bouncing around at unfamiliar
levels.

Adjusting to the energy shocks and slower
growth that began in the 1970's strained the
economic policy processes of all industrial
countries and made the participants feel
frustrated and inadequate. It's not obvious,
even with hindsight, that the fundamental
difficulties facing the industrial world in the
1970's can credibly be blamed on economists
or any particular structure of government
or economic policy responses, but all came
in for their share of the understandable
hostility.

The difficulties of the U.S. economy in the
1980's, by contrast, revolve heavily around
an economic policy mistake: the creation of
a large structural deficit in the federal budget.
I do not believe that the structure of our
economic decision process was the cause of
the mistake. Blaming the deficit on inherent
flaws in the policy process requires an explanation
of why the process did not cause
similar mistakes in the past. But the events
of 1981 which produced the deficit illustrate
several of the difficulties of economic policymaking
which make mistakes harder to

avoid:

the uncertainty of macroeconomic forecasting;


the isolation of monetary and fiscal
policy;

the contentiousness of economists and
their tendency to let their ideological positions
cloud their judgments about the
likely effects of particular policies.
That a tax cut unmatched by comparable
spending cuts would produce a deficit should
have surprised no economist. That the deficit
was so large reflected both economic and
political miscalculations. The Reagan Administration
has been faulted for masking

the deficit with a "rosy scenario," but the
fact is that most of the forecasting community,
including the Congressional Budget
Office, expected positive real growth in the
economy. The administration's official forecast
differed from the rest only in its degree
of optimism. Forecasters in and out of
government were oversanguine about growth
largely because they failed to realize how
serious the Federal Reserve was about reining
in the money supply to control inflation.
The Fed was not defying the administration,
which was touting the efficacy of monetary
stringency for controlling inflation, but hardly
anyone seemed to remember that the way
tight money controls inflation is by slowing
economic activity. Moreover, as our Association'
s President-elect, Robert Eisner, has
pointed out (1986, p. 146), the economics
community, unfamiliar with a world of high
inflation rates, overestimated the stimulative
effect of the existing deficit. Added to this
was the enthusiasm of the ideological proponents
of smaller government, some of whom


### ---Economics-1987-0-05.txt---
exaggerated the possible effects of lower tax
rates on supply and some of whom simply
hoped that deficits would pressure Congress
to cut back domestic spending. The size of
the deficits was also masked by the assumption
of unspecified future spending cuts, an
assumption reflecting the view that the U.S.
government was operating a lot of wasteful
programs with little public support which
Congress could soon be persuaded to reduce
or eliminate.

Both in the administration and in Congress,
decisions were made at a breakneck
pace, in a highly charged political atmosphere,
amid conflicting claims and competing
forecasts, with little attention to the consistency
of monetary and fiscal policy and
mostly by people with little experience in
evaluating the reasonableness of any set of
economic estimates. (See David Stockman,
1986, ch. 3.) When the dust settled, we found
ourselves with a serious recession that
nobody expected, and an escalating structural
budget deficit that nobody wanted. It
was hardly economic policy's finest hour.
The agonizing-and so far only partially
successful- struggle to correct the mistakes
of 1981 have kept the economic policy process
under stress and have continued to
dramatize some of its weakest aspects. The
struggle between the president and the Congress
over deficit solutions illustrates the
price we pay for the separation of powers.
The fact that fiscal policy has become an
exercise in damage control, while the Federal
Reserve makes all the important decisions
about the economy, underlines the separation
of monetary and fiscal policy. The
sensitivity of deficits to the pace of the economy
advertises the unreliability of macroeconomic
forecasts. The fact that all the actions
that could be taken to correct the deficit are
unpleasant ones drags out the annual agony
of budget setting interminably and dramatizes
how layered and cumbersome it has
become.

Small wonder that the strains of the last
few years, with a little help from the press,
have reinforced the negative stereotypes that
economists and political decision makers
have of each other. Political decision makers
see economists as quarrelsome folks who
cannot forecast, cannot agree, cannot express
themselves clearly, and have strong
ideological biases. Economists return the
favor by regarding politicians as shortsighted,
interested only in what is popular
with the electorate, and unwilling to face
hard decisions. All of the stereotypes are
partly right.

Politicians embody their stereotype in
economist jokes. Economists have retaliated
more massively by applying the tools of their
trade to the political system itself. Public
choice theory essentially asks the question:
what would economic policy be like if our
stereotype of politicians were entirely true?
The answer provides considerable insight
into observed political behavior and certainly
helps explain why the idealistic economist
so often fails to find the system simulating
the public interest motivation of the
philosopher queen.

III. Some Drastic Nonsolutions
Widespread concern that the economic
policy process is not working well has
spawned proposals for drastic change that
move in two quite different directions: one
toward circumscribing the discretion of
elected officials by putting economic policy
on automatic pilot and the other toward
making elected officials more directly responsible
to the voters for their policies.
The automatic pilot approach 'flows from
the perspective of public choice theory that
the decisions of democratically elected officials
interested in staying in office cannot
be counted on to produce economic policy
in the social interest, but are likely to be
biased toward excessive government spending,
growing deficits, special interest tax and
spending programs, and easier money. A
way to overcome these biases is to agree in
advance on strict rules of economic policy,
such as a fixed monetary growth path or
constitutionally required balance in the
federal budget.

Even if one accepts the premises, however,
firm rules are hard to define in a rapidly
changing world -no one seems to know what
"money" is anymore-and can easily lead
to perverse results. Recent experience with


### ---Economics-1987-0-06.txt---
trying to reduce the federal deficit along
the fixed path specified by the Gramm-Rudman-
Hollings amendment, for example, has
given us a taste of some of the possible
disadvantages of a balanced budget rule.
There is danger that specific dollar targets
for the deficit will require procyclical fiscal
policy, perhaps precipitating a recession that
would then make budget balance even less
attainable. Moreover, the effort to reach the
targets can induce cosmetic or self-defeating
measures, such as moving spending from one
fiscal year to another for no valid reason,
selling assets to reduce a current deficit while
exacerbating future ones, and accomplishing
desired purposes by regulatory or other nonbudgetary
means.

The Gramm-Rudman-Hollings experience,
however, has suggested the usefulness of a
different approach to deficit reduction than a
balanced budget rule; namely, a deficit neutral
amendment rule. If legislators advocating
a tax preference are required to propose
a rate increase to pay for it, special interest
tax legislation may falter. Similarly, the requirement
that a proposal for additional
spending be accompanied by a simultaneous
proposal to raise taxes or reduce another
spending program may be an effective brake
on deficits.

The other direction of reform reflects the
contrasting view that the separation of
powers and the diffusion of responsibility in
our government make it too difficult for the
electorate to enforce its will by holding officials
responsible for their policies. The
potential for deadlock would be reduced if
the United States moved toward a parliamentary
system, or found a way to hold
political parties more strictly accountable for
proposing or carrying out identifiable policies.


Casual examination of parliamentary democracies,
such as the Untied Kingdom and
Sweden, does not provide striking evidence
of the superiority of parliamentary systems
for making economic choices, even if one did
not have two hundred years of tradition to
contend with in changing our system. The
more modest notion that our system would
work more smoothly if political parties had
better defined positions and disciplined their
elected members more strictly may well be
right, but seems to fly in the face of current
history. Voters are showing less strong party
affiliation and more inclination to choose for
themselves among candidates, while members
of Congress tend increasingly to be
pragmatists willing to work out nonideological
compromises across party lines. These
trends seem likely to be the irreversible consequences
of greater education, sophistication,
and exposure to public issues among
voters and elected officials alike and to make
a resurgence of party discipline and loyalty
unrealistic.

IV. Making the Economic Policy System
Work Better

My own proposals involve less drastic
changes in the structure of our government.
They reflect a strong faith in the ability of
informed citizens and their elected representatives
to make policy decisions for the
common good, even to make substantial
sacrifices and take political risks to further
what they perceive as the long-run national
interest-once they understand what the
choices are. I also believe that the separation
of powers between the executive and legislative
branches works pretty well most of the
time. It provides needed protection against
overzealousness in either branch, albeit at
some risk of occasional stalemate.
The main problem, it seems to me, is that
our economic policy system has gradually
become so complex, diffused, and fragmented
that it impedes rather than fosters
informed choices on major issues. The fragmentation
imposes two kinds of costs. First,
it makes the decision process itself exceedingly
inefficient. Decisions are made too
often, in too great detail, and reviewed by
too many layers of decision makers in the
executive branch and in Congress. Too much
time is absorbed in procedure and in wrangling
over details, not enough on major decisions.
It's time to simplify the process, to
weed out some of the institutions, and to tip
the balance between substance and process
back toward substance.

Second, decisions are made separately that
ought to be made together, or at least with


### ---Economics-1987-0-07.txt---
attention to their impact on each other. The
separation of monetary and fiscal policy is
one example; the separation of tax and
spending decisions is another. Congress has
made a good deal of progress in recent years
in putting spending decisions together with
their revenue or deficit consequences, but
more could be done. I have seven steps to
suggest that might make the economic policy
process work more effectively.
First, seek out decisions that should be
made less frequently and arrange to do so.
This would economize decision-making time
and enhance the chances of thoughtful,
well-informed decisions. It would free up
time and energy for managing the government
enterprise more effectively, with a
longer planning horizon. It would also reduce
the inefficiency and sense of unfairness
that goes with frequent changes of the rules.
Making the federal budget every other year
would be a major advance. Major revisions
of the tax code should occur even less frequently.
Big ticket acquisitions, such as major
weapons systems, should be reviewed thoroughly
at infrequent intervals and then put
on a steady efficient track, not constantly
revisited.

With a two-year budget, there would occasionally
be major events, such as a sudden
escalation of international tension or a sharp
unexpected shift in the economic outlook,
that would justify reopening the budget in
midstream, but the temptation to tinker frequently
should be strongly resisted. The
argument that economists cannot forecast
accurately two years in advance, while quite
true, does not undermine the case for a
multiyear budget. It simply reinforces the
point that discretionary fiscal policy is
hazardous and ought to be viewed with great
skepticism whether the budget is annual or
biennial.

Second, seek out decisions that need not
be made at all and stop making them. Some
spending programs could be consolidated
into block grants or devolved to the states,
not necessarily in the interest of smaller
government, but in the interest of greater
responsiveness to local needs and a less
cluttered federal decision schedule. In other
cases, the responsibility is clearly federal-as
in defense-but Congress would be doing its
job more effectively if it concentrated on
major policy issues rather than on details of
program management.

Third, in the executive branch, consolidate
authority for tax, budget, andfiscalpolicy
in a single cabinet department. The department
could retain the name Treasury, but
might better be called the Department of
Economic Affairs. The Secretary of Economic
Affairs should have a high level chief
economist or economic council with a strong
professional staff. The chief economist should
work closely with the budget director who
also should report to the Secretary. The purpose
would be to bring together economic
decisions now made in OMB, CEA, and
Treasury under one high-level responsible
person, to relieve the president of the duty
of adjudicating among so many potentially
warring power centers, and to increase the
chances of building a highly professional
permanent economic staff one step removed
from the short-run political concerns of the
White House.

Fourth, streamline the congressional
committee structure to reduce the number of
steps in the budget process. The authorizing
and appropriating functions should be combined
in a single set of "program committees,
" one for each major area of public
spending. This would imply a single defense
committee, for example, and a social insurance
committee. The tax committees

should handle the revenue side-not additional
spending programs as at present.
The budget committees would be charged
with considering fiscal policy and putting the
spending and revenue sides together into a
budget to be passed by the whole congress.
The Joint Economic Committee should celebrate
the important contributions it made
to economic understanding in the days before
the budget process and then close up
shop.

Fifth, bring monetary and fiscal policy
into the same conversation. This end could be
furthered by closer formal links between the
central bank and the Department of Economic
Affairs to dramatize the need for consultation
and interaction. The Federal Reserve
chairman should make a report to the


### ---Economics-1987-0-08.txt---
budget committees of Congress laying out
recommended short- and longer-run economic
goals for the nation and discussing
combinations of monetary and fiscal strategies
to achieve them. The Fed's report should
be an important input to congressional deliberations
on fiscal policy.

Sixth, strive for a government-wide official
economic forecast to be updated on a
regular schedule. The main purpose of the
common forecast would be to reduce the
confusion generated by conflicting estimates,
but the increased interaction between the
Department of Economic Affairs, the Congressional
Budget Office, and the Federal
Reserve necessary to create such a forecast
would increase mutual understanding of what
is happening to the economy and what the
goals of policy should be. Occasionally, it
might be necessary for one of the agencies to
dissent and explain why it disagreed with the
forecast, but these occasions are likely to be
infrequent. There should also be more attention
than at present to the consequences for
policy of the forecast being wrong.
Finally, bring choices explicitly into the
decision process, both in executive branch deliberations
and, especially, in Congress. Those
proposing spending increases or tax reductions
should routinely be required to specify
what is to be given up and to offer both the
benefit and its cost as a package. In other
words, proposals should be deficit neutral.
V. What Economists Can Do

For their part, how can economists be
more useful in the policy process? The press
and politicians often sound as if they are
telling us to work harder: go back to your
computers and don't come out until you
known how the economy really works and
can give us reliable forecasts. But economists
know that the economic system is incredibly
complicated, and that increasing global interdependence
and rapidly changing technologies
and public attitudes are not making it
easier to understand. It is not likely in our
lifetimes that anyone will happen on a
paradigm that explains everything, or even
that forecasting will become appreciably
more accurate. Like the medical profession,
which also deals with an incredibly complex
system, we economists just have to keep
applying our imperfect knowledge as carefully
as possible and learning from the results.
Both doctors and economists need
humility, but neither should abandon their
patients to the quacks.

The objective of economists ought to be to
raise the level of debate on economic policy,
to make clear what they know and do not
know, and to increase the chances of policy
decisions that make the economy work better.
Much of the time that means telling the
public and politicians what they would rather
not hear: hard choices must be made. We
are stuck with being the dismal science.
Increased effort in three directions would
make economics more useful in the policy
process. First, economists should put much
more emphasis on their areas of agreement.
The press admittedly makes this difficult.
Agreement is not news, and the press' stereotype
of economists' diversity of views is so
entrenched that they will go to great lengths
to scare up a lonely dissenter to an almost
universally held economic platitude and give
her equal time.

Economists realize that the breakthrough
insights around which "schools" are built
are at best partial visions of the truth, but
our training leads us to elaborate and differentiate
these insights, to explain to ourselves
and to others where they lead in
different directions, not where they come
together. Yet areas of agreement are
wide -even in macroeconomics-and a
major effort to make this clearer to ourselves
and our audience would be useful.
Second, economists should devote more
serious attention to increasing the basic economic
literacy of the public, the media, and
the political community. While the print media
seem to me increasingly knowledgeable and
sophisticated about economic issues, television,
where most people get most of their
information, lags far behind. Television
coverage of the economy is heavily weighted
to isolated economic statistics reported without
context-the wholesale price index increased
two-tenths of a percent in October
-and talking heads disagreeing, briefly, for
some obscure reason. Some of the best newscasters
appear to have bad cases of economics
phobia.


### ---Economics-1987-0-09.txt---
Media bashing is not the answer. The
profession needs to take the lead in explaining
more clearly what is happening to the
economy, why it matters, and what the arguments
are about or ought to be about. This
means more than each of us taking a little
time to make a luncheon speech, write an op
ed piece, or appear on a talk show. It means
sustained efforts on the part of teams of
economists to figure out how to present economic
ideas more interestingly and understandably,
developing new graphics and other
teaching tools and getting feedback from
real audiences. The technology is available
and the audiences exist-the number of people
who will watch long hard-to-follow congressional
debates and hearings on cable

television is quite astonishing. We just need
to devote the kind of effort and ingenuity
that goes into explaining to audiences the
complex, fast-moving, jargon-ridden game
of football to our complex, fast-moving,
jargon-ridden game of economics.
Third, economists need to be more careful
to sort out, for ourselves and others, what
we really know from our ideological biases.
George Stigler pointed out in his presidential
address (1965) that economists beginning
with Adam Smith have not hesitated to make
strong assertions, both positive and negative,
about the effectiveness of government intervention
without offering serious evidence to
support their claims. For two hundred years,
"the chief instrument of empirical demonstration
on the economic competence of

the state has been the telling anecdote"
(pp. 11-12). In the more than two decades
since Stigler presided over our Association,
an enormous amount of useful empirical
work has been done, as he predicted it would
be, on the effectiveness of government programs,
the costs and benefits of regulation,
and so forth. Still the arguments among
economists about the merits of larger vs.
smaller government too often revolve around
anecdotes or, worse, misleading statistics
quoted out of context. My own anecdotal
evidence would lead me to believe that liberals
and conservatives are about equally
guilty.

My concern is not with economists taking
sides on policy issues or acting as advocates
of particular positions. Indeed, I think many
policy debates would be clarified if there
were more formal and informal opportunities
for economists to marshall the evidence
on each side and to examine and cross-examine
each other in front of some counterpart
of judge or jury.

We economists tend to be uncomfortable
in the role of partisans or advocates, preferring
to be seen as neutral experts whether we
are or not. Lawyers move more easily among
roles; and the best are able to serve with
distinction at different times as prosecutors,
defenders, experts, and judges. The system
works well when the roles are played competently
and the rules of evidence strictly
observed. Economists might increase their
usefulness to the policy process if they made
clear at any given moment which role they
were playing. More important, we need to
work hard to raise the standards of evidence,
to make clear to the public and the participants
in the political process what we are
reasonably sure we know and how we know
it, and where we are guessing or expressing
our preferences.
 ## Economics-1988-0


### ---Economics-1988-0-03.txt---
It is tempting to use the audience captured
by a presidential address to pontificate about
the sad state of economics. You probably
will conclude that I have surrendered to the
temptation. But I do recognize that my good
luck in becoming president of our Association
does not automatically endow me with
commanding wisdom over all of economics.
I will do my best to stick to my knitting.
And for many years much of my research
has been directly toward investment in human
capital and the understanding of family
behavior.

Modern economists neglected the behavior
of families until the 1950s. Since then
economic analysis has been used to explain
who marries whom and when (if ever) they
divorce, the number of children and investments
in each child's human capital, the
extent and timing of labor force participation
by married women, when elderly parents
rely on children for support, and many other
family choices. A fair conclusion, I believe
(need I remind you of my biases?), is that
the economic approach contributes important
insights toward explaining the large decline
in birth rates during the past 100 years,
the rapid expansion in the labor force participation
of married women after the 1950s,
the explosive advance in divorce rates during
the past two decades, and other major
changes in the family. Family economics is
now a respectable and growing field.
Yet perhaps because family economics is a
new field, only a small literature considers
the implications for other parts of economics.
The family is such an important
institution that progress in understanding
how it behaves is justification enough for
any discipline. But most economists, including
the audience here, are not particularly
concerned about family behavior. Your interest
must be stimulated through a demonstration
that its study helps in the analysis of
other problems.

In this address I try to maintain your
interest by exploring the contribution to
macroeconomics from the progress in family
economics. This is a challenge not only because
macro behavior is a central part of
economics but also because its link to the
family may seem remote and unimportant.
By macroeconomics I mean the analysis of
economywide behavior. Much of the time is
spent on long-term economic growth, although
I also discuss short and long cycles in
economic activity, and the interaction between
overlapping generations through Social
Security, transmission of inequality, and
in other ways.

Of course, one paper even by a macro
expert cannot do justice to these topics, and
I do not pretend to be such an expert. My
purpose is to help you recognize that many
conclusions in these and presumably other
macro areas change radically when family
choices get the attention they deserve. I
apologize for the technical nature of some of
the discussion that may seem out of place in
a presidential address.

I. The Malthusian and Neoclassical Models
In considering the relation between economic
growth and the family, it is natural to
begin with Thomas Malthus's great contribution.
Although usually called the Malthusian
theory of population growth, a more
appropriate name is the Malthusian theory


### ---Economics-1988-0-04.txt---
of wages and average income. His first
monograph, subtitled "With Remarks on the
Speculations of Mr. Godwin, M. Condorcet,
and Other Writers," begins with an objection
to the conclusion of these writers that the
economic position of mankind will continue
to improve over time. In the process of rebutting
their arguments, Malthus develops
his famous theory of population growth and
reaches much more pessimistic conclusions
about the long-term economic prospects of
the average family.

You will recall that the Malthusian model
assumes diminishing returns to increases in
the level of population-that is, to increases
in employment-when land and other capital
are fixed. The analytical heart of his model
(I am not concerned with the details of what
he actually said) is consistent with constant
returns to the scale of labor and capital, as
long as the capital stock, including usable
land, does not respond to changes in wages
and interest rates.

The response of fertility and mortality to
changes in income determine the Malthusian
supply of population. Population grows more
slowly when wages are low because the average
person marries later and thereby has
fewer children (the preventive check on
population), and also because deaths increase
when families are poorer (the positive
check). Historical studies indicate that the
effect of the economy on age at marriage was
considerably greater, at least in Europe, than
was its effect on death rates (see Ronald D.
Lee, 1987b, pp. 450-51). Therefore, I will
ignore the positive effect and consider only
the preventive check through changes in the
number of children.

The long-run equilibrium wage rate is
found at the point on the positively inclined
population supply curve where the average
family has two children. The economy's production
function then determines the stationary
level of population that is consistent
with this long-run wage rate. There is no
presumption that this equilibrium wage is at
the subsistence level, especially if the positive
check through death rates is not important.
In this model tastes for marriage
and children, not vague notions of subsistence,
determine long-run wages.

The long-run wage is stable in the Malthusian
model when shocks push the system
out of equilibrium. For example, if an infectious
disease destroys much of the population,
as the Black Death destroyed perhaps
25 percent of certain European populations
during the fourteenth century, the decline in
population raises the marginal productivity
of labor. The resulting rise in wages encourages
families to marry earlier and have more
children. Population begins to grow and its
increase over time lowers wage rates back
toward equilibrium. Ultimately, this dynamic
process restores both the wage rate
and the level of population to their long-run
levels.

If the amount of usable land increases,
wages rise and that stimulates higher birth
rates. Again, the growth in population continues
to lower wage rates until eventually
the long-run wage is restored. However,
population is permanently higher because
the amount of land is greater.
This example brings out that the equilibrium
wage is more immune to shocks in
the Malthusian system than is the level of
population. Indeed, if tastes are stable over
time-the Malthusian model, along with
George Stigler and myself (1977), assumes
de gustibus non est disputandum -and if
technology does not continue to improve,
the equilibrium wage rate remains fixed by
the point on the stable supply curve where
the typical couple has two surviving children.
The Malthusian model does help some in
explaining very long-term changes in
European wage rates prior to the nineteenth
century (Lee, 1987b, gives a good analysis of
the evidence). People evidently married
earlier when wages were above the equilibrium
level and married later when they
were below.

It is ironic that Malthus's first essay on
population was published in 1798 at the
close of the eighteenth century. Although his
system was accepted by many leading
economists of the nineteenth century (see
John Stuart Mill, 1848, Book I, ch. X), events
after publication were not kind to the theory.
Fertility eventually fell sharply rather
than rose as wage rates and per capita incomes
continued to advance during much of


### ---Economics-1988-0-05.txt---
the nineteenth and twentieth centuries in the
United States, Western Europe, and Japan.
The contradiction between the theory and
events explains why most economists during
the first half of this century showed little
interest in explaining long-term trends in
income and population. But the subject
is too important to remain neglected, and
Robert Solow, David Cass, and others developed
the neoclassical growth model in the
1950s and early 1960s. This model incorporates
two major advances over the Malthusian
model. Each person maximizes utility
that depends on present and future consumption.
More important is the recognition
that changes in the capital stock respond to
rates of return on investments. Unfortunately,
the neoclassical model also takes a
sizable step backward from Malthus by assuming
that fertility and other dimensions of
population growth are independent of wages,
incomes, and prices.

I trust that the basic properties of a simple
neoclassical model are familiar. What may
not be generally appreciated is that despite
the different assumptions, the analytic structures
of the neoclassical and Malthusian
models are quite close and many of their
implications are similar. If technology and
preferences do not change over time, both
models have stable steady-state levels of per
capita income. The neoclassical equilibrating
mechanism works through changes in the
rate of investment, while the Malthusian
mechanism works through changes in the
rate of population growth. To illustrate, if
the capital-labor ratio exceeds its steady-state
level, the rate of return on capital is below
and the wage rate is above their steady-state
levels. In the neoclassical model this discourages
investment, which lowers the capitallabor
ratio over time (with exogenous population
growth). In the Malthusian model this
encourages population growth, which also
lowers the capital-labor ratio over time (with
exogenous investment in capital). We have
seen that a shock to population in the
Malthusian model has no effect on the level
of population or per capita income in the
long run. Similarly, in the neoclassical model
a shock to the capital stock (perhaps wartime
destruction of capital) has no long-run effect
on the aggregate capital stock or per capita
income.

The persistent growth in per capita incomes
during the past two centuries is no
easier to explain within the neoclassical
framework than within the Malthusian. Of
course, the neoclassical model postulates exogenous
technological progress to "explain"
continuing growth in per capita incomes, but
the need to rely on "exogenous" progress is
a confession of failure to explain growth
within the model. Moreover, the Malthusian
model can equally well postulate exogenous
progress to "explain" persistent growth.
II. The Family and Economic Growth
After a short while the economics profession
became disenchanted with the neoclassical
model, presumably because it too did
not help in understanding progress. The excitement
reflected in hundreds of papers that
extended and elaborated this model in the
1950s and 1960s gave way during the past
fifteen years to a lack of interest in the
analytics of growth that is a little reminiscent
of the situation during the first half
of the century.

Fortunately, a more relevant growth model
is available through combining the best features
of the neoclassical and MIalthusian
models and by adding a focus on investment
in knowledge and skills. The neoclassicists
are right to emphasize endogenous capital
accumulation and utility maximization.
Malthusians are right to stress the response
of fertility and other components of population
growth to changes in the economy, and
that these responses can greatly influence
economic change.

I will sketch out a modified neoclassical
model where parents choose both the number
of children and the capital (human or
physical) bequeathed to each child. Parental
altruism or "love" toward children provides
a powerful framework for the analysis of
both the quantity and so-called quality of
children. Altruism means that the utility of
parents depends on the utility of each child.
The assumption of altruism is realistic for
the vast majority of families, although
parent-child interactions are determined also


### ---Economics-1988-0-06.txt---
by other motives. Presumably, the altruism
per child is negatively related to the number
of children, so that an additional child lowers
the utility per child to parents in the same
way as (please excuse the analogy) an additional
car lowers the utility per car.
Such altruism is easily grafted onto the
neoclassical utility function by letting parents'
utility depend on their own life-cycle
consumption and separately on their degree
of altruism per child, the number of children,
and the utility of each child. This formulation
has the important implication that preference
for parents' relative to children's consumption
(so-called time preference) is not
exogenous but rises as the number of children
increases.

The resources available to parents from
the capital they inherit and labor earnings
are spent either on own consumption, on the
costs of rearing children, or on transfers to
children of human and other capital. Since
child rearing is time intensive, the cost of
rearing children is positively related to the
value of parents' time. Income per capita
would rise between the parents' and the
child's generations if the total capital bequeathed
to each child exceeds the capital
inherited by each parent.

Parents choose optimal values of their own
consumption, the number of children, and
capital transferred to each child while taking
into account the cost of rearing children and
the dependence of their utility on the utility
of children. This analysis has many implications
for the behavior of fertility that Robert
Barro and I explore elsewhere (see 1987 and
1988). Here I concentrate on a few that alter
implications of the neoclassical model about
capital accumulation and growth.
If the number of children demanded by
the typical family is positively related to the
income of parents (the Malthusian assumptions)
, or at least if it is not strongly negatively
related, then this model also has stable
steady-state levels of the capital-labor ratio
and per capita income. But these steady
states depend on variables that change the
demand for children.

One example is the consequences of an
extended but temporary decline in income
and productivity-perhaps due to the disorganization
induced by a lengthy depression.
In the neoclassical model this has no
long-run effect on either per capita or aggregate
income. In our modified model an
extended decline in productivity can permanently
lower aggregate income because birth
rates may fall when productivity, wages, and
interest rates fall. Recall the sharp decline in
birth rates during the Great Depression.
Just over a decade age, Barro (1974)
showed that a dose of family economics
radically alters traditional conclusions about
the effects of budget deficits on private savings.
For example, deficits to finance Social
Security payments tax future generations to
support the elderly. Altruistic parents who
leave bequests to their children do not seek
an intergeneration redistribution of incomes,
so they would increase their bequests to offset
the effect on children of future taxes. If these
families are common, Social Security payments
and other public expenditures financed
by taxes on future generations would not
have much effect on private savings. This is
the so-called Ricardian equivalence theorem.
A larger dose of family economics gives
more radical implications in some respects
but also has more conventional implications
for the relation between Social Security and
savings. Various comments on Ricardian
equivalence emphasize that some families do
not leave bequests; I will discuss these families
in Section IV. Development economists
have long recognized that parents value
children who provide support during old age.
A Social Security system that replaces child
support of parents with public support raises
the net cost of children to parents (not to
society) since they are no longer as useful to
elderly parents. As a result, a Social Security
system tends to reduce the demand for
children. Social Security also reduces the
demand for children by parents who do not
receive support but provide bequests. The
net cost of children to these parents also
increases when they raise bequests to offset
the effect of Social Security taxes on children.
For reasons given earlier, a lower demand
for children raises the capital bequeathed to
each child. Therefore, Social Security and
other public transfers between generations
would raise private savings per child, and as


### ---Economics-1988-0-07.txt---
a result, raise wage rates and the capital-labor
ratio in the next generation. Yet total private
savings of the present generations would fall,
as in a conventional life-cycle analysis with
no bequests, if the decline in fertility exceeds
the greater saving for each child.
Consider next an example from tax incidence.
A tax on income from capital initially
lowers after-tax returns and discourages investment.
In the neoclassical model, capital
then falls over time until the after-tax rate of
return again equals the given rate of time
preference. In public finance jargon, a tax on
capital would be fully shifted in the long
run.

A difficulty with this conclusion is the
neoclassical assumption that fertility is fixed,
which is especially inappropriate for very
long-term changes in incidence. Fertility
would fall as capital fell in response to the
tax if fertility is positively related to per
capita income. A fall in fertility lowers preference
for present consumption and raises
the demand for investment in each child
through the interaction between the quality
and quantity of children. Then the equilibrium
after-tax rate of return must also
fall, and the tax on capital is only partially
shifted even in the long run.

The conclusion is more radical if fertility
is negatively related to per capita income
(for reasons discussed next). Fertility then
increases when the stock of capital falls.
Since the increase in fertility lowers investment
per child, the equilibrium after-tax rate
of return would have to increase. That is, we
have the paradox that a tax on capital is
eventually shifted by more than 100 percent!
Let me assure the theorists that this strange
result does not violate the second-order conditions.


Does a negative relation between fertility
and per capita income imply that children
are an "inferior" good (to use the economist's
infelicitous language)? The answer is no because
the cost of rearing children increases
when the capital-labor ratio and per capita
income rise since wage rates and the value of
parents' time spent on children rise along
with the capital-labor ratio. Fertility would
fall if the positive effect on fertility of an
increase in income is weaker than the negative
effect due to the rise in cost. The substitution
effect often dominates the income
effect in rich countries, for child care in these
countries requires considerable time and energy
of parents.

If fertility is negatively related to per capita
income, an increase in the capital-labor ratio
above its steady-state level would reduce
fertility and thereby encourage more investment
per child. The capital-labor ratio would
continue to increase over time if this positive
effect on investment dominates the negative
effect of a lower rate of return. Consequently,
a negative relation between fertility
and per capita income can destabilize what
is otherwise a stable steady state (see the
formal analysis in Robert Tamura, 1986).
Demographers have long been aware that
fertility eventually declines as a country
develops. Less well appreciated (although
see the earlier literature by R. R. Nelson,
1956; Robert M. Solow, 1956, pp. 90-91;
S. C. Tsiang, 1964, and others on low-level
" traps") is that a negative relation between a
country's fertility and its income can destabilize
a steady-state equilibrium and cause
a protracted period of rising per capita incomes.
However, although a decline in fertility
is an important stimulus in early stages of
development, it alone cannot explain sustained
growth over a century or longer. In
the absence of other forces, a growing economy
with neoclassical production functions
but without continuing technological progress
eventually moves to a stable steady
state with low fertility and high per capita
incomes.

A promising approach to sustained growth
that complements the role of fertility builds
on the special properties of education and
other learning. The important property for
this purpose is that investments in education
and other human capital are more productive
when past investments are larger. That
is to say, accumulation of knowledge and
skills in the past eases the acquisition of
additional knowledge. The mastery learning
concept in education pedagogy uses this
property to organize the teaching of mathematics
and other subjects to children (see
Benjamin S. Bloom, 1976). Such a production
technology implies that rates of return


### ---Economics-1988-0-08.txt---
on investments in human capital may not
fall and may even rise as the stock of human
capital grows.

Perhaps it was reasonable in Malthus's
time to neglect investments in human capital,
but there is little excuse for the neglect in
neoclassical growth theory. Modern economies
spend enormous amounts on education
and other training of children, and parents'
investments in children are a far more important
source of an economy's capital stock
than are bequests or the life-cycle accumulation
of physical capital. Dale Jorgenson and
Barbara Fraumeni (1987) estimate that human
capital comprises over 70 percent of the
total capital stock in the United States. This
estimate may be too low because it does not
include the contribution of human capital to
output in the household sector (the authors
do try to estimate household output). Seventy
percent may be higher than the true
fraction because it makes no allowance for
the contribution of "raw labor" to output. I
would guess that the true ratio of human
capital to the total capital stock may be as
high as 90 percent or as low as 50 percent.
Of course, even this lower percentage signifies
a large contribution. The neglect of
human capital in wealth and income accounts
greatly distorts comparisons of savings propensities
and the accumulation of wealth.
Only recently have growth models begun
to appreciate the potential of the learningby-
having property of human capital for
generating sustained growth in per capita
incomes (see Paul Romer, 1986; Robert E.
Lucas, Jr., 1988; and Robert G. King and
Sergio Rebelo, 1986; pioneering earlier work
includes Kenneth J. Arrow, 1962; Yoram
Ben-Porath, 1967; Hirofumi Uzawa, 1965;
and Sherwin Rosen, 1976). Kevin M. Murphy
and I are developing an analysis that combines
such a human capital technology with
unskilled labor, physical capital, and endogenous
fertility that results from altruism. (See
Gary S. Becker, 1971, pp. 204, 207-208, for
an earlier effort to combine human capital,
unskilled labor, and physical capital.) Our
model has a " Malthusian" equilibrium where
per capita income is constant and low and
fertility is high. However, if this equilibrium
receives big enough technology and other
shocks-good luck may be required-the
economy takes off toward a perpetual growth
equilibrium with a decline in fertility and
increased investment per child. Knowledge
continues to grow through its embodiment
in additional human capital.

Family economics is critical to the analysis
since choices about number of children
and investments in each child's human
capital helps determine whether the economy
ends up at a "good" (i.e., growth) equilibrium
or at a "bad" (i.e., Malthusian) equilibrium.
Obviously, we do not have the full
answer to economic growth-public policies,
conglomeration effects, and other considerations
are surely important-but I do believe
that our story contributes a sizable part of
the answer.

III. Short and Long Cycles

Let me now turn briefly to the relation
between family behavior and cycles in aggregate
output and other variables. For centuries
marriages, births, and other family
behavior have been known to respond to
fluctuations in aggregate output and prices.
In an early use of regression analysis in the
social sciences, G. Udny Yule (1906) demonstrated
that English marriages and births
in the nineteenth century moved together
with the business cycle. Subsequent studies
showed that higher order as well as first
births, divorce rates, and possibly the labor
force participation of secondary workers
all fluctuated procyclically in many countries
(see, for example, Becker, 1960, and
Morris Silver, 1965). Birth rates in the United
States apparently became countercyclical
after many married women entered the labor
force. Children are cheaper during recessions
because the value of time spent on children
by working mothers is low then (see William
P. Butz and Michael P. Ward, 1979). Investments
in education and other human capital
are much less procyclical than investments
in physical capital also because the foregone
value of time spent in school is cheaper
during bad times (Linda N. Edwards, 1975).
Of course, none of the competing macro
models of business cycles-be they Keynesian,
monetarist, neoclassical, or real-rely


### ---Economics-1988-0-09.txt---
on family behavior to cause business cycles.
However, declining population growth was a
major cause of the secular stagnation feared
by Alvin H. Hansen (1939) in his presidential
address to our Association almost fifty
years ago. Family behavior may play more
than a negligible role even in generating
ordinary business cycles. For example, an
increase in the labor supply of married women
or young people when household work
or school becomes less attractive can induce
cyclical responses in aggregate output and
other variables. Cycles started by shifts in
labor supply induce a negative relation between
wage rates and aggregate output over
business cycles. This would help explain why
cyclical fluctuations in real wages appear to
be less positively related to cyclical fluctuations
in aggregate output than is implied by
business cycles models that emphasize the
demand side.

Although family behavior presumably has
only a small part in the generation of
ordinary business cycles, it is likely to be
crucial to long cycles in economic activity.
Malthus claimed that family choices
cause long-term fluctuations in the economy
through the lagged effects first of marriages
on births and then of births on the size of
the labor force (see Maw Lin Lee and David
Loschky, 1987). Modern demographic analysis
generates long cycles in population growth
rates through the relation between aggregate
fertility and the age distribution, and perhaps
also between fertility and the size of a
cohort (see, for example, James C. Frauenthal
and Kenneth E. Swick, 1983, and Ronald
Lee, 1987a). In our modified Malthus-neoclassical
model, family choices cause long
cycles not only in population growth, but
also in capital, output, and other variables if
the elasticity of the degree of altruism per
child with respect to the number of children
declines as families get larger, a reasonable
assumption. Fertility and per capita income
then fluctuate in generation-long cycles whenever
the economy is disturbed away from the
steady state (for a proof, see Jess Benhabib
and Kazuo Nishimura, 1986).

In the 1920s, the Russian economist
Nicholas D. Kondratieff claimed that capitalist
economies exhibit long-term fluctuations
of about fifty years' duration in output
and prices (see Kondratieff, 1935). Simon
Kuznets (1958) later argued that long-term
fluctuations only last about twenty years. If
long cycles of the Kondratieff or Kuznets
type exist-we will need another 200 years
of data to determine whether they do exist
or are just a statistical figment of an overactive
imagination-they almost certainly will
depend on fertility and other family decisions
that biologically require a long time to
implement.

IV. Overlapping Generations

The intrinsic risks faced by the elderly,
sick, and unemployed are surely no greater
in rich countries like Germany and the
United States than in poor countries like
China and India, nor do these risks rise as a
country develops. Yet the first large-scale
Social Security program was introduced by
Germany a mere 100 years ago. China, India,
and numerous other countries still have
only modest programs that exclude many of
their old people. We take publicly financed
schools for granted, but they were unimportant
until the latter half of the nineteenth
century. Public and private programs that
protect against the consequences of illness
and unemployment are even newer and less
common than Social Security and public
schools.

Throughout history the risks faced by the
elderly, young, sick, and unemployed have
been met primarily by the family, not by
state transfers, private charity, or private
insurance. Children usually cared for elderly
or infirm parents, the unemployed looked to
their families for temporary support, and
parents have spent much time, money, and
energy to rear and train their children. Despite
the rapid growth of Social Security
payments in the past few decades, almost 20
percent of women aged 65 and over in the
United States still live with their children.
The altruism and love of parents, children,
spouses, and other relatives have helped protect
family members against the hazards of
childhood, old age, and other risks. When
altruism is insufficient-unfortunately, it
often is-what sociologists call social norms


### ---Economics-1988-0-10.txt---
frequently emerge that pressure children,
parents, spouses, and other relatives into
helping out family members in need. In addition,
family members use their frequent
interaction with one another to raise the
level of guilt experienced by a member when
he or she does not help out.

The formal analysis of the interaction
among overlapping generations began with
Paul A. Samuelson's brilliant paper in 1958.
This spawned an enormous literature that
continues up to the present. Although
Samuelson had relevant obiter dicta about
social compacts, altruism, and family obligations,
his model and that of most of the
subsequent literature assumes that each person
enters the analysis as a young adult
without personal connections to older cohorts.
A long review of overlapping generation
models in the recent New Palgrave Dictionary
(see John Geanakoplos, 1987) has no
discussion whatsoever of familial relations
between members of overlapping generations.
I claim that the neglect of childhood
and of the intimate relations among parents,
children, husbands-wives, and other family
members misled these studies sometimes into
focusing on minor problems and diverted
attention away from some important consequences
of the overlapping of generations
(the discussion in the next few paragraphs
draws partly on Becker and Murphy, 1988).
One example of the emphasis on unimportant
problems is the concern with the
plight of older people when there are few
durable assets that can finance consumption
at old age. In an influentual literature on the
demand for money, the social role of money
is even attributed to a durability that
enables older people to finance consumption
by selling to the next generation money
accumulated when young (see, for example,
Thomas Sargent, 1987, ch. 7, and Neil
Wallace, 1980). Yet when anthropologists
study simple societies that do not have money
or other durable assets, they find that old
people finance their consumption mainly
by relying for support on children and
other kin. Indeed, children have been an
important resource and money balances an
unimportant resource of the elderly in practically
all societies, whether simple or complicated.


General equilibrium theorists are concerned
about the continuum of equilibria,
inefficiency, and other problems that arise in
models where overlapping-generations persist
indefinitely into the future (see, for example,
Geanakoplos, 1987, or Timothy J.
Kehoe, 1987). Although these problems
would not completely disappear, I conjecture
that they would be much less important
if overlapping-generations models incorporated
the informal trades and assistance
available to parents, children, and other
members of the same family.

Ever since Plato's Republic, philosophers
have worried about whether parents invest
sufficiently in the health, skills, and morals
of their children. Overlapping-generations
models usually neglect childhood and concentrate
on savings by young adults and
their trades with old adults. The treatment of
children by parents not only is so important
in its own right, but it also greatly influences
the relations between older and younger
adults (Allan Drazen, 1978, is one of the few
earlier studies that recognizes the importance
of investments in children for overlapping-
generations models.)

I cannot do more on this occasion than
present the bare bones of an analysis of how
families respond to the demands of both old
age and childhood. The analysis is straightforward
when altruistic parents leave bequests
to their children. The combination of
altruism and bequests eliminates any difficulties
in financing the wealth-maximizing
investment in children's health, training, and
other human capital. For if the marginal rate
of return on additional human capital exceeds
the rate on assets, both parents and
children would be better off with additional
capital. Parents can save less to offset the
negative effect on their consumption of
greater spending on their children's human
capital, and they can reduce bequests to
offset the effect of lower savings on consumption
at old age.

Bequests also partly insulate parents from
many risks of old age. The opportunity to
draw on bequests provides an annuity-like


### ---Economics-1988-0-11.txt---
protection against an usually long life and
other risks of old age. For example, parents
who live longer than expected reduce bequests
to help finance consumption in the
additional years. If bequests are not a large
part of children's assets, bequests can give
elderly parents excellent protection against
various hazards, and yet changes in bequests
do not have much influence on children's
welfare. In effect, children help support their
parents in old age, although their support is
not fully voluntary.

The analysis is less simple when parents
do not leave bequests, perhaps because they
are not very altruistic or because they expect
their children to be better off than they are.
These families tend to underinvest in children
and underprotect parents against the hazards
of old age because bequests are not available
to finance investments and old-age support.
Social norms, feelings of guilt, and similar
mechanisms may greatly moderate the degree
of underinvestments and underprotection.
They can induce even selfish parents to
invest in children and selfish children to care
for sick or poor parents. Economists neglect
concepts like norms and guilt because no
one really knows how they evolve. Moreover,
sociologists (perhaps I should say "we" sociologists
since I am now officially also a sociologist)
are too prone to use norms as a
deus ex machina to explain behavior that is
difficult to explain in other ways. Nevertheless,
there can be little doubt that norms and
other intangible mechanisms do greatly affect
the relations between family members in
many societies, although presumably, they
do not work as well as bequests in linking
generations together.

Parents in richer countries have more resources
to spend on children and to protect
against the hazards of old age. Why then
have public expenditures on both the young
and old grown rapidly during the 100 years
as western countries as they have become
richer? One reason is that social norms are
weaker in the anonymous urban communities
of industrial countries where elderly
parents often live far from adult children. A
more analytically tractable reason is the high
rates of return in modern industrial societies
on investments in the health and training of
children. Recall my discussion of the role of
human capital in economic development.
Parents are eager to finance profitable investments
in children called for by economic
development, as long as they can draw on
gifts and bequests that they would give to
children. But gifts and bequests would become
nil in many families that invest a lot in
their children. These families would underinvest
in children, particularly when pressure
from norms is weak. The growth in public
support of schooling and other investments
in children as countries develop would then
appear to be mainly a response to the positive
effect of economic development on the
benefits from human capital.

Since families that do not leave bequests
are vulnerable to the hazards of old age, it is
not difficult to understand why public expenditures
on Social Security and medical
care for the elderly have also grown rapidly
in industrial countries. However, you may be
surprised to find out that public expenditures
on the old have not been at the expense
of the young. Since 1940 in the United States,
the ratio of expenditures per child under age
22 to expenditures per adult age 65 or over
has hardly changed. Our analysis that combines
investments in human capital with oldage
support does explain why expenditures
on the old and young grew in tandem. By
contrast, the popular view of generation
fighting-that public expenditures on the
elderly grew rapidly because the old became
politically powerful as they became more
numerous-cannot explain why expenditures
on children grew just as rapidly.
The overlapping-generation framework is
also a natural one to consider inequality and
the transmission of wealth and poverty across
generations. Families help perpetuate inequality
because children inherit abilities and
other "endowments" from parents. Moreover,
parents are the major source of the
assets and human capital of children. This
enormous influence of the family led my
esteemed teacher, Frank H. Knight, to claim
that ".where the family is the social unit, the
inheritance of wealth, culture, educational
advantages, and economic opportunities


### ---Economics-1988-0-12.txt---
tend toward the progressive increase of inequality.
.." (1935, p. 50).

Abilities and other endowments regress
downward from parents to children in successful
families where parents earn a lot, and
they regress upward in unsuccessful families
where parents earn little. The poor underinvest
in each child also because they have
larger families and less stable marriages.
Therefore, children from poorer families tend
to earn more than their parents but below
the average of their generation, and children
from richer families tend to earn less than
their parents but above their generation's
average.

Earnings depend not only on endowments
but also on investments in human capital.
Our earlier analysis implies that richer families
do not tend to underinvest in their
children's human capital because these
families leave gifts and bequests. Poorer
families do tend to underinvest in children
because they are not likely to leave gifts and
bequests. The poor underinvest in each child
also because they have large families and less
stable marriages. Therefore, the relation between
the earning of fathers and sons in
richer families would depend mainly on the
relation between endowments, while the relation
between earnings of fathers and sons
in poorer families would depend also on the
degree of underinvestment in children. Put
differently, without offsetting government
subsidies to investments in the human capital
of poorer children, low earnings would be
more persistent across generations than high
earnings -the so-called "culture of poverty"
across generations would exceed the "culture
of privilege."

In every country with data that I have
seen-this includes the United States and
several European countries (see Table 1 in
Becker and Nigel Tomes, 1986), a few Asian
countries, and some Latin American countries
(James J. Heckman and Joseph V. Hotz,
1986, consider the evidence for Panama)
earnings strongly regress to the mean between
fathers and sons. Probably much less
than 40 percent of the earnings advantages
or disadvantages of fathers pass to sons, and
few earnings advantages or disadvantages
survive three generations. Evidently, abilities
and other endowments that generate earnings
are only weakly transmitted from
parents to children. This tendency to go
from "shirtsleeves to shirtsleeves" in three
generations began long before industrialization
and government support of education
and other human capital. The fourteenth
Arab historian and philosopher, Ibn Khaldtun
said (I owe this reference to my wife, Guity
Nashat), "Prestige is an accident that affects
human beings. It comes into being and decays
inevitably.... It reaches its end in a
single family within four successive generations.
" (1958, p. 279)... "As a rule, no dynasty
lasts beyond the [span] of three generations.
" (p. 343)

In all these countries, low earnings as well
as high earnings are not strongly transmitted
from fathers to sons, and Knight's claim
about family life causing growing inequality
is inconsistent with the evidence. Still, data
for both the United States and England do
appear to confirm the implication of our
theory that low earnings persist more than
high earnings across generations (see W.
Stanley Siebert, 1987). Of course, incomes of
the rich regress down more slowly between
generations than do their earnings because
rich children receive gifts and bequests
from parents (see Becker and Tomes, 1986,
Table 2).

V. Concluding Remarks

I was attracted to the family by its obvious
importance in all countries, no matter
what the economic system or stage of development.
People spend much of their time
in a dependency relation-toward parents
when children and toward grown children in
old age-marriage is a crucial step for most
people, children absorb time, energy, and
money from their parents, divorce often
causes economic hardship and mental depression,
and so forth. Economic studies of
the family are growing at a steady pace and
they are influencing the way other social
scientists look this fundamental institution.
The economic analysis of family behavior
stimulated the development of techniques


### ---Economics-1988-0-13.txt---
and prospectives that already has affected
many parts of microeconomics, especially
agricultural and labor economics, but also
the study of industrial organization and
preference theory. For example, the treatment
of marriage as a sorting of men and
women into small "partnerships" through a
reasonably efficient marriage market influenced
the analysis of how workers and
managers are allocated to different firms.
Viewing divorce as a joint decision by
husbands and wives based largely on information
gathered from living together encouraged
some studies of employment separations
to blur the analytical distinction
between quits and layoffs and to emphasize
the information about working conditions
and productivity gathered from on-the-job
experience.

The message of this address, however, is
not the importance of the family per se, even
though family welfare is the principal goal of
a well-run economic system. Nor that analytical
techniques developed to understand
family choices are valuable in other parts of
economics. The message is that family behavior
is active, not passive, and endogenous,
not exogenous. Families have large
effects on the economy, and evolution of the
economy greatly changes the structure and
decisions of families. I illustrated how families
and the economy interact through a discussion
of economic growth and other issues
in macroeconomics. A heightened awareness
of the interaction between economic change
and family choices will hasten the incorporation
of family life into the mainstream of
economics.
 ## Economics-1989-0


### ---Economics-1989-0-03.txt---
Four decades ago, in a brilliant review of
Burns and Mitchell's classic work on business
cycles, Tjalling Koopmans (1947)
sounded a warning against "Measurement
Without Theory." Without theoretical hypotheses
and constructs to orient investigation
and interpret findings, economists risk
the fate of the blind man studying the elephant.


Recent excursions in vector autoregressions
possibly aside, modem econometrics
has generally followed the path of specifying
theoretical relations that make sense in terms
of economic behavior and using empirical
data of time-series or cross sections or both
to estimate the parameters of those relations.
A critical problem, only one surface of which
is touched by Robert Lucas' famous "Critique"
(1976, 1981a), is that key arguments
in most functions are expected values-or
worse, probability distributions-of future
variables. These are generally not observed
and we are reduced to estimating parameters
of current and past variables, for which we
have data we can put into our computers,
and making explicit or, more often, implicit
assumptions about the relation between them
and the relevant but unobservable expectations.


The frailties of all this have long been
apparent in estimation of demand and supply
functions for individual commodities. We
cannot, after all, even be sure of confirming
the "law of demand," that lower prices will
increase quantities demanded. They probably
will not, if they are associated with
expectations of still lower prices in the
future.

Similar critical problems emerged in estimation
of key relations in macroeconomics.
Will an increase in the quantity of money
reduce long-term interest rates? Not, we are
told, if it generates an increased expectation
of inflation or, perhaps, an expectation that
the increase in money will be short-lived and
be reversed in future action by the monetary
authority. What, though, were the expectations,
with regard to inflation or future monetary
operations, that generated the data used
in estimating money demand functions? Can
we be sure that these same expectational
relations apply when we try to forecast the
results of a proposed easing of monetary
policy? And is the assumption that expectations
are "rational" of much help if they are
frequently, as John M. Keynes argued, built
on such uncertain knowledge as to be a
fragile phenomenon of social interaction?
The problem of bringing in critical expectational
variables is brought front and center
in Franco Modigliani's life cycle (1954) and
Milton Friedman's permanent income (1957)
versions of Keynes' consumption function.
Serious problems remain here in considerable
part because of our difficulties predicting
agents' reactions to uncertainty regarding
not only future income but expected
length of life. We thus lack two parameters
defining the resources per unit of time available
for lifetime or permanent consumption.


Difficulties are most excruciating with regard
to investment, which is entirely forward


### ---Economics-1989-0-04.txt---
looking. We introduce as arguments of investment
functions such variables as current
and past output, sales, or utilization of capacity,
current or past profits, cash flow or
measures of liquidity, and current or past
interest rates, depreciation rates, and relative
rental price or user costs of capital. Yet our
theory tells us that the arguments we generally
need are again the expected future values
of these variables. Firms should invest if
they expect the future demand for output to
be high, if they expect the cost of capital to
be higher in the future than now, and if they
look to higher future profits as a consequence
of current investment, but little if at
all in response to current or past values of
these variables.

But these difficulties in finding proxies for
usually unobservable expectations of the future
are only part of the overall problem.
Somehow, econometricians, theorists, and
economic analysts of all stripes have lost
essential communication with the compilers
and synthesizers of their data. As a consequence,
popular discourse, policymaking,
and basic principles of economics have suffered
inordinate confusion. I shall offer this
indictment, in particular, with regard to discussion
of the major macroeconomic variables
with which we are most concerned:
income, output, employment, prices, and
productivity; consumption, saving, investment,
and capital formation; wealth, assets
and liabilities, debt, and deficits. To put
matters bluntly, many of us have literally
not known what we are talking about, or
have confused our listeners-and ourselves
-into thinking that what we are talking
about is directly relevant to the matters with
which we are concerned. In some cases the
confusions do not make all that much difference,
but often they do.'

I. Measures of Income and Product
Take income for starters. We all know
what we mean by income, don't we? Or
do we? The theoretical Hicks-Haig-Simons
(J. R. Hicks, 1940, 1946, 1948; Robert Haig,
1921; Henry Simons, 1938) concept of income
is that which we can consume while
keeping our real wealth intact. But this is a
far cry from the usual measures of individual
incomes, corporate profits or the aggregates
of personal and national income.
First, we allow for capital consumptionthe
depreciation of existing capital-in
measuring income. We may note that capital
consumption allowances with capital consumption
adjustment were 8.22 percent of
GNP in 1951 before the move to permanently
higher tax depreciation allowances
began, were 9.14 percent of GNP by 1960,
11.69 percent by 1983, and 11.98 percent in
1987. Did production really become more
capital-intensive, did the capital mix truly
turn that much less durable,2 or is it possible
that we are overstating capital consumption
and understating national and personal income
and net saving, both in absolute
amounts and as compared to the past?
What about capital gains and losses? If
the value of our stock or bond or house goes
up, can we not consume the gain in addition
to what is usually counted as income, and
still keep our wealth intact? We must indeed
distinguish between the nominal gain and
the real gain, that is, the increase in nominal
value over and above the increase that
would be necessary to compensate for general
inflation. But should not the real gain, in
conformity with the Hicks-Haig-Simons concept,
which I shall denote as "theory," be
included in properly measured income?


### ---Economics-1989-0-05.txt---
If nominal interest rates are 9 percent and
inflation runs at 4 percent, should we include
all of interest receipts in income? Or should
we not recognize that 4 percent of the value
of existing interest-bearing securities is being
eaten away each year by inflation? Must not
this amount be taken out of interest receipts,
to keep capital intact, before we can recognize
income? In fact, of course, we do not do
this in our measures of personal income or
national income or the distribution of income
or anywhere else, to my knowledge-or
even in the computation of taxable income.
Our national income statistics include imputations
for a number of nonmarket items
of output including, most importantly, net
rent of owner-occupied housing. But what
about automobiles and other durable goods?
Are the transportation services of a car consumption
when we rent one from Hertz or
Avis but not consumption if we own the car
ourselves? But if they are consumption, are
we not obliged to include their value, net of
the car's depreciation (but plus real capital
gains), in income?

If we pay a housekeeper we create income
according to conventional measures. Should
we not record income, as well, if the work of
the housekeeper is done by an unpaid member
of the household?3 If we did, we would
note an enormous increase, in particular, in
the amount of income earned by women,
and consumed by men, women, and children.
We would also be forced to lower our
estimates of the increases in national income
as women leave unpaid household work for
jobs in the marketplace; we would have to
offset the increase in market earnings with
the reduction in the value of nonmarket output.


We might also alter our views of the presumed
decline in productivity growth. Average
labor productivity growth falls, in conventional
measures, as women entering the
labor force continue to fill disproportionately
lower-paying and hence less productive
jobs. But if the jobs they take, while less
productive than the average of those in the
labor force, are actually more productive
than unpaid jobs in the home, as appears
likely if only on the evidence that women do
take them, average total labor productivity
will be increased even as average market
labor productivity may be declining.4
Conventional measures of income include
various items that might better be excluded,
more appropriately viewed as expenses related
to work. If a firm arranges transportation
for its workers or if workers have to
travel in connection with their jobs we do
not include the cost of the transportation
services in income. But if workers have to
pay for their own commuting expense-as
most do-the portion of wages devoted to
this is counted as part of income.
We might include in consumption and income,
though, at least some of the value of
three-martini and other lunches and the variety
of amenities-fitness centers, club memberships,
vacation retreats, and conventions
combining as much play as work in exotic
locations-supplied in and out of corporate
offices. And we might impute income to capital
invested by government and impute output
to the value of services produced by this
capital. The value of public education, we
may note, is taken as the wages and salaries
paid to teachers and janitors. We do not
include, as we would if education were provided
by private business, any of capital
costs that would be measured by interest on
government debt, depreciation on buildings
and equipment or "profit."

We do include in gross national product
all of what we measure as government output.
Yet much of it is clearly intermediate in
character. If private firms are forced to hire
more security guards, the result is either no
change in national income or product if the
extra cost is taken out of profits, or an
increase in nominal but not real product if
prices rise. But if government hires more
police, nominal and real national income
and product increase. It may well be argued
3I am informed (by Lawrence Klein) that Norway
once proposed the inclusion of housewives' output in
national income and product and, for a time, estimated
it.

4I am indebted to Nancy Barrett for this point. See
Barrett and Ines Bustillo (1985).


### ---Economics-1989-0-06.txt---
that in both cases the security services are
merely inputs which should not be taken to
change our measure of final product. With
the massive increases in expenditures for national
defense in recent years-whether justifiable
or not-the implications for our
measures of growth in GNP are substantial.5
II. Deficits and National Saving
and Investment

Substantial as are the differences between
income as usually measured and its theoretical
construct, they are relatively small compared
to the departures of conventionally
measured saving and investment from their
theoretical counterparts.6 Personal saving,
the difference between disposable personal
income and outlays, of which the latter is
chiefly consumption, is immediately suspect
because of the measures of both income and
consumption, as is business or corporate saving
because of questionable measures of
costs, investment, and profits.7 But the problem
is much larger.

Investment is the acquisition or production
of capital, which in turn contributes to
current and future output. Should not the
production of new automobiles then qualify
as investment, whether they are bought by
business or by households or by government?
Should research expenses not be considered
investment along with expenditures
for plant and equipment? Are the education
and training of our young not perhaps the
most important investment in future productivity
that we can make?8 Yet we count such
private expenditures as consumption and the
vast if insufficient amount of output in the
form of public education is excluded from
investment under the apparently somewhat
pejorative heading of "government spending.
"

With gross income identically equal to
gross output, gross saving is identically equal
to gross investment, including increases in
net claims on the rest of the world, and net
saving is thus the claims on net additions to
capital. Saving should then include the portion
of income used to invest in the human
capital of a college education as well as the
portions used to buy corporate bonds or put
into a pension fund.

We have been bombarded for some time
with the arguments in political circles, but
also from many economists, utilizing data
from conventional measures and accounts,
that " national saving" is too low, disastrously
low. This so-called national saving is
the sum of the personal and corporate saving,
which we have had reason to question,
and government saving or the government
budget surplus.9 These measures are far from
the theoretical constructs to which we might
expect them to correspond.

First, since government budget surpluses
-or deficits-are taken as government receipts
minus all government outlays, without
distinguishing between current and capital


### ---Economics-1989-0-07.txt---
expenditures, a major adjustment is immediately
in order, consistent with the practice of
private business, to exclude the capital expenditures
and include in current outlays
only depreciation charges. Second, just as
not all of nominal interest receipts should be
included in income, not all of government
nominal interest payments should be counted
in outlays. Rather these should be adjusted
for the changing real value of government
debt, due both to inflation itself and the
changing market price of securities associated
with changing interest rates, which latter
may relate in part to changing rates of inflation.
We should in effect be including in
government revenues the inflation tax on the
private holders of government securities or
the money backed by central bank holdings
of those securities, or including in outlays
only real interest payments.'0
These two adjustments, for capital expenditures
and for inflation, make such a huge
difference in the Federal government budget
as to wipe out the much decried "budget
deficit."" The first of them has an identical
effect in raising our measure of national saving,
greatly depressed in its conventional
measure by the negative contribution of the
presumed fiscal dis-saving."

Gross saving is identically equal to gross
private domestic investment plus net foreign
investment. The supposed "twin deficit" in
the U.S. balance of trade or, more exactly,
the current account,"3 is the other side of the
coin of negative foreign investment, that is,
net acquisition by foreigners of claims on the
United States or its residents. If we measure
those claims incorrectly and hence measure
net foreign investment incorrectly, we correspondingly
mis-measure gross saving. By understating
gains in the value of our claims on
foreign assets, we understate business and,
to a certain extent, personal income and
saving.

But here again there is a major difference,
not noted by the public and politicians and,
I must add, not noted by most economists
either, between the official measures and
those that would correspond to meaningful
economic theory.'4 Particularly, direct investment
by U.S. firms abroad and by foreign
firms in the United States are both
taken at original cost. There is then no adjustment
for the changes in value of the
resulting assets in their own currencies or,
taking into account changing exchange rates,
the value of those changes in dollars.
Saving should correspond to increases in
wealth. National saving should correspond
to increases in national claims to wealth, at
home and abroad, net of increasing foreign
claims to domestic wealth. But to be meaningful,
these should surely relate to claims
measured in real, current, or market value.
In fact, they do not. Because they do not, we
have been confronted with repeated assertions
that the United States has become " the
world's greatest debtor nation," which is
"0It should be clear, of course, that this, meaningful
measure of real interest, which involves subtracting the
loss in the real value of debt due to inflation, cannot be
gotten by simply dividing nominal interest by a price
deflator. Deflators have rather to be applied to successive
nominal values of debt to calculate changes in its
real value. These changes are reflated to current prices
and subtracted from nominal interest payments. Alternatively,
the rate of inflation may be subtracted from
nominal rates of interest to calculate real rates of inter-
est which are in turn multiplied by the nominal value of
debt. This last measure, however, will not capture the
current gains or losses in the market value of debt
associated with changing expectations of future interest
rates.

" Using Office of Management and Budget data (U.S.
Government, 1988), I estimate the first adjustment, the
excess of federal investment expenditures over corresponding
capital consumption allowances, at about $70
billion. With an inflation rate of 3.6 percent and federal
debt held by the public at some $2,000 billion, the
second comes to another $72 billion. The federal budget
for fiscal 1988 is thus moved from its reported deficit of
$155 billion to a virtual balance!
12The second, inflation tax adjustment would also
reduce the measure of government dis-saving, or increase
government saving, but would correspondingly
decrease private saving or increase net foreign investment.
To the extent, because foreigners own some U.S.
government debt, our real obligations to the rest of the
world are reduced and properly measured net foreign
investment (as explained below) is increased, the inflation
tax adjustment also adds to gross saving.
'3Which adds to the trade deficit the net payments to
foreigners of property income, interest on government
debt, and transfers.

This despite some warning by the Bureau of Economic
Analysis. See, in particular, Jack Bame (1985).


### ---Economics-1989-0-08.txt---
generally construed to be a national and
international calamity.

Calamity or not, despite the official figures
which, for the end of 1987, show a negative
"international position" of the United States
of $368 billion, in terms of theoretically
meaningful measures the assertions are simply
not true. The market values of direct
investment of the United States abroad, having
been undertaken generally in the more
distant past, have appreciated much more in
the currencies of the countries in which the
investment took place than has the dollar
value of foreign investment in the United
States. They have gained enormously more
in dollar value from the fall in dollar exchange
rates since early 1985. Further, the
official accounts take Treasury gold holdings,
which are viewed in effect as an offset
to foreign central bank holdings of dollars,
at its old "statutory" value of $42 an ounce,
roughly a tenth of its value at world market
prices. Adjustments for the value of direct
investment and the value of Treasury gold
have been more than sufficient to wipe out
the entire U.S. "debt" to the rest of the
world.'5

It may be argued, with some validity, that
whatever our current net creditor or net
debtor position, continued current account
deficits will increase U.S. liabilities relative
to assets regarding the rest of the world. But
with proper measures even this judgment
must be hedged. If continued current account
deficits are not financed by foreign
central bank intervention, the dollar is likely
to fall further. As it does, the dollar value of
U.S. assets abroad will rise. This suggests an
adjustment to our measure of the current
account deficit if we are to keep it identically
equal to a true capital account surplus that
measures the increase in the value of foreign
capital in the United States net of increases
in the value of U.S. capital in the rest of the
world.

In order of magnitude, the differences between
conventional measures and those
which might relate more closely to relevant
theoretical constructs are frequently major.
My own estimates for a comprehensive " total
incomes system of accounts" (TISA) put net
national product in 1981 at 30 percent more
than its Bureau of Economic Analysis (BEA)
counterpart,'6 and the contrast is even more
striking when we turn to investment. BEA's
real gross private domestic investment in
1981 was less than one quarter of TISA real
gross capital accumulation (exclusive of capital
gains). Where BEA net private domestic
investment was 5.5 percent of BEA net national
product, TISA net domestic capital
accumulation in current dollars was 19.5
percent of TISA net national product.
Similar proportions show up in the measures
of capital stocks. Business nonresidential
structures and equipment, to which so
much attention is usually given on the ground
of its purported contribution to productivity,
amounted to only 11 percent of total TISA
capital, intangible and tangible, land and
"reproducible," in all sectors-government,
households, and nonprofit, as well as business.
Government and government enterprise
structures and equipment came to 8
percent of the total, household durables and
residential capital to 13 percent, and the
intangible capital of R&D, education, training,
and health to no less than 48 percent.'7
Intertemporal and international comparisons
of total saving, investment, and capital


### ---Economics-1989-0-09.txt---
may or may not look strikingly different
from the pictures presented of narrower bodies
of data. It may indeed be hard to know
until we have the fruits of some redirection
of economic research. But is it not clearly
these broader, conceptually relevant measures
on which we should be focusing in
analysis, evaluation, and the determination
of public policy?

III. New Behavioral Relations, Theory
and Policy

All of this is not just a matter of changing
the numbers we associate with familiar variables.
Rather, it alters old relations and reveals
important new ones. Take the wellknown
loglinear production function, for example.
In its most common Cobb-Douglas
form, we are accustomed to relating output
to private "capital" and "labor." David
Aschauer (1988) reports significant contributions
to private output from government infrastructure
capital. I have added arguments
for R&D and human capital and found both
significantly positive while the coefficient of
tangible capital is sharply negative.'8 I am
hardly ready to claim on the basis of these
single-equation estimates that more tangible
capital will generally reduce output, and take
my results with some reserve, but I suggest
that a broader view of capital would give
pause to enthusiastic advocates of business
investment tax credits and other "incentives"
of this genre.

Revision of the traditional consumption
function indicates that, along with the expected
coefficient in the neighborhood of .9
for labor income after taxes, we get a (too
high, admittedly) coefficient of about .3 for
the real net debt of the government and .06
for tangible capital. (Household tangible
capital, perhaps a proxy for permanent income,
has a coefficient of about .2.) Working
with investment functions, I find that the
acceleration principle explains much of business
investment in structures and equipment.
But increases in market output do not appear
to have generated subsequent accumulation
of the much larger amounts of other
capital which seem to have so much to do
with total output and productivity.
Measures of investment closer to their theoretical
construct would throw light on some
of the limitations of "q-theory," which purports
to relate business capital expenditures
to the ratio, q, of the market value of the
firm to the replacement cost of its "capital.
" When that ratio is high, it is presumed
that firms will sell shares or otherwise see fit
to raise funds to add to their fixed capital.
When q is low, it will rather pay to buy fixed
capital by buying other firms or simply to
use funds to buy back outstanding stock
rather than buy more expensive physical assets.


But does not this theory relate very largely
to a view of dichotomous labor and "capital"
as the factors of production? In truth, firms
invest much in the experience, skills, and
dedication of their labor, which then constitute
an important component of capital
value. Much of the value of the firm constituting
the numerator of the q-ratio is determined
by TISA components of capital that
do not enter into its replacement-cost denominator.
To the extent these were substitutes
for conventionally included capital,
they might even bring about a negative relation
between the conventional measures of
business investment and q. If all capital were
included, the values of q would likely be
vastly different and critical divergences between
average and marginal q exposed. The
remoteness of the relevance of a comprehensive
q to investment in the small spectrum
of business plant and equipment would
become clear, and with that an explanation
of some of the well-known difficulties in
empirical implementation of the q-theory
approach.19

18See Eisner (1989), Table D.I.
19Further difficulties in giving flesh to investment
and production theory may well stem from problems in
getting sound measures of real output. These depend
considerably on the quality of our price deflators, too
often-despite warnings from the BEA of the need for
further work on a number of problems, including measure
of quality change, weighting, and import and input
prices-taken for granted. If, as there is good ground to
suspect, they have frequently overstated price inflation
while correspondingly understating quality improveOne


### ---Economics-1989-0-10.txt---
of the more egregious examples of
misplaced concreteness of theoretical concepts
involves the role of "money" in many
recent macroeconomic models. The "money"
of the models is frequently a pure-breed of
uniquely perfectly liquid cash. It is somehow
increased or decreased by the government or
monetary authority without changes in the
quantities of other assets. Some theorists then
find that in certain ideal types of costlessinformation,
perfectly competitive, priceflexible,
market-clearing economies, changes
in the quantity of money have no real effects
and merely bring proportionate changes in
prices.20 Then, to my astonishment, some of
our colleagues derive implications for the
optimal paths of M-1 or M-2 or "monetary
policy."

That the various monetary aggregates
which enter into our empirical formulations
are heterogeneous collections of frequently
endogenous elements in a broad spectrum of
financial assets and liabilities seems, in some
quarters at least, frequently to escape attention.
The theoretical concept of assets of
"outside," government money is a far cry
from the multitide of largely private instruments
that enter into transactions and
wealth. To the extent that these are or can be
readily quantified, they are a poor fit indeed
to the money of much economic theory and
the relations in which it is embodied.
While monetary policy is bedeviled by
confusion with regard to measures and variables
of analysis, fiscal policy is one big
mess. And confusion here has contributed
mightily to misdirection in macroeconomic
theory and policy.

As I have pointed out elsewhere,2' motivation
for the macroeconomic, rational expectations
revolution, or counterrevolution,
stemmed at least in part from the perception,
as so attributed by Robert Lucas and
Thomas Sargent, that "massive government
budget deficits and high rates of monetary
expansion" in the 1970s were accompanied
not by the decreasing unemployment predicted
by Keynes, but by growing unemployment
and growing inflation.22 I have already
alluded to problems with measures of "money.
" The measured "massive budget deficits"
observed by Lucas and Sargent, and by so
many others, were simply not the deficits of
meaningful economic theory.

The significance of a deficit, any deficit, is
that it adds to net debt. When any agent
spends more than its income it must borrow
or sell off assets. When government runs a
deficit it must therefore add to private assets,
either in the form of holdings of government
debt or the assets government has liquidated.
It is this increase in private assets that, even
more clearly in neoclassical than Keynesian
theory, as noted many years ago by Gottfried
Haberler (1941), A. C. Pigou (1943,
1947), and Don Patinkin (1948, 1951, and
1965), induces an increase in private spending.


To have this effect, as the neoclassical
argument made clear, the increase in private
assets must be real. But if the government
"deficit" is accompanied by substantial and
rising inflation, the real, market value of
outstanding government debt may well decline
more than the amount of the nominal
deficit. In that case, those alleged deficits
become real surpluses, which economic theory,
Keynesian and neoclassical alike, indicate
should be a depressant to consumption
and, unless one has inordinate faith that
goods markets always clear, to real aggregate
demand and output.

That is exactly what happened in the late
1970s. The cumulative total of $153 billion
of nominal deficits in the Carter years, from
1977 to 1980, were real surpluses totaling
$72 billion. 24 The federal budget did not


### ---Economics-1989-0-11.txt---
move to significant real deficit until well into
1982. Should any theorist-or policymaker
-have been surprised that the economy evidenced
mounting unemployment leading up
to the mini-recession of 1980? Or should
they have been surprised that, with the addition
of very restrictive monetary policy, imposed
as " the only game in town" to combat
inflation given the supposed fiscal stimulus,
this led on to the deep recession of 1982?
Had appropriate measures been used to
illuminate previous policy, we might have
been better prepared to see the predictable
consequences of the very large real deficits
inaugurated in the latter half of 1982. We
might then have been all the more incredulous
of those who perceived a recovery from
the trough of a "real business cycle." 25 Who
indeed can really believe that the 12 million
new jobs, about which some have boasted,
were those of otherwise idle workers lured
by higher real wages-brought on, shall we
say, by lower oil prices or higher marginal
products stemming from sweeping new productivity
gains? Rather, it should have been
clear-dare I say "perfectly clear"?-that it
was the old-fashioned Keynesian stimulus of
real budget deficits that has contributed
mightily to cutting unemployment in half,
from its recession high of almost 11 percent.
There should indeed be some consternation
among those who find the current 5.3
percent unemployment as much as a full
percentage point below their misnamed
"natural" rate. Where, I might add, is that
supposedly excess-demand "accelerating inflation"
we were taught to fear? Perhaps
waiting to be confused again with the supply
shocks of a new war or oil cartel in the
Middle East!

IV. Provision for the Future: The Case
of Social Security

There is a perennial, legitimate concern in
any nation, and certainly in our mixed economy,
as to the allocation of resources to the
joys of today and the needs of tomorrow. It
is widely argued, however much, as I have
suggested, on the basis of improper measures,
that our presumed budget and trade
deficits will tarnish our golden years or contribute
undue and possibly unsustainable
burdens to our children and grandchildren.
Flowing from this argument have been a
number of proposals of new accounting for
our vast Social Security system. However
well-intentioned, they threaten to make our
fiscal mess all the worse.

There have, on the one hand, been suggestions
that we bring our huge "contingent
liabilities" into the general federal accounting
framework. We would, for example, include
the present value of expected payouts
to future retirees, net of their expected "contributions"
or tax payments, as federal debt.
Increases in this net debt would then add to
our measure of the deficit; decreases would
reduce it. At first glance, this would seem
sound. The budget would be truly "unified."
But would debt and deficits so measured
really match our theoretical constructs? Can
we expect agents to react in the same fashion
to uncertain and currently illiquid potential
receipts decades in the future as they would
to liquid current cash or Treasury bills? Must
we not recognize that few have any idea
what their Social Security benefits will be, or
what taxes they can expect to pay before
they receive them, whatever is written into
present, and very changeable legislation? Are
Martin Feldstein's changing estimates of net
Social Security wealth (1974, 1982) robust
arguments of anybody's consumption function?
26

In principle, contingent and uncertain liabilities
and assets should be taken into account
in macroeconomic analysis. My objection
to confusing them with the already
mis-measured components of the current
budget should not be taken to deny this. The
move in the opposite direction, though, to
declare Social Security trust funds "off budget"
and then ignore them in determining
appropriate fiscal policy is treacherous. Some
advise us now, for example, that a "balanced"
overall budget, despite the infirmities
I have pointed out in that measure, is not
25As it may be defined by E. C. Prescott (1986) and
others who have developed the concept.
26For a negative view on this see, among others,
Eisner (1983).


### ---Economics-1989-0-12.txt---
enough. We must rather balance the " budget"
exclusive of Social Security trust funds.
These latter will be showing increasing surpluses
over the years ahead until, by moderate
estimate, they will in the year 2030 attain
a total accumulation of $12 trillion. From
that point they would presumably be drained
down to support the final declining years of
the baby boomers of a few decades ago.
Is the fiction of putting what the government
collects from the public in a different
pot likely to prevent serious deflationary
consequences of years of major real budget
surpluses? Or would the equal fiction that
they are not federal spending avoid the inflationary
consequences of large payouts in
excess of revenues half a century from now?
Fiscal policy based on such measures can
indeed create havoc.

This is all the more so because of the
interaction of these faulty proposed measures
and others that I have cited. Provision
of generous real support to large cohorts of
future retirees by reduced cohorts of those
working may well prove a problem. Aside
from having more people working and fewer
retired, it can only be met by raising the
productivity of those working. That may be
accomplished in part by increasing their endowment
of business plant and equipment,
which dominates conventional measures of
investment, although it may be doubted that
much of current business capital expenditures
will have much directly to do with
productivity in 2030.27

I may add that neither good (Keynesian!)
theory nor empirical evidence suggests that
budget surpluses and reduced consumption
are likely to contribute to more business
investment of any kind. To be confident that
they would, one would have to believe that
the reductions in interest rates that they
might generate would increase investment
demand more than the reductions in consumption
and output would decrease it. The
reasons for doubt are of course to be found
in John M. Keynes, and their formal theoretical
formulation in Oscar Lange's (1938), (I
hope not forgotten) distinguished article of
half a century ago on the "Optimal Propensity
to Consume." Some decades of work on
investment functions and the role of output
and accelerator effects versus that of interest
rates and costs of capital, to which I confess
to having contributed my share,28 would
seem to offer ample empirical support for
that doubt.

But fuller and more appropriate measures
of investment and productivity might make
clear that the best and perhaps the only
feasible way to provide the sustenance of our
future aged is to develop our public, social
infrastructure and endow our young with all
of the education, training, research output,
and good health that our society is capable
of offering. One or two more generations
with large proportions of illiterates and
semiliterates will hardly produce a labor
force adequately equipped for productive
employment in the 21st century. This suggests
to me that the Social Security surpluses
should be invested in education and training,
research and development, public health, and
the public infrastructure which is the necessary
foundation for private production. If we
had federal budget and national accounting
measures that properly classified all of this
vital capital accumulation, the choice of wise
public policy, and the economic analysis on
which it would build, might be much easier.
V. Conclusion

Economic science can boast of many advances
over the years. Too many sins of
policymakers have been falsely attributed to
a profession whose counsel has too frequently
been compromised or ignored. I am
fond, to illustrate, of comparing the challenge
to prescribe a (painless?) cure for inflation
to the cry of the patient who comes to
the doctor complaining of myriad ills and
says "Doc! Make me well, but don't tell me
to exercise, lose the hundred pounds that I


### ---Economics-1989-0-13.txt---
am overweight, cut down my drinking, or
quit smoking." We cannot very easily cure
inflation if we are told that we cannot touch
import quotas and tariffs, price supports, a
variety of entrenched monopolies and anticompetitive
and other costly regulative interventions
in the economy. But we can pride
ourselves on rigorous application of our often
finely honed tools of analysis and our
ability to offer sound policy advice, whether
it is followed or not.

So I would not want my remarks this
evening to be taken as a mea culpa, for
myself or for our profession. But we are not
perfect, and I have endeavored to alert
us-or recall our attention to-certain critical
failings that keep us further from perfection
than we should be.

I have hence noted some pitfalls in using
current and past variables in analyses that
depend critically upon expectations of the
future. I have warned of confusion in employing
narrowly defined measures of income
and product in evaluating flows and
trends in comprehensive earnings and output.
I have argued that particularly large
dangers abound in basing policy on conventional
measures of private and public saving,
investment and capital. I have suggested that
usual estimates of some of the critical behavioral
relations of macroeconomics may be
suspect because of a failure to match theoretical
constructs with appropriate empirical
counterparts.

Very generally, I conclude, it is important
in economics-as elsewhere-to know what
we are talking about.29

29I should not close without suggesting that of complimentary
importance is having the data to talk about.
At the risk of increasing that budget deficit, however we
measure it (or reducing the surplus), it would seem
appropriate, in line with the report of the American
Economic Association Committee on Federal Statistics
(see Thomas Juster, 1988), to give the BEA and other
statistical agencies the resources to develop more of the
measures appropriate for economic analysis.
 ## Economics-1990-0


### ---Economics-1990-0-01.txt---
The federal income tax has been under
attack by the economics profession for more
than a decade. The attack comes from two
directions: supply-siders who believe that
progressive income taxation impairs economic
incentives,' and more traditional
economists who would substitute a progressive
expenditure tax for the income tax.2 At
one time, support for the expenditure tax
was confined to a few members of our profession,
including such distinguished names
as John Stuart Mill, Irving Fisher, Nicholas
Kaldor, and James Meade. Today, it is fair
to say that many, if not most, economists
favor the expenditure tax or a flat rate income
tax. This group has joined the opponents
of progressive taxation in the attack on
the income tax.

Despite an incessant barrage from both
groups, no country in the world is planning
to abandon the income tax or is even considering
a personal expenditure tax. A wave of
tax reform, beginning with the U.S. reform
in 1986, has been sweeping the world, aimed
at improving the income tax, not at eliminating
it. Tax preferences formerly regarded as
sacrosanct are being removed and there is a
distinct movement toward comprehensive income
taxation.3 However, individual income
tax rates are being cut, tax progressivity has
been declining almost everywhere, and reliance
on the income tax has been diminishing.


It will come as no surprise to this audience
that I approve of the base-broadening feature
of the current tax reform movement,
but I believe that the reduction in the redistributive
effect of the income tax has gone
too far. In this paper, I shall show that the
progressivity of the U.S. tax system-never
very pronounced, except during and immediately
after the two world wars-has been
declining for more than two decades and
that the Tax Reform Act of 1986 reversed
this decline, but only slightly. Consequently,
we have a long way to go to improve the
equity of the tax system. I believe this can be
done without punitive tax rates that will hurt
economic incentives.

I begin with a brief review of recent
changes in the U.S. distribution of income
and follow this with an analysis of the effect
of taxes on the income distribution. I next
examine arguments for and against the income
tax, with particular emphasis on its
effects on economic incentives and its merits
when compared with the expenditure tax. I
then evaluate the income tax as it emerged
from the 1986 tax reform and conclude with


### ---Economics-1990-0-02.txt---
an agenda for further reform in the context
of the current fiscal crisis. I believe that,
when the nation gets around to eliminating
or substantially reducing the federal deficit,
the income tax should play an important
role.4

Distribution of Income and Tax Burdens
It is well known that, after several decades
of relative stability, the U.S. pre-tax income
distribution has become much more unequal
in the last ten years. Official statistics understate
the increasing inequality. At the same
time, the tax system as a whole-and the
income tax in particular-has become less
equalizing, so that the trend toward inequality
is even more pronounced after tax than
before tax.

Distribution of Income. The longest continuous
and comparable income distribution
series available to us comes from the annual
Current Population Survey (CPS) of the
Census Bureau. The figures show that the
share of total income received by the highest
fifth of the nation's families fell from 1948 to
1952, remained unchanged between 1952 and
1981, and then rose from 1981 to 1988. By
1988, the share of the top fifth was the
highest ever recorded. The figures for the top
5 percent are similar, except that their share
in 1987 had not quite recovered to the 1952
high (Table 1).

It is well known that very high incomes
are virtually unrepresented in the CPS distribution
and that official census statistics
greatly understate income inequality in any
year. What is not recognized is that the CPS
data greatly understate the increase in inequality
that has occurred during the 1980s
because very high incomes have been increasing
much faster than the incomes in the
lower part of the distribution.5 This can be
TABLE 1-BEFORE-TAX INCOME SHARES, CENSUS DATA,
SELECTED YEARS, 1948-1988, PERCENT.
Top 5 percent Top 20 percent

Year of families of families

1948 17.1 42.4

1952 17.4 41.5

1957 15.6 40.4

1962 15.7 41.3

1967 15.2 40.4

1972 15.9 41.4

1977 15.7 41.5

1981 15.4 41.9

1987 16.9 43.7

1988 17.2 44.0

Source: Bureau of the Census. Income includes transfer
payments (for example, Social Security benefits, unemployment
compensation, welfare payments, etc.) but
excludes capital gains. Distribution includes only families
and excludes single persons living alone.
seen by examining changes in the shares of
the top income recipients reported in the
annual Statistics of Income published by the
Internal Revenue Service (Table 2).6
Like the CPS data, the tax data show that
the very rich in the United States-defined
as either the top 1 percent or the top 5
percent of the income distribution-enjoyed
about the same income increases as the average
income recipient in the 1950s, 1960s, and
1970s, but their share of total income has
been rising in the 1980s. From 1952 to 1981,
the share of the top 1 percent of the tax
units remained in a very narrow range-between
8 and 9 percent of the total income
reported on tax returns. Since 1981, their
share has skyrocketed to 14.7 percent in
1986. The same trends are shown by the top
2, 5, 10, and 15 percent of the tax units.
Much of the increase in the share of the
top tax units reflects the large increase in
realized capital gains that accompanied the
bull market of the 1980s. But salaries and
other incomes of the top units have also
been increasing faster than average.7 In fact,
4See Musgrave (1989) for a statement of similar
views.

5Using Pareto distributions based on income tax
data to approximate the upper tail of the U.S. distribution,
Rudy Fichtenbaum and Hushang Shahidi (1988)
calculated that the CPS underestimation of the Gini
coefficient rose from 1.7 percent in 1967 to 7.6 percent
in 1984.

6For the method of calculation, see Pechman (1989),
ch. 1.

7According to Statistics of Income, the shares of adjusted
gross income other than capital gains increased
from 6.59 percent in 1981 to 7.64 percent in 1986 for


### ---Economics-1990-0-03.txt---
TABLE 2-BEFORE-TAX INCOME SHARES, TAX DATA, SELECTED YEARS,
1948-1986, PERCENT

Top 1 percent Top 2 percent Top 5 percent Top 10 percent Top 15 percent
Year of Tax Units of Tax Units of Tax Units of Tax Units of Tax Units
1948 9.8 13.4 20.2 27.9 34.3

1952 8.7 12.1 18.7 26.7 33.4

1963 8.8 12.3 19.4 28.2 35.5

1967 8.8 12.3 19.6 28.3 35.5

1972 8.0 11.4 18.7 27.8 35.4

1977 7.8 11.3 18.9 28.3 36.1

1981 8.1 11.5 19.0 28.6 36.5

1986 14.7 18.2 26.6 36.8 45.1

Source: Statistics of Income. Income excludes transfer payments, but includes realized
capital gains in full.

the movement toward inequality must have
been even greater than the tax data show
because they do not include the large
amounts of income taxpayers were able to
shelter before the enactment of the Tax Reform
Act of 1986.

Many economists and statisticians have
examined these trends, but nobody has been
able to explain them fully. The declining
share of incomes received by the lower income
classes has been attributed to the increase
in the number of single-parent families,
slow growth in earnings of production
workers, the disappearance of middleincome
jobs, and other factors.8 But these
explanations do not account for the recent
explosion of earned and property incomes of
those in the top tail of the distribution.
The trend toward greater inequality has
developed despite the existence of an income
tax in the United States for seventy-six years
and of an estate tax for eighty years. Clearly,
the tax system never reduced inequality very
much and other forces in the 1980s have
swamped whatever equalizing effect it may
have had earlier. I turn now to an examination
of the burdens imposed by the tax system
and how they have affected the distribution
of income after tax.

Distribution of Tax Burdens. I have been
estimating federal, state, and local tax burdens
by income classes for the last two
decades on the basis of the Bookings
MERGE files.9 These files are based on the
CPS surveys, modified at the top by the
incomes reported on federal individual income
tax returns. As shown in Table 3, the
tax burdens of the bottom 90 percent of the
income distribution did not change very
much from 1966 to 1985. By contrast, the
tax burdens of the top ten percent of income
recipients fell, especially those of the top 5
percent and 1 percent. Effective tax rates of
the top 5 percent dropped by one-fifth between
1966 and 1985 (from 32.7 percent to
26.0 percent); for the top 1 percent, the
reduction was more than one-third (from
39.6 percent to 25.3 percent).
Tax burdens of the highest income recipients
fell because top federal individual tax
rates were reduced throughout this period,
from 70 percent in 1966 to 50 percent in
1985. Furthermore, the federal corporation
income tax dwindled to relative obscurity,
falling from 4.1 percent of GNP in 1966 to
1.6 percent in 1985. The proliferation of
personal deductions (for example, state and
local taxes, interest payments, and IRAs),
tax-exempt bonds, and tax shelters were also
major factors in the reduction of the tax
burdens in the top part of the income distribution.
The reduction in the corporate tax
the top one percent of tax units, from 17.31 to 18.77
percent for the top 5 percent, and from 35.11 to 36.56
percent for the top 15 percent.
8See, for example, Frank Levy (1988) and Sheldon
Danziger, Peter Gottschalk, and Eugene Smolensky
(1989).

9See Pechman and Benjamin Okner (1974) and Pechman
(1985).


### ---Economics-1990-0-04.txt---
reflected primarily the investment incentives
introduced in the 1960s and liberalized in
the 1970s and 1980s, as well as a reduction
in the profitability of the corporate sector.10
Since 1985, the distribution of tax burdens
has changed largely because of the enactment
of the landmark Tax Reform Act of
1986. This act increased the progressivity of
the tax system, most notably by raising the
personal exemptions, standard deductions,
and the earned income credit, and by shifting
about $25 billion of tax annually from
individuals to corporations. However, this
change in tax policy restored only a small
fraction of the progressivity lost in the preceding
two decades. At the very top of the
income distribution, the 1986 federal tax reform
restored about half the reduction in
effective tax rates between 1980 and 1985,
but left them far below the 1966 levels: the
top 1 percent paid only 26.8 percent in taxes
in 1988 as compared with 39 percent in 1970
(Table 3).

The inescapable conclusion from these
figures is that the well-to-do in our society
had very large reductions in tax rates in
recent years, while the tax rates at the low
and middle income levels have not changed
much. Since the before-tax distribution has
become much more unequal in the 1980s, it
follows that inequality has increased even
more on an after-tax basis.'1

Transfer Payments. The other major element
of government policy affecting the distribution
of income is the system of transfer
payments, or negative taxes. This system inTax-Transfer


### ---Economics-1990-0-05.txt---
Table

50-

c~~~~~~~~~~ - - - - - -- - -7 7 -7
a) ....

C !

a -50- .,LEGEND

co - - - -

X, L,taxes (Variant lc)

? : f ........~~~~~~~~~~~~~~~~---------
transfers

.~~~~~~~~~~~~~~~~~~~~~~~ .............
taxes mnus

-100* transfers

0 50 100

Population Percentile

FIGURE 1. FEDERAL, STATE, AND LOCAL TRANSFERS AND TAXES AS A PERCENT
OF MARKET INCOME BY INCOME PERCENTILE, 1985
cludes programs of public assistance that are
designed explicitly to help the poor, but it
also includes others that are not designed
primarily for this purpose (for example, retirement
and unemployment benefits and

health insurance). To evaluate the impact of
the tax-transfer system on the distribution of
income, cash and in-kind transfers must be
added to market incomes while taxes are
deducted.

While I cannot separate the effects of
transfer payments and of taxes on the recent
changes in the after-tax income distribution,
12 a snapshot for a recent year-
1985-suggests what happened (Figure 1).
When family units are arrayed by their incomes
from market production (wages,
salaries, interest, dividends, etc.), the U.S.
tax system is only mildly progressive. On the
other hand, transfer payments are highly
progressive. Taxes in 1985 were regressive in
the lowest deciles and proportional thereafter,
while transfer payments declined from
over 200 percent of market incomes in the
lowest decile to 1.4 percent in the highest.
On balance, families in the lowest three
deciles received more in transfers than they
paid in taxes, while those in the top seven
deciles paid more in taxes than they received
in transfers.

Clearly, the tax-transfer system is progressive,
mainly because of transfers, not taxes.
What we are doing in the United States is
financing redistributive transfers with taxes
that are roughly proportional to incomes.
Moreover, the tax system has been getting
12The income distributions used in the Brookings
MERGE file for 1966 were prepared on the basis of
incomes including transfer payments. After so many
years, it is impractical to reclassify the income and tax
data for 1966 by income excluding transfer payments.


### ---Economics-1990-0-06.txt---
less progressive in the last two decades, while
the ratio of transfers to income has been
increasing.-3 In other words, the recent increases
in transfer payments in the United
States have been financed by the low and
middle income groups, while the rich have
been getting tax cuts.

What Role for the Income Tax?

Most people support tax progressivity on
the ground that taxes should be levied in
accordance with ability to pay, which is assumed
to rise more than proportionately with
income. Economists have long had trouble
with the "ability to pay" concept. In recent
years they have revived the old notion that
consumption measures ability to pay better
than income does. I believe that the person
in the street is right and that we should
continue to rely on the income tax to raise
revenue in an equitable manner.
Ability to Pay. In the latter half of the
nineteenth century, progressive income taxation
was justified by "sacrifice" theories that
emerged from discussions of ability to pay.
Under this doctrine, ability to pay is assumed
to increase as incomes rise, and the
objective is to impose taxes on a basis that
would involve "equal sacrifice" in some
sense. If the marginal utility of income declines
more rapidly than income increases
and the relation between income and utility
is the same for all taxpayers, equal sacrifice
leads to progression."4 Whether or not one
believes in sacrifice theory, the ability to pay
idea has been a powerful force in history and
has doubtedly contributed to the widespread
acceptance of progressive taxation.15 Young
has found that the equal sacrifice model fits
most U.S. tax schedules in the postwar period,
with the notable exception of the
schedule adopted in 1986. Similar results
hold for Italy, West Germany, and Japan.16
Henry Simons vigorously attacked sacrifice
theory although he argued strongly that
the purpose of the progressive income tax is
to reduce economic inequality.17 Simons was
vague on how far progression should be
pushed, but he clearly felt that it had not yet
gone too far in most countries. His prescription
was the pragmatic one that the tax rates
should not impair economic incentives. In
his policy statements, he argued in favor of a
broad base and graduated rate schedule that
rises to a maximum of 50 percent.18
I agree with Simons that the income tax
should be used to reduce the great disparities
of welfare, opportunity, and economic power
arising from the unequal distribution of income.
I also recognize that this view is not
widely held and has probably not been the
major rationale for income tax legislation in
the United States or in most other countries.
The income tax is widely used primarily
because it raises large amounts of revenue in
a moderately progressive way. Recent income
tax reforms have concentrated mainly
on eliminating tax preferences to improve


### ---Economics-1990-0-07.txt---
horizontal equity; where income tax rates
had been pushed to very high levels, they are
being moderated. Curiously, the world appears
to be moving toward a consensus on
the Simons' view that the income tax should
be levied on a broad base with graduated
rates reaching a maximum of 50 percent or
less, though not for his reasons.
Economic Incentives. The effects of the
progressive income tax on incentives to work
and to save are hard to measure. As is well
known, the substitution and income effects
of taxation work against each other, and the
net result cannot be predicted.
Sample surveys have revealed that professional
personnel do not vary their hours of
work in response to high tax rates."9 However,
recent econometric studies suggest that
the pre-1986 income and payroll taxes reduced
the work effort of primary earners in
the United States by about 8 percent, while
secondary earners-who have a greater opportunity
to vary their labor input-reduced
their work effort by as much as 30 percent.20
According to this approach, the 1986 reform
may have increased labor supply of married
men by only 1 percent and of married women
by less than 3 percent, largely because the
marginal tax rates of most workers were not
reduced very much.2" Burtless estimates that
the Reagan tax and transfer policies increased
average annual taxes of men aged
25-54 by no more than 2-4 percent and of
women in the same age group by no more
than 3.5 percent.22

Historical trends in U.S. labor supply are
not consistent with the finding that taxes
have reduced work effort. Adult males have
been reducing their labor supply over the
last forty years, largely through earlier retirement
little of which is the effect of tax rates.
The labor force participation of women has
risen sharply in recent years, despite high
marginal rates resulting from the requirement
that married couples must file joint
returns to benefit from income splitting.
Studies in other countries are not reliable
enough to support conclusions about the relationship
between taxes and labor supply.
The effect of taxes on saving is even more
ambiguous. A few studies claim that they
have found a significant response to an increase
in the real after-tax return on saving;
others find that the response, if any, is close
to zero.23 The reduction in the personal saving
rate in the United States in the 1980s
confounded most economists in view of the
reductions in the marginal tax rates, the incentive
provided by individual retirement accounts
(IRAs), and the high real interest
rates, all of which should have increased the
incentive to save.

The strongest conclusion one can draw
from the available evidence is that the incentive
effects of taxation have been relatively
small. Yet the supply siders were convinced
that the incentive effects were so large that
rate cuts would increase revenues when tax
rates- are reduced.24 U.S. tax rates were cut
sharply in 1981 and 1986, but these cuts had
little effect on labor supply and no effect on
saving. Under the circumstances, so long as
tax rates are not pushed to punitive levels,
incentive considerations do not justify neglect
of the distributional objective of tax
policy.

Income vs. Expenditure Tax. The revival
of interest in the expenditure tax can be
traced to the difficulties of taxing income
from capital under the income tax. However,
economists and tax lawyers have also found
19See Break (1957) and Holland (1969).
20These estimates are based on a comparison with
work effort under a system of lump-sum taxes. The
estimates would be reduced substantially if the basis of
comparison were a proportional income tax. For summaries
of recent studies and their implications for policy,
see Bosworth (1984), ch. 5 and Gary Burtless and
Robert Haveman (1987). The methodology of these
studies is explained by Hausman (1987).
21See Hausman and Poterba (1987).
22See Burtless (1989).

23' ,3.For a review of the various studies, see Evans
(1983) and Bosworth (1984), chaps. 3 and 5.
24 President Ronald Reagan was one of those who
belieed that tax rate cuts would actually increase revenues
(see Reagan, 1981, p. 710). The supply-side view
is defended in Raboy (1982). For an analysis of this
view, see Meyer (1981).


### ---Economics-1990-0-08.txt---
efficiency reasons to prefer the expenditure
tax and these need to be addressed. S
A basic difference between the income and
expenditure taxes is in the time perspective
of the two taxes. The perspective of the
income tax is relatively short run-a year or
several years to allow for short-run income
fluctuations. Consumption is more stable
than income and is alleged, therefore, to be a
better measure of long-term well-being. In
fact, under certain simplifying assumptions,
the bases of taxes on the discounted present
value of income and expenditure are the
same over a lifetime. Assuming perfect capital
markets, constant discount rates that apply
equally to all people under all circumstances,
tax rates that are constant and
proportional, and no gifts and bequests, the
present values of lifetime expenditures of
people with the same (discounted) lifetime
incomes are the same regardless of when the
incomes are consumed.26

Advocates of the expenditure tax regard
the lifetime perspective as a major advastage
because it permits them to pretend that taxing
consumption is equivalent to taxing personal
endowments. A tax on endowments, if
they could be measured, would avoid the
distortionary effects of either an income tax
or an expenditure tax. If it is assumed that
lifetime consumption approximates exidowment,
then taxing consumption at flAtXand
constant rates treats equally all taxpayers
with the same endowment.27 The totaly' unrealistic
assumptions underlying this line of
reasoning strain credulity, but it doestteem
to lie behind the strong support for thQ expenditure
tax by many economists.

The lifetime perspective has little merit
even without the endowment rationale. In
my view, it is difficult enough to measure
economic circumstances over relatively short
periods. Taxation of lifetime consumption
(or income) hardly seems appropriate in a
world of changing tax rates, substantial family
instability, economic and political change,
and uncertainty. Except for the attractiveness
of the arithmetic, lifetime economic
circumstances as measured by discounted
lifetime incomes or consumption cannot be
regarded as satisfactory indexes of ability to
pay.28 Moreover, taxation of annual consumption
expenditures at graduated rates
would destroy the identity of lifetime taxes
of taxpayers with the same (discounted) lifetime
incomes.

The expenditure tax is alleged to be superior
to the income tax on the additional
ground that the income tax reduces the return
on saving and therefore encourages current
as against future consumption. Even if
saving remained unchanged, the distortion
generates a welfare loss for consumers. It has
been pointed out by many economists that
this effect must be balanced against the welfare
cost of further distorting the choice
between labor and leisure. There is no theoretical
basis for judging whether the welfare
gain from eliminating the intertemporal distortion
of consumption would exceed the
welfare loss from increasing the intratemporal
distortion of the labor-leisure choice.29
A tax that omits saving from the tax base
can be shown to be the same as a tax applying
only to labor income and exempting all
property income.30 Several expenditure tax


### ---Economics-1990-0-09.txt---
advocates have, in fact, proposed a tax on
labor income on grounds of simplicity and
administrative feasibility.3" Most people
would be appalled by a proposal to substitute
a wage tax for income tax, yet that is
essentially what expenditure tax proponents
are advocating.

Many economists are attracted to the expenditure
tax because it would not tax income
from capital and would thus eliminate
all the income tax problems arising from the
use of the realization principle for calculating
capital gains and losses and from the
accounting conventions for inventories, depreciation,
and depletion used in arriving at
net business profits. There would also be no
need to adjust the tax base for inflation, as
consumption would be measured appropriately
in current dollars. These are serious
problems for income taxation and I shall
deal with them later, but it would be unfortunate
to abandon the income tax for administrative
and compliance reasons alone.

The transition from the income tax to an
expenditure tax would be troublesome. The
retired elderly would draw down assets, some
of which had previously been taxed under
the income tax, to finance current consumption
that would be taxed yet again. To avoid
this double tax, some method would need to
be devised to identify consumption from
previously taxed accumulations. Grandfathering
all assets at the time an expenditure
tax is initiated would leave a big loophole
for people with large amounts of
untaxed accrued capital gains. But I have
not seen any practical method of making the
necessary distinctions in order to prevent
wholesale tax avoidance and to achieve
equity.32

Under an expenditure tax, taxpayers who
save could accumulate large amounts of
wealth over a lifetime. Many, but by no
means all, expenditure tax advocates support
wealth or estate and gift taxes to prevent
excessive concentrations of wealth. But the
history of transfer taxation in this country
and abroad provides little assurance that
effective death and gift taxes would be levied
to supplement an expenditure tax.
Proponents of expenditure taxation often
compare the merits of a comprehensive expenditure
tax with the income tax as it has
developed. It is hard to believe that an expenditure
tax would be enacted without numerous
exemptions and exclusions. In fact,
most of the eroding features of the income
tax (for example, preferences for housing,
fringe benefits, child care, state-local borrowing,
etc.) might be carried over to the expenditure
tax. Thus, an expenditure tax is no
less immune to erosion than the income tax
and, in such circumstances, it loses much of
its attractiveness.

I conclude that income is a better indicator
of ability to pay than consumption and
that the major upheaval of substituting an
expenditure tax for an income tax cannot be
justified on theoretical or practical grounds.
How Much Progression? The effective degree
of progression of the income tax depends
on the comprehensiveness of the tax
base as well as on the tax rates. We have
learned from experience that high, graduated
tax rates do not assure progressivity of the
income tax. For most of the period since the
end of World War II, the top U.S. income
tax rates were 70 percent or higher. Yet little
equalization resulted because of the erosion
of the base of the individual and corporate
income taxes and because of increases in the
payroll tax for Social Security.33 According
the second, then

Si = WI - Ci

C2 = w2 + (1 + r )s

cl(l + r) +c2 =(1 + r)(wi-sl) + w2+ (1 + r)sl
cl(1+ r)+c2= (1+ r)w + w2.

31See Hall and Rabushka (1985), Bradford (1986),
ch. 14, and McLure, Mutti, Thuroni, and Zodrow (1988),
ch. 9.

32For a discussion of these problems and possible
solutions, see Aaron and Galper (1985), pp. 78-79 and
99-103, and U.S. Department of the Treasury (1977),
chaps. 4 and 6.

33The combined employer-employee payroll tax rose
from 6 percent in 1960 to 12.26 percent in 1985 and


### ---Economics-1990-0-10.txt---
to estimates of the Congressional Budget
Office (CBO), in 1977, when the top income
tax rate was 70 percent and the general
corporate tax rate was 48 percent, the Gini
coefficient of inequality was 0.4502 before
tax and 0.4185 after federal taxes (more than
half of which were individual and corporate
income taxes). In 1980, when the top tax rate
was still 70 percent (though only on unearned
income) and the corporate rate was
46 percent, the Gini coefficient was 0.4627
before tax and 0.4320 after the same taxes.
Thus, as measured by the Gini coefficient,
the equalization achieved by the federal tax
system declined from a modest 0.0317 points
(7.0 percent) in 1977 to an even more modest
0.0307 points (6.6 percent) in 1980 (Table 4).
Two major pieces of tax legislation were
enacted during the 1980s. One increased inequality,
the other reduced it. The 1981 Act,
enacted after Ronald Reagan's sweeping victory
in the 1980 presidential election, increased
inequality by reducing income tax
rates by 23 percent across the board (with a
top rate on ordinary income of 50 percent),
lowering the capital gains rate to a maximum
of 20 percent, introducing generous
deductions for individual retirement accounts,
and providing very liberal depreciation
allowances for business investment on
top of the previously enacted investment tax
credit. The Tax Reform Act of 1986 reduced
inequality by increasing personal exemptions
and the standard deduction, equalizing the
tax rates on capital gains and ordinary income,
and closing numerous loopholes. At
the same time, income tax rates were reduced
to a maximum of 33 percent on individuals
and 34 percent on corporations.
Despite the large rate cuts at the top of
the income scale, the 1986 act increased income
tax progression, though not to the 1980
level. By 1984, the equalization provided by
the federal tax system had declined to 0.0184
points (3.8 percent) in terms of the Gini
coefficient. As a result of the 1986 act, the
degree of equalization increased to 0.0218
points (4.4 percent) in 1988 (Table 4). While
this change is modest, it is noteworthy as the
first movement toward greater income tax
progressivity at least since 1964, when the
Kennedy-Johnson tax cut was enacted.
I suggest that a minimal goal of federal
tax policy in the next several years should be
to restore the equalization achieved by the
federal tax system in the mid-1970s.34 While
this may appear to be a modest goal, it turns
out to be a rather ambitious undertaking,
particularly if income tax rates are to be
kept to moderate levels. Before calculating
the tax rates, it is necessary first to establish
the appropriate tax base for a modem income
tax.

Reform of the Income Tax

The proper base for the income tax was
described fifty years ago by Henry Simons,
who argued that it should conform with an
economic definition of income.35 Admittedly,
the use of a comprehensive income tax contradicts


### ---Economics-1990-0-11.txt---
the principle of optimal taxation that
tax rates should vary with a number of elasticities.
However, the optimal tax models are
based on strong assumptions that are often
implausible or virtually impossible to validate.
Consequently, there is no empirical
basis for determining how different commodities
and sources of income should be
taxed. Moreover, the compliance and enforcement
costs of such a system could be
large enough to more than offset the potential
inefficiencies of a uniform tax. In the
absence of reliable data, it is safer to rely on
the comprehensive approach rather than to
introduce tax differentials that will generate
their own distortions.36

According to Simons and others, income
is the sum of an individual's consumption
and change in net worth during a particular
time period. For a long time, the federal
income tax base was a far cry from a comprehensive
definition of income. In 1986,
however, Congress reversed its previous
practice and enacted a wholesale tax reform
that moved the income tax a long way toward
the Simons' ideal. This remarkable
piece of legislation can provide the basis for
achieving the distributive objectives discussed
earlier with moderate tax rates.
The 1986 Tax Reform.37 The Tax Reform
Act of 1986, a major step toward comprehensive
income taxation, greatly improved
the fairness and efficiency of the tax system.
The major accomplishments of the act are as
follows:

By doubling personal exemptions and increasing
the standard deduction, the act relieved
about 5 million poor people from
paying any income tax. This step restored
the principle (abandoned by Congress in
1978) that people who are officially designated
as "poor" should not be required to
pay income tax. The principle was perpetuated
by the resumption in 1989 of an automatic
annual adjustment of the exemptions
and standard deduction for inflation.
Significant increases were made in the
earned income credit for wage earners with
families. These increases eliminated almost
the entire Social Security payroll tax (including
the employer's share) for those eligible
for the full credit and reduced the tax burden
for many low-income workers.

For the first time since 1921, realized capital
gains were made taxable as ordinary income.
This is the keystone of comprehensive
tax reform: it reduces the incentive to convert
ordinary income into capital gains and
removes one of the major elements of tax
shelter arrangements. Moreover, this change
made it possible to reduce tax rates without
reducing the progressivity of the income tax.
A good start was made to reverse the
erosion of the individual income tax base.
For example, unemployment benefits, which
were previously taxable only if a married
taxpayer's income exceeded $18,000 ($12,000
if single), were made taxable regardless of
the size of income. Deductions for state and
local sales taxes were eliminated and those
for consumer interest were phased out. For
administrative reasons, deductions for unreimbursed
business expenses, costs incurred
in earning investment income, and other
miscellaneous expenses were allowed only to
the extent that they exceed a floor of two
percent of income.

Some of the most egregious loopholes and
special tax benefits were eliminated. Many
tax shelters were rendered unprofitable by
denying deductions for losses from passive
activity against income from anything but
passive activities.3' Tax subsidies for borrowing
(other than for mortgages) were eliminated
by another limitation on the deduction
for interest expenses to the amount of
investment income reported on the individual'
s tax return.39 Deductions for contribu-
36See Stern (1987), p. 51, Aaron (1989), pp. 10-12,
and Slemrod (forthcoming).

37For an analysis of the structural features of the
1986 tax reform, see Pechman (1987a).
38A passive activity is a trade or business in which the
taxpayer (or spouse) does not materially participate. All
rental activities are regarded as passive.
39However, the act did not change the deductibility
of interest on borrowing to finance business invest-
ments.


### ---Economics-1990-0-12.txt---
tions to individual retirement accounts were
curtailed. Deductible business expense accounts
for meals, travel and entertainment
were limited to 80 percent of outlays. Tax
preferences benefiting defense contractors,
banks, oil companies and other industries
were narrowed. On top of all these changes,
the minimum tax for both individuals and
business was retained and strengthened.
Finally, the individual and corporate tax
rates were cut drastically. Under the individual
income tax, two rates-15 and 28 percent-
were substituted for the earlier schedule
of 14 rates, which rose to a maximum of
50 percent. However, the benefits of the lowest
rates and of the personal exemptions
were phased out for higher income taxpayers
at a 5 percent rate. As a result, the new
individual income tax rate structure has four
brackets, with rates of 15, 28, 33, and 28
percent (see Table 7). The general corporate
rate was cut from 46 percent to 34 percent.
Despite these large rate cuts, the act was
expected to be roughly revenue neutral in
total over the first five years, but to shift
about $25 billion of tax annually from individuals
to corporations.

The distributional effect of the 1986 act is
distinctly progressive, especially if the increase
in corporate tax liabilities is taken
into account. I have calculated the change in
average effective tax rates of the nation's
families on the basis of the distribution of
income estimated from the Brookings
MERGE file (Table 5). Total federal tax
burdens decline in the lower nine deciles and
rise in the top decile. In the lower deciles,
the tax reductions result from increases in
the personal exemptions, standard deduction,
and the earned income credit under the
individual income tax. The increases at the
top reflect the broadened individual income
tax base, as well as the increase in corporate
tax liability, which is assumed to fall on
owners of capital in these calculations. However,
as already noted, this increase in progressivity
only partially reversed the reductions
that had taken place in the 1970s and
early 1980s.

The Unfinished Agenda. Despite the progress
made in 1986, the federal income tax in


the United States falls considerably short of
the comprehensive income target.40 I assume
that we shall continue to tax capital gains on
a realization, rather than accrual, basis, and
that gifts and bequests will be taxed under a
separate transfer tax. Nevertheless, a great
deal more could be done to broaden the tax
base for equity, efficiency, and revenue reasons.


The personal exemptions, standard deductions,
and rate bracket limits are adjusted
annually for inflation, but the tax base is
not. Of the two types of adjustment, adjustment
of the tax base would be by far the
more important. Perhaps the major reason
why the income tax tends to be in disrepute
is the discrimination against capital income
inherent in a nominal income tax. An inflation
adjustment of asset prices should be


### ---Economics-1990-0-13.txt---
incorporated in the tax law as part of the
computation of real capital gains and losses,
real interest income expense, and real inventory
and depreciation allowances. The adjustment
of interest is admittedly difficult,
but the widespread use of computers should
ease the administrative and compliance
problems.

Restoration of a tax differential between
capital gains and ordinary income should be
resisted at all costs. Equalization of the tax
rates lowers the incentive to convert other
income into capital gains, simplifies business
and financial decisions, and reduces income
tax complexity. Aside from the correction
for inflation, the one additional reform
needed in the capital gains tax is to include
in the tax base unrealized capital gains
transferred by gift or at death. Taxing such
gains would reduce the lock-in effect of the
tax on transfers of assets and eliminate a
source of horizontal inequity.
A major neglected problem in most countries
is the erosion of the tax base from the
exclusion of employee fringe benefits. Trade
unions, as well as employers, staunchly defend
the continued exclusion of fringe benefit
income, but in fact the largest subsidies
go to the highest paid employees. Loopholes
for union members and other workers are no
more defensible than those for the rich. Taxation
of fringe benefits would encourage their
conversion into cash compensation, thus giving
employees more control over the disposition
of their income and the choice of the
providers of their services. Australia and New
Zealand have shown the way to reform in
this area by taxing fringe benefits (other
than contributions to pension plans) at the
corporate tax rate. This method of handling
a difficult, but urgent, problem is simple and
effective.

Social Security benefits continue to receive
favorable treatment, even though the elderly
can no longer be regarded as a disadvantaged
group. Under current law, the medical
insurance subsidies they receive are not subject
to tax, and less than half of retirement
and disability benefits is taxable to married
couples with income above $32,000 ($25,000
if single). The value of the medical insurance
subsidies should be subject to tax in full"4
and retirement and disability benefits should
be treated like private pensions without any
income thresholds, which would mean that
roughly 85 percent of the benefits would be
currently taxable.

The treatment of owner-occupied housing
remains unsatisfactory. I assume that the
exclusion of imputed rent from the tax base
and the deduction of mortgage interest by
most homeowners are sacrosanct, but it is
possible to limit the encouragement of borrowing
without promoting rearrangements of
debt for tax purposes. The solution is to
broaden the limitation on deductions of investment
interest to include all interest payments.
That is, a deduction for all interest
payments would be allowed, but limited to
the amount of reported investment income.
To accommodate the home-owner lobby, the
limit could be raised to net investment plus
an arbitrary amount, say, $10,000-enough
to take care of the vast majority of home
owners. The broader interest limitation
would remove the discrimination against
borrowing for other purposes and the incentive
to substitute home equity loans for other
types of borrowing.

Deductions other than for interest are still
too generous. The Simons' definition of income
includes all sources of income, without
any deductions for the uses of that income.
For equity reasons, it is appropriate to permit
deductions for such unusual expenses as
medical payments and casualty losses. I
would retain the deduction for state income
taxes to moderate interstate tax differentials.
42 However, the property tax is
largely a benefit tax and therefore should not
be deductible. Nor is it necessary to allow a
deduction for the first dollar of charitable
contributions on incentive grounds. Little or
4'The tax would be imposed on the insurance value
of hospital and Medicare subsidies rather than on the
dollar benefits actually received. See Congressional
Budget Office (1989).

42See Pechman (1987b), pp. 267-69, for illustrations
of aggregate tax rates with various combinations of
federal and state tax rates.


### ---Economics-1990-0-14.txt---
no charitable giving would be lost43 and
much revenue would be gained or reductions
in tax rates would be possible if the federal
deduction for property taxes were disallowed
and the deduction for charitable contributions
were restricted to amounts in excess of
two percent of income. In addition, the taxexemption
for interest on newly issued state
and local bonds should be removed.
Although income tax compliance is better
in the United States than in most other
countries, roughly 15 percent of individual
income is still unreported, according to IRS
estimates.44 Extension of withholding to interest
and dividends would improve compliance.
Congress enacted a withholding system
for these income items in 1982, but repealed
it the following year under pressure from the
financial institutions. Since information returns
are required for annual interest and
dividend payments of $10 or more, the
marginal costs of compliance with a withholding
system would be small.

One of the major features of the 1986 tax
law was to telescope the schedule of tax rates
into two acknowledged and two concealed
brackets, a bizarre four-bracket rate structure
of 15, 28, 33, and 28 percent. The reduction
in the number of brackets was a response
to the flat tax proposals that were
being promoted when the tax reform bill
began its journey through Congress, while
the unsightly bulge in the rate schedule was
motivated by revenue considerations. It is
not necessary to return to 14 brackets, but
there is room for more rate graduation without
the bulge.

In this connection, consideration needs to
be given to improving the structure of estate
and gift taxes to compensate for their low
average rates. These taxes were almost gutted
by increases in the exemptions and reductions
in the tax rates when income tax
rates were cut in 1981. Now that the top
income tax rates are even lower, it is time to
rely more heavily on the estate and gift
taxes.

The reduction in the tax rates led to two
additional changes in the 1986 act that I
believe were unfortunate. Congress eliminated
the deduction for two-earner couples
and ended the privilege of averaging income
for tax purposes.45 Both provisions should
be restored in the interest of horizontal equity.


Finally, contrary to the prevailing view
among public finance experts, Congress
clearly believes that a separate, unintegrated
corporate tax is essential for effective income
taxation. A separate tax prevents individuals
from avoiding the income tax by accumulating
earnings at the corporate level, although
some might question whether corporations
should be taxed at a higher rate than the top
bracket individual rate. But in its present
form, the corporate tax encourages debt financing.
It is alleged to be a major cause of
the recent upsurge in leveraged buyouts and
mergers. The remedy is not to allow a deduction
or credit for dividends received at the
individual level. Rather, the deduction of
interest by corporations should be denied
while reducing their tax rate to the neighborhood
of 15 percent to maintain the revenues
now produced by the corporate tax.46 The
corporate tax would become a low-rate tax
on net corporate income before distributions.


Tax Rates and Progressivity. The reforms I
have suggested would greatly increase the
income tax base and permit a realignment of
the tax rates to achieve the distributional
objectives described earlier. At calendar year
1990 levels, the tax base would increase from
$2.4 trillion to $2.8 trillion (Table 6). The
increase in the base leaves enough room to
cut rates in the lowest taxable income brackets
and still keep the top tax rates at reasonably
modest levels.


### ---Economics-1990-0-15.txt---
TABLE 6-ADJUSTED GROSS INCOME AND TAXABLE INCOME UNDER THE TAX
REFORM ACT OF 1986 AND UNDER A COMPREHENSIVE INCOME TAX
1990

Billions of Dollars

Adjusted Taxable

Item Gross Income Income

Tax Reform Act of 1986 3545 2407
Plus:

Personal Deductionsa 0 68

Transfer Paymentsb 226 164

Fringe Benefitsc 187 185

Two-earner Deductiond -82 -81

Othere 43 42

Equals: Comprehensive Taxf 3919 2785
Source: Congressional Budget Office.
aAllows flat standard deduction of $4,000; interest deduction limited to investment
income plus $10,000; tax deduction limited to income taxes; 10 percent floor on
deductions for medical expenses and casualty losses; 2 percent floors on deductions for
charitable contributions and miscellaneous expenses; and no standard deduction for the
elderly and the blind.

bIncludes 85 percent of Social Security retirement and disability benefits for all
taxpayers, workers' compensation, and veterans' benefits; and 50 percent of the
insurance value of hospital insurance benefits.
cIncludes premiums paid by employers for health and life insurance and other fringe
benefits; interest on life insurance policies; and IRAs of persons covered by employer
pension plans.

d20 percent of earnings of spouses with lower earnings up to a maximum of $70,000.
eIncludes unrealized capital gains transferred by gift or at death. interest on newly
issued state and local securities, and all preference items now subject to the minimum
tax. fAn increase in the earned income credit under Plan II (see Table 7) does not affect
adjusted gross income or taxable income.
In redesigning the rate structure, I suggest
scrapping the multiple schedule system which
was developed to reduce the tax advantage
of married couples relative to single people
under income splitting. It is simpler to use
one set of rates and to rely on the personal
exemptions to take into account differences
in ability to pay of families of different size.47
The restored deduction for two-earner couples
(20 percent of the earnings of the spouse
with the lower earnings up to $70,000) would
help avoid a significant marriage penalty.
The same revenue and progressivity of
present law could be generated by a tax
schedule ranging from 7 percent on the first
$5,000 of taxable income to 26 percent on
taxable income in excess of $35,000, without
the bulge in tax rates under current law (see
Plan I, Table 7).48 Thus, a wide margin
exists for increasing progressivity at the top
of the income scale, while keeping rates
moderate. To restore the progressivity of the
federal tax system to its 1977 level, the range
of graduation would have to be expanded
and the rate of graduation increased. A starting
rate of 4 percent on the first $5,000 of
taxable income rising to 48 percent on tax-
47To avoid the old community property problem,
(see Pechman, 1987b, pp. 102-7) the brackets in the
rate schedules for married couples filing separate returns
would be one-half the size of the brackets for
single people.

48Exemptions and the standard deduction would remain
the same as in 1989, that is, exemptions would be
$2,000 per capita and the standard deduction would be
$3,100 for single persons, $4,550 for heads of households,
and $5,200 for married couples, all adjusted for
inflation.


### ---Economics-1990-0-16.txt---



### ---Economics-1990-0-17.txt---
able income above $250,000 would accomplish
this objective (see Plan II, Table 7).49
Another change to increase progressivity
would be to increase the earned-income
credit for low-income families. Today, the
credit is the same for all families, regardless
of the number of children. The credit would
be more effective in combatting poverty if it
increased with family size. For example, the
current 14-percent credit could be maintained
for families with one child and four
percentage points, or roughly $250, could be
added for each additional child. With this
modification, the earned income credit would
increase the likelihood that a family with
several children could earn enough to remain
outside the welfare system.

Table 8 reports the average effective federal
income tax rates by population deciles
under the schedule that restores overall progressivity
to 1977 levels (Plan II) and under
the schedule that matches 1990 distribution
with a broadened base (Plan I). The average
effective rates in Plan II are lower than those
under present law in the bottom nine deciles
and higher in the top decile. For the top 1
percent of the family units, the average effective
rate rises from 21 percent to 30 percent,
which cannot be regarded as punitive.
I recognize that few people would go as
far as I would in broadening the tax base.
But that does not mean that the objective of
greater progressivity must be abandoned.
Even if there were no additional base-broadening
between now and 1990, the same degree
of progressivity that prevailed in 1977
could be achieved with rates ranging from 7
percent at the bottom to 56 percent at the
top.

Conclusion

I conclude that there is no good reason for
the disenchantment of economists with the
income tax. The main rival of the income tax
-the consumption expenditure tax- is distinctly
inferior on theoretical as well as practical
grounds. The endowment or lifetime
perspective of the expenditure tax is indefensible
in a world of financial, political, and
family instability. The transition problems in
moving from an income tax to an expenditure
tax are extremely difficult. There is also
a danger that the substitution of an expenditure
tax for the income tax would greatly
increase the concentration of wealth. Moreover,
the public regards income, not expenditure,
as the best index of ability to pay,
and it would be unwise to abandon this
familiar and widely approved basis of taxation.


The 1986 reforms have greatly improved
the federal income tax by broadening the
base and lowering rates. But the progressivity
of the federal tax system has been declining
for the last two decades. As a result, the
distribution of before-tax income, which has
been growing more unequal in the 1980s, has
become even more unequal on an after-tax
basis. I have suggested that the goal of tax
policy should be to restore the progressivity
of the income tax at least to its level in the
mid-1970s.

The 1986 tax reform went a long way
toward comprehensive income taxation, but
much more can be done to enlarge the tax
base and to remove the preferences for capital
income. Among the more urgent basebroadening
reforms are the inclusion in taxable
income of capital gains transferred by
gift or at death, elimination of the tax exemption
for newly issued state-local securities,
taxation of employee fringe benefits,
treatment of Social Security benefits like private
pensions, reduction of the tax subsidy
for home owners, pruning of the personal
deductions, and withholding on interest and
dividends. To correct the measurement of
capital income for inflation, asset prices
should be adjusted for changing prices in
order to convert nominal to real incomes for
tax purposes. The two-earner deduction and
income averaging should be restored to reduce
the marriage penalty and equalize the
treatment of fluctuating and stable incomes.
49 Between 1977 and 1990 the weight of the relatively
regressive payroll tax in federal revenues increased.
Thus, the income tax must be more progressive in 1990
than it was in 1977 to restore overall progressivity to its
1977 level.


### ---Economics-1990-0-18.txt---
A comprehensive income tax along these
lines would permit further rate reductions
throughout the income scale if the degree of
progression enacted in 1986 were to be retained.
However, the progressivity of the
federal tax system has declined since the
mid-1970s, even after taking into account
the effect of the 1986 act. To restore the
degree of progressivity of the mid-1970s, the
rate of graduation of the tax rates would
need to be increased. I estimate that this can
be accomplished with a rate schedule ranging
from 4 percent at the bottom to 48
percent at the top of the taxable income
scale-a moderate schedule of rates by any
standard.

It is clear from this analysis that the revenue
potential of the income tax has not
been exhausted in this country. Even if the
base is not broadened, the income tax can be
used to raise considerable additional revenues
in order to eliminate the recurring
federal deficits. Each percentage-point increase
in the individual and corporate income
tax rates would bring in about $30
billion in 1994, so that three points would
come close to balancing the overall budget in
that year. A top individual income tax rate
of 31 percent and a corporate rate of 37
percent cannot be regarded as punitive or
harmful to economic incentives.
What is inappropriate in my view would
be to introduce a value-added tax, as some
are suggesting. The value-added tax is regressive
and imposes unnecessarily heavy
burdens on the lower income classes. With
tax rates as low as they are today, more
revenues should come from the income tax,
the tax paid by those who have the ability to
pay. In view of the recent reductions in the
progressivity of the federal tax system, it
would be unconscionable to enact the distinctly
inferior alternative of a value-added
tax.
 ## Economics-1991-0


### ---Economics-1991-0-03.txt---
I

As the Second World War was drawing
near its resolution, economic theory entered
a phase of intensive mathematization that
profoundly transformed our profession. In
several of its main features that phase had
no precedent, and it will have no successor.
Assessing it requires a multidimensional
analysis acknowledging the contributions to
economics that were made, as well as the
tensions among economists that were
heightened.

The development of mathematical economics
during the past half-century can be
read in the total number of pages published
each year by the leading periodicals in the
field, an index that I will follow at first.
From 1933, the date when they both started
publication, to 1959, those periodicals were
Econometrica and the Review of Economic
Studies, and the index tells of the decline
from a high point, above 700 pages in 1935
to the lowest point, below 400 pages in
1943-1944. But 1944 marked the beginning
of a period of explosive growth in which
Econometrica and the Review of Economic
Studies were joined in 1960 by the International
Economic Review, in 1969 by the
Journal of Economic Theory, and in 1974 by
the Journal of Mathematical Economics. In
1977, these five periodicals together published
over 5,000 pages. During the period
1944-1977, the index more than doubled
every nine years. By that measure, 1944 was
a sharp turning point in the history of mathematical
economics. It was also the year in
which John von Neumann and Oskar
Morgenstern published the Theory of Games
and Economic Behavior.

While the professional journals in the field
of mathematical economics grew at an unsustainably
rapid rate, the American Economic
Review underwent a radical change
in identity. In 1940, less than 3 percent of
the refereed pages of its 30th volume ventured
to include rudimentary mathematical
expressions. Fifty years later, nearly 40 percent
of the refereed pages of the 80th volume
display mathematics of a more elaborate
type.

At the same time, the mathematization of
economists proceeded at an even faster pace
in the 13 American departments of economics
labeled by a recent assessment of
research-doctorate programs in the United
States (Lyle V. Jones et al., 1982) as "distinguished"
or "strong" according to the

scholarly quality of their faculties. Every
year the Fellows of the Econometric Society
(ES) certify new members by election into
their international guild, which increased in
size from 46 in 1940 to 422 in 1990. For
those 13 departments together, the proportion
of ES Fellows among professors was
less than 1 percent in 1940; it is now close
to 50 percent. It equals or exceeds 50 percent
for six of them, which were among
those assessed as the eight strongest. So
mathematized a faculty expects its students
to have what it considers to be minimal
mathematical proficiency, and knowledge of
calculus and linear algebra is required, or
forcefully recommended, for admission to
all 13 graduate programs.

Several scholarly recognitions lay additional
emphasis on the role that mathematical
culture is now playing in our profession.
Of the 152 members of the economics section
of the American Academy of Arts and
Sciences, 87 are Fellows of the Econometric
Society; and of the 40 members of the economics
section of the National Academy of


### ---Economics-1991-0-04.txt---
Sciences of the United States, 34 are ES
Fellows. From 1969 to 1990, 30 economics
Nobel awards were made, and 25 of the
laureates are, or were, ES Fellows. Since it
was first presented to Paul Samuelson in
1947, the John Bates Clark medal of the
American Economic Association has been
given to 21 economists, of whom 20 are ES
Fellows; and of the 26 living past presidents
of our Association, 13 are ES Fellows.
One may wish that those counts had not
been made. One may argue about points of
their interpretation. But they belong in our
common knowledge, and their thrust is unequivocal.
They indicate how extensive the
mathematization of economics and how
deep the accompanying change of our field
were over the past five decades.
The perception of the depth of that
change is reinforced by a comparison of the
levels of mathematics required in 1940 and
in 1990 to follow the development of economic
theory in every direction it was taking.
Fifty years ago, basic undergraduate
preparation in mathematics was almost always
sufficient. Today, graduate training in
mathematics is necessary. If, instead of being
a follower, one wishes to be an active
participant in that development along its
most technical avenues, a high degree of
mathematical professionalism is called for.
Several faculty members of the 13 departments
of economics mentioned previously
were actually identified as mathematicians
by their doctorates; four of them served as
chairmen of those departments during the
past 25 years. If still sharper focus brings
out the intellectual leaders of that development,
prominent among them is John von
Neumann, one of the foremost mathematicians
of his generation.

In that development process, mathematical
economics was continuously redefined as
new territories were included within its outward-
moving frontier and as topics that were
once at that frontier became standard parts
of the graduate, if not of the undergraduate,
economic-theory curriculum.

II

Before the contemporary period of the
past five decades, theoretical physics had
been an inaccessible ideal toward which
economic theory sometimes strove. During
that period, this striving became a powerful
stimulus in the mathematization of economic
theory.

The great theories of physics cover an
immense range of phenomena with a
supreme economy of expression. Of this,
James Clerk Maxwell (1865) had given a
notable example, as he described the electromagnetic
field by means of eight equations
at the time when mathematical economics
was born and came of age in the
middle of the 19th century. This extreme
conciseness is made possible by the privileged
relationship that developed over several
centuries between physics and mathematics.
In turn, the former presented the
latter with open problems, or found to questions
raised by physical theory ready-made
answers discovered by mathematicians in
their abstract universe. Sometimes the
causal linkage of research done in each one
of the two fields could not easily be unraveled;
and, on occasion, the same scientist
made inextricably intertwined contributions
to both disciplines.

The benefits of that special relationship
were large for both fields; but physics did
not completely surrender to the embrace of
mathematics and to its inherent compulsion
toward logical rigor. The experimental results
and the factual observations that are at
the basis of physics, and which provide a
constant check on its theoretical constructions,
occasionally led its bold reasonings to
violate knowingly the canons of mathematical
deduction.

In these directions, economic theory could
not follow the role model offered by physical
theory. Next to the most sumptuous
scientific tool of physics, the Superconducting
Super Collider whose construction cost
is estimated to be on the order of $1010
(David P. Hamilton, 1990; see also Science,
5 October 1990), the experiments of economics
look excessively frugal. Being denied
a sufficiently secure experimental base, economic
theory has to adhere to the rules of
logical discourse and must renounce the
facility of internal inconsistency. A deductive
structure that tolerates a contradiction
does so under the penalty of being useless,


### ---Economics-1991-0-05.txt---
since any statement can be derived flawlessly
and immediately from that contradiction.
In its mathematical form, economic theory
is open to an efficient scrutiny for logical
errors. The rigor that has been reached
as a consequence is in sharp contrast to the
standards of reasoning that were accepted
in the late 1930's. Few of the articles published
then by Econometrica or by the Review
of Economic Studies would pass the
acid test of removing all their economic
interpretations and letting their mathematical
infrastructure stand on its own. The
greater logical solidity of more recent analyses
has contributed to the rapid contemporary
construction of economic theory. It has
enabled researchers to build on the work of
their predecessors and to accelerate the cumulative
process in which they are participating.


But a Grand Unified Theory will remain
out of the reach of economics, which will
keep appealing to a large collection of individual
theories. Each one of them deals
with a certain range of phenomena that it
attempts to understand and to explain.
When it acquires an axiomatic form, its
explicit assumptions delimit its domain of
applicability and make illegitimate overstepping
of its boundary flagrant. Some of those
theories take a comprehensive view of an
economic system and bring insights into the
solutions of several global problems. For
instance, prices contribute to achieving an
efficient use of resources, to equalizing supply
and demand for commodities, and to
preventing the formation of destabilizing
coalitions. In every case, a theoretical explanation
must be provided. The assumptions,
which cannot be satisfied by all economic
observations, are the present outcome of a
continuing weakening process.

A global view of an economy that wants
to take into account the large number of its
commodities, the equally large number of
its prices, the multitude of its agents, and
their interactions requires a mathematical
model. Economists have successfully constructed
such a model because the central
concept of the quantity of a commodity has
a natural linear structure. The action of an
agent can then be described by listing the
quantity of its input or output for each
commodity (opposite signs differentiating
inputs from outputs). That list can be treated
as the list of the coordinates of a point in
the linear commodity space. Similarly, the
price system of an economy can be treated
as a point in the linear price space, dual of
the commodity space, whose dimension is
also the number of commodities.
In those two linear spaces, the stage was
set for sometimes dazzling mathematical
developments that began with the elements
of differential calculus and linear algebra
and that gradually called on an ever broader
array of powerful techniques and fundamental
results offered by mathematics. Thus,
the three roles of prices given earlier as
instances were illuminated by basic mathematical
theorems: the first, the achievement
of an efficient use of resources, by results of
convex analysis; the second, the equalization
of supply and demand for commodities,
by results of fixed point theory; the third,
the prevention of the formation of destabilizing
coalitions, by results of the theory of
integration and of nonstandard analysis. In
those three cases, the lag between the date
of a mathematical discovery and the date of
its application to economic theory decreased
over time. It was notably short for
nonstandard analysis, founded at the beginning
of the 1960's by Abraham Robinson1
and applied to economics by Donald Brown
and Abraham Robinson (1972).

The last, and most recently developed, of
those three instances can be chosen, as can
either of the other two, for a more detailed
illustration. Competition is perfect when every
agent's influence on the outcome of
economic activity is insignificant. The influence
of their totality on that outcome is,
however, significant. It is to solve the problem
of aggregating negligible quantities so
as to obtain a nonnegligible sum that integration
was invented. In this perspective,
the application of integration theory to the
study of economic competition is entirely
natural. That application requires the set of
agents to be large-larger than the set of
integers. Treating the set of the agents of an
economy as the rich collection of the points
1See the preface in Robinson (1966).


### ---Economics-1991-0-06.txt---
of an interval of real numbers has long been
familiar in descriptions of economic data. It
became familiar in economic theory as well
after Robert J. Aumann (1964) showed that,
in a pure exchange economy composed of
insignificant agents, the formation of destabilizing
coalitions is prevented if and only if
all those agents base their decisions on a
price system.

The concept of a convex set (i.e., a set
containing the segment connecting any two
of its points) had repeatedly been placed at
the center of economic theory before 1964.
It appeared in a new light with the introduction
of integration theory in the study of
economic competition: if one associates with
every agent of an economy an arbitrary set
in the commodity space and if one averages
those individual sets over a collection of
insignificant agents, then the resulting set is
necessarily convex.2 But explanations of the
three functions of prices taken as examples
can be made to rest on the convexity of sets
derived by that averaging process. Convexity
in the commodity space obtained by aggregation
over a collection of insignificant
agents is an insight that economic theory
owes in its revealing clarity to integration
theory.

An economist who experiences such an
insight belongs to the group of applied
mathematicians, whose values he espouses.
Mathematics provides him with a language
and a method that permit an effective study
of economic systems of forbidding complexity;
but it is a demanding master. It ceaselessly
asks for weaker assumptions, for
stronger conclusions, for greater generality.
In taking a mathematical form, economic
theory is driven to submit to those demands.
The gains in generality that it has achieved
as a result, in little more than a century,
stand out when the first formulations of
the theories of general equilibrium (Leon
Walras, 1874-1877) and of the core of an
economy (Francis Y. Edgeworth, 1881 pp.
34-8) are placed side by side with the recent
treatments of those subjects to which
The New Palgrave is an introduction and a
bibliographical key (John Eatwell et al.,
1987-1989). Walras's consumers and producers
have been freed from many of their
constraining characteristics; Edgeworth's
universe of two consumers and two commodities
has been vastly expanded.

Mathematics also dictates the imperative
of simplicity. It relentlessly searches for
short transparent proofs and for the theoretical
frameworks in which they will be
inserted. Participating in that pursuit, economic
theory was sometimes drawn by drives
toward greater generality and toward greater
simplicity in the same direction, rather than
in opposite directions. Cohort after cohort,
students of consumer theory have learned
about the concept of decreasing marginal
rate of substitution for two commodities on
an indifference curve and about its extension
to the multicommodity case. Notably
more general, and notably simpler, is the
concept of convexity of the set of points
preferred to a given point in the commodity
space. Welfare economics presents another
instance. One of its main theorems formulates
precisely the principle enunciated by
Adam Smith (1776). If all the agents of an
economy are in equilibrium relative to a
price system, then they utilize their collective
resources optimally. The proof of that
theorem (Kenneth J. Arrow, 1951) has become
so simple that it can be given without
mathematical symbols. It is, at the same
time, of utmost generality; in relating two
basic concepts of economic theory to each
other, it uses no assumption.

In its attempts to attain its many objectives,
economic theory was helped by greater
abstraction. Preference theory supplies an
example again. Significant research efforts
were expended on solutions of the integrability
problem. That problem can be bypassed
altogether, and greater simplicity can
be achieved by moving from the commodity
space to the more abstract space of the
pairs of its points. In this space, whose
dimension is twice the number of commodities,
the pairs of commodity points indifferent
to each other are now assumed to
form a smooth (hyper)surface. As another
instance of the generality permitted by abstraction,
consider the notion of a commodity,


### ---Economics-1991-0-07.txt---
which can be treated as a primitive
concept, with an unspecified interpretation,
in an axiomatic economic theory. A newly
discovered interpretation can then increase
considerably the range of applicability of
the theory without requiring any change in
its structure. Thus, by making the transfer
of a good or service between two agents
contingent on the state of the world that
will obtain, Arrow (1953) made possible the
immediate extension of the economic theory
of certainty to an economic theory of uncertainty
by a simple reinterpretation of the
concept of a commodity. The theory of financial
markets has been influenced by that
view of uncertainty, and their practice has
not been unaffected. Finally, take the problem
of existence of a general equilibrium,
once considered to be one of the most abstract
questions of economic theory. The
solutions that were proposed in the early
1950's paved the way for the algorithms for
the computation of equilibria of Herbert E.
Scarf (1973) and for several of the developments
of applied general equilibrium analysis
(Scarf and John B. Shoven, 1984). In this
case, abstraction in economic theory led to
the study of fundamental problems of great
generality, but also to a broad range of
applications.

III

The list of advances that the mathematization
of economic theory helped or permitted
is already long; and in one aspect it may
appear lengthy. Ceteris paribus, one cannot
prefer less to more rigor, lesser to greater
generality, or complexity to simplicity; but
other things are not equal, and in the estimate
of many members of our Association
the cost of that mathematization sometimes
outweighs its benefit. Two of its presidential
addresses notably confronted that difficult
analysis and stressed the price that economics
paid for its increased use of mathematics.
Wassily Leontief's (1971) observations
were factual, and Robert A. Gordon's
(1976) comments relevant when they were
made in 1970 and in 1975. They still are
today, for, in spite of their authorities, enhanced
by the platform from which they
were speaking, and in spite of the wide
diffusion of their critiques, neither Leontief
nor Gordon altered the course of the development
they were assessing. In the past two
decades, economic theory has been carried
away further by a seemingly irresistible current
that can be explained only partly by the
intellectual successes of its mathematization.


Essential to an attempt at a fuller explanation
are the values imprinted on an
economist by his study of mathematics.
When a theorist who has been so typed
judges his scholarly work, those values do
not play a silent role; they may play a decisive
role. The very choice of the questions
to which he tries to find answers is influenced
by his mathematical background.
Thus, the danger is ever present that the
part of economics will become secondary, if
not marginal, in that judgment.
The reward system of our profession reinforces
the effects of that autocriticism. Decisions
that shape the career of an economic
theorist are made by his peers. Whether
they are referees of a journal or of a research
organization, members of an appointment
or of a promotion committee,

when they sit as judges in any capacity, their
verdicts will not be independent of their
own values. An economist who appears in
their court rarely ignores his perception of
those values. If he believes that they rate
mathematical sophistication highly, and if
he can prove that he is one of the sophisticates,
the applause that he expects to receive
will condition his performance.
The same effects are also amplified by the
relentless pressure to publish exerted by his
environment. There are indeed instances of
extreme restraint in scientific publication,
and some of them have become legend. The
mathematical papers of Bernhard Riemann
(1826-1866) take 506 pages in the volume
that collected them (Riemann, 1876). The
molecular structure of DNA was announced
by James Watson and Francis Crick (1953)
in a one-page article. But it is easier to
explain those examples away than to follow
them. The environment of a scholar demands
papers, and the temptation to supply
them without restraint may become overpowering


### ---Economics-1991-0-08.txt---
to an economic theorist who has
developed proficiency in his research style.
The precocious development of that proficiency
is a comparative advantage that a
mathematical approach bestows on him.
The spread of mathematized economic
theory was helped even by its esoteric character.
Since its messages cannot be deciphered
by economists who do not have the
proper key, their evaluation is entrusted to
those who have access to the code. But
acceptance of their technical expertise also
implies acceptance of their values. Our profession
may take pride in its exceptional
intellectual diversity, one of whose clearest
symbols is an Ely lecture given by an economic
historian at a session chaired by a
mathematical economist. Yet that diversity
is strained by the increasing impenetrability
to the overwhelming majority of our Association
of the work done by its most mathematical
members.

IV

The bond that ties economists together in
their study of a common subject has not
been tested only by differences in methodologies.
It has also been tried by differences
in ideologies. In their endeavors to make
their field into a science, economists must
renounce a favorite mode of thinkingwishful
thinking; they must be impartial
spectators of a play in which they are the
actors. While they attempt to keep that
inhuman stance, they are pressed to give
immediate answers to societal questions of
immense complexity and thereby to abandon
the exacting slowness of the step-by-step
scientific approach. Divisions according to
methodologies and ideologies, criticism from
outside and from inside, and intellectual
fashions that sweep our discipline make each
one of its steady developments remarkable.
The mathematization of economic theory
was one of them for a century and a half.
During the past five decades it became one
of the prime movers in the transformation
of our field. The extent of that mathematization
has given rise to discordant assessments
of its effects and to attempts to
change its heading. The quality of assessments
of the phase that economic theory
underwent and the effectiveness of attempts
to alter the course of its evolution will gain
from a detailed analysis of the processes
that led to its present state.
 ## Economics-1992-0


### ---Economics-1992-0-03.txt---
Global warming from carbon dioxide was
an esoteric topic 15 years ago, unknown to
most of us. But in a few years, helped along
by some hot summers, it has climbed to the
top of the international agenda. Cabinets,
Parliaments, and heads of government have
issued pronouncements on reducing carbon
emissions, and in June of this year more
than a hundred governments will be represented
by ministers or heads of government
at a great United Nations Conference on
Environment and Development to be held
in Rio de Janeiro. Together with nongovernmental
organizations representing labor,
business, students, environmentalists,
scientists, and groups concerned with health
and child development and family planning,
these representatives are expected to need
25,000 hotel rooms. A "framework agreement"
is widely expected, together with
some institutional arrangements that will
keep global environmental issues permanently
on every government's agenda. And
at the center of these issues will be the
phenomenon that has come to be known as
the "greenhouse effect."

The greenhouse effect itself is simple
enough to understand and is not in any real
dispute. What is in dispute is its magnitude
over the coming century, its translation into
changes in climates around the globe, and
the impacts of those climate changes on
human welfare and the natural environment.
These are beyond the professional
understanding of any single person. The
sciences involved are too numerous and diverse.
Demography, economics, biology, and
the technology sciences are needed to project
emissions; atmospheric chemistry,
oceanography, biology, and meteorology are
needed to translate emissions into climates;
biology, agronomy, health sciences, economics,
sociology, and glaciology are needed
to identify and assess impacts on human
societies and natural ecosystems. And those
are not all.

There are expert judgments on large
pieces of the subject, but no single person
clothed in this panoply of disciplines has
shown up or is likely to. So, I venture to
offer my judgment.

First on the principle. The metaphor of
the greenhouse is not quite appropriate, but
the basic idea is not in dispute. The earth is
bathed in sunlight, some reflected and some
absorbed. If the absorption is not matched
by radiation back into space, the earth gets
warmer until the intensity of that thermal
radiation matches the absorbed incoming
sunlight. Some gases in the atmosphere that
are transparent to sunlight absorb radiation
in the infrared spectrum, blocking that outward
radiation and warming the atmosphere.
When the atmosphere has warmed
enough to intensify the thermal radiation so
that it matches the absorbed incoming sunlight,
equilibrium is achieved at the higher
temperature. These so-called "greenhouse"
gases can be identified in the laboratory.
Carbon dioxide is one of them; methane is
another, as is nitrous oxide, as are the chlorofluorocarbons
(CFC's).

The principle has been in practice for
decades. On a clear day in January, the
earth and its adjacent air in Orange County
California warm nicely, but the warmth radiates
rapidly away during the clear nights,
and frost may threaten the trees. Smudge
pots, burning cheap oil on a windless night,
produce substances, mainly carbon dioxide,
that absorb the radiation and protect the
trees with a blanket of warm air. Greenhouses,
in contrast, mainly trap the air
warmed by the earth's surface and keep it


### ---Economics-1992-0-04.txt---
from rising to be replaced by cooler air. The
phenomenon should have been called the
"smudgepot effect," but it is too late to do
anything about it.

A first step in pursuing this phenomenon
is to assess how much warming might go
with an enhanced concentration of these
gases. That cannot be done in the laboratory;
there are too many feedbacks. A
warmer atmosphere will contain more water
vapor; water vapor itself is a greenhouse
gas. Changes in temperature and humidity
will change cloud cover; clouds can reflect
or absorb incoming or outgoing light according
to their composition and altitude. The
average temperature is only one dimension;
temperatures at different altitudes and different
latitudes matter. But a starting point
has been the change in average surface atmospheric
temperature expected to accompany
a specified increase in the concentration
of greenhouse gasses; and arbitrarily,
but reasonably, the base case is taken as a
doubling of the concentration.
A moment on why a doubling is the
benchmark. To compare estimates of warming,
people must use the same hypothesized
concentration of greenhouse gases in the
atmosphere. (Alternatively, they could use
the same hypothesized temperature increase
and estimate the corresponding concentration.
) Doubling, like a half-life in reverse,
is a natural unit if it is within the
range of practical interest, and it is. A doubling
is expected sometime in the next century,
so it is temporally relevant; and a
doubling is estimated to make a substantial
but not cataclysmic difference. If fixation on
a doubling seems to imply an upper limit on
any expected increase, the implication is
unfortunate: enough fossil fuel exists to support
several doublings.

In 1979, a committee of the National
Academy of Sciences (NAS) (1979 p. 2)
estimated the change in average temperature
to accompany a doubling of carbon
dioxide in the atmosphere: three degrees
Celsius, with a range of 1.5 degrees to either
side. (In the last 15 years other greenhouse
gases have received attention; these
other gases can be converted to their carbon
dioxide equivalents and the original
estimate applied to the mixture.) The NAS
appointed another committee a few years
later to reexamine that estimate, and the
new committee saw no reason to change it
(NAS, 1982 p. 51). An intergovernmental
panel on climate change (IPCC), consisting
of scientists from many nations, revisited
the estimate in 1990 and concluded, from
the several climate models they had examined,
that "the models results do not justify
altering the previously accepted range of 1.5
to 4.5 degrees C" (IPCC, 1990 p. xxv). Thus,
the estimate appears to be robust over time,
but the spread of uncertainty remains large:
the upper limit is three times the lower
limit. (No quantitative interpretation of
these upper and lower "limits" has been
made public. Both National Academy reports
referred to them as "probable error.")
II

The uncertainties are even greater in
translating a temperature change into climates.
The media support a popular view
that things will just get hotter; a news magazine
cover was a sweating global face. But
the laboratories that do the meteorology do
not simply predict warming; they do not
even predict that the most noticeable effects
will necessarily be temperature changes.
Among the great driving forces of weather
and climate is the temperature differential
between equatorial and polar regions; convection
currents coupled with the rotation
of the earth are engines of atmospheric
circulation and, ultimately, ocean circulation.
The models predict greater temperature
change in the polar regions than near
the equator. This change in gradient can
drive changes in circulation. The results may
be warmer in some places and colder in
others, wetter in some places and drier in
others, cloudier in some places and sunnier
in others, stormier in some places and less
stormy in others-generally a complex of
changes that would bear no easy relation to
an average change in global temperature.
The change in average temperature is
useful as an index of climate change. It is


### ---Economics-1992-0-05.txt---
thought, and the models demonstrate, that
the greater the change in average temperature
the greater the departure of current
climates from what they are now. Thus,
while it is wrong to think that what is going
to happen can be readily characterized as
"warming" it is not erroneous to take that
average warming as a rough measure of the
extent or severity of change to be expected.
Unfortunately the widespread reference to
"global warming" promotes the notion that
things will simply get hotter. (Interestingly,
virtually all public discussion is on hotter
summers, not warmer winters; a hundred
years ago popular discussion of a warming
trend would likely have concentrated on the
milder winters to be expected.)
If three degrees Celsius is taken as an
index of climate change to come within the
next century or so, how big is that compared
with what has happened within the last century,
or the last 10,000 years? From what I
have just said, this cannot be answered in
terms of whether anyone would notice the
difference if every night and every morning,
every winter and every summer, temperatures
were exactly three degrees higher than
they otherwise would have been. The question
is: how would a three-degree change in
a global average compare with what has
been experienced in the past?

The answer is that for 10,000 years, since
the disappearance of the last ice age, average
temperature appears never to have varied
over anything like three degrees. A band
of one degree Celsius would cover the current
estimates of what average temperatures
have been since the dawn of recorded
history. We will be moving into a climatic
regime that has never been experienced in
the current interglacial period.
"Mankind will undergo greater climate
change in the next 100 years than has been
experienced in the last 10,000." Properly
qualified, the statement is true; what it neglects
is that peoples have been migrating
over great distances for at least several
thousand years. Goths and Vandals, Huns,
West Europeans who populated North and
South America, Southerners who went
North during the Great Depression, and
Northeasterners who moved southwest after
World War II all experienced changes in
climate greater than any being forecast by
the models. Almost everybody who attends
this lecture in New Orleans will have undergone
a greater change in the past few days
than is expected to occur in any fixed locality
during the coming century.

The changes that the models produce are
gradual both in time and in space. The
models do not produce discontinuities. Climates
will "migrate" slowly. The climate
of Kansas may become like Oklahoma's,
Nebraska's like that of Kansas, South
Dakota's like Nebraska's, but none of these
is expected to become like the climates of
Oregon, Louisiana, or Massachusetts.
A caution: the models probably cannot
project discontinuities-just gradual change
-because nothing goes into the models that
will produce catastrophes. There may be
phenomena that could produce drastic
change, but they are not known with enough
confidence to introduce them into the models.
So the reassuring gradualness may be
an artifact of the methodology. I will return
to this point later.

This greenhouse problem, if problem it
proves to be, is truly one of the "global
common." A ton of carbon emitted anywhere
on earth has the same effect as a ton
emitted anywhere else. And carbon dioxide
has a long residence time in the atmosphere:
a century or more. There may be
ways to remove it, but it doesn't disappear.
The greenhouse influence on any national
territory depends solely on the global concentration,
not in any way on what part of
the total is due to a nation's own emissions.
As I shall detail later, the costs of reducing
carbon emissions will be large compared
with any other emissions that have caused
concern. The costs of phasing out CFC's
will be in the billions of dollars per year for
some years, and complete elimination is expected
to be feasible. The cost of reducing
sulfuric acid may be in the tens of billions of
dollars. Proposals to hold emissions of carbon
dioxide constant (with a linear increase
of concentration in perpetuity) or to reduce
emissions by 50 percent below what they


### ---Economics-1992-0-06.txt---
would otherwise be, beginning perhaps in
2010, are expected to cost in the hundreds
of billions in perpetuity.

There are a few numbers worth carrying
in mind. There are 700 billion tons of carbon
in the atmosphere. (Quotations are
sometimes in tons of carbon dioxide, rather
than carbon; the figure is then 32 times as
large, about 2,600 billion.) Annual emissions
are 6 billion tons. Close to half disappears
somewhere, and a little over half
remains in the atmosphere; so the concentration
is increasing by one-half percent per
year. It has increased 25 percent in the last
hundred years. (Concentration is reported
more often than tonnage; it is currently
about 350 parts per million.) And there are
upwards of ten trillion tons of carbon fuels
out there to be burned; if it were all burned
and half stayed in the atmosphere, the concentration
could double at least three times.
If the carbon in the atmosphere has already
increased by a quarter, has the average
temperature gone up as predicted? And
were the recent hot American summers that
stirred popular interest harbingers of greenhouse
summers to come?

To the first question, the answer is that
average global temperature-summer and
winter, both hemispheres, night and
day-has apparently risen by half a degree
in the last hundred years, but whether "as
predicted" depends on what qualifications
one reads into the predictions. The pattern
differed between the Northern and Southern
Hemispheres. The global average rose
during the first 40 years of this century, was
level for the next 40 years, and rose during
the past decade. This pattern demonstrates
that, whether or not we are witnessing the
greenhouse effect, there are other decadeslong
influences that can obscure any smooth
greenhouse trend. (The carbon concentration
is not at issue; it is well measured and
shows steady rise on a decade scale.) There
are known phenomena that could account
for the irregular temperature increase of
the past century, and whether we are witnessing
the "signal" probably depends on
whether one wants high confidence to reject
a null hypothesis or is about to bet money
on whether, another 25 years from now,
looking back, all doubt will have been removed.
I don't know what bets are being
placed by "greenhouse scientists," but they
are cautious in public on the question.
To the second question-do the hot
American summers of the past few years
announce the arrival of a greenhouse, confirming
predictions?-the answer is in two
parts: maybe it's the greenhouse; but it's not
what the greenhouse models predict. The
global average in the four hot years of the
past seven was only 0.2 degrees above the
level of the preceding 40 years; and sudden
hot American summers are not what the
models predict.

III

In anticipating the impact on human welfare
or natural systems, two kinds of uncertainty
are unlikely to be dispelled soon. One
is simply the question of what the changes
will be in each region or locality. Current
models are severely limited in their agreement
with each other, in their handling of
such topographical variables as mountain
ranges, and in the fineness of the grids they
superimpose on the globe. There is no great
confidence that the models will be greatly
improved within the next decade or two. A
chaos-like process may defeat efforts to improve
local predictions; and uncertainties in
gross phenomena, such as the behavior of
ocean currents under changed climatic conditions,
may not be much better understood
soon.

Even if we had confident estimates of
climate change for different regions of the
world, there would still be uncertainties
about the kind of world it is going to be 50,
75, or 100 years from now. Imagine it were
1900 and the climate changes associated
with a three-degree average temperature increase
were projected to 1992. On what
kind of world would we superimpose either
a vaguely described potential change in climate
or even a specific description of
changes in the weather in all the seasons of
the year, even for our own country. There
would have been no way to assess the impact
of changing climates on air travel, electronic
communication, the construction of


### ---Economics-1992-0-07.txt---
skyscrapers, or the value of California real
estate. Most of us worked outdoors; life
expectancy was 47 years (it is now 75); barely
a fifth of us lived in cities of 50,000 or more.
Anticipating the automobile, we might have
been concerned with whether wetter and
drier seasons would bring more or less mud,
not anticipating that the nation's roads would
become thoroughly paved. The assessment
of effects on health would be without antibiotics
or inoculation. And in contrast to
most contemporary concern with the popular
image of hotter summers to come, I
think we would have been more concerned
about warmer winters, later frost in autumn,
and earlier thaw in the spring.
If the world, both North America and the
other continents, is going to change as much
in the next 90 years as it has changed in the
90 just past, we are going to be hard put to
imagine the effects of climate changes.
Another thought experiment: suppose the
kind of climate change expected between
now and, say, 2080 had already taken place
since 1900. Ask somebody 50, 60, or 80
years old what is different compared with
when he or she was a child. Would the
climate change be noticed? Even ask a 70-
year-old farm couple living on the same
farm where they were born: would the
change in climate be among the most dramatic
changes in either their farming or
their lifestyle? I expect changing from horses
to tractors and from kerosene to electricity,
the arrival of the telephone and the automobile
and the paving of roads, the development
of pesticides and artificial fertilizer,
the discovery of soy beans and the development
of hybrid corn, and even improvements
in outdoor clothing, veterinary
medicine, and agricultural practices generally
would swamp the climate change. And
if instead of living and working conditions
we inquire about changes in wildlife and
natural ecosystems, changes in regional climates
would have been competing, in their
impact on nature, with population growth
and economic development.

A conclusion we might reach is that a
climate change would have appeared to
make a vastly greater difference to the way
people lived and earned their living in 1900
than to the way people live and earn their
living today. Today very little of our gross
domestic product is produced outdoors, susceptible
to climate. Agriculture and forestry
account for less than 3 percent of GDP, and
little else is much affected. Some activities
-tourism and holidays, professional sports,
and school teaching-are seasonal, but
many of the seasonalities are conventions
that reflect the influence of climate in earlier
times. (Children were needed in the
fields in summer and could start school when
the harvest was in; hockey and basketball
used to be winter sports because one depended
on ice and the other could fit in a
building.)'

Manufacturing rarely depends on climate,
and where temperature and humidity used
to make a difference, air conditioning has
intervened. When Toyota chooses among
Ohio, Alabama, and Southern California for
locating an automobile assembly, geographical
considerations are important, but not
because of climate. Minerals are extracted
where they happen to occur, and oil fields
and coal mines inhabit all kinds of climates
and are little affected. The U.S. Postal Service'
s vow that neither snow nor rain nor
heat nor gloom of night will "stay these
couriers from the swift completion of their
appointed rounds" sounds quaint in the era
of e-mail and fax.

Finance is little affected by climate; similarly
for health care, or education, or broadcasting.
Transportation can be affected, but
improvements in all-weather landing and
take-off in the last 30 years are greater than
any differences that climate makes. If the
average effect is a warming, iced waterways
and snow removal may decline in importance.
Construction is affected, mainly by
cold, and if the average effect is in the
direction of warming, construction may benefit
slightly.

It is really agriculture that is affected. But
even if agricultural productivity declined by
a third over the next half century, the per
1An imaginative discussion is in Jesse H. Ausubel
(1991).


### ---Economics-1992-0-08.txt---
capita GNP we might have achieved by 2050
we would achieve only in 2051. Considering
that in most of the developed countries-the
United States, Japan, France, the United
Kingdom, the Netherlands, and Israel-the
agricultural problem has been protecting
farmers, that agricultural productivity in
most parts of the world continues to improve,
and that many crops and cultivated
plants will benefit directly from enhanced
photosynthesis due to increased carbon
dioxide, one cannot be certain that the net
impact on agricultural productivity will be
negative or, if negative, will be noticed in
the developed world.

I conclude that in the United States, and
probably Japan, Western Europe, and other
developed countries, the impact on economic
output will be negligible and unlikely
to be noticed.2 And there is no reason to
believe that in these countries there could
be a noticeable impact on health. Any influence
of climate on health in this country
would be more in the regional distribution
of the population than in changes in local
and regional climates.

Comfort is worth considering. Fortunately,
the climate models predict a greater
warming in winter than in summer. Most
people in the United States, Japan, and
Western Europe go south for vacation, both
summer and winter; and when people move
upon retiring in the United States they typically
move toward warmer climates. In future
years, elderly people may suffer more
heat stroke in summer in St. Louis, but we
can hope for fewer broken bones from ice
in Boston. (Inhaling air richer in carbon
dioxide has no effect on health.)
IV

This complacent assessment cannot be
extended to the much larger population of
the underdeveloped world. The livelihoods
earned in agriculture and other climatesensitive
outdoor activities, 3 percent in the
United States, comprise 30 percent and
more of all livelihoods in most of the developing
world. Reliable forecasts of likely
climate changes in the different areas so
dependent on agriculture are simply not
available, so no assessment, region by region,
of the effect on productivity can be
provided. There is no strong presumption
that the climates prevailing in different regions
50 or 100 years from now will be less
conducive to food production. But there is
also no assurance that climate changes will
not be harmful, and even if on balance the
impact is neutral, there may be large areas
with large populations that suffer severely.
Those people are vulnerable in a way that
Americans, Western Europeans, and
Japanese are not.

Nor can the impact on health be dismissed
or readily subsumed among generally
improving health conditions, as for the
developed world. Numerous parasitic and
other vector-borne diseases affecting hundreds
of millions of people are sensitive to
climate. Again, there is no strong presumption
that malaria mosquitos, to take an example,
will on balance benefit from climate
changes, but the risk is there.
It is with the less-developed countries that
we have to be most careful about superimposing
the climates of the future on the
economies and societies of today. As it was
in our own country during this century, the
trend in developing countries is to be less
dependent on agriculture and less vulnerable
to climate in transportation and other
activities and health. If per capita income
growth in the next 40 years compares with
the 40 years just past, vulnerability to climate
change should diminish, and the resources
available for adaptation should be
greater. I say this not to minimize concern
about climate change, but to anticipate the
question of whether developing countries
should make sacrifices in their development
to minimize the emission of gases that may
change climate to their disadvantage. Their
best defense against climate change may be
their own continued development.
This is a point worth emphasizing. Some
environmentalists argue that developing


### ---Economics-1992-0-09.txt---
countries should sacrifice some of their
hopes for economic development in the interest
of slowing the climate change that
may prove disastrous. But the advice contains
a contradiction Any disaster to developing
countries from climate change will be
a disaster to their economic development.
What is desired is to optimize development
by investing in greenhouse-gas abatement
only when that appears, subject to all the
uncertainties, to contribute more to their
development in the future than the alternative
direct investment in development. It is
not economic growth versus environment; it
is growth with the environment taken into
account.

A related point: population growth is important
for the climate change, in two respects.
One is that carbon emissions in developing
countries are positively driven by
population; population growth does not
merely dilute carbon emissions per capita,
but for a number of reasons more people
means more carbon. If China succeeds in
holding population growth to near zero for
the next couple of generations, it may do as
much for the earth's atmosphere as would a
heroic Chinese anticarbon program coupled
with 2-percent annual population growth.
The other population effect is simply that
the most likely adverse impact of climate
change on human productivity and welfare
would be on food production. In the poorest
parts of the world, the adequacy of food
depends on the number of mouths and
stomachs. In a hundred years, adverse
changes in climate for food production
would be far more tragic if the countries we
now associate with the developing world
had populations totaling 12 billion than if
they totaled 9 billion. For the developing
world, the increasing concentration of people
is probably more serious than the increasing
concentration of carbon dioxide.
At this point, I appear to have reached
the conclusion that the developed world has
no self-interest in expensively curtailing carbon
consumption and that the developing
world cannot afford to incur economic
penalties to slow the greenhouse effect.
There is a mismatch between those who
may be vulnerable to climate change and
those who can afford to do anything about
it.

V

Why should the rich developed countries
care enough about climate change to do
anything about it? The answer must depend
partly on how expensive it is going to be to
do anything about it. Abatement programs
have been examined in a number of econometric
models that suggest we might want
to treat as pertinent the sacrifice of perhaps
2 percent of world GNP in perpetuity.
A strong argument for trying seriously to
slow climate change is that the developing
countries are vulnerable and we care. Developed
countries are currently providing
$50 billion per year of assistance to the
developing world; we would be talking about
expending or forgoing perhaps 4-8 times
that much to slow emissions and slow climate
change. Whether people in the developed
democracies could be mobilized to
contribute so much to benefit, half a century
from now, the people in the countries
we now call developing I do not know, but I
believe that if the developed countries were
prepared to invest, say, $200 billion per year
in greenhouse-gas abatement, explicitly for
the benefit of developing countries 50 years
or more from now, the developing countries
would clamor to receive the resources immediately
in support of their continued development.
There would undoubtedly be

abatement opportunities so cheap that they
could compete with direct aid to developing
countries, but it would be hard to make the
case that the countries we now perceive as
vulnerable would be better off 50 or 75
years from now if 10 or 20 trillions of dollars
had been invested in carbon abatement
rather than in their economic development.
A second argument for an expensive program
of carbon abatement is that, while our
production of material goods and services
may not suffer from climate change, our
natural environment may be severely damaged.
Natural ecosystems will be destroyed;
plant and animal species will become extinct.
Places of natural beauty will be degraded.
Valuable chemistries of plant and


### ---Economics-1992-0-10.txt---
animal life will be lost before we learn their
genetic secrets. And the earth itself deserves
our respect. For many people, something
close to religious values are at stake.
This issue is doubly difficult to assess. It is
difficult to know how to value what is at
risk, and it is difficult to know just what is at
risk. Even if climate changes at each point
in time could be predicted accurately, the
impacts on natural ecosystems could not yet
be determined. And the benefits of slowing
climate change by some particular amount
would be even more uncertain. We know
that carbon fuels are not going to be discontinued;
the issue is the marginal gains, from
carbon abatement and a slowing of climate
change, in the survival of species and
ecosystems and the preservation of enjoyable
environments. This is an issue that
simply has not been addressed.
The third argument for spending heavily
to slow climate change is that the conclusions
I reported earlier may be quite wrong.
I said that the climate models predict that
climates will change slowly and not much;
the models do not produce discontinuities,
surprises, catastrophes. What is known about
weather and climate constitutes an equilibrium
system.

The possibility has to be considered that
if global temperature increases, not by the
median estimate of three degrees Celsius
for a doubling of carbon in the atmosphere,
but by four or five degrees and continues to
rise beyond the doubling because carbon
fuels are still in use worldwide, some atmospheric
or oceanic circulatory systems may
switch to alternative equilibria, producing
regional changes that are both sudden and
extreme.

Have any such possibilities been thought
of? One that was thought of but diminished
upon further investigation was the possibility
that the west Antarctic ice sheet might
glaciate into the ocean and raise the sea
level by 20 feet. As recently as 15 years ago,
the best scientific judgment was that this
could happen within 75 years as a result of
global warming. This prospect naturally attracted
attention, and further investigation
with the help of newly available satellite
sensing of glacial movement led to reassuring
estimates that if that catastrophic rise in
sea level were to happen it would take at
least a few hundred years and be gradual,
not sudden. But there isn't any scientific
principle according to which all alarming
possibilities prove to be benign upon further
investigation.

A currently discussed likely source of discontinuous
change is in the way oceans behave.
Amsterdam is north of Newfoundland,
yet is warmer, courtesy of the Gulf
Stream. There is some indication that in
earlier interglacial periods ocean currents
may have pursued different courses. If a
current like the Gulf Stream, or the
Japanese Current for the United States,
switched into an alternative pattern, the climatic
consequences might be both sudden
and severe. (Paradoxically, global warming
might freeze Western Europe.)

Insurance against catastrophes is thus an
argument for doing something expensive
about greenhouse emissions. But to pay a
couple percent of GNP as insurance premium,
one would hope to know more about
the risk to be averted. I believe research to
improve climate predictions should be concentrated
on the extreme possibilities, not
on modest improvements to median projections.


I said that current estimates suggest that
it might cost a couple percent of GNP to
postpone the doubling of carbon in the atmosphere
by several decades. Is 2 percent a
big number or a small one?

That depends on your perspective and on
what the comparison is. In recent years 100
billion dollars per year in budgets or taxes
has been a politically unmanageable magnitude
in the United States. On the other
hand, subtracting 2 percent from GNP in
perpetuity lowers the GNP curve by not
much more than the thickness of a line
drawn with a number-two pencil, or to formulate
it as I did earlier, it postpones the
GNP of 2050 until 2051. I say this not to
belittle the loss of 10 trillion dollars from
the American GNP over the next 60 years,
but only to point out that the insurance
premium, if we choose to pay it, will not
send us to the poorhouse. The proper question
is whether, if we were prepared to
spend 2 percent of our GNP in the interest
of protecting against damage due to climate


### ---Economics-1992-0-11.txt---
change, we might find better use for the
money.

I have mentioned one use: directly investing
to improve the economies of the poorer
countries. Another would be direct investment
in preserving species, ecosystems, or
wilderness areas. There is concern that many
ecosystems could not migrate as rapidly as
climate may change in the coming century;
there has been little investigation of what
might be done to facilitate the migration of
ecosystems if the alternative is to invest 5 or
10 trillions of dollars in the reduction of
carbon emissions.

VI

What can be done to reduce or offset
carbon emissions? Reducing energy use and
the carbon content of energy have received,
I believe properly, most of the attention,
especially the attention of economists. There
are other possibilities to mention.
Trees store carbon. In growing, they take
it out of the atmosphere. When they rot or
burn it goes back into the atmosphere. A
new forest will absorb carbon until it reaches
maturity (i.e., maximum carbon density) in
75 or 100 years. If it then merely replenishes
itself, with new growth replacing the
oxidized dead trees, it holds its carbon but
does not absorb more. If trees are harvested,
the lumber that becomes house

frames or furniture may last a hundred years
or more; removing mature trees and storing
them anaerobically is possible but expensive.
The most recent report of the National
Academy of Sciences considered that reforestation
in the United States might sequester
2-3 percent of current global

carbon dioxide emissions.3 The prospects
for that kind of reforestation in the rest of
the world are not nearly so promising, and
we should conclude that reforestation can
contribute, but not greatly.

Stopping or slowing deforestation is important
for reasons other than carbon emissions
but is quantitatively more important
than reforestation. Reforestation is unlikely
to take up as much as 100 billion tons of
carbon; deforestation, in areas where deforestation
is likely, could contribute several
hundred billion tons of carbon, partly because
forest subsoils contain carbon typically
greater than the amount in the trees
themselves, and this carbon is subject to
oxidation when the trees are removed.
Carbon can be "scrubbed" from stack
gases, probably not with any known technology
that would make such removal economically
competitive with reducing emissions.
(Part of the expense is disposing of sludge;
where gaseous carbon might be pumped
into the ocean or into underground cavities,
economical disposal may prove feasible.)
Parallel to reforestation is the idea of enhancing
oceanic photosynthesis, by "fertilizing"
the oceans, possibly with iron, if enough
of the carbon residues from the enhanced
growth will sink rather than remain near the
surface. Experiments would probably be reversible
and modest in scale; their political
acceptability may be tested in the near future.


Finally-although nothing is final in a
subject as new as the one we are talking
about-there are numerous possibilities for
putting substances or objects in orbit or in
the stratosphere to reflect something like 1
percent of incoming sunlight to offset a large
part of the radiation imbalance caused by
greenhouse gases. Some of these are as
apparently innocuous as stimulating cloud
formation, and some are as dramatic as
huge mylar balloons in low earth orbit. Until
very recently these possibilities were
nearly unmentionable, but they have recently
been dignified by inclusion, along with
caveats about "large unknowns concerning
possible environmental side effects," in the
1991 report of the National Academy of
Sciences. I shall not pursue them here, except
for two observations. First, if in decades
to come the greenhouse impact begins to
confirm the more alarmist expectations, and
if the economic sacrifices required to reduce
3Their estimate is 10 percent of U.S. emissions at
"low to moderate cost" on economically or environ-
mentally marginal crop and pasture lands and nonfederal
forest lands in the United States (National
Academy of Sciences, 1991 p. 57). A review of the
issues in both afforestation and deforestation by
Andrew Plantinga is in Joel Darmstadter (1991); an
optimistic estimate of the afforestation option is that of
Robert J. Moulton and Kenneth R. Richards (1990).


### ---Economics-1992-0-12.txt---
emissions prove unmanageable for economic
or political reasons, some of these
"geoengineering" options will invite attention.
Second, if they do, and especially if
they prove to be within the budgetary capabilities
of individual nations, international
greenhouse diplomacy will be transformed.
VII

What remains nearly certain is that the
main responses to the greenhouse threat
will be adapting to climate as climate
changes and reducing carbon emissions.
(CFC's are potent greenhouse gases and, if
unchecked, might rival carbon dioxide in
decades to come; but international actions
are making good progress and are among
the cheapest ways of reducing greenhouse
emissions.)

Like estimates of warming, estimates of
the costs of reducing emissions require some
common but arbitrary objective to be comparable.
A doubling of carbon became the
conventional benchmark for warming estimates;
no such benchmark for reduced carbon
emissions has been adopted for estimating
costs. (In principle, the estimates could
adopt that doubling: the issue could be formulated
as the cost of retarding the doubling
time by a decade, two decades, or half
a century.) Most estimates take as their
target a reduction of emissions either to a
specified fraction of what they would be in
the absence of controls, or to some fixed
ratio to the emissions of 1990 or the projected
emissions of 2000 or 2010. The estimates
examine minimum-cost trajectories,
implicitly or explicitly assuming something
like a uniform tax on the carbon content of
fuel as the policy instrument. They typically
make some assumption about a "fallback"
energy technology, at least for electricity,
available at some price in some decade of
the next century. They have to project estimates
of non-price-induced improvements
in the use or avoidance of energy by industries
and households. And if they deal with
global emissions, they have to make some
assumption about the distribution of abatement
efforts among nations, especially
among the developing countries, which, including
China, account for about a quarter
of emissions now and would be expected to
account for half by the middle of the next
century.

Any estimate of the cost of abatement
needs therefore to specify at least half a
dozen target assumptions. Furthermore, the
estimates are produced by people and institutions
that do not simultaneously estimate
the costs associated with climate change,
either damages or costs of adapting; the
estimates do not optimize the combined
costs of abatement and climate change. A
"not unreasonable" target for reduction
might be delaying a doubling by, say, four
decades. One decade might be too trivial, a
century too ambitious, and four decades an
objective in which most audiences would be
interested. But nobody who makes such an
estimate wishes to be interpreted as proposing
that when all the uncertainties about
climate changes and their impacts have been
resolved, if they ever are resolved, the optimum
reduction in emissions will be found
to retard doubling by 40 years, or any other
specified period of time.

All I can do to summarize a multitude of
estimates is to specify an order of magnitude
that many economists and the Congressional
Budget Office would not consider
outrageous. That is the figure I mentioned
earlier, possibly 2 percent of GNP for the
developed countries and a similar, but even
much more uncertain, percentage of GNP
for the developing world. The uncertainty
for the developing world is partly due to the
estimates being mainly derived from the
American economy.4

Two characteristics of these estimates
need to be emphasized. One is that they
tend to assume optimal technological adjustment,
as in response to a carbon tax. To
the extent that carbon emissions are controlled
by direct regulatory measures, there
may be the usual expected inefficiencies,
and I leave the reader to make his own
adjustment.


### ---Economics-1992-0-13.txt---
The second is that, since the early years
of the energy crisis in the 1970's, there have
been enthusiastic portrayals of currently
available technologies, ranging from light
bulbs to electric motors, double-glazed windows
and improved internal-combustion engines,
that for some reason have not been
successfully marketed. The interest continues,
and the recent National Academy of
Sciences study gave sympathetic attention,
but no analysis, to a number of proposals
for residential, commercial, industrial, and
transportation energy management and for
improved electricity production and fuel
supply and concluded that, including reductions
in CFC's, "The United States could
reduce or offset its greenhouse gas emissions
by between 10 and 40 percent of 1990
levels at low cost or at some net saving, if
the proper policies are implemented" (1991
p. 73).

All of these ideas are completely orthogonal
to the econometric estimates. The
Academy panel that produced the report
was unable to offer an explanation for why
these low-cost or negative-cost technologies
have not caught on. Its quantitative assessment,
including an allowance for elimination
of CFC's, ranged from as little as 10
percent to as much as 40 percent of current
U.S. emissions; CFC's aside, their range of
possibility is from zero to about 30 percent.
Whatever the correct figure, this is probably
a once-and-for-all backlog of accumulated
technologies, which once exploited may be
permanent but not progressive. But the
strong suggestion is that there is a lot to be
accomplished in the next two or three
decades.

VIII

With these qualifications, let us look at
that 2 percent of GNP as a permanent
reduction over the coming century. I consider
it altogether improbable that the developing
world, at least for the next several
decades, will incur any significant sacrifice
in the interest of reduced carbon (nor would
I advise developing countries to do so).
Anything done to reduce emissions in China,
India, or Nigeria will be at the expense of
the richer countries.

Financing energy conservation, energy
efficiency, and switching from high-carbon
to lower-carbon or noncarbon fuels in Asia
and Africa would not only be a major economic
enterprise but a complex effort in
international diplomacy and politics. If successful,
it would increase the costs to the
developed world by at least another percent
or two on top of the 2 percent I mentioned.
It is furthermore not easy to hide the transfer
of resources on the order of a couple of
hundred billion dollars, dollars "budgeted"
somehow or other, compared with hiding
some of the costs due to regulation, such as
automobile fuel-efficiency standards in the
United States. The kind of thing we are
talking about is inducing the Chinese,
through our somehow offsetting their cost,
to forgo a massive electrification based on
coal and the cheapest coal-combustion technology.
Without engaging in blackmail, the
Chinese can assert that it is not in their
interest to do that at their own expense,
even if they are the keystone of a "social
contract" and no other nation will do anything
unless the Chinese fully participate.
I shall sketch what I can imagine as a
major attack on the greenhouse problem.
And I should be explicit about what I cannot
imagine. For reasons that I would be
delighted to elaborate but for which I cannot
take space here, a universal uniform
carbon tax is not a solution that I can imagine.
My reason is simple. A carbon tax sufficient
to make a big dent in the greenhouse
problem would have to be roughly equivalent
at least to a dollar per gallon on motor
fuel, and for the United States alone such a
tax on coal, petroleum, and natural gas
would currently yield close to half a trillion
dollars per year in revenue. No greenhouse
taxing agency is going to collect a trillion
dollars per year in revenue; and no treaty
requiring the United States to levy internal
carbon taxation at that level, keeping the
proceeds, would be ratified by the Senate.
Reduce the tax by an order of magnitude
and it becomes imaginable, but then it becomes
trivial as greenhouse policy.5
5A careful treatment of the universal carbon tax is
provided by James M. Poterba (1991).


### ---Economics-1992-0-14.txt---
Tradable permits have been proposed as
an alternative to the tax. There are two
main possibilities: (i) estimating "reasonable"
emissions country by country and establishing
commensurate quotas or (ii) distributing
tradable rights in accordance with
some "equitable" criterion, such as equal
emissions per capita (a possibility that has
actually been discussed). Depending on how
restrictive the aggregate of such tradable
emission rights might be, the latter is tantamount
to distributing trillions of dollars in
discounted value and making, for a country
like Nigeria, the outcome of its population
census the country's major economic policy.
If, instead, quotas are negotiated to correspond
to every country's currently "reasonable"
emissions level, they will surely be
renegotiated every 5 or 10 years, and selling
an emissions right will be perceived as evidence
that a quota was initially too generous.
It is unlikely that governments will engage
in trades that acknowledge excessive
initial quotas.

I do not foresee negotiated national quotas
subject to serious enforcement, especially
enforcement through financial penalties.
I think any international regime for
carbon abatement can seriously include only
the developed countries, and I exclude from
this category the countries that we used to
call the Eastern Bloc. I can easily imagine
institutional arrangements that are universalist,
some kind of "framework agreement"
to which every country subscribes, with specific
commitments to be negotiated later.
But I expect serious commitments to be
undertaken only by the countries that can
afford to, and I am undecided whether an
institutional pretense of a universalist system
has advantages or, instead, the developed
world should proceed independently
and unencumbered with the need for a universalist
facade.

The model that I find most helpful in
conceptualizing a greenhouse regime among
the richer countries is the negotiations
among the countries of Western Europe for
distributing Marshall Plan dollars among
themselves and the negotiations, beginning
in 1951, on "burden sharing" in NATO.
There was never a formula for distributing
Marshall Plan dollars; there was never an
explicit criterion, such as equalizing living
standards, equalizing growth rates, maximizing
aggregate output or growth, or establishing
a floor under levels of living. Baseline
dollar-balance-of-payments deficits were a
point of departure, but the negotiations took
into account investment needs, traditional
consumption levels, war-induced capital
needs, opportunities for import substitution
and export promotion, and opportunities to
substitute intra-European trade for trade
with hard-currency countries.

The United States insisted that the recipients
argue out and agree on shares. In the
end, they did not quite make it, the United
States having to make the final allocation.
But all the submission of data and open
argument led, if not to consensus, to a reasonable
appreciation of each nation's needs.
The negotiations were professional; they
were assisted by a proficient secretariat. The
resources involved for most recipient countries
were immensely important. Good relations
were observed throughout; and proficiency
in debate, acceptance of criteria, and
negotiating etiquette steadily improved.
That is the only model I find plausible,
and I believe distribution of Marshall Plan
and defense-support funds to Europe is the
only model of multilateral negotiation involving
resources commensurate with the
cost of greenhouse abatement. (In the first
year, Marshall Plan funds were about 1.5
percent of U.S. GNP and-adjusting for
overvalued currencies-probably 5 percent
of OEEC GNP).

What that model suggests is that the main
participating countries in a greenhouseabatement
regime would submit for each

other's scrutiny and cross-examination plans
for reducing carbon emissions. The plans
would be accompanied by estimates of
emissions or emissions reduction from some
projected level, but any commitments undertaken
would be to the policies, not the
emissions. And not all of the plans would
necessarily be commitments.

The United States, for instance, could
present a plan for the introduction of a new
generation of nuclear power reactors beginning
sometime in the next century, but it is


### ---Economics-1992-0-15.txt---
difficult to see how the federal government
can commit itself to what reactors public
utilities will be purchasing 20 years from
now. The United States can have a plan to
mandate fuel-efficiency standards for automobiles,
but it takes 10 years for the standards
to work their way into the automobile
fleet, and there is no accounting procedure
that will estimate the effect on motor-fuel
consumption of any level of average fuel
efficiency a decade from now.

The current popular expectation is that
participation in any greenhouse regime will
take the form of commitments to specified
percentage reductions of emissions below
those of some specified year, like 1990 or
2000. I cannot help believing that adoption
of such a commitment is an indication of
insincerity. A serious proposal would specify
policies, like taxes, regulations, and subsidies
and would specify programs (like research
and development), accompanied by
very uncertain estimates of their likely effect
on emissions. In an international public
forum, governments could be held somewhat
accountable for the policies they had
or had not put into effect, but probably not
for the emissions levels achieved.
Such a modest beginning will require
finding a way to sublimate the current international
enthusiasm for a new universalist
greenhouse regime into institutional arrangements
that are helpful but noncommittal
when the U.N. Conference on Environment
and Development convenes next

June. This will require an understanding
among the developed countries that it is
initially up to them to find a way to mobilize
their populations in support of national
greenhouse policies.

Ix

A major commitment to financing emissions
abatement in the developing world is
surely too far away to need specific plans
now. A developing-world carbon-abatement
effort would, in principle, be altogether different
from foreign aid as we have known it
since World War II. In principle it would all
be directed, from whatever sources and
through whatever channels, to protecting
that same global common. There would be,
for the first time, a single criterion: economizing
carbon. In the abstract, aid recipients
in the war on greenhouse gases would
not compete; they would not make IndiaPakistan
comparisons, or Arab-Israel, or
Poland-Czechoslovakia. All would in principle
benefit equally from maximum carbon
conservation, wherever it could be achieved.
Trees may grow more rapidly, in carbon
content, in Madras or Szechuan or Borneo
or Alaska or South Carolina, but if someone
were willing to finance the growth of a tree
to absorb carbon dioxide, the citizens of
those states should not have the slightest
care where the tree were to be planted; they
all benefit solely from the carbon fixed in
the tree and benefit more, the faster the
tree grows, no matter where it grows.
It wouldn't work that way, of course.
Somebody gets the shade, or leases land for
the tree; and if it's not a tree but a nuclear
power plant to supplant coal, there are local
impacts that make huge differences, and
negotiations over sharing the cost differential
between the coal and the nuclear plants.
But it is worth noticing that if there were a
"pure" carbon-abatement or carbon-absorbing
technology, one that accomplished nothing
else, there should be no dispute about
locating it wherever it would be most effective.
That is new in foreign aid and foreign
investment.

If the developed countries ever manage
to act together toward the developing countries,
their bargaining position is probably
enhanced by the fact that cleaner fuels and
more efficient fuel technologies bring a
number of benefits other than reduced carbon,
and recipients of greenhouse aid will
be actively interested parties, not merely
neutral agents attending to the global atmosphere.
At the same time, large nations like
India and China will be aware of the extortionate
power that resides in ambitious
coal-development projects.

On a greatly reduced scale, there may be
something constructive to do more immediately.
There is a huge difference between
transferring "technology" and transferring
capital goods that embody technology or,
going further, financing entire investments


### ---Economics-1992-0-16.txt---
(local construction, etc.) in which the technology
is embedded. The difference in cost
is at least an order of magnitude. While the
developed countries are feeling their way
into some common attack on their own carbon
emissions, a tangible expression of their
interest and an effective first step would be
to establish a permanent means of funding
technical aid and technology transfer for
developing countries, as well as research,
development, and demonstration in
carbon-saving technologies suitable to those
countries. Eventually the rural Chinese
household may cook more efficiently with
nuclear-powered electricity, but for another
generation or two what is important is less
carbon-wasteful ways of cooking and heating.


Maybe there is a role here for the carbon
tax. Western Europe, North America, and
Japan will be burning 3 or 4 billion tons of
carbon per year for the next decade. Taxing
themselves, that is, contributing in proportion
to the carbon they consume, at one,
two, or three dollars per ton, they could
contribute to a fund that might begin at $3
billion per year and grow to $10 billion. The
carbon tax is a little arbitrary here, and a
U.S. administration may be wary about a
precedent that carries over when the tax
rises an order of magnitude, but compared
with alternative criteria for sharing costs it
might not even be a bad precedent.
 ## Economics-1993-0


### ---Economics-1993-0-03.txt---
I. The Allocation Task of Yesteryear
Nearly two score years ago, on the occasion
of Columbia's bicentennial celebration,
Sir Dennis Robertson gave an address entitled
"What Do Economists Economize," the
burden of which was that, since presumably
economists are the most expert economizers,
they should economize the most precious
thing in the world, namely, love, or
altruism. This would be done in part by so
arranging things that in the ordinary conduct
of life individual choices made on the
basis of self-interest in terms of market
prices would at least be consistent with maximizing
social welfare, so that the exercise
of scarce resources of altruism could be
concentrated on situations where Adam
Smith's unseen hand could not be made to
serve. To me, one implication of this was
that economists should see to it that market
prices correctly reflect the relevant marginal
social cost of various alternatives. I have
devoted a major part of my career to the
promotion of such marginal-cost pricing, but
thus far with a notable lack of practical
success outside academia.

At the time of Robertson's address, indeed,
there was a certain euphoria prevailing
among at least part of the economics
profession over the prospect of curbing the
business cycle and maintaining a high level
of economic activity through Keynesian fiscal
policy. Under these circumstances it was
reasonable to think that the chief remaining
job of the economist was to assure a
Pareto-efficient allocation of a given aggregate
of resources. The event, however,
proved otherwise. The conventional wisdom
of regarding budget deficits as improvident
prodigality, and government debt as the
legacy of a craven deferral of burden to the
future, resumed command.

One eminent economist is said to have
remarked, in effect, that it was the function
of the science of public finance to see to it
that nothing of importance is ever done or
left undone merely for financial reasons.
Alas, the financial reasons have thus far
carried the day, and we have not had anything
approaching real full employment
since the Korean War, or indeed in peacetime
at any time since 1925, if then, at least
in terms of the Beveridge definition of full
employment as a situation wherein there
are at least as many unfilled job openings as
there are unemployed individuals seeking
work.

In the Eisenhower years, the conventional
wisdom held sway in spite of the
absence of serious contraindications to the
Keynesian prescription. In the 1960's, the
simple Keynesian analysis began to be called
into question by the emergence of stagflation,
a phenomenon not contemplated by
the earlier Keynesian models. A new relationship,
the Phillips curve, relating the
evolution of inflation to the level of unemployment
was added to the economists'

armamentarium, with its "non-inflationaccelerating
rate of unemployment" or

NIARU.

This NIARU is of course not a fixed
datum, but varies over time and place according
to the sociopolitical ambience, the
mechanics of the labor market, and the
vigor of competition. It may have been rising
over time as a result of the increased
sophistication and differentiation of products,
real and factitious, giving sellers, as
the ones most knowledgeable about the
characteristics of their products and their
markets, considerable leeway to raise their
prices without unacceptable loss of sales.
This process is ultimately held in check only


### ---Economics-1993-0-04.txt---
by the presence of underutilized labor and
other resources. Currently in the United
States the NIARU appears to be around
4-6 percent.

In some quarters this NIARU has even
been termed the "natural" rate of unemployment,
in one of the most vicious euphemisms
ever coined. Some have even gone
so far as to define "full employment" as
being the NIARU. But while 5-percent unemployment
might be barely tolerable if it
meant that everyone would be taking an
additional two weeks of vacation every year
without pay, it is totally unacceptable as a
social goal when it means unemployment
rates of 10, 20, or even 40 percent among
disadvantaged groups, with resulting increases
in poverty, homelessness, poor
health, drug addiction, and crime. Yet the
hard political fact is that at such a NIARU
the great majority of the voting population,
including most of the politically active upper
and middle classes, will have relatively
little personal experience of severe unemployment,
while nearly everyone will have
some direct experience of inflation. Many
seem to feel that if only prices would stop
rising they would benefit correspondingly by
having their income go further, giving relatively
little thought to the effect on their
incomes. Even those with large mortgages
or other debts, who would actually gain
from inflation, tend to concur in the notion
that they suffer from it. It is thus extremely
difficult to get political support for antiunemployment
measures that are perceived

as involving a threat of inflation, at least
until unemployment reaches 7 percent or
more, at which point unemployment becomes
a more widespread threat.

Actually it is the uncertainty as to the
rate of inflation, and not its level, that does
the damage. An assured, moderate rate of
inflation can be adapted to by adjusting
nominal rates of interest and the terms of
long-term contracts involving money payments.
The "menu cost" of changing price
tags and catalog quotations is probably less
important than the mental effort required
of consumers in forming an idea of what an
appropriate current price is for infrequently
purchased items, such as furniture or clothing.
An inflation rate assured to stay between
5 percent and 6 percent, say, might
even have advantages. Monetary policy
would be more powerful in stemming a
downturn in that very low and even negative
real rates of interest would become feasible
as a stimulus to investment. It might in
principle be easier to keep inflation within a
1-percent range between 5 percent and
6 percent, than to keep it within a 2-percent
range between - 1 percent and + 1 percent,
given the smaller real value of noninterest-
bearing moneys in circulation, even
allowing for the superior political focusing
power of a target of 0 percent as compared
to one of 5.5 percent.

The base of the income tax would be
broadened also, making it possible to have a
tax that is more progressive and more productive
of revenue with lower marginal rates
and less of a distortionary effect. A tax
based on nominal accrued income would in
effect be a tax on a base consisting of real
income plus a percentage of net worth.
While this is not what is meant by an ideologically
pure income tax, in terms of its
practical effects it can be deemed a superior
tax.

It is the possibility of substantial changes
in the rate of inflation, either up or down,
that does the damage. Such changes involve
a disappointment of expectations and a redistribution
of wealth and income derived

from a given national product that is capricious
and often inequitable, but it does not
of itself substantially reduce the amount to
be distributed. Unemployment, on the other
hand, directly and definitely reduces the
total product to be distributed. Unanticipated
changes in the rate of inflation, up or
down, may be considered to be a form of
legitimized embezzlement, whereas unemployment
is vandalism.

Nevertheless, the stance of the politicofinancial
establishment is still to look at the
bottom line as the ultimate reality, whether
of the corporation or the national budget,
and since money is the measure of all good
and evil in this kind of calculus, anything
that impugns the value of money is viewed
as a kind of sacrilege reinforced by a lurking
fear of starting down a slippery slope to


### ---Economics-1993-0-05.txt---
hyperinflation. We find the Federal Reserve
System poised to slam on the brakes at the
first sign of a resurgence of inflation, a
posture not calculated to inspire investment
in durable capital.

On the political side, we see the House
voting by a substantial majority in favor of a
constitutional amendment to require a balanced
budget, fortunately falling short of
the required two-thirds. This was done in
spite of the fact that the nominal budget as
currently computed is not a valid measure
of any significant economic quantity. The
nominal deficit would be reduced by selling
the Pentagon to a life insurance company
subject to a long-term lease-back and repurchase
option; this at least would do no
harm, unlike the sale of natural resources to
private exploiters which would actually decrease
the real heritage handed down to the
future, on the pretext of reducing the transfer
requirements embodied in the national
debt.

II. Recycling Savings Through Public
Capital Formation

From a classical standpoint, of course,
the difficulty is that no account is taken
of the distinction between transactions on
current account and on capital account. If
AT&T, General Motors, and households
had been constrained to operate under the
restrictions of the proposed balanced-budget
amendment, we would now have far fewer
telephones, automobiles, and houses. A
capital budget, with a vast expansion of
government capital outlays on roads,
bridges, research, education, and the like,
financed by borrowing, might go a considerable
way toward improving the unemployment
situation. But there is no assurance
that it could do the whole job.
III. Eliminating the Corporation Income Tax
Other classical approaches to improving
the unemployment situation exist but have
their own political opposition and in any
case are too weak to make much of a dent
in a very large need. One such measure
would be the abolition of the corporation
income tax, which is by far the most serious
hurdle in the way of private capital formation
of a kind requiring equity funding. Unlike
the capital-gains tax, the corporate income
tax is a tax largely above or before the
market, requiring a rate of return on investment
sufficient to cover the corporation tax
and leave a rate of return after tax comparable
to other investments, whereas the
capital-gains tax operates largely as a reduction
in the return to the investor after or
below the market, comparable to the reduction
of net income to the taxpayer resulting
from the personal income tax on other income.
In addition, the corporation tax
causes inefficient allocation of investment
between equity-type and loan-type investments;
it encourages thin equity and resulting
bankruptcies and reorganizations, and it
lubricates takeovers and mergers of dubious
intrinsic merit.

Reduction of the tax on capital gains, on
the other hand, might actually depress economic
activity if the additional savings out
of the tax reduction were to exceed the
additional capital formation induced. This is
the more likely in that most of the tax
reduction is likely to be saved immediately,
whereas the inducement to capital formation
is in terms of a tax reduction in a
relatively remote future, subject to legislative
vicissitudes. At best, special treatment
of capital gains greatly increases the complexity
of the tax law and diverts investment
flows from their most efficient use. There is
nothing to indicate that investments likely
to yield returns in forms defined by the tax
code as capital gains will have any superior
social value: gains from land speculation, in
particular, add nothing to the real availability
of resources.

As for the corporate income tax, in spite
of its many defects from the standpoint of
economic efficiency, it has enormous political
popularity due to the fact that nearly
everyone thinks that it is paid by someone
else. Indeed economists have differed widely
in their assignment of the "burden" of the
tax, owing to a failure to specify, or even to
consider, the macroeconomic policy changes
necessarily involved in a change in the tax.
Unlike most other taxes, the corporation tax


### ---Economics-1993-0-06.txt---
inflicts a double whammy on the economy
in that it both extracts income from the
stream of purchasing power and reduces the
recycling of savings through investment. If
imposed on a revenue-neutral basis it causes
unemployment, while if a budgetary adjustment
is made to maintain employment constant,
its burden can be thought of as falling
on future wage earners, who will have less
capital with which to work.

Problems of the deferral of income
through undistributed profits, as well as the
deferral of taxation to the time of realization
of capital gains, would ideally be met
by putting the personal income tax on a
cumulative basis, along lines I developed
while working with Carl Shoup in 1938,
whereby the deferral of the reporting of
income, by whatever means, merely involves
the borrowing of the deferred tax at a suitable
rate of interest. About two-thirds of
the internal revenue code would become
redundant, with the possible exception of
the need to deal with the international jet
set and revolving-door marriages; large
numbers of tax techies would be able to
apply themselves to more productive employment.


Failing this, an approximation to a level
playing field might be had by imposing a
small annual tax on the accumulated undistributed
surplus of corporations, roughly
equal to the interest on the stockholders'
postponed individual income tax. Similarly,
there should be a surcharge on realized
capital gains, proportionate to the length of
time held, to offset the gain from the deferral
of the tax.

If there is nevertheless a need to cater to
a political demand for something that can
be labeled a corporation tax, this might be
satisfied by levying a corporation tax on
dividends, interest, and retained earnings at
a rate corresponding to the first-bracket rate
of the individual income tax and exempting
such interest and dividends from this "normal"
rate, going back to the pre-1934 practice
of dividing the income tax into a normal
tax and a progressive surtax. To even things
up neatly, normal tax paid on other forms of
income should be deductible in computing
the base for the progressive surtax paid by a
minority of taxpayers. It would still be appropriate
to have an undistributed surplus
tax to correspond to this surtax.
IV. Tax-Exempt Bonds

Another measure that might slightly improve
investment allocation would be to replace
the exemption of interest on state and
local bonds by a taxable tax credit at a rate
that would maintain the market value of the
bonds. Low-bracket taxpayers would be little
affected, while the entire loss of revenue
to the Treasury would accrue as a subsidy to
the issuers. Upper-bracket taxpayers would
no longer have an incentive to invest in such
bonds rather than in riskier investments
more suitable to their status.
V. Taxing Imputed Income

A more important but politically more
difficult measure would be to require the
inclusion in taxable income of the rental
value of owner-occupied residences. This
would not only improve the equity and progressivity
of the income tax but go a substantial
way toward making more units

available for rental and, to a modest extent,
promoting the construction of additional
affordable rental housing and abating the
problem of homelessness. A similar case
can be made for including in the income tax
base a net rental value of nonbusiness automobiles
(equal to interest on the market
value of the car), in this case reducing the
discrimination against the use of public
transit.

VI. Shifting Property Taxes from
Improvements to Land

A measure that could provide a powerful
stimulus to investment in property improvements
would be to replace part or all of the
property tax by a tax on land value only, a
proposal that can be traced all the way back
to Franqois Quesnay and the French physiocrats
but which is more recently associated
with the name of Henry George. This


### ---Economics-1993-0-07.txt---
would remove the very serious deterrent
effect of the property tax on improvements.
Unfortunately from the standpoint of a national
employment policy, this tax is largely
levied by local governments, which are often
constrained by constitutional provisions or
state laws. Nevertheless some means of
bringing pressure to bear on these governments
to make this change might be found.
Some Pennsylvania governments are already
doing this. When levied for municipal purposes,
it might be appropriate to exempt
from the tax a flat amount per square foot
as representing the value of circumambient
agricultural land for which the urban government
can claim no credit; this would also
mitigate discriminations at jurisdictional
boundaries.

It is perhaps worth noting that the significance
of a government debt would be drastically
different in a community relying exclusively
on a land-value tax. Such a debt
would in effect be a collective mortgage on
the land, especially if it can be assumed that
land values in the community will vary proportionately
over time. Since the interest on
the community debt will generally be lower
than interest charged on individual mortgages,
it can be in the general interest of all
the taxpayers of the community for the government
to borrow as much as the market
will take, even to finance current outlays,
provided a suitable margin is left to deal
with emergencies. On the other hand such
debt financing performs no recycling of savings,
there is no room for Keynesian fiscal
policy, and Ricardian equivalence is in full
sway. This does not detract, however, from
the powerful stimulating effect of a reduction
in the tax on improvements.

VII. Limitations of "Supply-Side" Measures
Under current conditions, however, such
"supply-side" measures designed to operate
by reducing the cost of capital are likely to
be severely limited in their effect as long as
nearly all types of capital facilities are idle
or underutilized. Very little "widening" investment
is likely to take place as long as
there is excess capacity in place. At most,
some "deepening" investment in new products
or technologies may take place, or there
may be corners of the economy where relatively
rapid growth has kept capacity fully
utilized. Even in such cases, investment in
capital facilities may depend more on appraisals
of an uncertain market for the
product than on the cost of capital.
This is likely to be true not only of tax
policy but even more of monetary policy. In
any attempt to emerge from present rates of
unemployment even only down to the
NIARU within any reasonable time period,
monetary policy is likely to prove a weak
reed, sometimes aptly described as pushing
on a string. The main difficulty is that monetary
policy bears primarily on short-term
interest rates and credit availability and in
its usual practice does not directly control
long-term rates, which are the important
rates for most decisions involving real
durable capital formation. The posture of
the Federal Reserve System in holding itself
ready to slam on the brakes at the first sign
of resurgent inflation is poorly adapted to
bringing long-term rates down. It does not
appear that the Fed has either the will or
the resources to do enough about long-term
rates to do very much to increase capital
formation, especially when idle and underused
capacity pervades much of the economy.


VIII. Savings Recycling by Government
This brings us inevitably around to fiscal
policy. Here it is necessary to stop thinking
of the conventional nominal budget deficit,
or even of a current-account deficit in a
budget drawn up in terms of distinguishing
capital and current-account items, and to
start thinking of fiscal policy in terms of its
role in recycling savings, in excess of what is
recycled by private investment, into the
stream of purchasing power. The conventional
wisdom seems to argue that increased
employment requires that the economy
grow, growth requires investment, and investment
requires savings; therefore let's
encourage saving through IRA's, tax expenditure
rather than income, and tighten our


### ---Economics-1993-0-08.txt---
belts to restore the economy to its normal
state of health.

It doesn't work that way. Savings are not
like a sack of potatoes which if not sold at
the current price will stay on hand and put
a downward pressure on the price until sold.
Savings not immediately taken up to create
capital simply vanish in reduced income,
without even exerting a downward pressure
on interest rates. If I yield to the allurements
of tax concessions to IRA's to the
point of not having my hair cut, this puts $8
more in my bank account, but $8 less in the
barber's account; there is nothing that makes
it any easier for anyone to obtain funds with
which to create capital, nor anything that
makes the prospect more attractive. As
Gertrude Stein remarked, "the money is
always there, it's the pockets that keep
changing." If the barber reacts by curtaling
his consumption, this further reduces national
income and saving. I may succeed in
my attempt to save, but only by reducing the
saving of others by even more. Savings are
an extremely perishable entity. Say's law
fails as soon as part of the income generated
in the process of producing the supply
is shunted off into savings that fail to get
converted into new capital goods.
On the other hand, if some genius invents
a new product or process and obtains a
credit or borrows the funds needed to finance
the capital involved in its production,
this added real wealth is, ipso facto, someone'
s saving. Instead of Say's law, we have
"capital formation creates its own saving."
Similarly, if the government borrows funds
created by credit expansion and recycles
them into purchasing power through outlays,
whether on current or capital account,
this creates both income out of which additional
savings will be attempted and demand
that may induce the private investment
to meet it.

Not all deficit financing, however, results
in recycling of savings, whether measured by
the current capriciously defined nominal
deficit or by a more rational definition involving
accounting for government assets.
We have seen that in a community relying
exclusively on a land tax, recycling does not
take place. Nor would the sale of the Pentagon,
or the purchase of an office building
currently being rented by the government,
offset by bond transactions, involve any
change in the level of recycling. Government
recycling is in principle the excess of those
government outlays that are regarded by
their recipients as income over those government
receipts that are regarded by their
payors as reductions in their disposable income.
Even this is subject to some caveats: if
government investment in a power plant, for
example, substitutes for investment that
would otherwise have been made by private
enterprise, there is no net recycling.
On the whole, however, recycling tends to
vary in rough correlation with the nominal
deficit, and the strength of the notion in the
minds of the public and their representatives
that deficits are bad and that the
"budget" should be balanced may make it
difficult to achieve an adequate level of
recycling. Some help in this respect may be
obtained by going to a capital budget system,
in which balance would be sought only
for the current-account part of the budget,
borrowing for the capital account being justified
by comparisons with corresponding
private practices and by the thought that
future generations being burdened with the
debt would also reap benefits from the capital
passed on to them. While this may constrain
choice away from what rational voters
would have chosen as the optimal level of
government capital formation, there would
seem to be sufficient scope for government
capital investment to provide sufficient recycling
to bring about full employment, particularly
if investments in education, research,
space exploration, and the like are considered
eligible for treatment as capital investment.
Some of these projects, even if they
would not stand scrutiny aside from their
function in justifying income recycling, may
nevertheless have the same kind of justification
as the building of the Egyptian pyramids
had for Keynes. On general welfare
grounds, one might well prefer recycling in
terms of borrowing to finance health care to
borrowing to finance space stations, but if
borrowing for health care is deemed to create
an ideologically sinful current-account
deficit, space stations it will have to be.


### ---Economics-1993-0-09.txt---
IX. The Need for Direct Inflation Control
Long before the economy reaches a really
satisfactory level of full employment, however,
as employment gets to the NIARU
level, and inflation threatens to accelerate,
the Fed is likely to try to slam on the
brakes, and demands for a more stringent
budget balancing and cutback of "government
waste" are likely to be heard in the
halls of Congress. To get anywhere near a
satisfactory level of unemployment, some
method of dealing with inflation will have to
be devised. We are short of tools.
In effect, the economy can be thought of
as having three major parameters that we
would like to control: the level of employment
of human and other resources, the
price level, and the division of the resulting
total product between provision for current
wants and investment in growth and the
future. At the same time, we have only two
major policy tools: monetary and fiscal policy.
In an era when inflation was not a
threat, one could think of these two tools as
controlling the level of employment and the
rate of growth, with low interest rates combined
with a deficit or surplus sufficient to
maintain full employment leading to high
investment and growth, and conversely.
However, with a need to control inflation as
well, relying on only two dimensions of control
is like trying to fly an airplane without
ailerons, which were the third dimension of
control that was the key to the success of
the Wright brothers. A new tool is needed.
Over the past three decades a number of
proposals for direct control of inflation have
been made, but none has achieved general
acceptance. Wartime control of specific
prices, accompanied by rationing, was accepted
as an emergency measure and

worked in part because of patriotic willingness
to conform and in part because, being
temporary, past prices could be continued
without becoming absurd. As a permanent
scheme this is probably unworkable
and certainly unacceptable. More recent
schemes have involved tax incentives of
various kinds to provide a countervailing
downward pressure against the inherent
inflationary tendency of an imperfectly
competitive system. Such schemes have
generally suffered from difficulties in measuring
price changes at an individual-firm
level, capriciousness of results when tied to
such taxes as the corporation income tax,
and possible time lags in adjusting the
strength of the incentives to changing circumstances.


X. Market-Based Inflation-Control Plans
A few years ago David Colander came to
visit me and reported on a proposal by
Abba Lerner for a market in rights to raise
prices. Those wishing to raise their prices
would be required to purchase the right
from those prepared to lower their prices,
thus assuring a constant overall price level.
While this neatly circumvents the problem
of adjusting the strength of incentive to
changing inflationary pressures, the problem
remains of how to measure price
changes in the face of quality changes, new
products, and variations in the terms of sale
such as delivery, reliability, service, credit
terms, tie-in sales, and the like.
More pregnant was the question of how
to deal with cases in which prices paid to
suppliers have risen. A somewhat similar
problem arises with gross receipts taxes,
which discriminate in favor of vertically integrated
operations and against situations in
which the product passes through several
hands on the way to the market. In Europe
this problem has been solved by shifting
from gross receipts taxes and retail sales
taxes to value-added taxes, which immediately
suggests that instead of a market in
rights to raise prices we have a market in
rights to value added.

XI. Control with Marketable Gross
Markup Warrants

For semantic reasons I have chosen to
speak in terms of "gross markups" rather
than value added, as being more suggestive
of something to be restrained rather than
promoted. In principle, gross markups simply
refers to the excess of sales revenue over
amounts paid for nonprime inputs. In operation,
warrants for gross markups for a


### ---Economics-1993-0-10.txt---
prospective accounting period would be issued
to each firm on the basis of the gross
markups for a corresponding preceding period,
plus or minus adjustments for changes
in prime inputs such as labor and invested
capital. These warrants would be issued in
sufficient total face value to correspond to
the value at a desired price level of the
output expected to be produced by the inputs
against which the warrants were issued.
They would be freely tradable for cash in a
competitive market, and if at the end of the
accounting period a firm is found to have
retained or acquired fewer warrants than
the actual amount of its gross markups for
the period, a penalty tax would be assessed.
This tax would not be a substantial source
of revenue, but would serve merely as an
enforcement device. It could be set at a
level fairly certain to be higher than the
market price of the warrants.

Adjustment of the warrant issue for
changes in investment could be made simply
on the basis of a uniform percentage of
such change. Adjustment for changes in employment
is somewhat more difficult: a flat
amount per employee or man-hour takes
too little account of variations in qualifications,
while to allow adjustments equal to
payrolls would run a danger of allowing
inflationary wage increases. Some formula
such as a percentage of payrolls plus a flat
amount per employee might be satisfactory;
such a formula would involve a certain bias
in favor of the employment of low-skill labor,
which may be considered desirable in
view of the fact that this is where the unemployment
problem is most serious.

Administration would seem to pose no
insurmountable problems. Determination of
gross markups is essentially no different than
the assessment of a value-added tax such as
is widespread in Europe. Adjustment for
investment can be made on the basis of
accounts already needed for income-tax
purposes, while adjustments for employment
can be related to the social-security
records. Some special methods may have to
be developed for dealing with the selfemployed
and very small firms, and possibly
some classes of firms could be excluded
from the scheme, as is sometimes done with
the value-added tax.

XII. Prospects for Rapidly Reaching
Genuine Full Employment

With such a scheme in place, what can we
plan for in terms of getting from where we
are to full employment? Currently unemployment
is reported as about 7.5 percent,
and full employment can be reckoned at
about 1.5 percent, giving a slack to be made
up of 6 percent. Using Okun's ratio of percentage
change in GNP to percentage

change in reported unemployment of 2.5,
we have a slack of 15 percent to be made
up. If this slack can be taken up within two
years, this will be 7.5 percent per year; if to
this we add 2.5 percent for growth in the
labor force and in productivity, we get 10-
percent annual growth in GNP over two
years. After two years, we hit the fullemployment
ceiling, and growth thereafter
will be limited to the labor force and productivity
factor, possibly between 2 percent
and 4 percent.

Is public finance up to the job of reaching
the goals thus defined in terms of the limits
of our real resources? Possibly, but it requires
breaking new ground. One would
have to begin with increasing government
recycling as rapidly as possible by 8-10 percent
of GNP in order to inaugurate the
1.0-percent growth rate. How rapidly this
could be done would of course depend on
the political and legislative ambience. From
some points of view the fastest and easiest
way to do this is by tax cuts. Unfortunately,
if tax cuts are temporary they tend to be
viewed as windfalls to be saved rather than
spent, so that only part of the tax cuts are
effectively recycled. Alternatively, if not announced
as temporary, tax cuts tend to create
a resistance to later tax increases called
for by full-employment conditions and large
debt-service requirements. This is especially
threatening in the present context of political
campaigning on the basis of promises of
no new taxes. Perhaps the best tax cut would
be a cut in the payroll taxes, as promising
the maximum proportion of recycling, if this


### ---Economics-1993-0-11.txt---
can be done in the face of outcries that this
would be jeopardizing the financial soundness
of the social-security system.
Outlays on actual programs, on the other
hand, are somewhat harder to start and
stop rapidly. There is also the need not to
get too far ahead of the effective operation
of whatever anti-inflation program is put in
place, whether the program of gross markup
warrants proposed above or some other, lest
anticipatory speculation and inflation get
out of hand. The exact program for the
start-up period will require careful study.
What happens after the first few months
will depend to a large extent on what Keynes
called the "animal spirits" of the financial
community. At one extreme there could be
such horror and alarm at the violation of
the conventional wisdom concerning the
sinfulness of deficits as to produce a
widespread hibernation and flight to foreign
shores. More likely, once the financial community
has become convinced of the seriousness
of the administration's purpose to
bring about full employment, and once it is
anticipated that demand will shortly use up
the spare capacity of existing productive
facilities, private capital formation may pick
up to the point of absorbing and recycling
individual savings sufficiently so that government
recycling may for the time being
become unnecessary. At the same time, government
revenues from increased GNP will
increase and outlays for unemployment insurance
and welfare will decrease. Also,
there may be a need to shut down those
governmental programs that compete for
real resources with private capital formation,
in order to avoid a real "crowding out"
(as contrasted with the financial crowding
out alleged to occur as a result of government
borrowing associated with a tax cut).
As a result, a brief period of budget balance
or even of surplus may become appropriate.
As the economy hits the ceiling of full
employment, however, still another transition
becomes necessary. For a while capital
formation may continue on its momentum,
recycling savings but producing excess capacity
that either cannot find labor with
which to operate or cannot find markets in
which to sell its product. Within a short
time after hitting the full-employment ceiling,
capital formation will have to drop from
that appropriate to a 10-percent growth rate
to that suited to a far slower growth rate. At
this point attempted savings may again exceed
what can be absorbed by private capital
formation, even at very low rates of
interest. Other ways to recycle the excess
will again become necessary, one of which
will be renewed government recycling.
XIII. Long-Term Excess of Demand Saving
over Private Investment

There is, indeed, no principle of economics
that says that there will always be a
feasible rate of interest that will equate
desired savings and private capital-formation
under conditions of steady full employment.
Current trends seem to be such as to
make such a possibility unlikely. One factor
has been a spate of capital-saving innovations
and practices. Fiber optics, when fully
utilized, costs less per unit of service than
previous technologies by orders of magnitude,
leaving ductways planned for copper
conductors forever surplus; electronic exchanges
occupy a fraction of the space
formerly required by equivalent electromechanical
exchanges; just-in-time practices
reduce investment in inventory; improved
communications enable more freight to be
carried on a single track line with sidings
than was formerly carried by a full two-track
line; a man assembling electronic gear with
a soldering iron uses far less capital than
the man in the pulpit of a rolling mill, and
service industries generally use less capital
per employee than manufacturing, mining,
or transportation.

Moreover, before gross investment can
begin to recycle private savings, it must first
recycle funds set aside in depreciation,
amortization, depletion, and obsolescence
charges, while rapid obsolescence due to
accelerating technological progress makes
capital formation relatively insensitive to
changes in interest rates. Very low or negative
interest rates may stimulate investment
in nondepreciating assets such as land, but


### ---Economics-1993-0-12.txt---
even this is limited by the possibility that
speculative bubbles may burst, and in any
case relatively little recycling is produced
thereby, except to the extent that the enhanced
asset values cause owners to feel
wealthier and spend more.

On the savings side, increased longevity
and the high cost of old-age illness lead to
increased savings through funded pensions
and other provisions for retirement. For this
purpose, the lower the rate of interest, the
greater is the amount of current savings that
must be put aside to provide a given level of
retirement security. More recently the increased
concentration of income among the
very wealthy, who have a high propensity to
save, not so much for eventual consumption
but largely to accumulate chips with which
to play financial games and exercise economic
power, has further added to the savings-
recycling problem. Some recycling may
take place through investment abroad, reflected
in a positive trade balance and the
production of goods for export, though it is
uncertain how far this can be carried in the
face of political instability, the danger of
creating repayment problems, and the resistance
of foreign governments that do not
have an effective full-employment policy of
their own to our exporting our unemployment
to them in this way.

On balance, it may prove impossible, for
the foreseeable future, to maintain a steady
state of genuinely full employment without
a substantial amount of government recycling
of savings, a chronic budget deficit,
and a long-term increasing trend in the national
debt, however distasteful this may be
to those ideologically addicted to a balanced
budget. It may even prove necessary
for the debt to grow at a rate faster than the
growth of GNP. The burden of servicing
this debt might be kept within bounds by
reducing real interest rates, close to zero if
need be, though this might imply a higher
level of private investment than would be
chosen on its own merits. Even contemplating
such prospects calls for a significant
expansion in our range of habitual thought.
XIV. The Task Before Us

This, then, is the challenge I lay before
the economics profession. There is no reason
inherent in the real resources available
to us why we cannot move rapidly within
the next two or three years to a state of
genuinely full employment and then continue
indefinitely at that level. We would
then enjoy a major reduction in the ills of
poverty, homelessness, sickness, and crime
that this would entail. We might also see
less resistance to reductions in military expenditure,
to liberalization of trade and immigration
policy, and to conservation and
environmental protection programs.
I lay before you a plan I believe can
accomplish this. It involves government recycling
of excess savings plus a method of
keeping inflation under control. I believe it
can do the job while preserving the essentials
of a free-market system. There may be
some details to be worked out, but I am
confident that the basic concept is sound
and workable.

We simply cannot carry on as we have
been doing without falling apart as a community
and losing what is left of our status
of world leadership. If you don't think that
something like this can be made to work,
then it is up to us to get together to find
something that will. Otherwise, if we continue
to tie our hands with financial shibboleths
and models that tacitly assume a fixed
total of resource utilization, we are no better
than the feckless castaway whose contribution
to the solution of the problem of
dealing with cases of canned goods was "let's
just assume we have a can-opener."
 ## Economics-1994-0


### ---Economics-1994-0-03.txt---
Forty years ago economists discovered the
"residual." The main message of this literature,
that growth in conventional inputs
explains little of the observed growth in
output, was first articulated by Solomon
Fabricant in 1954 and emphasized further
by Moses Abramovitz (1956), John Kendrick
(1956), and Robert Solow (1957).1 The pioneers
of this subject were quite clear that
this finding of large residuals was an embarrassment,
at best "a measure of our ignorance"
(Abramovitz, 1956 p. 11). But by
attributing it to technical change and other
sources of improved efficiency they turned
it, perhaps inadvertently, from a gap in our
understanding into an intellectual asset, a
method for measuring "technical change."
Still, it was not a comfortable situation, and
a subsequent literature developed trying to
"explain" this residual, or more precisely, to
attribute it to particular sources (Griliches
1960, 1963a,b, 1964; Edward Denison, 1962;
Dale Jorgenson and Griliches, 1967). The
consensus of that literature was that, while
measurement errors may play a significant
role in such numbers, they could not really
explain them away. The major sources of
productivity growth were seen as coming
from improvements in the quality of labor
and capital and from other, not otherwise
measured, sources of efficiency and technical
change, the latter being in turn the
product of formal and informal R&D investments
by individuals, firms, and governments,
and the largely unmeasured contributions
of science and other spillovers. The
prescription of additional investments in education,
in science, and in industrial R&D
followed from this reading of history as did
also the hope and expectation that the recently
observed rates of "technical change"
would continue into the future.
This general view of the sources of growth
was put into doubt by the events of the
1970's and 1980's. Beginning in 1974 (or
perhaps already in 1968) productivity growth
slowed down significantly in the United
States and abroad, and it has not fully recovered
yet, at least as far as national aggregates
are concerned. The many explanations
that were offered for these events were not
very convincing (see e.g., Denison, 1979;
Martin Baily and Robert Gordon, 1988;
Griliches, 1988). As time went on and the
direct effects of the energy-price shocks
wore off but the expected recovery did not
come or came only weakly, more voices
were heard arguing that the slowdown might
not be temporary; that the energy-price
shocks just revealed what was already there
-a decline in the underlying trend of technical
change in the world economy; that the
growth opportunities that had opened up in
the late 1930's and had been interrupted by
World War II have been exhausted, reflecting
perhaps the completion of an even
longer cycle, going back to the beginnings of
this century (see e.g., Alfred Kleinknecht,
1987; Gordon, 1993a). Even more ominously,
the slowdown was blamed on diminishing
returns to science and technology in
general and the onset of widespread socioeconomic
sclerosis (see e.g., William Nordhaus,
1972, 1989; Mancur Olsen, 1982;


### ---Economics-1994-0-04.txt---
F. M. Scherer, 1983, 1986; Robert Evenson,
1984; Baily and A. K. Chakrabarti, 1988).
This is a rather pessimistic view of our
current situation, and I would like to argue
that the observed facts do not really support
it. But that will not be easy, both because
some of the "facts" are contradictory and
because our measurement and observational
tools are becoming increasingly inadequate
in the context of our changing economy.
Nevertheless, I will review some of the
evidence for such views and argue with their
interpretation. There are several possibilities
here: (i) this view is true and that is sad;
(ii) it is not true and recovery is around the
corner if not already underway; (iii) it may
be true, but whatever is or is not happening
has little to do with diminishing returns to
science or industrial R&D. Or, (iv) it may
be that we just do not know. As is the case
with global warming, we may not have an
adequate understanding of the mechanisms
producing growth or adequate data to adjudicate
whether there has or has not been an
underlying trend shift. If that is true, as is
most likely, the question arises as to why we
don't know more after years of research
done by so many good people. What is it
about our data and data acquisition structure,
and possibly also our intellectual
framework, that prevents us from making
more progress on this topic?

In discussing this range of topics, I will
concentrate primarily on the R&D component
of this story-not because it can explain
much of the productivity slowdown (it
cannot), and not just because this is where I
have done most of my recent work, but
because it illustrates rather well the major
point I want to make here tonight: that our
understanding of what is happening in our
economy (and in the world economy) is constrained
by the extent and quality of the
available data. I will also allude briefly to
similar issues which arise in interpreting the
productivity contribution of computers in
the economy. Parallel tales about data constraining
our understanding could also be
told about other potential productivityslowdown
villains: energy-price shocks, insufficient
investment in'physical capital, and
possible declines in human-capital investments.
Having reached the verdict of "not
proven," largely on account of insufficient
evidence, I shall make a number of more
general remarks on the state of our data
and the possible reasons for it. The major
message that I will be trying to convey is
that we often misinterpret the available data
because of inadequate attention to how they
are produced and that the same inattention
by us to the sources of our data helps explain
why progress is so slow. It is not just
the measurement of productivity that is affected.
Other fields of empirical economics
are also struggling against the limitations
imposed by the available data. Great advances
have been made in theory and in
econometric techniques, but these will be
wasted unless they are applied to the right
data.

I. The "Facts"

There are three sets of "facts" to look at:
what has happened to productivity, what
has happened to investment in R&D and
science, and what has happened to the relationship
between them. Sometime in the

late 1960's measured productivity growth in
the United States started to slow down.
After a mild recovery in the early 1970's,
the world economy was hit by two successive
oil-price shocks which dropped economic
growth rates in most of the developed
economies to levels significantly below
those experienced in the 1960's and early
1970's. While the effects of the oil-price
shocks wore off and real energy prices declined
to close to their earlier levels, productivity
growth rates did not recover much.
At this point, and also somewhat earlier,
many observers started wondering whether
something more fundamental than just an
energy-price-shock-induced business cycle
was afoot. Standing in the early 1980's and
looking back at the recent past, one would
have observed a decline in total patents
granted in the United States beginning in
the early 1970's and a decline in the share
of GNP being devoted to industrial R&D
starting in the mid-1960's, the timing looking
suspiciously appropriate for declining
productivity growth rates 5-10 years later.


### ---Economics-1994-0-05.txt---
0.030

0.025 -,,"

"Measurable" ,"

g*% /

,.020 Total

0.020 -^~^

"Unmeasurable"

0.015 ,_5 ,, ,,l

0.010t-

0.005 l6 ... 66 ... 67 ... l7. 1l8' 1l85 .. l llllll
1950 1955 1960 1965 1970 1975 1980 1985 1990
FIGURE 1. GROSS DOMESTIC PRODUCT PER MAN-HOUR (THOUSANDS OF 1982
DOLLARS, UNITED STATES, 1948-1990)
Notes: Measurable sectors are agriculture, mining, manufacturing, transportation,
communications, and public utilities; unmeasurable sectors are construction, trade,
finance, other services, and government. Values for 1977-1987 are based on 1987
weights; values for 1948-1976 are based on 1982 weights (the series are linked at
1977).

One could also see a continuous and worrisome
decline in the number of patents received
per corporate R&D dollar (see below)
. But there were also many other events
clouding this picture, making one wonder
whether faltering R&D and scientific efforts
are really the culprits behind our current
woes.

A number of discordant facts are important
for an understanding of what happened.
First, the productivity-growth decline
in many other countries was larger,
absolutely, than in the United States, and
there it was not associated with declines in
R&D investment.2 Second, as illustrated in
Figure 1, the sectors where the productivity
slowdown has persisted in the United States
are largely outside of manufacturing, communications,
and agriculture (see Gordon,

1987). Besides mining and public utilities,
which were affected more specifically by the
energy-price shocks, it has lingered particularly
in construction, finance, and other services
where output measurement is notoriously
difficult. Third, the decline in patent
grants in the 1970's was just a bureaucratic
mirage, an example of fluctuations induced
by changes in the data-generating process (a
budgetary crisis in the Patent Office) rather
than a reflection of the underlying activity
itself.3 The number of patent applications
did not decline significantly during this period,
but also it did not grow. The latter
fact, coupled with a continuous upward
growth in the absolute level of companyfinanced
R&D, resulted in a persistent de-
2For example, the rate of growth in total factor
productivity declined between the 1960's and the 1970's
by 4.5 percent in Japan, 3.3 percent in France, and
"only" 2 percent in the United States (see Organization
for Economic Cooperation and Development,
1993).

3See Griliches (1990) for more details on this story.


### ---Economics-1994-0-06.txt---
cline in the patents per R&D ratio in the
United States (and also in most of the other
countries for which we have data). This
raised the specter of diminishing returns to
R&D and offered the hypothesis of "exhaustion
of inventive opportunities" as a
potential explanation for the productivity
slowdown.

This hypothesis has been examined recently
by various authors. There are basically
two styles of analysis: one focuses directly
on the link, if any, between R&D and
productivity growth (see e.g., Griliches,
1986a; Bronwyn Hall, 1993; Scherer, 1993),
while the other uses patents as indicators of
the output of the R&D effort and looks at
what has happened to the "knowledgeproduction
function" (see e.g., Griliches,
1990; Ricardo Caballero and Adam Jaffe,
1993; Robert Evenson, 1993; Samuel
Kortum, 1993). The bridge that is missing
between these two approaches would examine
the units in which patents affect productivity
growth and ask whether they have
stayed constant over time. Without such
constancy, no clear interpretation is possible.


II. Productivity Growth and the Role of R&D
In parallel to the aggregate "residual"
literature, a more micro-oriented approach
had developed. It took the study of technical
change, diffusion, and the role of formal
R&D as its main challenge, with the hope
of bringing more of it within the realm of
economic analysis, helping thereby also to
explain some of this residual away. Using
modern language, one can interpret Edwin
Mansfield's and my own early work on diffusion
and on the role of R&D in agriculture
and manufacturing as trying to endogenize
as much of technical change as was
possible (Griliches, 1957, 1958, 1964;
Mansfield, 1961, 1965). Other important
contributors to this literature were Richard
Nelson, Scherer, Jacob Schmookler, and
Nestor Terleckyj. By expanding the notion
of capital to include also R&D capital and
estimating its effects, this literature documented
the contribution of public and private
investments in R&D and their spillovers
to the growth of productivity.4 But the magnitude
of the estimated effects was modest,
not enough to account for the bulk of the
observed residual or the fluctuations in it
(Griliches, 1988). The experience here was
similar to other attempts to account for the
residual, such as using "embodiment" theories
to magnify the potential effects of capital
accumulation (Denison, 1962; Nelson,
1962) or looking for increasing returns to
scale (Griliches and Vidar Ringstad, 1972).
These various effects are real and nonnegligible,
but not large enough.

There is one other way of trying to make
something more out of the R&D story: the
possibility that the productivity impact of
R&D has declined over time-that the coefficients
have changed. This hypothesis has
been investigated repeatedly by a number of
researchers with mixed results. Studies that
used data through the 1970's and early
1980's found no decline in the relevant coefficients.
More recent studies that analyze
data through the late 1980's report more
mixed results, varying strongly with how the
computer industry and its deflator are handled
in the analysis.5 At the same time, the
stock market's valuation of R&D fell significantly,
both in terms of ex post returns to


### ---Economics-1994-0-07.txt---
TABLE 1-INDUSTRY TFP GROWrH REGRESSIONS: COEFFICIENTS OF THE R&D-SALES
RATIO BY PERIOD, THREE-DIGIT SIC LEVEL (N = 143 OR 142)
Row Period With computers Without computers
1 1958-1973 0.332 (0.066) 0.317 (0.066)
2 1973-1989 0.357 (0.072) 0.134 (0.059)
3a 1978-1989 0.300 (0.073) 0.115 (0.062)
3b 1978-1989 "revised" 0.461 (0.070) 0.348 (0.070)
Notes: The equations include also dlog(energy/capital) as an additional utilization
variable. Standard errors are shown in parentheses. The ratio of company-financed
R&D to total sales in 1974 is from Scherer (1984) for row 1; this ratio is updated for
1984 from National Science Foundation (1992) for rows 2 and 3. Row 3b shows
total-factor-productivity growth revised downward for computers and upward for
electronic components and drugs (computers = SIC 357).
R&D in the 1980's (Michael Jensen, 1993)
and the market's view of current R&D investments
(Bronwyn Hall and Robert Hall,
1993; B. Hall, 1993).

My own recent foray into this type of
analysis of industry data at the three-digit
SIC level is summarized in Table 1.6 It
reports estimates from regressions of growth
rates in total factor productivity (TFP)
on the rate of investment in R&D (the
R&D-sales ratio), where the estimated coefficient
can be interpreted as the excess
gross rate of return to R&D (Griliches,
1979). The earlier 1958-1973 period yields
an estimate on the order of 0.33, while the
estimate for the later 1973-1989 period even
rises a bit, to 0.36. So far, so good! But
when one excludes the outlier computer
industry (see Fig. 2) the estimated coefficient
falls from 0.36 to 0.13 for 1973-1989
and even lower for 1979-1989. Only one
observation out of 143 does this!7
These results raise a major data conundrum:
is it right to treat the computer inComputers


o 0.10-

0.0

oE~~~~~+ +

S 0.05

t~~~~~~ 4,

-0.05

0.000 0.025 0.050 0.075 0.100 0.125
R&D-Sales Ratio in 1984

FIGURE 2. TOTAL-FACTOR-PRODUCTIVITY GROWTH
(PER ANNUM) AND RESEARCH INTENSITY
IN U.S. MANUFACTURING, THREE-DIGIT
SIC LEVEL, 1978-1989

dustry as an outlier and exclude it from
such calculations just because the productivity
measure may be better there? It is quite
possible that if other technologically advanced
industries (such as instruments,
communications equipment, and pharmaceuticals)
had their price indexes adjusted
in a similar fashion, Figure 2 would look
much better, with the computer industry not
being as much of an outlier and with the
whole period showing much higher (social)
returns to R&D. That this is indeed the
case can be seen in Figure 3, where only
three such adjustments are made, but be-
6The total-factor-productivity numbers come from
the National Bureau of Economic Research data base
(Wayne Gray, 1992). The R&D numbers come from
Scherer (1984), updated to 1984 using 2.5-digit-level
information from National Science Foundation (1992).
'Updating the Griliches and Lichtenberg (1984) results
for 28 2.5-digit SIC industries and using a possibly
more appropriate R&D-by-product-field measure yields
essentially similar results, as does a parallel computation
at the more aggregated two-digit SIC level using
unpublished Bureau of Labor' Statistics data on total
(five-factor) productivity.


### ---Economics-1994-0-08.txt---
fore I discuss it, I need to digress briefly
and remind you about the developments in
computer price measurement.

Quality change is the bane of price and
output measurement. Until 1986, computer
prices were treated as unchanged in the
national income accounts. It took 25 years
for the recommendations of the Stigler
committee (Griliches, 1961; National Bureau
of Economic Research, 1961) to have a
noticeable effect on official practice, but
when they did, they did it with a bang! In
1986 the Bureau of Economic Analysis
(BEA) introduced a new computer price
index, based on hedonic regression methods,
into the national accounts and revised
them back to 1972 (Rosanne Cole et al.,
1986).8 This index was falling by about 15
percent per year or more (as compared to
the assumed value of zero before), and that
had several major implications, including
the fact that it made the apparent recovery
in manufacturing productivity in the 1980's
much stronger, about one-third of the total
coming from the introduction of this price
index alone (Gordon, 1993b).

There was nothing wrong with the price
index itself. It was, indeed, a major advance,
and the BEA should be congratulated for
making it, but the way it was introduced
created some problems. First, it was a
unique adjustment. No other high-tech
product had received parallel treatment, and
thus it stuck out like a sore thumb. This had
the unfortunate consequence that the productivity
growth in the computer industry
itself was seriously overestimated, because
some of its major inputs, such as semiconductors,
were not similarly deflated. Second,
it was introduced into a framework with
fixed weights, wrecking havoc on it. Using
fixed 1982 weights and a sharply falling price
index implied the absence of a "real" computer
industry in the early 1970's and a very
rapid growth in its importance, leading to a
more than doubling of the share of machinery
in total manufacturing output by the




late 1980's. This last problem has largely
been solved recently with the introduction
of "benchmark-weighted" estimates of gross
domestic product (GDP) and the moving
away from fixed-weights national income accounting
(Allan Young, 1992). But the first
problem, the uniqueness of this adjustment
in the face of similar, though perhaps not as
extreme, problems elsewhere remains to
haunt us.

What I have done in Figure 3 (and in row
3b of Table 1) is to adjust the estimated
TFP growth in the computer industry downward
by deflating materials purchases in
this industry, which to a significant extent
consist of purchases of other computer components
and semiconductors, by the same
output price index. I have also substituted a
similar price index in the semiconductors
(electronic components) industry and also
adjusted the growth of TFP in the pharmaceuticals
industry upward to reflect the
exclusion of price declines due to the introduction
of generics in the current measurement
procedures. (I shall come back to
discuss this last adjustment later on.) So
adjusted, Figure 3 does not look all that


### ---Economics-1994-0-09.txt---
11 14

10 13

Patents per 9 - - 12 Patents per
R&D Dollar - Scientist and

Engineer

8 -11

7 - 10

6 9

5

4 7

1920 1930 1940 1950 1960 1970 1980 1990
FIGURE 4. DOMESTIC PATENT APPLICATIONS PER COMPANY-FINANCED R&D IN INDUSTRY
(DASHED LINE; IN 1972 DOLLARS) AND PER SCIENTIST AND ENGINEER
(SOLID LINE), LOG SCALE

bad, and row 3b in Table 1 indicates no
decline in the R&D coefficient even without
the computer industry.

What is one to make of these conflicting
stories? It seems that the observed decline
in the R&D coefficients did not begin seriously
until the latter half of the 1970's, with
the second oil-price shock and the rise in
the dollar exchange rate. The abruptness of
the decline argues against a "supply-side"
explanation in terms of exhaustion of inventive
opportunities. It is more likely that the
peculiar aggregate shocks of that time went
against R&D-intensive industries: first, because
they hit energy-intensive industries
such as chemicals and petroleum refining
more severely; and second, because the subsequent
rise in value of the dollar and the
expansion in imports that followed hit some
of the more high-tech R&D-intensive industries
even harder, leading to declines in
"competitiveness," losses of rents, and the
appearance of excess capacity. The subsequent
rise in the R&D coefficients (if it did
in fact occur), the rise in corporate R&D
investments through most of the 1980's, and
the rise in patenting in, the late 1980's (as
we shall see), all argue against interpreting
these coefficient movements as reflecting
"real" declines in the once and future
"potency" of R&D. What did happen,
though, was a sharp widening of the differential
between social and private returns to
R&D. The internationalization of R&D, the
rise in the technical and entrepreneurial
skills of our competitors, and the sharp rise
in the dollar exchange rate in the mid-1980's,
all combined to erode, rather rapidly, the
rents accruing to the earlier accumulated
R&D capital and to the technical-expertise
positions of many of our enterprises. This
rise in the rate of private obsolescence and
the fall in the "appropriability" of R&D led
to sharp declines in both profitability and
real product prices. The latter, if they were
actually reflected in the appropriate price
indexes, would show up as an increase in
productivity, rather than a decline.
Before accepting this inconclusive verdict,
one still has to face the evidence of declining
patent-to-R&D ratios. Figure 4 plots
domestic patent applications divided by total
company-financed R&D expenditures in
U.S. industry (in 1972 dollars) and by the
total number of scientists and engineers in
industry. Looking at the right half of this


### ---Economics-1994-0-10.txt---
plot (the last couple of decades) we see a
more or less continuous decline with a small,
but possibly significant, turnaround in the
late 1980's. Similar trends can be seen also
in other countries, even in Japan (Evenson,
1991). But before one takes this as an indicator
of our recent problems, one should
glance also at the left side of this figure,
which goes back to the early 1920's. How
long has this been going on? This ratio
keeps falling, both through good times
(while productivity growth rates were rising)
and bad times. If this was not a cause for
worry earlier, why should one worry about it
now?9

III. Patents: A Shrinking Yardstick?
To decide whether we should be worried
by what is happening with the patent numbers
we need to know what they measure.
Since I have discussed this at some length
elsewhere (Griliches, 1990), I will make only
two points here. First, the interpretation of
Figure 4 need not be pessimistic. Its message
may not be what meets the eye. And,
second, the meaning of both the numerator
and the denominators of the ratios plotted
in Figure 4 may have changed significantly
over time.

If patents can be taken as indicators of
invention, and if the value of an invention is
proportional to the size of its market (or
economy), then the fact that their total
numbers remained roughly constant over
long time periods is consistent with nondeclining
growth rates of output and overall
productivity.10 If inventions are "produced"
by a combination of current R&D and the
existing state of knowledge (incorporating
the accumulated effects of science and
spillovers from the previous research activities
of others), and if R&D is invested approximately
"optimally," then under reasonable
assumptions, a rise (or fall) in the
underlying knowledge stock will affect them
both in parallel fashion and will leave their
ratio unchanged.1" There will be, therefore,
no evidence in this ratio on the underlying
state of the "stock of knowledge." Moreover,
it will be declining with growth in the
size of the market, since a rise in the value
of inventions will push R&D up until present
costs equal again the present value of
future (private) returns.

The rate of growth of domestic patents
was close to zero during the last three
decades. That by itself should not be worrisome.
If their average value had been growing
at the same rate as the economy as a
whole, there would be no reason for us to
worry about it. But there were long periods
when the actual numbers were worse than
that. During 1965-1985 the number of domestic
patent applications declined by - 0.6
percent per year while company-financed
R&D expenditures were growing by 4.8 percent
per year, in constant prices. But a
negative growth rate in the number of inventions
and a positive one in R&D are

inconsistent with an unchanging inventions
production function, unless the overall pool
of available knowledge is declining, or more


### ---Economics-1994-0-11.txt---
likely, unless the relationship between inventions
and the number of patents applied
for has been changing.

The suspicion that the relationship between
the number of patents and the number
of inventions (weighted by their relative
economic importance) has been changing is
not new. Schmookler (1966) stops most of
his analysis with pre-World War II data,
believing that the meaning of the patent
statistics changed at that time. What needs
to be reconciled in the data is the sharp
contrast between the rapidly growing R&D
series during 1953-1968 (and earlier) and
the essentially flat patent series. There are a
number of not mutually exclusive possibilities
here:

(i) The fast-growing R&D expenditures,
fueled by the new global opportunities
that opened up in the post-World War
II period, were being invested in face
of rapidly diminishing returns.
(ii) Some of the observed growth in R&D
could be spurious, the result of reclassification
of informal technological activities
into formal R&D under the

pressure of tax accountants, publicrelations
experts, and R&D tax credits.

(iii) The rise of formal R&D-based invention
crowded out smaller, less valuable
individual-inventor-based patents, while
the rise in the cost of patenting (in
terms of the time costs of dealing with
the patent system) and the more recent
sharp rise in fees may have selected out
a large number of potentially low-valued
patents. Given the evidence that
the value distribution of inventions and
patents is extremely skewed, with only
a small fraction having a high present
value, such a crowding out could raise
average values significantly, though the
required rate is rather on the high
side.12

It is also likely that the threshold for what
is patentable has risen, given the large influx
of foreign patent applications into the
U.S. system all impinging on a relatively
slow-growing and budget-constrained patent
office.'3 On the other hand, the legal
status of patents in the United States has
improved significantly with the creation of a
special patents court, driving up the expected
private value of a patent. Given the
presence of so many opposing forces, there
is no compelling need to reply on the
exhaustion-of-inventive-opportunities hypothesis,
especially since patents-to-R&D
ratios were falling much more drastically
during the "good times" of the past than
recently.14 Moreover, if we do take these
numbers seriously, then good news is just
around the corner: domestic patent applications
have risen sharply in the last five years
(see Fig. 5), implying a potential resurgence
in the rate of technological change. This
leaves us, however, more or less where we
started, with the productivity slowdown
largely unexplained.

12There is scattered evidence on the rising "quality"
of patents from patent renewal data (see Mark
Schankerman and Ariel Pakes, 1986; Pakes and Margaret
Simpson, 1989) and from the rising number of
claims per patent (see X. Tong and J. D. Frame, 1992).
The latter, for example, rose at about 2.5 percent per
year between 1970 and 1990. That is about right for
this period but far too low for the 6+ percent earlier.
On the other hand, Cabellero and Jaffe (1993), using
citation data, find that the average "size" of a patent
did not grow during the last 20 years.
13There is some evidence that such crowding-out
may have occurred. Between 1966-1969 and 1981-1985
the "yield ratio" for domestic patent applications in
terms of grants received fell by about 15 percent (from
0.68 to 0.58) before recovering somewhat in the late
1980's (to 0.62). See Griliches (1990) for a survey of
these issues and citations to the relevant literature.
'A similar story is also told by other scattered
invention "output" indicators. In their study of innovations
in the chemical, textile, and machinery-tools industries,
Baily and Chakrabarti (1988) found a decline
in the number of innovations in the 1970's in two out
of these three industries, and some recovery thereafter.
Similar patterns were observed in a study of British
industrial innovations (see the figure in Gerhard
Mensch et al. [1991]). In both cases the timing is not
right for an explanation of the slowdown in the 1970's.
The impact on productivity is too fast. Rather, it is
likely to reflect the impact of the slowdown in the
growth of aggregate demand and the recessions of the
1970's. In both cases there is an upturn in the 1980's.


### ---Economics-1994-0-12.txt---
IV. Why Is the Glass Half-Empty?
Economists have not been very successful
in explaining what has happened to the
economy during the last two decades, nor
have they been able to agree on what should
be done about it. I will argue that data and
measurement difficulties may in fact be a
major source of this failure. This point will
be made not to provide us with an alibi, but
rather to temper the pretentiousness of
some of our pronouncements and to urge us
toward the more mundane task of observation
and measurement.

Why don't we know more after all these
years? Our data have always been less than
perfect. What is it about the recent situation
that has made matters worse?

The brief answer is that the economy has
changed and that our data-collection efforts
have not kept pace with it. "Real" national
income accounts were designed in an earlier
era, when the economy was simpler and had
a large agricultural sector and a growing
manufacturing sector. Even then, a number
of compromises had to be made to get measurement
off the ground. In large sectors of
the economy, such as construction and most
of the services, government, and other public
institutions, there were no real output
measures or relevant price deflators. Imagine
a "degrees of measurability" scale, with
wheat production at one end and lawyer
services at the other. One can draw a rough
dividing line on this scale between what I
shall call "reasonably measurable" sectors
and the rest, where the situation is not
much better today than it was at the beginning
of the national income accounts. Table
2 shows the distribution of nominal GDP by
major industrial sector. In the early postWorld
War II period, the situation was not
all that bad: about half of the overall economy
was "measurable" in this sense. By
1990, however, the fraction of the economy
for which the productivity numbers are half
reasonable had fallen to below one-third.
Figure 6 tells the same story with employment
numbers. Measurement problems have
indeed become worse. Our ability to interpret
changes in aggregate total factor productivity
has declined, and major portions
of actual technical change have eluded our
measurement framework entirely.15


### ---Economics-1994-0-13.txt---
TABLE 2-THE DISTRIBUTION OF GNP BY MAJOR INDUSTRIAL SECTOR,
IN CURRENT PRICES (PERCENTAGES)
Industry 1947 1959 1969 1977 1990
Agriculture 8.8 4.1 3.0 2.8 2.0
Mining 2.9 2.5 1.8 2.7 1.8

Construction 3.9 4.8 5.1 4.8 4.4
Manufacturing 28.1 28.6 26.9 23.6 18.4
Transportation and utilities 8.9 9.1 8.6 9.1 8.7
Wholesale trade 7.1 6.9 6.7 7.0 6.5
Retail trade 11.7 9.9 9.8 9.6 9.3
Finance, insurance, and real estate 10.1 13.8 14.2 14.4 17.7
Other services 8.6 9.7 11.5 13.0 18.9
Government 8.6 10.2 12.6 12.5 12.2
"Measurable" sectorsa 48.7 44.3 40.3 38.2 30.9
Note: Numbers before 1977 are not strictly comparable, since the latest revision was
carried back only to 1977.

Source: Tables 6.1 and 6.2 of the National Income and Products Accounts (1928-1982)
and Survey of Current Business (May 1993).
aAgriculture, mining, manufacturing, and transportation and utilities.
An example of the consequences of this
shift is what has come to be known as the
"computer paradox." We have made major
investments in computers and in other
information-processing equipment. The
share of "information" equipment in total
producer investment in durable equipment,
in current prices, has more than doubled,
from about 17 percent in 1960 to 36 percent
in 1992. Computers alone went up from less
than 1 percent to 11 percent of the total;
and that does not allow for improvements in
the quality of this equipment, which has
been happening at a very fast rate-on the
order of 15-30 percent per year (see Jack
Triplett, 1989; Berndt and Griliches, 1993).
Why has this not translated itself into visible
productivity gains? The major answer to
this puzzle is very simple: over threequarters
of this investment has gone into
our "unmeasurable" sectors (see Table 3),
and thus its productivity effects, which are
likely to be quite real, are largely invisible in
the data.

That there were gains is not really in
doubt. Just observing the changes in the
way banks and airlines operate, and in the
ways in which information is delivered to
firms and consumers, would lead one to
conclude that we are in the midst of a major
technical revolution. Effective distances are
declining rapidly in many parts of the world.
The rise of ATM networks in banking has
resulted in substantial though largely unmeasured
time savings for consumers. It is
less clear, however, whether the large expansion
of the securities industry has been
associated with a similar productivity increase
or was primarily a response to a real
decline in the cost of rent-seeking induced
by the falling price of informationprocessing
(see Timothy Bresnahan et al.,
1992).

There is also some scattered evidence for
the positive contribution of computers in
manufacturing, but given the needle-in-the
haystack aspect of this problem, it is not
particularly strong (see e.g., Alan Krueger,
dustries produce largely intermediate products and services.
But personal consumption expenditures account
only for about 68 percent of GDP, while services
represent 56 percent of personal consumption. Thus, it
is unlikely that looking at consumption data in more
detail would change the tenor of my remarks much. A
cursory look at Personal Consumption Expenditures
(Bureau of Economic Analysis, 1990) yields a rough
estimate of 47 percent of total consumption expenditures
not easily measurable in real terms. The two
largest difficult items consist of hard-to-measure services
in the medical, insurance, legal, entertainment,
and education areas (23 percent) and housing-related
services (21 percent).


### ---Economics-1994-0-14.txt---
1991; Donald Siegel and Griliches, 1992;
Erik Brynjolfson and Lorin Hitt, 1993; Igal
Hendel, 1993). Some of the gains from computers
have been reflected in higher wages
of their operators and in the more general
rise in the returns to education and "skill"
(Chinhui Juhn et al., 1993). More generally,
we may be just at the beginning of the
computer era, early in its diffusion and
learning stages, with most of the productivity
contributions still to come, as we learn
how to use computers more effectively and


### ---Economics-1994-0-15.txt---
integrate them more efficiently into the existing
production structures (Paul David,
1991).

Similar arguments, can be (and have been)
made about the difficulties in measuring the
contribution of R&D to productivity growth
(see Griliches, 1979). From one-third to over
half of all industrial R&D is "sold" to the
government, either in the form of research
contracts and prototypes or indirectly in the
form of weapons and space equipment, and
its direct productivity effects do not show up
in the data at all. Private R&D investment
is also likely to have followed the economy
and shifted its targets toward the fastergrowing
sectors, with more invention and
technical change occurring exactly where we
have more trouble in measuring them.
Not only has the economy shifted into
uncharted waters, but even in the "measurable"
sectors accelerating rates of change
have destroyed the basis for some of the
older compromises. Currently, new goods
are introduced into the various official price
indexes rather slowly. While attempts are
being made to reduce the revision cycle in
the producer price index from five to two
years for some of the more high-tech goods,
this may still not be fast enough. In the
personal-computers market, for example,
the life of a model has recently fallen to a
year or less (Berndt et al., 1993).
Dealing with the quality-change problem
by treating every version of a product sold
to a different type of customer as a separate
commodity, as is currently the predominant
official practice, creates its own problems.
By linking out the decline in prices experienced
by consumers in their shift to supermarkets,
discount stores, and mail-order
purchases, it underestimates significantly not
only the output of se;rvices, but also the
output of some of the more "standard"
manufacturing industries (Marshall Reinsdorf,
1993). A prime example of that is the
treatment of generics in the pharmaceutical
price indexes. The stylized facts are as follows:


(i) Generics are introduced at roughly half
the price of the original brand.
(ii) The brand price, however, does not,
decline (it sometimes even goes up),
with the ex-monopolist depreciating
optimally her original position and with
generics gaining between half and
three-quarters of the market for the
particular drug.

(iii) But because generic versions are
treated as separate commodities, in
spite of what the FDA says, the price
index does not fall, and since the value
of shipments declines as the market
shifts to generics (and to hospital and
HMO formularies), so does measured
"output" in this industry and the associated
productivity measures (Griliches
and lain Cockburn, 1993).

This might explain the rather strange
fact that during the last decade pharmaceuticals,
an industry with one of the highest
R&D-sales ratios, had a rather dismal
productivity-growth performance. This was
the period with an increasing penetration of
generics, which should have reduced measured
prices in this industry but did not.
The measurement environment has deteriorated
also in other ways. There is less
willingness on the part of firms and consumers
to respond to detailed questions,
and our government has done little to emphasize
the importance of good economic
data to its own functioning or the overall
understanding of our economy. The consequence
of such deterioration can be illustrated
by the uncertainty about the level of
industrial investment in basic research, an
investment which many think is crucial to
our long-run economic performance
(Griliches, 1986a). Because the question that
asks about the allocation of total R&D expenditures
by the "character of work" is not
mandatory and is also not an easy one to
answer, less than half of all the firms surveyed
in 1988 answered it. As a result of
such nonresponse, the best that can be done
is to produce a "reasonable" range of estimates,
based on alternative imputation algorithms,
from $2.5 to $8.2 billion (and a
"central" guess of $3.9 billion), which leaves
us really in the dark as to what has happened


### ---Economics-1994-0-16.txt---
to such investments recently (Eileen I.
Collins, 1990).

V. Data Woes

Why are the data not better? The facts
themselves are not in dispute. Every decade
or so a prestigious commission or committee
produces a report describing in detail
various data difficulties and lacunae: the
Stigler committee report on government
price statistics (National Bureau of Economic
Research, 1961) is still a living document,
as are the related Ruggles report
(Richard Ruggles, 1977), the Rees productivity
report (National Academy of Sciences,
1979), the Bonnen report (J. T. Bonnen,
1981), the Creamer GNP improvement report
(D. Creamer, 1977), the recent OTA
report (Office of Technology Assessment,
1989), and many others. But life goes on,
and change in this area is very slow. Why? I
don't really have good answers to this question,
and the topic itself is much larger than
can be handled in this address, but at least
three observations come to mind:
(i) The measurement problems are really
hard.

(ii) Economists have little clout in Washington,
especially as far as data-collection
activities are concerned. Moreover,
the governmental agencies in

these areas are balkanized and underfunded.


(iii) We ourselves do not put enough emphasis
on the value of data and data

collection in our training of graduate
students and in the reward structure of
our profession. It is the preparation
skill of the econometric chef that
catches the professional eye, not the
quality of the raw materials in the meal,
or the effort that went into procuring
them (Griliches, 1986b).

In many cases the desired data are unavailable
because their measurement is really
difficult. After decades of discussion we
are not even close to a professional agreement
on how to define and measure the
output of banking, insutrance, or the stock
market (see Griliches, 1992). Similar difficulties
arise in conceptualizing the output
of health services, lawyers, and other consultants,
or the capital stock of R&D. While
the tasks are difficult, progress has been
made on such topics. The work of Jorgenson
and Barbara Fraumeni (1992) on the
measurement of educational output is an
example both of what can be done and of
the difficulties that still remain. But it is not
reasonable for us to expect the government
to produce statistics in areas where the concepts
are mushy and where there is little
professional agreement on what is to be
measured and how. Much more could be
done, however, in an exploratory and research
mode.16 Unfortunately, the various
statistical agencies have been both starved
for funds and badly led, with the existing
bureaucratic structure downplaying the research
components of their enterprise when
not being outright hostile to them, research
being cut first when a budget crunch happens
(Triplett, 1991).

Our current statistical structure is badly
split, there is no central direction, and the
funding is heavily politicized. How else can
one explain that the national income accounts
and the BEA as a whole receive only
one-third, and health and education statistics
each less than one-half of the funds
allocated to agricultural statistics?17 How
does one explain the failure of the most


### ---Economics-1994-0-17.txt---
recent attempt at getting more money for
economic statistics, the late "Boskin initiative"
? Central economic statistics do not
have a clear constituency that lobbies on
their behalf. Recent governments seem not
to care enough, or to have enough energy to
fight for something that has a more distant
horizon than the next election. One hopes
for some improvement in this situation from
the current administration. It has people
who know better in reasonably important
positions. Still, with the main focus on the
daily crisis and the continuing budget battles
with Congress, I am not all that optimistic.
But if we want progress in this area,
if we care, we need to make our opinions
heard. We need to convince Congress (and
ourselves) that the requests for additional
funding of the statistical infrastructure are
justified as investments in general knowledge
and more informed policy formation;
that they are not just self-serving, intended
to allow us to publish more articles or run
thousands more regressions; that it is indeed
important to know what is happening
and to understand where we might be going
or drifting.18

We need also to make observation, data
collection, and data analysis a more central
component of our graduate teaching. How
can we expect our community to fight for
the budgets of the BEA, BLS, or Census, if
the average student doesn't really know how
the data that they use are manufactured or
what the national accounts are made of.19
We also need to teach them to go out and
collect their own data on interesting aspects
of the economy and to rely less on "given"
data from distant agencies.'o There are encouraging
signs that some of this is happening,
especially in the micro area. One is
much more cheered by work such as that of
Robert Fogel (1986) on heights and nutrition,
Alan Krueger and Orley Ashenfelter
(1992) on twins, Richard Levin et al. (1987)
on the appropriability of technology,
Rebecca Henderson and Cockburn on pharmaceutical
R&D, Richard Freeman and

Harry Holtzer (1986) on inner-city youths,
Schankerman and Pakes (1986) on patent
renewal data, Manuel Trajtenberg (1990a)
on CT scanners, and Trajtenberg (199Gb)
and Adam Jaffe et al. (1993) on patent
citations, where researchers go out, collect,
and create new data sets, than by the
20,000th regression on the Robert Summers
and Alan Heston (1991) data set, illuminating
as it may be. But unless we transmit this
message to our students, we will not be able
to convince others that this is a cause worth
supporting.

VI. Expanding the Framework

Is there something possibly wrong with
the way we ask the productivity question,
with the analytical framework into which we
force the available data? I think so. I would
focus on the treatment of disequilibria and
the measurement of knowledge and other
externalities. The current measurement
framework proceeds as if all investment
and employment decisions are made at
known and common factor and product
prices, throwing all of the heterogeneity
and uncertainty-the surprises and the disappointments-
into the residual category.

An alternative view would see measured
productivity growth as a summation of
above- (and below-) average returns to various
current investment decisions and capital
gains (or losses) on existing physical- and
180ne should probably worry also about the overall
level of support for economic research. As a percentage
of total academic research funding, it fell from
1.5 percent in 1979 to 1.2 percent in 1990. While the
number of economists doing academic research was
rising at 5.5 percent per year, funds per researcher
were falling in real terms at - 2.3 percent per year, and
the Federal share in these funds was also dropping
from 48 percent to only 27 percent (in 1989). At the
same time, real funds per researcher in the academy as
a whole were rising at 0.4 percent per year (National
Science Board, 1991). What is it that we have been
doing wrong?

19The recent shift toward a "three essays" Ph.D.
thesis is also not conducive to a serious involvement
with data creation.

20Unfortunately, the usage is apt. Data already
means "given" rather than collected or observed.


### ---Economics-1994-0-18.txt---
human-capital stocks.2' The appearance of
such investment opportunities is the essence
of growth and change. They are largely disequilibrium
phenomena, resulting in a

lurching from one "steady state" to another
rather than something smooth and exponential.
The presence of locally increasing
returns, network externalities, asymmetric
information, and heterogeneous expectations,
the appearance of new products and
technologies, and the changes in the political
and regulatory environments are all
sources of such "excess" returns, while the
ex post fixity of much of the investment in
both physical and human capital causes capital
gains and losses and unanticipated "obsolescence"
in the various stocks. We will
have to figure out how to take the residual
apart along such lines to make more
progress in understanding its proximate
sources.

Our theories tend to assume that we are,
indeed, at the frontier and that we can only
either move along it or try to shift it, the
latter being a difficult and chancy business.
In fact we may be far from our existing
"frontiers." Harvey Leibenstein's (1966)
ideas about X-efficiency, or more correctly
X-inefficiency, did not get much of a sympathetic
ear from us. They were inconsistent
with notions of equilibrium, the absence of
unexploited profit opportunities, and the
possibilities for economic arbitrage. But real
economic growth is the consequence of both
the appearance of such disequilibria and
the devising of ways of closing them. How
quickly they are eliminated depends on the
strength of incentive systems within enterprises,
and on their organizational quality.
In spite of the large growth in the literature
on organizations, we have not yet developed
useful ways of quantifying their strengths
and weaknesses. Nor are we close to having
measures of such factors as the "work ethic"
or aspects of the property-rights system
which are likely to contribute much to the
observed differences in productivity across
nations.

The "new" growth theories have various
externalities as their centerpiece (see Solow
[1991] for a recent review). It is somewhat
ironic that they have come to the fore just
when growth started declining and notions
of eternal exponential growth began to lose
their luster. Knowledge externalities are obviously
very important in the growth process,
but they do not help us to explain what
has happened in the last two decades. There
is no reason to believe that they have declined
over time. If anything, the communication
and transportation advances should
have expanded the availability of such externalities.
22 But we have no good models for
the measurement of such processes.
Knowledge is not like a stock of ore,
sitting there waiting to be mined. It is an
extremely heterogenous assortment of information
in continuous flux. Only a small part
of it is of any use to someone at a particular
point of time, and it takes effort and resources
to access, retrieve, and adapt it to
one's own use. Thus models of externalities
must perforce be models of interaction between
different actors in the economy. We
have, however, very few convincing models
of such interactions, and the identification
problems are severe (see e.g., Charles Manski,
1993). Our measurement frameworks
are not set up to record detailed origin and
destination data for commodity flows, much
less so for information flows. We do have
now a new tool for studying some of this:
citations to patents and the scientific literature
(see e.g., Jaffe et al., 1993), but anyone
currently active in the e-mail revolution and
participating in the conferences and workshops
circuit knows how small this tip is
relative to the informal-communications
iceberg itself.


### ---Economics-1994-0-19.txt---
VII. The Glass Half-Full?

After a long detour I come back to the
original question: why don't we know more
about the sources of productivity growth
and the causes for its recent slowdown?
Why does it feel as if the glass is still
half-empty? First note that in a trivial sense
we are doing better: the residual is smaller.
But that is the bad news, not the good. It is
smaller not because we have succeeded in
providing a substantively fuller explanation
of output growth, but rather because measured
output growth declined, leaving some
of these explanations in the dust. But we
are also doing better substantively. We know
much more about the components of growth
and where our measures are lacking. After
decades of work and contributions by Denison,
Jorgenson, Kendrick, and many others,
the conceptual and measurement underpinnings
of the growth accounts are in much
better shape today. We now have extensive
micro data on firms, their productivity, their
R&D expenditures, and other variables. We
have more data on individual investments in
education and training, and we also have
more asset detail on capital formation. More
international data are now available, with
the OECD both collecting R&D data and
computing TFP numbers for many countries,
and with Summers and Heston (1991)
providing comparable real GNP numbers
for many countries. Finally, we have much
more computing power and better econometric
techniques and frameworks for attacking
many of the problems that arise in
the analysis of such data. So what is still
missing?

We are caught up in a mixture of unmeasurement,
mismeasurement, and unrealistic
expectations. The productivity situation is
both better than we think and also worse. It
is likely that there have been significant
unmeasured productivity advances in many
of the service sectors (Bresnahan, 1986;
Baily and Gordon, 1988). Moreover, rising
R&D investment rates in the mid-1980's
and the recent rise in the number of patent
applications augur well for the future. Also,
productivity growth rates are probably underestimated
even in the "measurable" sectors
because they are based on "book value"
estimates of physical- and human-capital
stocks and do not reflect the capital losses
-the obsolescence that occurred, first as
the result of the various energy-price shocks,
and later as the result of increased international
competition and the melting away of
much of the previously existing monopoly
rents to both types of capital. That is actually
bad news. We are not as wealthy as we
thought, but productivity growth, based on
the lower remaining levels of input, is probably
higher than we have measured it.
A cautionary remark needs to be added
here: productivity growth contributes to the
potential for welfare, but it is not the same
thing. Welfare can move in the opposite
direction if the resources released by productivity
growth do not find adequate employment
in other, economically valuable,
activities (including leisure). Also the physical,
economic, and political environments
can change, both positively and negatively,
overwhelming the productivity story.23 So
even though I have been focusing on it here
tonight, it is not the be-all of economic
welfare. But as George Bernard Shaw used
to say when he was accused of money-grubbing:
"Yes, I know that money is not happiness,
but it is a pretty good substitute."
Nevertheless, the issues I have been discussing
here tonight are important. Much
depends on whether the "truth" is closer to
the upper ("measurable") line in Figure 1,
or the lower one. The country's mood is
affected by bad data and incorrect perceptions.
Are we really not much better off
than we were in the 1960's? Would we
really like to exchange the commodity assortment
we have today for that of

yesteryear? Our health system, warts and
all? The air pollution? The civil-rights situation?
The fear of nuclear war? These are
23Between 1970 and 1989, average hours of work
per worker went down by 7 percent, air pollution went
down significantly, and the crime rate came close to
doubling (Baily et al., 1993). Of course, these data are
also problematic (see Scott Boggess and John Bound,
1993).


### ---Economics-1994-0-20.txt---
not just idle intellectual curiosities. They
affect what we feel about ourselves and the
future.

Returning to the topic of technical
change, our expectations of what economics
can deliver here may also be excessive. It is
unlikely that we can have a fully "endogenous"
theory of technical change. Yes, both
the rate and direction of inventive activity
are subject to economic influences and analysis.
So also is the diffusion of innovations.
But the outcome of inventive activity is not
really predictable. True "innovation" is an
innovation. If it were knowable in advance
it would not be one, and the innovators
would not be able to collect any rents. In
that sense it is futile to expect that we could
control it fully or predict it well.24 Given the
fundamental uncertainties entailed in the
creative act, in invention, and in innovation,
there is no reason to expect the fit of our
models to be high or for the true residual to
disappear. We should, however, be able to
"explain" it better ex post even if we cannot
predict it.

The metaphor of the glass half-empty is
also misleading. As we fill it, the glass keeps
growing. A major aspect of learning is that
the unknown keeps expanding as we learn.
This should be looked at positively. It is
much better this way-especially for those
of us who are engaged in research!
 ## Economics-1995-0


### ---Economics-1995-0-02.txt---
While Aristotle agreed with Agathon that
even God could not change the past, he did
think that the future was ours to make-by
basing our choices on reasoning. The idea
of using reason to identify and promote
better-or more acceptable-societies, and
to eliminate intolerable deprivations of different
kinds, has powerfully moved people
in the past and continues to do so now. In
this lecture I would like to discuss some
aspects of this question which have received
attention in the recent literature in socialchoice
and public-choice theories. The contemporary
world suffers from many new as
well as old economic problems, including,
among others, the persistence of poverty
and deprivation despite general economic
progress, the occurrence of famines and
more widespread hunger, and threats to our
environment and to the sustainability of the
world in which we live. Rational use of the
opportunities offered by modern science and
technology, in line with our values and ends,
is a powerful challenge today.
I. Problems and Difficulties

How are we to view the demands of rationality
in social decisions? How much guidance
do we get from Aristotle's general
recommendation that choice should be governed
by "desire and reasoning directed to
some end"? There are several deep-seated
difficulties here.

The first problem relates to the question:
whose desires, whose ends? Different persons
have disparate objects and interests,
and as Horace put it, "there are as many
preferences as there are people." Kenneth
Arrow (1951) has shown, through his famous
"General Possibility Theorem" (an
oddly optimistic name for what is more
commonly-and more revealingly-called
Arrow's "impossibility theorem"), that in
trying to obtain an integrated social preference
from diverse individual preferences, it
is not in general possible to satisfy even
some mild-looking conditions that would
seem to reflect elementary demands of reasonableness.
1 Other impossibility results

have also emerged, even without using some
of Arrow's conditions, but involving other
elementary criteria, such as the priority of
individual liberty.2 We have to discuss why
these difficulties arise, and how we can deal
with them. Are the pessimistic conclusions
that some have drawn from them justified?
Can we sensibly make aggregative socialwelfare
judgments? Do procedures for social
decision-making exist that reasonably
respect individual values and preferences?


### ---Economics-1995-0-03.txt---
Second, another set of problems relates
to questions raised by James Buchanan
(1954a,b), which were partly a response to
Arrow's results, but they are momentous in
their own right.3 Pointing to "the fundamental
philosophical issues" involved in
"the idea of social rationality," Buchanan
(1954a) argued that "rationality or irrationality
as an attribute of the social group
implies the imputation to that group of an
organic existence apart from that of its individual
components" (p. 116). Buchanan was
perhaps "the first commentator to interpret
Arrow's impossibility theorem as the result
of a mistaken attempt to impose the logic of
welfare maximization on the procedures of
collective choice" (Robert Sugden, 1993 p.
1948). But in addition, he was arguing that
there was a deep "confusion surrounding
the Arrow analysis" (not just the impossibility
theorem but the entire framework used
by Arrow and his followers) which ensued
from the mistaken idea of "social or collective
rationality in terms of producing results
indicated by a social ordering" (Buchanan,
1960 pp. 88-89). We certainly have to examine
whether Buchanan's critique negates the
impossibility results, but we must also investigate
the more general issues raised by
Buchanan.4

Third, Buchanan's reasoned questioning
of the idea of "social preference" suggests,
at the very least, a need for caution in
imposing strong "consistency properties" in
social choice, but his emphasis on procedural
judgments may be taken to suggest,
much more ambitiously, that we should
abandon altogether consequence-based
evaluation of social happenings, opting instead
for a procedural approach. In its pure
form, such an approach would look for
"right" institutions rather than "good" outcomes
and would demand the priority of
appropriate procedures (including the acceptance
of what follows from these procedures)
. This approach, which is the polar
opposite of the welfare-economic tradition
based on classical utilitarianism of founding
every decision on an ordering of different
states of affairs (treating procedures just as
instruments to generate good states), has
not been fully endorsed by Buchanan himself,
but significant work in that direction
has occurred in public choice theory and in
other writings influenced by Buchanan's
work (most notably, in the important contributions
of Robert Sugden [1981, 1986]).
This contrast is particularly important in
characterizing rights in general and liberties
in particular. In the social choice literature,
these characterizations have typically been
in terms of states of affairs, concentrating
on what happens vis-a-vis what the person
wanted or chose to do. In contrast, in
the libertarian literature, inspired by the
pioneering work of Robert Nozick (1974),
and in related contributions using "gameform"
formulations (most notably, by Wulf
Gaertner, Pattanaik, and Suzumura [1992]),
rights have been characterized in procedural
terms, without referring to states of affairs.
We have to examine how deep the
differences between the disparate formulations
are, and we must also scrutinize their
respective adequacies.

Fourth, the prospects of rationality in social
decisions must be fundamentally conditional
on the nature of individual rationality.
There are many different conceptions of
rational behavior of the individual. There is,
for example, the view of rationality as canny
maximization of self-interest (the presumption
of human beings as "homo economicus,"
used in public choice theory, fits into this
framework). Arrow's (1951) formulation is
more permissive; it allows social considerations
to influence the choices people make.
Individual preferences, in this interpretation
reflect "values" in general, rather than
being based only on what Arrow calls
"tastes" (p. 23). How adequate are the respective
characterizations of individual rationality,
and through the presumption of
rational behavior (shared by most economic


### ---Economics-1995-0-04.txt---
models), the depiction of actual conduct
and choices?

Another issue, related to individual behavior
and rationality, concerns the role of
social interactions in the development of
values, and also the connection between
value formation and the decision-making
processes. Social choice theory has tended
to avoid this issue, following Arrow's own
abstinence: "we will also assume in the present
study that individual values are taken
as data and are not capable of being altered
by the nature of the decision process itself"
(Arrow, 1951 p. 7).5 On this subject,
Buchanan has taken a more permissive position-
indeed emphatically so: "The definition
of democracy as 'government by discussion'
implies that individual values can
and do change in the process of decisionmaking"
(Buchanan, 1954a p. 120).6 We

have to scrutinize the importance of this
difference as well.

This is a long and somewhat exacting list,
but the different issues relate to each other,
and I shall try to examine them briefly and
also comment on some of their practical
implications.

II. Social Welfare Judgments and Arrow's
Impossibility Theorem

The subject of welfare economics was
dominated for a long time by the utilitarian
tradition, which performs interpersonal aggregation
through the device of looking at
the sum-total of the utilities of all the people
involved. By the 1930's, however,
economists came to be persuaded by arguments
presented by Lionel Robbins (1938)
and others (influenced by the philosophy of
"logical positivism") that interpersonal
comparisons of utility had no scientific
basis.7 Thus, the epistemic foundations of
utilitarian welfare economics were seen as
incurably defective.

Because of the eschewal of interpersonal
comparability of individual utilities, the
"new welfare economics" that emerged tried
to rely only on one basic criterion of social
improvement, the Pareto criterion. Since
this confines the recognition of a social improvement
only to the case in which everyone'
s utility goes up (or someone's goes up
and no one's goes down), it does not require
any interpersonal comparison, nor for that
matter, any cardinality of individual utilities.
However, Pareto efficiency can scarcely
be an adequate condition for a good society.
It is quite insensitive to the distribution of
utilities (including inequalities of happiness
and miseries), and it takes no direct note of
anything other than utilities (such as rights
or freedoms) beyond their indirect role in
generating utilities. There is a need, certainly,
for further criteria for social welfare
judgments.

The demands of orderly, overall judgments
of "social welfare" (or the general
goodness of states of affairs) were clarified
by Abram Bergson (1938, 1966) and extensively
explored by Paul Samuelson (1947).
The concentration was on the need for a
real-valued function W of "social welfare"
defined over all the alternative social states,
or at least an aggregate ordering R over
them, the so-called "social preference." In
the reexamination that followed the Bergson-
Samuelson initiative (including the development
of social choice theory as a discipline)
, the search for principles underlying a
social welfare function played a prominent
part.

Arrow (1951) defined a "social welfare
function" as a functional relation that specifies
a social ordering R over all the social
states for every set of individual preference
orderings. In addition to assuming-not especially
controversially-that there are at
5Arrow (1951) himself points out "the unreality of
this assumption" (p. 8).

6The importance of politics as discussion has also
been stressed in the Habermasian tradition; on this see
Jon Elster and Aanund Hylland (1986) and Jiirgen
Habermas (1994). See also Albert Hirschman (1970)
and the works inspired by his writings.
7Robbins (1938) himself was opposed not so much
to making interpersonal comparisons, but to claiming
them to be "scientific."


### ---Economics-1995-0-05.txt---
least three distinct social states and at least
two (but not infinitely many) individuals,
Arrow also wanted a social welfare function
to yield a social ordering for every possible
combination of individual preferences; that
is, it must have a universal domain. A second
condition is called the independence of
irrelevant alternatives. This can be defined in
different ways, and I shall choose an extremely
simple form. The way a society ranks
a pair of alternative social states x and y
should depend on the individual preferences
only over that pair-in particular,
not on how the other ("irrelevant") alternatives
are ranked.

Now consider the idea of some people
being "decisive": a set G of people-I shall
call them a group G-having their way no
matter what others prefer. In ranking a pair
x and y, if it turns out that x gets socially
ranked above y whenever everyone in group
G prefers x to y (no matter what preferences
those not in G have), then G is decisive
over that ordered pair (x, y). When a
group G is decisive over all ordered pairs, it
is simply "decisive."

Arrow required that no individual (formally,
no single-member group) should be
decisive (nondictatorship), but-following
the Paretian tradition-also demanded that
the group of all individuals taken together
should be decisive (the Pareto principle).
The "impossibility theorem," in this version
(presented in Arrow [1963]), shows that it is
impossible to have a social welfare function
with universal domain, satisfying independence,
the Pareto principle, and nondictatorship.


The theorem can be proved in three simple
steps.8 The first two steps are the following
(with the second lemma drawing on
the first).

FIELD-EXPANSION LEMMA: If a group
is decisive over any pair of states, it is decisive.
9

GROUP-CONTRACTION LEMMA: If a

group (of more than one person) is decisive,
then so is some smaller group contained in
it.

The final step uses the Group-Contraction
Lemma to prove the theorem. By the Pareto
principle, the group of all individuals is decisive.
Since it is finite, by successive partitioning
(and each time picking the decisive
part), we arrive at a decisive individual, who
must, thus, be a dictator. Hence the impossibility.


### ---Economics-1995-0-06.txt---
III. Social Preference, Social Choice,
and Impossibility

The preceding discussion makes abundant
use of the idea of "social preference."
Should it be dropped, as suggested by
Buchanan? And if so, what would remain of
Arrow's impossibility theorem?
We have to distinguish between two quite
different uses of the notion of "social preference,
" related respectively to (i) the operation
of decision mechanisms, and (ii) the
making of social welfare judgments. The first
notion of "social preference" is something
like the "underlying preference" on which
choices actually made for the society by
prevailing mechanisms are implicitly based
-a kind of "revealed preference" of the
society.11 This "derivative" view of social
preference would be, formally, a binary representation
of the choices emerging from

decision mechanisms.

The second idea of "social preference"
as social welfare judgments-reflects a view
of the social good: some ranking of what
would be better or worse for the society.
Such judgments would be typically made by
a given person or agency. Here too an aggregation
is involved, since an individual
who is making judgments about social welfare,
or about the relative goodness of distinct
social states, must somehow combine
the diverse interests and preferences of different
people.

Buchanan's objection is quite persuasive
for the first interpretation (involving decision
mechanisms), especially since there is
no a priori presumption that the mechanisms
used must -or even should -necessarily
lead to choices that satisfy the requirements
of binary representation (not to
mention the more exacting demands of an
ordering representation).12 On the other
hand, the second interpretation does not
involve this problem, and even an individual
when expressing a view about social welfare
needs a concept of this kind.13 When applied
to the making of social welfare judgments
by an individual or an agency, Arrow's
impossibility theorem thus cannot be disputed
on the ground that some organic existence
is being imputed to the society. The
amelioration of impossibility must be sought
elsewhere (see Section IV). However,
Buchanan's critique of Arrow's theorem
would apply to mechanisms of social decision
(such as voting procedures).

Would the dropping of the requirement
that social choices be based on a binary
relation-in particular a transitive ordering
-negate the result in the case of social
decision mechanisms? A large literature has
already established that the arbitrariness of
power, of which Arrow's case of dictatorship
is an extreme example, lingers in one
form or another even when transitivity is
dropped, so long as some regularity is demanded
(such as the absence of cycles).'4
There is, however, cause for going further,
precisely for the reasons identified by
Buchanan, and to eschew not just the transitivity
of social preference, but the idea of
social preference itself. All that is needed
from the point of view of choice is that the
decision mechanisms determine a "choice
function" for the society, which identifies
11On some analytical problems involved in deriving
"the revealed preference of a government" by observing
its choices, see Kaushik Basu (1980).
12 Binariness requires a combination of two types of
choice consistency: basic "contraction consistency" (a)
and basic "expansion consistency" (y). These conditions
are quite exacting, and they have to be further
strengthened to get transitivity and other additional
properties (on this, see Sen [1971, 1977a], Rajat Deb
[1983], and Isaac Levi [1986]).
13On this, see Harsanyi (1955 p. 310): "Of course,
when I speak of preferences 'from a social standpoint,'
often abbreviated to 'social' preferences and the like, I
always mean preferences based on a given individual's
value judgments concerning 'social welfare."'
14This has been established in a sequence of results,
presented by Gibbard, Hansson, Andreu Mas-Colell,
Hugo Sonnenschein, Donald Brown, Georges Bordes,
Kelly, Suzumura, Douglas Blair, Robert Pollak, Julian
Blau, Deb, David Kelsey, and others; for critical
overviews, see Blair and Pollak (1982), Suzumura
(1983), and Sen (1986a).


### ---Economics-1995-0-07.txt---
what is picked from each alternative "menu"
(or opportunity set).'5

However, provided some conditions are
imposed on the "internal consistency" of
the choice function (relating decisions over
one menu in a "consistent" way to decisions
over other-related-menus), it can be
shown that some arbitrariness of power
would still survive.16 But the methodological
critique of James Buchanan would still
apply forcefully, as reformulated in the following
way: why should any restriction
whatever be placed a priori on the choice
function for the society? Why should not
the decisions emerging from agreed social
mechanisms be acceptable without having
to check them against some preconceived
idea of how choices made in different situations
should relate to each other?

What happens, then, to Arrow's impossibility
problem if no restrictions whatever
are placed on the so-called "internal consistency"
of the choice function for the society?
Would the conditions relating individual
preferences to social choice (i.e., the
Pareto principle, nondictatorship, and independence)
then be consistent with each

other? The answer, in fact, is no, not so. If
the Pareto principle and the conditions of
nondictatorship and independence are redefined
to take full note of the fact that
they must relate to social choices, not to any
prior notion of social preference, then a very
similar impossibility reemerges (see Theorem
3 in Sen [1993]).

How does this "general choice-functional
impossibility theorem" work? The underlying
intuition is this. Each of the conditions
relating individual preferences to social decisions
eliminates-either on its own or in
the presence of the other conditions-the
possibility of choosing some alternatives.
And the conjunction of these conditions can
lead to an empty choice set, making it "impossible"
to choose anything.

For example, the Pareto principle is just
such a condition, and the object of this
condition in a choice context, surely, is to
avoid the selection of a Pareto-inferior alternative.
Therefore this condition can be
sensibly redefined to demand that if everyone
prefers x to y, then the social decision
mechanism should be such that y should
not get chosen if x is available.17 Indeed, to
eliminate any possibility that we are implicitly
or indirectly using any intermenu consistency
condition for social choice, we can
define all the conditions for only one given
menu (or opportunity set) S; that is, we can
consider the choice problem exclusively over
a given set of alternative states. The Pareto
principle for that set S then only demands
that if everyone prefers some x to some y
in that set, then y must not be chosen from
that set.

Similarly, nondictatorship would demand
that there be no person such that whenever
she prefers any x to any y in that set S,
then y cannot be chosen from that set.
What about independence? We have to
modify the idea of decisiveness of a group
in this choice context, related to choices
over this given set S. A group would be
decisive for x against y if and only if,
whenever all members of this group prefer
any x to any y in this set S, then y is not to
be chosen from S. Independence would now
demand that any group's power of decisiveness
over a pair (x, y) be completely independent
of individual preferences over pairs
other than (x, y). It can be shown that there
is no way of going from individual preferences
to social choice satisfying these


### ---Economics-1995-0-08.txt---
choice-oriented conditions of independence,
the Pareto principle, nondictatorship,
and unrestricted domain, even without
invoking any "social preference," and without
imposing any demand of "collective rationality,
" or any intermenu consistency
condition on social choice.18

The morals to be drawn from all this for
Buchanan's questioning of "social preference"
would appear to be the following.
The "impossibility" result identified in a
particular form by Arrow can be extended
and shown to hold even when the idea of
"social preference" is totally dropped and
even when no conditions are imposed on
"internal consistency" of social choice. This
does not, however, annul the importance of
Buchanan's criticism of the idea of social
preference (in the context of choices emerging
from decision mechanisms for the society)
, since it is a valid criticism on its own
right. But the "impossibility" problem identified
by Arrow cannot be escaped by this
move.

IV. On Reasoned Social Welfare Judgments
How might we then avoid that impossibility?
It is important to distinguish the
bearing of the problem in the making of
aggregative social welfare judgments, as opposed
to the operation of social decision
mechanisms. I start with the former.
It may be recalled that the BergsonSamuelson
analysis and Arrow's impossibility
theorem followed a turn in welfare economics
that had involved the dropping of
interpersonal comparisons of utility. As it
happens, because of its utilitarian form, traditional
welfare economics had informational
exclusions of its own, and it had been
opposed to any basic use of nonutility information,
since everything had to be judged
ultimately by utility sum-totals in consequent
states of affairs. To this was now
added the exclusion of interpersonal comparisons
of utilities, without removing the
exclusion of nonutility information. This
barren informational landscape makes it
hard to arrive at systematic judgments of
social welfare. Arrow's theorem can be interpreted,
in this context, as a demonstration
that even some very weak conditions
relating individual preferences to social welfare
judgments cannot be simultaneously
satisfied given this informational privation.19
The problem is not just one of impossibility.
Consider the Field-Expansion Lemma:
decisiveness over any pair of alternatives entails
decisiveness over every pair of alternatives,
irrespective of the nature of the states
involved. Consider three divisions of a given
cake between two persons: (99,1), (50,50),
and (1, 99). Let us begin with the assumption
that each person-as homo economicus -
prefers a larger personal share of the cake. So
they happen to have opposite preferences.
Consider now the ranking of (99,1) and
(50, 50). If it is decided that (50, 50) is better
for the society than (99,1), then in terms of
preference-based information, person 2's
preference is getting priority over person l's.
A variant of the Field-Expansion Lemma
would then claim that person 2's preference
must get priority over all other pairs as well,
so that even (1,99) must be preferred to
(50, 50).20 Indeed, it is not possible, given
the assumptions, to regard (50, 50) as best of
the three; we could either have (99,1), giving
priority to person l's preference, or
(1,99), giving priority to 2's preference. But
not (50, 50). I am not arguing here that
(50,50) must necessarily be taken to be the
best, but it is absurd that we are not even
permitted to consider (50,50) as a claimant
18For exact statements of the conditions and a proof
of the theorem, see Sen (1993).
19On this issue, see Sen (1977b, 1982a).
20Formally, person 2 is "almost decisive" over the
first pair (in the sense of winning against opposition by
all others-in this case, person 1), and an alternative
version of the Field-Expansion Lemma shows that he
will be almost decisive (indeed fully decisive) over all
other pairs as well (see Lemma 3a in Sen [1970 pp.
43-44]). Note that "field expansion" is based inter alia
on the use of the condition of "unrestricted domain,"
allowing the possibility that the individuals involved
could have had other preferences as well.


### ---Economics-1995-0-09.txt---
to being the best element in this cakedivision
problem.

It is useful to consider what arguments
there might be for considering (50,50) as a
good possibility, and why we cannot use any
of these arguments in the information
framework resulting from Arrow's conditions.
First, it might seem good to divide the
cake equally on some general non-welfarist
ground, without even going into preferences
or utilities. This is not permitted because of
-the exclusion of evaluative use of nonutility
information, and this is what the FieldExpansion
Lemma is formalizing. Second,

presuming that everyone has the same
strictly concave utility function, we might
think that the sum-total of utilities would be
maximized by an equal division of the cake.
But this utilitarian argument involves comparability
of cardinal utilities, which is ruled
out. Third, we might think that equal division
of the cake will equate utilities, and
there are arguments for utility-centered
egalitarianism (see James Meade, 1976). But
that involves interpersonal comparison of
ordinal utilities, which too is ruled out. None
of the standard ways of discriminating between
the alternative states is viable in this
informational framework, and the only way
to choose between them is to go by the
preference of one person or another (since
they have opposite preferences).
To try to make social welfare judgments
without using any interpersonal comparison
of utilities, and without using any nonutility
information, is not a fruitful enterprise. We
do care about the size and distribution of
the overall achievements; we have reasons
to want to reduce deprivation, poverty, and
inequality; and all these call for interpersonal
comparisons-either of utilities or of
other indicators of individual advantages,
such as real incomes, opportunities, primary
goods, or capabilities.21 Once interpersonal
comparisons are introduced, the impossibility
problem, in the appropriately redefined
framework, vanishes.22 The comparisons
may have to be rough and ready and often
open to disputation, but such comparisons
are staple elements of systematic social welfare
judgments. Even without any cardinality,
ordinal interpersonal comparisons permit
the use of such rules of social judgment
as maximin, or lexicographic maximin.23 This
satisfies all of Arrow's conditions (and many
others), though the class of permissible social
welfare rules that do this is quite limited,
unless cardinality is also admitted,
along with interpersonal comparisons (see
Louis Gevers, 1979; Kevin Roberts, 1980a).
With the possibility of using interpersonal
comparisons, other classes of possible rules
for social welfare judgments (including
inter alia, utilitarianism) become usable.24
While the axiomatic derivations of different
social-welfare rules in this literature
are based on applying interpersonal comparisons
to utilities only, the analytical
problems are, in many respects, rather similar
when people are compared in terms of
some other feature, such as real income,
holdings of primary goods, or capabilities to
function. There are, thus, whole varieties of


### ---Economics-1995-0-10.txt---
ways in which social welfare judgments can
be made using richer information than in
the Arrow framework.

This applies also to procedures specifically
aimed at making social welfare judgments
and other aggregative evaluations,
based on institutionally accepted ways of
making interpersonal comparisons: for example,
in using indexes of income inequality
(see Serge Kolm's [1969] and Anthony
Atkinson's [1970] pioneering work on this),
or in aggregate measures of distributioncorrected
real national income (Sen, 1976a),
or of aggregate poverty (Sen, 1976b).5 This
links the theory of social choice to some of
the most intensely practical debates on economic
policy.26 While Arrow's impossibility
theorem is a negative result, the challenge it
provided has led, dialectically, to a great
many constructive developments.
V. On Social Decision Mechanisms
Moving from the exercise of making social
judgments to that of choosing social
decision mechanisms, there are other difficulties
to be faced. While systematic interpersonal
comparisons of utilities (and other
ways of seeing individual advantage) can be
used by a person making social welfare
judgment, or in agreed procedures for social
judgments (based on interpreting available
statistics to arrive at, say, orderings of aggregate
poverty or inequality or distribution-
corrected real national income), this is
not an easy thing to do in social-decision
mechanisms which must rely on some standard
expressions of individual preference
(such as voting), which do not readily lend
themselves to interpersonal comparisons.
The impossibility problem, thus, has
greater resilience here. While it is also the
case that the critique of James Buchanan
(and others) of the idea of "social rationality"
and the concept of "social preference"
applies particularly in this case (that of
judging social decision mechanisms), the impossibility
problem does indeed survive, as
we have seen, even when the concept of
social preference is eschewed and the idea
of social rationality in the Arrovian form is
dropped altogether (Section III). How, then,
can we respond to the challenge in this
case?

We may begin by noting that the conditions
formulated and used by Arrow, while
appealing enough, are not beyond criticism.
First, not every conceivable combination of
individual preferences need be considered
in devising a social decision procedure, since
only some would come up in practice. As
Arrow had himself noted, if the condition of
unrestricted domain is relaxed, we can find
decision rules that satisfy all the other conditions
(and many other demands) over substantial
domains of individual preference
profiles. Arrow (1951), along with Duncan
Black, had particularly explored the case of
"single-peaked preferences," but it can be
shown (Sen, 1966) that this condition can be
far extended and generalized to a much less
demanding restriction called "value restriction.
"27

25The literature on such measures is now quite
large. Different types of exercises are illustrated by Sen
(1973), Frank Cowell (1977), Blackorby and Donaldson
(1978,1980), Siddiq Osmani (1982), Sudhir Anand
(1983), Atkinson (1983, 1989), S. R. Chakravarty (1983),
Anthony Shorrocks (1983), Suzumura (1983), James E.
Foster (1984, 1985), Ravi Kanbur (1984), Michel
Le Breton and Alain Trannoy (1987), W. Eichhorn
(1988), Peter J. Lambert (1989), and Martin Ravallion
(1994), among many other contributions.
26The policy discussions include those surrounding
the influential Human Development Reports, produced
by the United Nations Development Programme. Another
strong force in that direction has been the sequence
of UNICEF reports on The State of the World's
Children. Policy issues related to such social judgments
have been discussed by Paul Streeten et al. (1981),
Nanak Kakwani (1986), Jean Dreze and Sen (1989),
Alan Hamlin and Philip Pettit (1989), Keith Griffin and
John Knight (1990), Anand and Ravallion (1993),
Partha Dasgupta (1993), and Meghnad Desai (1995).
27,, Value restriction" turns out to be necessary and
sufficient for this class of domain conditions for consistent
majority rule when individual preferences are linear
orderings, though the conditions are more complex
in the general case of weak orderings (see Sen and
Prasanta Pattanaik, 1969; see also Ken-ichi Inada, 1969,
1970). These relations can be generalized to all
Arrovian social welfare functions and for nonmanipulable
voting procedures (on which see Maskin [1976]
and E. Kalai and E. Muller [1977]). Other types of
conditions have been proposed by Tullock (1967) (with


### ---Economics-1995-0-11.txt---
The plausibility of different profiles of
individual preferences depends on the nature
of the problem and on the characteristics
of individual motivations. It is readily
checked that with three or more people, if
everyone acts as homo economicus in a
cake-division problem (always preferring
more cake to oneself over all else), then
value restriction and the related conditions
would all be violated, and majority rule
would standardly lead to intransitivities. 'It
is also easy to show that in the commodity
space, with each concentrating on her own
commodity basket, the Arrow conditions
could not be all satisfied by any decision
mechanism over that domain. Majority rule
and other voting procedures of this kind do
cause cycles in general in what is called
"the economic domain" (of interpersonal
commodity space), if everyone votes in a
narrowly self-interested way.

However, majority rule would be a terrible
decision procedure in this case, and its
intransitivity is hardly the main problem
here. For example, taking the most deprived
person in a community and passing on half
her share of the cake divided between two
richer persons would be a majority improvement,
but scarcely a great welfare-economic
triumph. In view of this, it is perhaps just as
well that the majority rule is not only nasty
and brutish, but also short in consistency.28
The tension between social welfare judgments
(of different kinds explored, for example,
by Meade [1976], Arrow [1977],
Mirrlees (1982), William J. Baumol [1986],
or John Broome [1991]) and mechanical decision
rules (like majority decision) with
inward-looking, self-centered individuals is
most obvious here. Also, as Buchanan
(1994a, b) has argued, the acceptability of
majority rule is, in fact, related to its tendency
to generate cycles, and the endemic
cyclicity of majority decisions is inescapable,
given the endogeneity of alternative proposals
that can be presented for consideration.
In practice, in facing political decisions,
the choices may not come in these stark
forms (there are many issues that are mixed
together in political programs and proposals)
, and also individuals do not necessarily
only look at their "own share of the cake"
in taking up political positions and attitudes.
29 The "public choice" school has
tended to emphasize the role of logrolling
in political compromises and social decisions.
While that school has also been rather
wedded to the presumption of each person
being homo economicus even in these exercises
(see Buchanan and Tullock, 1962),
there is a more general social process here
(involving a variety of motivations) that can
be fruitfully considered in examining decision
mechanisms. Central to this is the role
of public discussion in the formation of
preferences and values, which has been emphasized
by Buchanan (1954a,b).

The condition of independence of irrelevant
alternatives is also not beyond disputation
and, indeed, has led to debates-explicitly
or by implication-for a very long
time. It was one of the issues that divided
J. C. Borda (1781) and Marquis de Condorcet
(1785), the two French mathematicians,
who had pioneered the systematic
theory of voting and group decision procedures
in the 18th century. One version of
the rule proposed by Borda, based on adding
the rank-order numbers of candidates in
each voter's preference list, violates the independence
condition rather robustly, but it
is not devoid of other merits (and is frequently
used in practice).30 Other types of
voting rules have also been shown to have
different desirable properties.3'


### ---Economics-1995-0-12.txt---
In examining social decision mechanisms,
we have to take the Arrow conditions seriously,
but not as inescapable commandments.
Our intuitions vary on these matters,
and Arrow's own theorem shows that not
everything that appeals to us initially would
really be simultaneously sustainable. There
is a need for some de-escalation in the grim
"fight for basic principles." The issue is not
the likely absence of rationally defendable
procedures for social decisions, but the relative
importance of disparate considerations
that pull us in different directions in evaluating
diverse procedures. We are not at the
edge of a precipice, trying to determine
whether it is at all "possible" for us to hang
on.

VI. Procedures and Consequences
I turn now to the general issue, identified
earlier, of the contrast between relying respectively
on (i) the "rightness" of procedures,
and (ii) the "goodness" of outcomes.
Social choice theory, in its traditional
form, would seem to belong to the latter
part of the dichotomy, with the states of
affairs judged first (the subject matter of
"social preference" or "social welfare
judgements"), followed by identification of
procedures that generate the "best" or
"maximal" or "satisficing" states. There are
two issues here. First, can consequences
really be judged adequately without any notion
of the process through which they are
brought about? I shall also presently question
whether this presumption of processindependence
is the right way of seeing the
claims of social choice theory. Second, can
we do the converse of this, and judge procedures
adequately in a consequence-independent
way? This issue I take up first.
Sugden (1981, 1986), who has extensively
analyzed this dichotomy (between procedural
and consequence-based views), explains
that in the public choice approach, which he
supports, "the primary role of the government
is not to maximize the social good, but
rather to maintain a framework of rules
within which individuals are left free to
pursue their own ends" (Sugden, 1993
p. 1948). This is indeed so, but even in
judging a "framework of rules" in this way,
we do need some consequential analysis,
dealing with the effectiveness of these
frameworks in letting individuals be actually
"free to pursue their own ends." In an
interdependent world, examples of permissive
rules that fail to generate the freedom
to pursue the respective individual ends are
not hard to find (see Sen, 1982b).
Indeed, it is not easy to believe that the
public-choice approach is-or can bereally
consequence-independent. For example,
Buchanan's support of market systems
is based on a reading of the consequences
that the market mechanism tends to produce,
and consequences certainly do enter
substantially in Buchanan's evaluation of
procedures: "To the extent that voluntary
exchange among persons is valued positively
while coercion is valued negatively, there
emerges the implication that substitution of
the former for the latter is desired, on the
presumption, of course, that such substitution
is technologically feasible and is not
prohibitively costly in resources" (Buchanan,
1986 p. 22). While this is not in serious
conflict with Buchanan's rejection of any
"transcendental" evaluation of the outcomes
(p. 22), nevertheless the assessment
of outcomes must, in some form, enter this
evaluative exercise. 32

There are, however, other-more purely
procedural-systems to be found in this literature.
If the utilitarian tradition of judging
everything by the consequent utilities is
one extreme in the contrast (focusing only
on a limited class of consequences), Nozick's
(1974) elegant exploration of libertarian
"entitlement theory" comes close to the
other end (focusing on the right rules that
cover personal liberties as well as rights of
holding, using, exchanging, and bequeathing
32Buchanan (1986) expresses some basic sympathy
for "libertarian socialists" (as opposed to antilibertarian
socialists) but attributes what he sees as their
well-intentioned but mistaken opposition to markets to
their not having "the foggiest notion of the way the
market works" and to their being "blissfully ignorant
of economic theory" (pp. 4-5). Consequential analysis
incorporated in economic theory is precisely what
Buchanan is invoking here to dispute the libertarian
socialist position.


### ---Economics-1995-0-13.txt---
legitimately owned property). But the possibility
of having unacceptable consequences
has to be addressed by any such procedural
system. What if the results are dreadful for
many, or even all?

Indeed, it can be shown that even gigantic
famines can actually take place in an
economy that fulfills all the libertarian rights
and entitlements specified in the Nozick
system.33 It is, thus, particularly appropriate
that Nozick (1974) makes exceptions to
consequence-independence in cases where
the exercise of these rights would lead to
"catastrophic moral horrors."34 Because of
this qualification, consequences are made to
matter after all, and underlying this concession
is Nozick's good sense (similar to
Buchanan's) that a procedural system of
entitlements that happens to yield catastrophic
moral horrors (we have to have some
consensus on what these are) would
be-and should be-ethically unacceptable.
However, once- consequences are
brought into the story, not only is the purity
of a consequence-independent system lost,
but also the issue of deciding on the relative
importance of "right rules" and "good consequences"
is forcefully reestablished.

I turn now to the other side of the dichotomy:
can we have sensible outcome

judgments in a totally procedure-independent
way? Classical utilitarianism does indeed
propose such a system, but it is hard
to be convinced that we can plausibly judge
any given utility distribution ignoring altogether
the process that led to that distribution
(attaching, for example, no intrinsic
importance whatever to whether a particular
utility redistribution is caused by charity,
or taxation, or torture).35

This recognition of the role of processes
is not, in fact, hostile to social choice theory,
since there is nothing to prevent us
from seeing the description of processes as
a part of the consequent states generated
by them.36 If action A is performed, then
"action A has been done" must be oneindeed,
the most elementary-consequence
of that event. If Mr. John Major were to
wish not merely that he should be reelected
as Prime Minister, but that he should be
"reelected fairly" (I am not, of course, insinuating
that any such preference has been
expressed by Mr. Major), the consequence
that he would be seeking would have procedural
requirements incorporated within it.
This is not to claim that every process can
be comfortably placed within the description
of states of affairs without changing
anything in social choice theory. Parts of the
literature that deal with comparisons of decision
mechanisms in arriving at given states
would need modification. If, in general, processes
leading to the emergence of a social
state were standardly included in the characterization
of that state, then we have to
construct "equivalence classes" to ignore
some differences (in this case, between some
antecedent processes) to be able to discuss
cogently the "same state" being brought
about by different decision mechanisms. To
make sense of such ideas as, say, "path
independence" (on which see Plott [1973]),
so that they are not rendered vacuous,
equivalence classes of this type would certainly
have to be constructed (on the concepts
of equivalence classes and invariance
conditions, see Sen [1986b]).

The contrast between the procedural and
consequential approaches is, thus, somewhat
overdrawn, and it may be possible to
combine them, to a considerable extent, in
an adequately rich characterization of states
of affairs. The dichotomy is far from pure,
and it is mainly a question of relative concentration.


### ---Economics-1995-0-14.txt---
VII. Liberties, Rights, and Preferences
The need to integrate procedural considerations
in consequential analysis is especially
important in the field of rights and
liberties. The violation or fulfillment of basic
liberties or rights tends to be ignored in
traditional utilitarian welfare economics not
just because of its consequentialist focus,
but particularly because of its "welfarism,"
whereby consequent states of affairs are
judged exclusively by the utilities generated
in the respective states.37 While processes
may end up getting some indirect attention
insofar as they influence people's utilities,
nevertheless no direct and basic importance
is attached in the utililtarian framework to
rights and liberties in the evaluation of states
of affairs.

The initial formulation of social choice
did not depart in this respect from the utilitarian
heritage, but it is possible to change
this within a broadly Arrovian framework
(see Sen, 1970, 1982a), and a good deal of
work has been done in later social choice
theory to accommodate the basic relevance
of rights and liberties in assessing states of
affairs, and thus to evaluate economic, political,
and social arrangements. If a person is
prevented from doing some preferred thing
even though that choice is sensibly seen to
be in her "personal domain," then the state
of affairs can be seen to have been worsened
by this failure. The extent of worsening
is not to be judged only by the magnitude
of the utility loss resulting from this (to
be compared with utility gains of others, if
any), since something more is also at stake.
As John Stuart Mill (1859 p. 140) noted,
"there is no parity between the feeling of a
person for his own opinion, and the feeling
of another who is offended at his holding
it."38 The need to guarantee some "minimal
liberties" on a priority basis can be incorporated
in social choice formulations.
It turns out, however, that such unconditional
priority being given even to minimal
liberty can conflict with other principles of
social choice, including the redoubtable
Pareto principle. The "impossibility of the
Paretian liberal" captures the conflict between
(i) the special importance of a person'
s preferences over her own personal
sphere, and (ii) the general importance of
people's preferences over any choice, irrespective
of field. This impossibility theorem
has led to a large literature extending, explaining,
disputing, and ameliorating the result.
39 The "ways out" that have been sought
have varied between (i) weakening the priority
of liberties (thereby qualifying the minimal
liberty condition), (ii) constraining the
field-independent general force of preferences
(thereby qualifying the Pareto principle)
, and (iii) restricting the domain of permissible
individual-preference profiles. As
in the case of the Arrow impossibility problem,
the different ways of resolving this conflict
have variable relevance depending on
the exact nature of the social choice exercise
involved.

There have also been attempts to redefine
liberty in purely procedural terms. The
last is an important subject on its own (quite
independently of any use it might have
as an attempt to resolve the impossibility),
and I shall presently consider it. But as has
37Utilities can be defined in terms of choices made,
desires entertained, or satisfactions received, but the
point at issue applies to each of these interpretations.
Utilitarian welfare economics has tended traditionally
to focus on satisfactions, partly because individual
choices do not immediately yield any basis for interpersonal
comparisons unless some elaborately hypothetical
choices are considered, on which see Harsanyi
[1955]), but also because "satisfaction" had appeared
to utilitarian economists as providing a more solid basis
for judging individual welfare. For example, this was
the reason given by A. C. Pigou (1951 pp. 288-89):
Some economists ... have employed the term
'utility' indifferently for satisfactions and for
desiredness. I shall employ it here to mean
satisfactions, so that we may say that a man's
economic welfare is made up of his utilities.
38The idea of "personal domains" and "protected
spheres" goes back to Mill (see Riley, 1987), and more
recently has found strong and eloquent expression in
the writings of Friedrich Hayek (1960).
39For general accounts of the literature, see Kelly
(1978), Suzumura (1983, 1991), Wriglesworth (1985),
Paul Seabright (1989), and Pattanaik and Suzumura
(1994a, b). For public-choice critiques, see Sugden
(1981, 1993) and Rowley (1993).


### ---Economics-1995-0-15.txt---
been noted by Gaertner, Pattanaik, and
Suzumura (1992), who have recently provided
the most extensive recharacterization
of liberty (in terms of "game forms"), the
impossibility problem "persists under virtually
every plausible concept of individual
rights" (p. 161).40

The decisive move in the direction of a
purely procedural view of liberty was made
by Nozick (1974), responding to my social
choice formulation and to the impossibility
of the Paretian liberal (Sen, 1970). This has
been followed by important constructive
contributions by Gardenfors (1981) and
Sugden (1981), and the approach has been
extended and developed into game-form
formulations by Gaertner et al. (1992). In
the game-form view, each of the players has
a set of permissible strategies, and the outcome
is a function of the combination of
strategies chosen by each of the players
(perhaps qualified by an additional "move"
by "nature"). The liberties and rights of the
different persons are defined by specifying a
permissible subset from the product of the
strategy sets of the different individuals. A
person can exercise his rights as he likes,
subject to the strategy combination belonging
to the permissible set.

In defining what rights a person has, or in
checking whether his rights were respected,
there is, on this account, no need to examine
or evaluate the resulting state of affairs,
and no necessity to examine what states the
individuals involved prefer. In contrasting
this characterization of preference-independent,
consequence-detached rights with the
social choice approach to rights, perhaps
the central question that is raised is the
plausibility of making people's putative
rights, in general, so dissociated from the
effects of exercising them. This is a general
issue that was already discussed at a broader
level (Section VI).

In some contexts, the idea of seeing rights
in the form of permission to act can be
quite inadequate, particularly because of
"choice inhibition" that might arise from a
variety of causes. The long British discussion
on the failure of millions of potential
welfare recipients from making legitimate
claims (apparently due to the shame and
stigma of having one's penury publicized
and recorded) illustrates a kind of nonrealization
of rights in which permission is not
the main issue at all.4' Similarly, the inability
of women in traditionally sexist societies
to use even those rights that have not been
formally denied to them also illustrates a
type of rights failure that is not helpfully
seen in terms of game forms (see Sen, 1992b
pp. 148-50). Even the questions that standardly
come up in this country in determining
whether a rape has occurred have to go
well beyond checking whether the victim in
question was "free" to defy.

Leaving out such cases, it might well be
plausible to argue that rights can be nicely
characterized by game forms in many situations.
However, even when that is the case,
in deciding on what rights to protect and
codify, and in determining how the underlying
purpose might be most effectively
achieved, there is a need to look at the
likely consequences of different game-form
specifications and to relate them to what
people value and desire. If, for example, it
appears that not banning smoking in certain
gatherings (leaving the matter to the discretion
of the people involved) would actually
lead to unwilling victims having to inhale


### ---Economics-1995-0-16.txt---
other people's smoke, then there would be
a case for considering that the game-form
be so modified that smoking is simply
banned in those gatherings. Whether or not
to make this move must depend crucially on
consequential analysis. The object, in this
case, is the prevention of the state of affairs
in which nonsmokers have to inhale unwillingly
other people's smoke: a situation they
resent and which-it is assumed-they
have a right to avoid. We proceed from
there, through consequential analysis (in an
"inverse" form: from consequences to
antecedents), to the particular game-form
formulation that would not achieve an
acceptable result. The fact that the articulation
of the game-form would be

consequence-independent and preferenceindependent
is not a terribly profound assertion
and is quite consistent with the fundamental
relevance of consequences and

preferences.

The contrast between game-form formulations
and social-choice conceptions of
rights is, thus, less deep than it might first
appear (see Sen, 1992b).42 As in other fields
considered earlier (Section VI), in this area
too, the need to combine procedural concerns
with those of actual events and outcomes
is quite strong.

VIII. Values and Individual Choices
I have so far postponed discussing individual
behavior and rationality, though the
issue has indirectly figured in the preceding
discussions (for example, in dealing with
norms for social choice, individual interest
in social welfare judgments, and determination
of voting behavior). The public choice
tradition has tended to rely a good deal on
the presumption that people behave in a
rather narrowly self-centered way-as homo
economicus in particular, even though
Buchanan (1986 p. 26) himself notes some
"tension" on this issue (see also Geoffrey
Brennan and Loren Lomarsky, 1993). Public
servants inter alia are to be seen as working
for their own well-being and success.
Adam Smith is sometimes described as
the original proponent of the ubiquity and
ethical adequacy of "the economic man,"
but that would be fairly sloppy history. In
fact, Smith (1776, 1790) had examined the
distinct disciplines of "self-love," "prudence,
" "sympathy," "generosity," and
"public spirit," among others, and had discussed
not only their intrinsic importance,
but also their instrumental roles in the success
of a society, and also their practical
influence on actual behavior. The demands
of rationality need not be geared entirely to
the use of only one of these motivations
(such as self-love), and there is plenty of
empirical evidence to indicate that the presumption
of uncompromising pursuit of narrowly
defined self-interest is as mistaken
today as it was in Smith's time.43 Just as it is
necessary to avoid the high-minded sentimentalism
of assuming that all human beings
(and public servants, in particular) try
constantly to promote some selfless "social
good," it is also important to escape what
may be called the "low-minded sentimentalism"
of assuming that everyone is constantly
motivated entirely by personal selfinterest.
44

420n related matters, see also Pattanaik and Suzu-
Imura (1994a, b).

A set of studies on this and related issues is
presented in Jane Mansbridge (1990).
44Efforts to explain every socially motivated action
as some kind of a cunning attempt at maximization of
purely private gain are frequent in part of modern
ecnonomics. There is an interesting question as to
whether the presumption of exclusive self-interested-
ness is a more common general belief in America than
in Europe, without being a general characteristic of
actual behavior. Alexis de Tocqueville thought so:
The Americans... are fond of explaining almost
all the actions of their lives by the principle of
self-interest rightly understood; they show with
complacency how an enlightened regard for
themselves constantly prompts them to assist
one another and inclines them willingly to
sacrifice a portion of their time and property to
the welfare of the state. In this respect, they
frequently fail to do themselves justice; for in
the United States as well as elsewhere people
are sometimes seen to give way to those disinterested
and spontaneous impulses that are
natural to man; but the Americans seldom
admit that they yield to emotions of this kind;
they are more anxious to do honor to their
philosophy than to themselves.
(Tocqueville, 1840 [Book II, Chapter VIII; in the 1945
edition, p. 122]).


### ---Economics-1995-0-17.txt---
This does not, however, negate an important
implication of the question raised by
Buchanan and others that public servants
would tend to have their own objective
functions; I would dissociate that point from
the further claim, with which it has come
mixed, that these objective functions are
narrowly confined to the officials' own selfinterest.
The important issue to emerge is
that there is something missing in a large
part of the resource-allocation literature (for
example, in proposals of algorithms for decentralized
resource allocation, from Oscar
Lange and Abba Lerner onward) which
make do without any independent objective
function of the agents of public action. The
additional assumption of homo economicus
is not needed to point to this general
lacuna.

While this has been a somewhat neglected
question in social choice theory
(though partially dealt with in the related
literature on implementation), there is no
particular reason why such plurality of motivations
cannot be accommodated within a
social choice framework with more richly
described social states and more articulated
characterization of individual choices and
behavior. In the formulation of individual
preference used by Arrow (1951) and in
traditional social choice theory, the nature
of the objective function of each individual
is left unspecified. While there is need for
supplementary work here, this is a helpfully
permissive framework-not tied either to
ceaseless do-gooding, or to uncompromising
self-centeredness.

Even with this extended framework, taking
us well beyond the homo economicus,
there remain some difficulties with the notion
of individual rationality used here.
There is a problem of "insufficiency" shared
by this approach to rationality with other
"instrumental" approaches to rationality,
since it does not have any condition of critical
scrutiny of the objectives themselves.
Socrates might have overstated matters a bit
when he proclaimed that "the unexamined
life is not worth living," but an examination
of what kind of life one should sensibly
choose cannot really be completely irrelevant
to rational choice.45 An "instrumental
rationalist" is a decision expert whose response
to seeing a man engaged in slicing
his toes with a blunt knife is to rush to
advise him that he should use a sharper
knife to better serve his evident objective.
This is perhaps more of a limitation in
the normative context than in using the
presumption of rationality as a device for
predicting behavior, since such critical
scrutiny might not be very widely practiced.
However, the last is not altogether clear,
since discussions and exchange, and even
political arguments, contribute to the formation
and revision of values. As Frank
Knight (1947 p. 280) noted, "Values are
established or validated and recognized
through discussion, an activity which is at
once social, intellectual, and creative."
There is, in fact, much force in Buchanan's
(1954a p. 120) assertion that this is a central
component of democracy ("government by
discussion") and that "individual values can
and do change in the process of decisionmaking.
"

This issue has some real practical importance.
To illustrate, in studying the fact that
famines occur in some countries but not in
others, I have tried to point to the phenomenon
that no major famine has ever

taken place in any country with a multiparty
democracy with regular elections and with a
reasonably free press (Sen, 1984).46 This
applies as much to the poorer democratic
countries (such as India, Zimbabwe, or
Botswana) as to the richer ones.47 This is
largely because famines, while killing millions,


### ---Economics-1995-0-18.txt---
do not much affect the direct wellbeing
of the ruling classes and dictators,
who have little political incentive to prevent
famines unless their rule is threatened by
them. The economic analysis of famines
across the world indicates that only a small
proportion of the population tends to be
stricken-rarely more than 5 percent or so.
Since the shares of income and food of
these poor groups tend normally to be no
more than 3 percent-of the total for the
nation, it is not hard to rebuild their lost
share of income and food, even in very poor
countries, if a serious effort is made in that
direction (see Sen, 1981; Dreze and Sen,
1989). Famines are thus easily preventable,
and the need to face public criticism and to
encounter the electorate provides the government
with the political incentive to take
preventive action with some urgency.
The question that remains is this. Since
only a very small proportion of the population
is struck by a famine (typically 5 percent
or less), how does it become such a
potent force in elections and in public criticism?
This is in some tension with the assumption
of universal self-centeredness, and
presumably we do have the capacity-and
often the inclination-to understand and
respond to the predicament of others.48
There is a particular need in this context to
examine value formation that results from
public discussion of miserable events, in
generating sympathy and commitment on
the part of citizens to do something to prevent
their occurrence.

Even the idea of "basic needs," fruitfully
used in the development literature, has to
be related to the fact that what is taken as a
"need" is not determined only by biological
and uninfluencible factors. For example, in
those parts of the so-called Third World in
which there has been increased and extensive
public discussion of the consequences
of frequent childbearing on the well-being
and freedom of mothers, the perception that
a smaller family is a "basic need" of women
(and men too) has grown, and in this value
formation a combination of democracy, free
public media, and basic education (especially
female education) has been very potent.
The implications of this finding are
particularly important for rational consideration
of the so-called "world population
problem."49

Similar issues arise in dealing with environmental
problems. The threats that we

face call for organized international action
as well as changes in national policies, particularly
for better reflecting social costs in
prices and incentives. But they are also dependent
on value formation, related to public
discussions, both for their influence on
individual behavior and for bringing about
policy changes through the political process.
There are plenty of "social choice problems"
in all this, but in analyzing them, we have to
go beyond looking only for the best reflection
of given individual preferences, or the
most acceptable procedures for choices
based on those preferences. We need to
depart both from the assumption of given
preferences (as in traditional social choice
theory) and from the presumption that people
are narrowly self-interested homo eco-
480n this general question, see Rawls (1971) and
Thomas Scanlon (1982). See also Daniel Hausman and
Michael McPherson (1993).

49See the discussion and the literature cited in Sen
(1994 pp. 62-71), particularly Dasgupta (1993). See
also Adam Przeworski and Fernando Limongi's (1994)
international comparisons, which indicate a fairly strong
association between democracy and fertility reduction.
In the rapid reduction of the total fertility rate in the
Indian state of Kerala from 4.4 in the 1950's to the
present figure of 1.8 (a level similar to that in Britain
and France and lower than in the United States), value
formation related to education, democracy, and public
discussion has played a major part. While the fertility
rate has also come down in China (though not as much
as in Kerala), China's use of compulsion rather than
consensual progress has resulted in relatively high
infant-mortality rates (28 per thousand for boys and 33
per thousand for girls, compared with Kerala's 17 per
thousand for boys and 16 per thousand for girls in
1991). Such public dialogues are, however, hard to
achieve in many other parts of India, despite democ-
racy, because of the low level of elementary education,
especially for women. These and related issues are
discussed in Dreze and Sen (1995).


### ---Economics-1995-0-19.txt---
nomicus (as in traditional public choice theory)
.

IX. Concluding Remarks

Perhaps I could end by briefly returning
to the questions with which I began. Arrow's
impossibility theorem does indeed identify a
profound difficulty in combining individual
preference orderings into aggregative
social welfare judgments (Section II). But
the result must not be seen as mainly a
negative one, since it directly leads on to
questions about how to overcome these
problems. In the context of social welfare
judgments, the natural resolution of these
problems lies in enriching the informational
base, and there are several distinct ways of
doing this (Section IV). These approaches
are used in practice for aggregative judgments
made by individuals, but they can
also be used for organized procedures for
arriving at social measures of poverty, inequality,
distribution-adjusted real national
incomes, and other such aggregative indicators.


Second, Buchanan's questioning of the
concept of social preference (and of its use
as an ordering to make-or explain-social
choices) is indeed appropriate in the case of
social decision mechanisms, though less so
for social welfare judgments (Section III).
The Arrow theorem, in its original form,
does not apply once social decision-making
is characterized in terms of choice functions
without any imposed requirement of
intermenu consistency. However, when the
natural implications of taking a .choicefunctional
view of social decisions are

worked out, Arrow's conditions have to be
correspondingly restated, and then the impossibility
result returns in its entirety once
again (Section III). The idea of social preference
or internal consistency of social
choice is basically redundant for this impossibility
result. So Buchanan's move does not
negate Arrow's impossibility. On the other
hand, it is an important departure in its own
right.

Coming to terms with the impossibility
problem in the case of social decision mechanisms
is largely a matter of give and take
between different principles with respective
appeals. This calls for a less rigid interpretation
of the role of axiomatic demands on
permissible social decision rules (Section V).
Third, Buchanan's argument for a more
procedural view of social decisions has much
merit. Nevertheless, there are good reasons
to doubt the adequacy of a purely procedural
view (independent of consequences), just
as there are serious defects in narrowly consequentialist
views (independent of procedures)
. Procedural concerns can, however,
be amalgamated with consequential ones by
recharacterizing states of affairs appropriately,
and the evaluation of states can then
take note of the two aspects together (Section
VI). This combination is especially important
in accommodating liberty and rights
in social judgments as well as social decision
mechanisms (Section VII).

Finally, there is room for paying more
attention to the rationality of individual behavior
as an integral component of rational
social decisions. In particular, the practical
reach of social choice theory, in its traditional
form, is considerably reduced by its
tendency to ignore value formation through
social interactions. Buchanan is right to emphasize
the role of public discussion in the
development of preferences (as an important
part of democracy). However, traditional
public choice theory is made unduly
narrow by the insistence that individuals
invariably behave as homo economicus
(a subject on which social choice theory
is much more permissive). This uncompromising
restriction can significantly misrepresent
the nature of social concerns and
values. But aside from this descriptive limitation,
there is also an important issue of
"practical reason" here. Many of the more
exacting problems of the contemporary
world-varying from famine prevention to
environmental preservation-actually call
for value formation through public discussion
(Section VIII).

On the rationality of social decisions,
many important lessons have emerged from
the discipline of social choice theory as well
as the public choice approach. In fact, we
can get quite a bit more by combining these
lessons. As a social choice theorist, I had


### ---Economics-1995-0-20.txt---
not, in fact, planned to be particularly evenhanded
in this paper, but need not, I suppose,
apologize for ending up with rather
even hands.
 ## Economics-1996-0


### ---Economics-1996-0-02.txt---
Interest in health economics has soared over
the past three decades, stimulated by intellectual
innovations, greater availability of data,
and, most importantly, a surge in health care
spending from 6 to 14 percent of GDP.' An
11 -fold increase2 in the number of Ph.D.s has
enabled many professional schools, government
agencies,3 and research institutes to add
health economists to their staffs. Nevertheless,
the health care debate of 1993-1994 benefited
much less than it could have from the results
of their research.

In this lecture I identify the primary sources
of modern health economics and describe interactions
between the discipline and the field
of health, drawing heavily on my personal experience.
I then turn to the question of why
economists did not have more impact on
health care reform. I report and analyze the
answers of health economists, economic theorists,
and practicing physicians to a survey I
conducted in 1995. My principal conclusion is
that value differences among economists, as
well as among all Americans, are a major barrier
to effective policy-making. I discuss the
implications of the importance of values for
economics and conclude the lecture with my
recommendations for health care reformrecommendations
based on my values as well

as my understanding of health economics.
I. The Past

In 1963 a seminal paper by Kenneth Arrow
discussed risk aversion, moral hazard, asymmetrical
information, philanthropic externalities,
and numerous other topics that have since
played major roles in health economics research.
4 He saw that uncertainty about health
status and about the consequences of care was
the key to understanding the health sector from
both positive and normative perspectives. As
Arrow wrote, "Recovery from disease is as
unpredictable as its incidence" ( 1963 p. 951 ).
At the same time that Arrow was depicting
the theoretical landscape, Martin Feldstein
was pioneering in the application of quantitative
methods such as 2-stage least squares,
principal component analysis, and linear programming
to the estimation of production
functions and other important economic aspects
of medical care. His numerous papers
analyzing the British National Health Service
formed the basis for his Ph.D. thesis at Oxford
University (Feldstein, 1967).

A third line of work that has had a significant
influence on health economics also began
in the early 1960's with the National
Bureau of Economic Research Conference
on Investment in Human Beings (1962) and
Gary S. Becker's treatise on human capital


### ---Economics-1996-0-03.txt---
(1964). The NBER conference volume included
Selma Mushkin's (1962) paper,

"Health As an Investment," and a few years
later the application of the human capital
model to health was given its fullest development
by Michael Grossman (1972).

Predating and postdating the theoretical and
econometric innovations of the 1960's is a
stream of research that focuses on health care
institutions, technology, and policy. As early
as 1932, Michael M. Davis and C. Rufus
Rorem (1932) were writing about the crisis in
hospital finance. Significant contributions to
this genre have been made by Henry Aaron,
Alain Enthoven, Rashi Fein, Eli Ginzberg,
Herbert Klarman, Dorothy Rice, Anne Scitovsky,
Anne and Herman Somers, Burton Weisbrod,
and many others. Although they are all economists,
much of their work does not appear in
economics journals, but rather in books and in
publications such as the New England Journal
of Medicine, Journal of the American Medical
Association, Milbank Memorial Fund Quarterly,
and Health Affairs.

In recent decades several leading health
economists have addressed theoretical, empirical,
and policy questions in various aspects of
their research (e.g., Joseph Newhouse, Mark
Pauly). Health economics has also been enlivened
and enriched by contributions from economists
who are primarily specialists in other
fields such as industrial organization, labor, finance,
and public economics (e.g., Sherwin
Rosen, Richard Zeckhauser). There has also
been a welcome infusion from another direction,
namely physicians who have earned
Ph.D.s in economics and who now contribute
to the economics literature (e.g., Alan Garber,
Mark McClellan).

Parenthetically, all this name-dropping has
a point. I want to underscore the varied intellectual,
methodological, and ideological sources
that have contributed to the health economics
enterprise. Research has often been described
as lonely work, and in one sense it is. But in
another sense it is the most collective of all
human activities. The philosopher Susan Haack
(1995) sees scientific research as analogous to
an attempt by many participants to fill out a
huge crossword puzzle. We have clues; we try
out possible answers; we check to see whether
they fit together. Occasionally, an Arrow or a
Becker comes up with one of the really big
answers that runs across the puzzle and makes
it easier to discover the smaller words that intersect
it. If several of the small answers don't
fit, however, we may have to modify or even
reject the larger one. It is good to remember
that all answers are provisional until the puzzle
is completed-and it never will be.5
Although I have mentioned only American
economists, note should be taken of many fine
health economists in England, Canada, and
other high-income countries. There is, however,
less of a global intellectual community
in this field than in some other branches of
economics'-or in other fields of health7-
because most health economics research is
applied and is (or is perceived to be) country
specific. More than 60 years ago Walton
Hamilton (1932) noted that "The organization
of medicine is not a thing apart which can
be subjected to study in isolation. It is an
aspect of culture whose arrangements are
inseparable from the general organization of
society" (p. 190). On the whole I agree
with Hamilton; there are, however, important
economic questions concerning technology
assessment and disease prevention that are
common to all high-income countries. This
type of research does not receive support commensurate
with its importance because funding
sources, both public and private, tend to
focus on national problems.

My involvement in health economics grew
out of my research on the service industries
(Fuchs, 1968, 1969). It was motivated in part
by a desire to gain a better understanding of
the postindustrial society that was emerging in
the United States and other developed coun-
' In an extension of the crossword puzzle analogy suggested
by Richard J. Zeckhauser in a 1995 personal communication,
it seems that economics might make more
progress if theorists didn't tend to concentrate on the lower
left-hand corner of the puzzle while empiricists work the
upper right-hand corner.

6 The relatively new International Health Economics
Association will hold its inaugural conference in Vancouver
in May 1996.

7 The Journal of the American Medical Association has
twenty international editions published weekly in eleven
languages, with 40 percent more recipients than the regular
U.S.-based edition (George D. Lundberg and Annette
Flanagin, 1995).


### ---Economics-1996-0-04.txt---
tries (Fuchs, 1966, 1978a). The growth of the
service economy and improved methods of
contraception were bringing women into paid
employment and dramatically changing gender
roles and relationships. Lower fertility and
longer life expectancy were transforming the
age distribution of the population, and this
transformation, along with the fragmentation
of the family and the declining influence of
traditional religion, were creating new social
and economic conditions. The health sector,
with its nonprofit institutions, professional
dominance, sharply skewed distribution of demand,
and the critical importance of the consumer
in the production process, seemed like
a fruitful area for investigation. I was particularly
interested in trying to understand the determinants
of health and the determinants of
health care expenditures.

With regard to health, my research has led
me to emphasize the importance of nonmedical
factors such as genetic endowment, the
physical and psychosocial environment, and
personal behaviors such as cigarette smoking,
diet, and exercise. Over time, advances in
medical science contribute significantly to reductions
in morbidity and mortality; at any
given point in time, however, differences in
health levels within or between developed
countries are not primarily related to differences
in the quantity or quality of medical
care.8

With respect to expenditures on medical
care, my research has led me to emphasize the
importance of supply factors, especially technology
and the number and specialty mix of
physicians.9 To be sure, conventional demand
factors such as price, income, and insurance
play significant roles, but in my judgment concentration
on them to the exclusion of (partly
exogenous) supply factors misses a big part of
the expenditures story. Despite many attempts
to discredit it,'? the hypothesis that fee-forservice
physicians can and do induce demand
for their services is alive and well.11
My views about health and health care expenditures
have been formed not only through
research but also through close interaction
with medical scientists, practicing physicians,
and other health professionals. Since 1968 I
have maintained a regular medical school faculty
appointment in addition to my appointment
in economics, and have participated
every year in a wide variety of health-related
activities. This dual life would have gained approval
from John Stuart Mill who, in The Principles
of Political Economy (1848, reprinted
1987), wrote, "It is hardly possible to overrate
the value ... of placing human beings in contact
with persons dissimilar to themselves, and
with modes of thought and action unlike those
with which they are familiar ... Such communication
has always been ... one of the primary
sources of progress" (p. 581).
The proposition that the discipline of economics
has a great deal to contribute to health
and medical care is not one likely to require
elaborate defense before this audience. (I have
had audiences that were less receptive to this
notion.) It might, however, be useful to report
briefly just what it was in economics that I
found to be most relevant in the invasion of
alien turf. (To avoid undue suspense, let me
say at once that it was not game theory.)
In my experience, the most important contribution
we make is the economic point of
view, which may be summed up in three
words: scarcity, substitutability, and heterogeneity.
This economic point of view stands
in stark contrast to the romantic and monotechnic
points of view that I found prevalent
among health professionals and health policymakers.
The romantic point of view refuses to
accept the notion that resources are inherently
scarce; any apparent scarcity is attributed to
some manmade problem, such as capitalism or
socialism, market failure or excessive government
interference. In the 1960's and 1970's,
many physicians said that there was no need
to limit expenditures for medical care if only


### ---Economics-1996-0-05.txt---
we would cut defense spending. In 1996, when
health care expenditures are almost four times
as large as the defense budget, this argument
is not heard as often. Because it denies the
inevitability of choice, the romantic point of
view is increasingly seen as impotent to deal
with the problems of health care."2
To be sure, it is not clear whether economic
research or the force of circumstances is bringing
about the change in point of view. I suspect
that there is a synergistic relationship in which
the former provides the language to give
expression to the latter. Or, as Max Weber
(1915; reprinted 1946) wrote, material and
ideal interests are the tracks on which society
rides, but ideas throw the switches (p. 280).
The monotechnic point of view, found frequently
among physicians, engineers, and others
trained in the application of a particular
technology fails to recognize the diversity of
human wants, or acknowledge the difference
between what is technically best and what is
socially desirable." "Optimal" care is defined
as the point where the marginal benefit is zero,
ignoring the fact that resources used for health
care have alternative uses that might yield
greater benefit. The "production" of health is
viewed narrowly as a function of inputs of
medical care, and the appropriate input mix
is assumed to be determined by technology
without regard to relative prices, explicit or
implicit. For example, Feldstein found that average
lengths of stay in British hospitals were
uniform across regions despite large regional
differences in the pressures for admission.'4
The monotechnic view often fails to consider
the heterogeneity of preferences, even
though for many health problems there are alternative
interventions: one drug versus another,
drugs versus surgery, or even "watchful
waiting" versus any intervention. Under the
influence of economists and other behavioral
scientists, physicians are now making such
choices with more attention to patient differences
in time preference, attitudes toward risk,
tolerance of pain, functional needs, and other
characteristics.

Among our specific tools, one of the most
useful is the idea of the margin. The key to
gaining acceptance for this principle is to have
people realize that most decisions involve a
little more or a little less, and that they will
make better decisions if they look at the costs
and benefits associated with having a little
more or a little less. This formulation is more
effective than postulating "maximization,"
which economists find useful for classroom or
research purposes, but sounds unreal to most
noneconomists.

David M. Eddy's research on the frequency
with which women should get Pap smears provides
a fine example of the use of marginal (or
incremental) analysis to assist in medical
decision-making. This screening test for cervical
cancer is of proven safety and effectiveness,
and before Eddy's work appeared most
experts recommended that women obtain this
test annually. Using mathematical models and
clinical studies of the natural history of the disease,
Eddy (a physician with extensive training
in operations research and economics)
calculated the incremental cost of 1 additional
year of life expectancy with screening regimes
ranging from once every 6 months to once
every 5 years. The results were striking. Some
screening has a high yield at low incremental
cost, but as the frequency of screening is increased
from once every 2 years to once a year
the incremental cost rises to close to $1 million
per additional year of life expectancy (Eddy,
1980, 1987, 1990).15

The impact of Eddy's research on health
policy is worth noting. The American Cancer
Society accepted his conclusions and the Society'
s recommendation to screen once every
3 years made the front page of the New York
'2 As a sign of the times, Sweden, Norway, Finland,
and the World Health Organization are sponsoring the first
international conference on priorities in health care in October
1996.

' Economists fall into their own monotechnic trap
when they offer policy advice under the assumption that
efficiency is society's only goal.
'4 See Feldstein (1967).

'5 To put this in perspective, consider the choice between
tissue plasminogen activator (TPA) and its cheaper
alternative, streptokinase, as the treatment to dissolve a
clot during a heart attack. The latest studies suggest that
the incremental cost of TPA rather than streptokinase is
$33,000 per year of life extended (D. B. Mark et al., 1995).
In the United States TPA is usually the treatment of
choice, but Canadians use streptokinase.


### ---Economics-1996-0-06.txt---
Times. The U.S. Surgeon General, the U.S. Preventive
Services Task Force, and the American
College of Physicians supported this position,
and many individual physicians changed their
practice accordingly. Intense opposition came
from the American College of Obstetricians and
Gynecologists and the American Society of Cytology.
The contending groups finally negotiated
a compromise along the following lines: "Pap
smears should be done annually; after two or
more negative examinations the frequency can
be decreased." 16

The economist's distinction between movement
along a function and a shift in the function
is a very useful one. It is particularly applicable
in discussing the relationship between medical
care and health. At any given time in developed
countries the effects of additional medical care
on health are usually small, but over time advances
in medical science have had significant
effects on health."7 Or consider the relationship
between infant mortality and per capita income.
At any given time income is a good predictor of
infant mortality, especially post-neonatal mortality
(28 days to one year). In log-log regressions
across the 48 states in 1937 and 1965, the
income elasticity of post-neonatal mortality was
-0.53 (0.11) and -0.49 (0.12) respectively.18
The decline in post-neonatal mortality between
1937 and 1965, however, was consistent with an
elasticity of -2.00. There was undoubtedly a
shift in the function associated with the introduction
of antibiotics and other advances in
medical science (Fuchs, 1974b). In 1991 the
elasticity was -0.73 (0.12) but the change from
1965 to 1991 was consistent with an elasticity
of -1.08, suggesting a further shift in the function,
but not nearly so large as the shift between
1937 and 1965.

Economists have much to contribute to the
health field. What can they expect in exchange?
The most immediate benefit to me was the pressure
to make my lectures and research results
accessible, relevant, and credible to intelligent
but untutored and often unsympathetic audiences.
I was obliged to write clearly and simply
and to reconsider assumptions and conclusions
in economics that I might otherwise have accepted
too readily. My experience was in accord
with that of Thomas Henry Huxley (1863) who
wrote, "Some experience with popular lecturing
has convinced me that the necessity of making
things plain to uninstructed people was one of
the very best means of clearing up the obscure
corners in one's own mind."

For example, one of the questions that troubled
me for a long time is why there is such
a strong correlation between health and years
of schooling. I originally believed that this
was another manifestation of the productivityenhancing
effect of education. Schooling could
increase an individual's knowledge about the
health effects of personal behavior and medical
care options or could enable a person to better
process and act upon information about health
(Grossman, 1975). Or schooling could increase
an individual's ability to develop strategies
of self control (Richard A. Thaler and
H. M. Shefrin, 1981). I began to doubt the
schooling-causes-health hypothesis, however,
when it was observed that the favorable effect
of an additional year of schooling on health
does not diminish with increased years of
schooling. It is just as strong for those with
more than a high school education as for those
with less and continues right through graduate
school on up to the doctoral level (Grossman,
1975).19 I began to suspect that perhaps the correlation
was the result of some underlying difference
among individuals that affects both
schooling and health.

To explore this question I examined survey
data on smoking behavior collected
by colleagues in the Stanford Heart Disease
Prevention Program as part of a health


### ---Economics-1996-0-07.txt---
education experiment designed to alter smoking
and other risks for heart disease (Nathan
Maccoby and Douglas S. Solomon, 1981).
Identical regressions of smoking on schooling
were estimated at age 17 and at age 24,
with schooling measured in both cases as the
number of years the individual would eventually
complete. The most striking result was
the absence of any increase in the size of the
schooling coefficient between the ages of 17
and 24. The additional schooling could not
be the cause of the differential smoking behavior
(and by extension the differential
health associated with smoking) at age 24
because the differences in smoking were already
evident at age 17, before the differences
in schooling had emerged (Philip
Farrell and Fuchs, 1982) .20

In my judgment, the most likely explanation
for the high correlation between health and
schooling is that both reflect differences in
time preference (Fuchs, 1982). Both health
and schooling are aspects of investment in human
capital; differences among individuals in
time preference that are established at an early
age could result in different amounts of investment
in health and education.21

Although I believe there have been many
fruitful interactions between economics and
health, the political debate over health care
reform in 1993-1994 benefited much less
than it could have from the insights of economists.
Possible explanations for the failure
of health economics research to have more
impact on policy are explored in the next
section.

II. The Present

George Stigler's Presidential Address to the
American Economic Association in December
1964 was distinctive in its emphasis on prophecy
over preaching. To be specific, Stigler predicted
that economics was "at the threshold of
its golden age" (Stigler, 1965 p. 17) because
"the age of quantification is now full upon us"
(p. 16). The growth of empirical estimation
was, for Stigler, "a scientific revolution of the
very first magnitude" (p. 17). He believed
that empirical research would have an impact
on policy far beyond anything possible from
theory alone because "a theory can usually be
made to support diverse policy positions. Theories
present general relationships, and which
part of a theory is decisive in a particular context
is a matter of empirical evidence" (p. 13).
With regard to health care, Stigler's prediction
of a vast expansion in empirical research
has been amply fulfilled. During the past 30
years economists have published thousands of
empirical articles on various aspects of health
and medical care. But the shallow and inconclusive
debate over health policy in 1993-
1994 contradicts his expectation that this
research would narrow the range of partisan
disputes and make a significant contribution to
the reconciliation of policy differences.22 What
went wrong?

One possibility is that the research was inconclusive.
If health economists cannot agree
among themselves, why should their research
have a salutary effect on public policy? Second,
even if the research were conclusive, it
would not be of much help to policy if the
results were not adequately disseminated to a
wider audience. A third possible explanation
is that the policy debate foundered on differences
in values, differences which could not
be reconciled by empirical research, however
conclusive and however well disseminated.
To gain some insight into these matters,
I prepared a 20-question survey concerning
health economics and health policy and sent it
to health economists, economic theorists, and
practicing physicians. The health economists
were those whom I considered to be the lead-
20 It is worth noting that the negative relation between
schooling and smoking is only evident for cohorts that
reached age 17 after the information about the effects of
smoking on health became available. It is also of interest
that the relationship has not diminished for more recent
cohorts even though the information about the negative
consequences of smoking has become more widely
available.

21 There are alternative or complementary "third variable"
explanations possible; compare Albert Bandura's
(1991) concept of self-efficacy.
22 Stigler's optimism regarding the impact of empirical
research on policy may have had more vindication in other
fields, but my research into family issues (Fuchs, 1983)
and gender issues (Fuchs, 1988a) do not lead me to such
a conclusion.


### ---Economics-1996-0-08.txt---
ing people in the field, plus some of the more
promising recent Ph.D.s. There were 46 respondents
(response rate 88 percent). The theorists
were also leaders in the field; I was
assisted in selecting them by two eminent theorists.
" There were 44 respondents (response
rate 63 percent). The practicing physicians
were reached through my personal contacts,
and include colleagues and friends of those
contacts. Nearly all are in private practice, not
teaching, research, or administration. They are
located on both the east and west coasts in
small towns and large cities. The practice settings
vary from solo to a group of over 100
physicians, and in organizational form from
traditional fee-for-service to capitation. They
include generalists, surgical specialists, and
nonsurgical specialists. There were 42 physician
respondents (response rate 89 percent).
The participants were asked to indicate
whether they agree or disagree with each of 20
relatively short statements; they were also
given the option of answering "no opinion."
Ten percent of the health economists' replies
were "no opinion"; the theorists used that option
19 percent of the time, and the physicians
11 percent. Participants were also invited to
qualify any of their replies by jotting comments
on the back of the survey. The percentage
of replies that were qualified was 8, 5, and
3 for the health economists, theorists, and physicians,
respectively. Participants were told to
assume that the statements refer to the United
States in 1995, other things held constant. For
statements with more than one part, "agree"
would indicate that the respondent agreed with
all parts of the statement. The order of the
questions was determined randomly, and respondents
were guaranteed anonymity.

Three experts24 from three different universities
who were not participants in the survey
were asked to identify which of the 20 questions
were relatively value-free ("positive"
questions) and which had substantial value aspects
("policy-value" questions). Their independent
replies were almost unanimous in
identifying seven of the questions as "positive,
" and thirteen as "policy-value." Table 1
shows the percent agreeing for each question,
with the two types of questions grouped separately.
Question numbers refer to the ordering
of the questions in the survey. The policyvalue
questions are presented in three groups:
four that pertain directly to national health insurance,
three that pertain directly to health
insurance company underwriting, and all others.
Questions for which the percentage agreeing
differs significantly from a 50-50 split (by
a chi-square test) are identified with asterisks.
We see in Table 1 that the degree of consensus
on positive questions among health
economists is extremely high.25 In six of the
seven cases the hypothesis that the observed
split differs from a 50-50 split simply by
chance is rejected with p < 0.01 and the seventh
with p < 0.05. There is also a high degree
of consensus among economic theorists, but
for two of the questions (12 and 13) the majority
of theorists gave replies opposite to
the majority of health economists. Consensus
among the physicians on the positive questions
was more rare. In no case did the split differ
from 50-50 with p < 0.01, and in only three
cases was the split significant at p < 0.05. For
one question (4) the majority of physicians
gave replies opposite to the majority of health
economists.26

When we turn to the policy-value questions,
agreement among the health economists drops
sharply. For example, in replies to the four questions
dealing with support for national health insurance,
the health economists never depart
significantly from a 50-50 split. On question 8,


### ---Economics-1996-0-09.txt---
Survey Health Economic Practicing
question economists theorists physicians
number' Question (n c 46) (n c 44) (n c 42)
A. Positive Questions:

4 The high cost of health care in the United States makes U.S. firms 9** 17** 64
substantially less competitive in the global economy.
9 Third-party payment results in patients using services whose costs 84** 93** 73*
exceed their benefits, and this excess of costs over benefits
amounts to at least 5 percent of total health care expenditures.
10 Physicians have the power to influence their patients' utilization 68* 77** 67
of services (i.e., shift the demand curve), and their propensity
to induce utilization varies inversely with the level of demand.
12 Widespread use of currently available screening and other 11** 83** 37
diagnostic techniques would result in a significant (more than
3%) reduction in health care expenditures (from what they
would otherwise be) 5 years from now.
13 The primary reason for the increase in the health sector's share of 81** 37 68*
GDP over the past 30 years is technological change in
medicine.

18 Differential access to medical care across socioeconomic groups 0** 17** 34*
is the primary reason for differential health status among these
groups.

19 In the long run employers bear the primary burden of their 13** 8** 43
contributions to employees' health insurance.
B. Policy-Value Questions:

National health insurance questions:
3 The U.S. should now enact some plan that covers the entire 62 65* 68*
population.

7 The U.S. should seek universal coverage through a broad-based 54 56 56
tax with implicit subsidies for the poor and the sick.
14 The U.S. should seek universal coverage through mandates, with 38 29* 46
explicit subsidies for the poor and the sick.
15 Given a choice between the Clinton health care plan or no federal 36 33* 28**
health care legislation for at least 5 years, the Clinton plan
should be approved.

Insurance company underwriting questions:
8 Insurance companies should be required to cover all applicants 51 29** 69*
regardless of health condition and not allowed to charge sicker
individuals higher premiums.

17 Health insurance premiums should be higher for smokers than for 71** 90** 85**
nonsmokers.

20 Health insurance premiums charged to individuals born with 14** 20** 13**
genetic defects (that result in above average use of medical
care) should be higher than those charged to individuals
without such defects.

All other policy-value questions:
I It is inequitable for the govemment to vary subsidies for health 62 36 86**
insurance by size of firm.

2 "Any willing provider" legislation (that requires health plans to 12** 12** 39
include any physician who wants to be included) is desirable
for society as-a whole.

5 National standardized health insurance benefit packages should be 42 51 63
established.

6 It is inefficient for the govemment to vary subsidies for health 66* 42 73*
insurance by size of firm.

11 Expenditures on medical R&D are greater than is socially 27* 29* 16**
optimal.

16 All health insurance plans should be required to offer "point of 30** 55 83**
service" options (that allow patients to obtain care outside the
basic plan at additional cost).
'Of those who agree or disagree. * Significantly different from 50 percent at p < 0.05.
Question numbers refer to order of questions in original survey. ** Significantly different from 50 percent at p < 0.01.


### ---Economics-1996-0-10.txt---
which would require insurance companies to
cover all applicants regardless of health condition
with no premium surcharge for the sick, the
health economists are evenly divided: 51 percent
agree and 49 percent disagree. Among economic
theorists there is slightly more agreement on
policy, but not as much as among practicing
physicians who, contrary to both groups of economists,
show more agreement on policy-value
than on positive questions.

The contrasts between the replies by group
and type of question are brought more sharply
into focus in Table 2, which shows the average
absolute difference between the percentage
agreeing and the percentage disagreeing.
Among health economists the extent of consensus
for the positive questions is significantly
larger than for the policy-value
questions regardless of whether the comparison
is between means or medians. Although
the sample sizes are very small (7 and 13 ), the
differences by type of question are so large we
can reject the null hypothesis with considerable
confidence.27

It is also worth noting that the extent of
agreement among health economists on the
positive questions is much higher than is usually
found in surveys of economists covering
a wide variety of fields. For example, in a
survey conducted by Richard M. Alston et al.
(1992) the authors identify ten questions as
"micro-positive" and seven as "micronormative.
"28 In order to achieve comparability
between their survey and mine, I
combined their "agree, with provisos" with
their "agree," and then calculated the mean
absolute difference between percentage agreeing
and percentage disagreeing.29 This difference
(22 percentage points) was much smaller
(and less statistically significant) than the difference
I found for the health economists.


### ---Economics-1996-0-11.txt---
Why is there so little agreement among economists
regarding policy-value questions when
there is so much agreement on the positive
questions? One possible explanation is differences
in values. Most health policy decisions
have significant implications for freedom, efficiency,
justice, and security. Health economists
(like other Americans) probably desire all these
goals, but (again like other Americans) they
probably differ in the values they attach to
them, or in the way they define them,3' and
these differences could lead to sharply different
views about policy.

Another possible explanation is that there
are positive questions embedded in the policyvalue
questions and that health economists disagree
with respect to those positive questions.
This is the view taken by Milton Friedman in
195332 although he subsequently modified his
position in 1966 and 1995.33 In order to gain
some insights concerning the roles of values
and embedded positive issues in policy differences
I take a closer look at the policy-value
questions bearing on national health insurance
(3, 7, 14, 15) and on insurance company underwriting
(8, 17, 20).

Consider, for instance, question 3 which
calls for some national plan to cover the entire
population. The 62-38 percent split among
health economists may well reflect differences
in values, with those who agree placing a high
value on providing all Americans with the
right to have access to health care. On the other
hand, it is readily apparent that there are many
positive questions embedded in this policyvalue
question. For instance, most economists
see a loss in efficiency from requiring everyone
to have the same health insurance, but they
probably differ in their estimates of the extent
of the loss. Some may even believe there is a
net gain in efficiency because of imperfections
in the private market for health insurance.
Strongly held differences about this positive
question could produce different answers to
question 3 even among respondents with similar
values.

Some of the positive questions embedded in
question 3 may be beyond the scope of conventional
economics. For instance, Professor
A may favor national health insurance in part
because she believes it will contribute to a
more stable and harmonious society.34 Professor
B may disagree with that prediction, and
is therefore less inclined to support national
health insurance.

The role of embedded positive questions
can also be easily discerned in the three questions
(8, 17, 20) dealing with insurance company
underwriting. Health economists strongly
support charging higher premiums for smokers
than for nonsmokers, but are strongly opposed
to charging higher premiums to individuals
born with genetic defects. On question 8, dealing
with requiring insurance companies to insure
the sick with no premium surcharge, the
health economists are evenly split. One of the
positive questions embedded in question 8 is
the reason for people's illness. If a respondent
thought that most illness was the result of genetic
differences, the reply would presumably
be consistent with the answer to question 20.
On the other hand, if most illness was assumed
to be the result of personal behaviors like cigarette
smoking, the reply would probably be
consistent with the one given to question 17.
Inasmuch as leading medical scientists have
strongly divergent views about the importance
of genetic factors in disease, it is hardly
surprising that health economists are unable
to reach agreement. The state of knowledge
about the links between genes and disease is
the same as for the health economists' policy-value questions
(0.77).

" For a discussion of alternative conceptions of justice,
see Amartya Sen (1987).

32 In Essays in Positive Economics, Friedman (1953)
wrote "Differences about economic policy among disinterested
citizens derive predominantly from different
predictions about the economic consequences of taking
action ... " (p. 5).

33 See Dollars and Deficits (1966 p. 6); personal communication
in 1995.

34 In 1974 I recommended universal comprehensive insurance
for several reasons, one of which was the speculation
that "a national health insurance plan to which all
(or nearly all) Americans belong could have considerable
symbolic value as one step in an effort to forge a link
between classes, regions, races, and age groups." I also
thought it important to add "It will be more likely to serve
that function well if not too much is expected of it-if it
is not oversold-particularly with respect to its probable
impact on health" (Fuchs, 1974a p. 150).


### ---Economics-1996-0-12.txt---
constantly changing. Thus, if cigarette smoking
were found to be determined primarily by
genetic factors, the answers to question 17
would probably change even in the absence of
any change in values.

Positive economic questions are also embedded
in the insurance company underwriting
issues. Most economists realize that requiring
health insurance companies to charge healthy
people the same premium as those with a genetic
disease will deter healthy individuals
from purchasing insurance. But economists
may well differ as to how large that effect will
be and how large a welfare loss it implies.
It is easy to see that there are positive questions
embedded in the policy-value questions,
but it is more difficult to believe that disagreement
over them, rather than differences in
values, explains the low level of consensus
among health economists with respect to the
policy-value questions. Note that the physicians
have a higher level of consensus about
the policy-value questions than do the health
economists. This probably reflects more homogeneous
values among physicians rather
than agreement about the embedded positive
questions. (Note the low level of agreement
among physicians on the explicit positive
questions.)

It may be that it is not so much disagreement
among health economists about the embedded
positive questions as it is uncertainty about
them that make differences in values the
driving force in replies to the policy-value
questions. Many psychologists and economists
have observed that uncertainty about a datum
causes most individuals to give it less weight
when making choices.:

Uncertainty among health economists concerning
the positive questions that are embedded
in the policy-value questions is suggested
by their use of the "no opinion" option. Unlike
the theorists, who chose "no opinion"
twice as often for the positive questions as for
the policy-value questions (28 percent versus
15 percent), the health economists chose "no
opinion" less often for the positive questions
than for the policy-value questions (8 percent
versus 11 percent).36 The role of uncertainty
was mentioned by Milton Friedman in 1966 as
a reason for qualifying his position about the
relative imeportance of scientific judgment and
value differences (Friedman, 1966 p. 6).
In order to investigate further the relationship
between policy-value and positive questions,
I developed two indexes based on the
answers to the national health insurance and
insurance underwriting questions. The first index
measures each respondent's support for
national health insurance. It is constructed by
assigning a value of 1 to agreement with each
of questions 3, 7, 14, and 15, a value of 0 for
disagreement with those questions, and a value
of 0.5 for no opinion. The sum of the values
was divided by 4, giving a range for the index
of 1 (indicating agreement with all four questions)
to 0 (indicating disagreement with all
four questions). The "actuarial" 37 model index
was based on answers to questions 8, 17,
and 20. In the case of question 8, "disagree"
was given a value of 1, and for questions 17
and 20 "agree" was given a value of 1. The
total score for each individual is divided by 3,
again yielding a range for the index from 1 to
0 (indicating complete support or complete rejection
of the actuarial approach).

The results are presented in Table 3. We
see that with respect to national health insurance
the support among the three groups is virtually
identical. There is considerable variation around
the mean for each group, and the amount of variation
is similar across thOe groups. Thirteen percent
of all respondents had an index value of 1,
while 15 percent completely rejected the notion
of national health insurance with an index value
of 0. Not surprisingly, there is a negative correlation
between the national health insurance
index and the actuarial model index. But there


### ---Economics-1996-0-13.txt---
TABLE 3-INDEXES OF SUPPORT FOR NATIONAL HEALTH INSURANCEa AND FOR AN ACTUARIAL MODEL
OF PRIVATE INSURANCE UNDERWRITINGb
Health, Economic Practicing

economists theorists physicians All
National health insurance index:
Mean 0.48 0.48 0.49 0.48

Standard error of the mean 0.05 0.05 0.05 0.03
Coefficient of variation (percent) 71 70 67 69
Percentage with index = 1 15 9 14 13
Percentage with index = 0 13 18 14 15
Actuarial model index:

Mean 0.46 0.61 0.44 0.50

Standard error of the mean 0.05 0.04 0.04 0.03
Coefficient of variation (percent) 71 42 64 60
Percentage with index = 1 7 16 7 10
Percentage with index = 0 22 5 14 14
Coefficient of correlation between
the two indexes -0.37t -0.34t -0.37t _0.35**
a National health insurance index is based on answers to survey questions 3, 7, 14 and 15.
'Actuarial model index is based on answers to survey questions 8, 17 and 20.
t Significant at p < 0.02.

** Significant atp < 0.01.

is a significant difference between the groups in
the extent of support for the actuariai model index.
The economic theorists have a value of
0.61, compared with 0.46 for the health economists
and 0.44 for the practicing physicians. The
theorists are as supportive of national health insurance
as are the other groups, but if insurance
is to be provided through the private market, the
theorists are more inclined than the other two
groups to have premiums reflect expected loss.
One reasonable interpretation of this result is that
the theorists give more weight to the efficiency
aspects of the actuarial model, whereas the
health economists and the practicing physicians
give more weight to the distributional aspects.
Is there a close relationship between the respondents'
scores on the indexes and their
responses to the positive questions? The correlation
coefficients presented in Table 4 show
that the answer is overwhelmingly in the negative.
For the national health insurance index
there is only one positive question (10) for one
group (the health economists) that reaches statistical
significance with p < 0.05. For the actuarial
model index, only questions 9 and 10
show a significant relationship for the health
economists, and questions 10 and 12 for all
groups taken together. Whatever it is that is determining
the respondents' positions with regard
to national health insurance or the actuarial approach,
it is not their views on the seven positive
questions.

Correlations between the indexes and the six
policy-value questions not utilized in their
construction also are typically low, with one
striking exception. Respondents agreeing with
question 5, which calls for national standardized
health insurance benefit packages, also
support national health insurance and just as
clearly reject the actuarial approach for private
insurance underwriting. The actuarial model
index is also negatively correlated with agreement
with question 1.

The weak relationship between the positive
questions and the two indexes is also revealed
in Table 5, which presents the results of regressing
the indexes on the positive questions.
38 In the national health insurance
38 The reliability of the OLS regressions was checked
in several ways: values for each respondent were predicted
from each regression and found to be always between 0
and 1; regressions run with the dependent variable transregression


### ---Economics-1996-0-14.txt---
the only statistically significant coefficient
is for question 10 for health economists.
Other things being equal, those who
agree with the induced-demand hypothesis are
more supportive of national health insurance
than those who disagree, but the effect on the
index (0.239) is less than changing one of the
four answers from disagree to agree. The actuarial
model regressions result in a few additional
significant coefficients but, in general,
the respondents' replies to the explicit positive
questions do not explain their position with respect
to such major policy issues as national
health insurance or insurance company underwriting
changes. It seems unlikely, then, that
their position on these policy issues can be explained
by differences in the embedded positive
questions.

Although I believe that differences in values
lie at the heart of the disagreement about
policy-value questions, I recognize that there
is scope for work on the embedded positive
questions and this work could contribute to a
narrowing of policy differences. One indication
of where research is needed is the percent
of health economists answering "no opinion"
on the individual policy-value questions. This
option was chosen most frequently (35 percent
of the time) for question 11 concerning the
optimality of expenditures on medical R&D.39
Given the importance of technologic change
in medicine both from the point of view of
health outcomes and of expenditures, this is
clearly a high-priority area for research. Two
other questions elicited a "no opinion" response
from one fifth of the health economists.
They are question 1 concerning the subsidies
for health insurance by size of firm (a key part
of the Clinton plan) and question 20 (about
differential premiums for persons born with
genetic defects). In the latter case the high percentage
responding "no opinion" may reflect
uncertainty regarding the magnitudes of the
efficiency and distributional implications of


### ---Economics-1996-0-15.txt---
TABLE 5-RESULTS OF ORDINARY LEAST SQUARES REGRESSIONS OF THE NATIONAL HEALTH INSURANCE INDEX
AND THE ACTUARIAL MODEL INDEX ON SEVEN POSITIVE QUESTIONS
Survey question Health Economic Practicing
number economists theorists physicians All groups
National health

insurance index:

4 0.206 -0.007 0.048 0.022 0.029
(0.165) (0.163) (0.158) (0.079) (0.088)
9 0.053 0.138 0.046 0.056 0.052
(0.139) (0.195) (0.162) (0.084) (0.086)
10 0.239* 0.032 -0.104 0.079 0.077
(0.112) (0.157) (0.151) (0.074) (0.075)
12 -0.167 0.221 -0.100 -0.053 -0.043
(0.154) (0.196) (0.128) (0.076) (0.084)
13 -0.169 0.027 -0.121 -0.088 -0.093
(0.124) (0.133) (0.123) (0.066) (0.071)
18 -0.776 -0.031 0.087 0.007 0.012
(0.699) (0.162) (0.133) (0.093) (0.094)
19 0.231 0.049 -0.016 0.087 0.089
(0.141) (0.198) (0.145) (0.080) (0.083)
ET dummy' -0.026

(0.086)

PP dummy' -0.024

(0.089)

Constant 0.402 0.201 0.598 0.438 0.454
(0.189) (0.198) (0.178) (0.099) (0.112)
R 2 0.287 0.066 0.080 0.052 0.053
Adjusted R2 0.156 -0.116 -0.110 -0.001 -0.017
F 2.18 0.36 0.42 0.98 0.76

Actuarial model

index:

4 -0.102 0.079 -0.029 -0.069 -0.029
(0.160) (0. 1 19) (0.131) (0.068) (0.073)
9 0.373** 0.027 -0.102 0.146* 0.142*
(0.135) (0.142) (0.135) (0.072) (0.072)
1 0 -0.224* -0.211 -0.013 -0. 187** -0. 190**
(0.108) (0.115) (0.125) (0.063) (0.062)
12 0.026 0.216 0.023 0.146* 0.091
(0.149) (0.143) (0.106) (0.065) (0.070)
13 0.094 0.149 0.090 0.041 0.090
(0.120) (0.097) (0.102) (0.056) (0.059)
18 0.432 0.068 0.113 0.114 0.109
(0.678) (0.118) (0.111) (0.079) (0.079)
19 -0.010 0.080 -0.075 -0.062 -0.028
(0.137) (0.145) (0.120) (0.068) (0.070)
ET dummy' 0.130

(0.071)

PP dummy' -0.033

(0.074)

Constant 0.234 0.491 0.454 0.446 0.391
(0.184) (0.144) (0.148) (0.085) (0.093)
R 2 0.279 0.166 0.114 0.145 0.182
Adjusted R2 0.146 0.004 -0.068 0.096 0.122
F 2.10 1.02 0.63 3.00 3.02*

Notes: Standard error in parentheses.
Health economist is the excluded class (ET = economic theorists and PP = practicing physicians).
* Significant at p < 0.05.

** Significant at p < 0.01.


### ---Economics-1996-0-16.txt---
eliminating premium differentials. Or, it may
reflect a reluctance to choose between conflicting
values.

Before leaving the survey it is worth considering
what it reveals about the ability of
health economists to disseminate their conclusions
about the positive questions to a wider
audience. Overall, one must conclude that they
have not been very successful, as revealed by
the political debate of 1993- 1994 and the media
coverage of policy issues. Consider, for
example, question 19 concerning whether in
the long run employers bear the primary burden
of their contributions to their employees'
health insurance. Although 87 percent of the
health economists disagreed with that statement,
politicians on both sides of the debate
assumed, erroneously, that it was correct.
Moreover, nearly all of the media made the
same error. Most of the politicians and most
of the media also showed little understanding
of questions 4, 12, 13, and 18.
I am as ready as the next economist to
criticize politicians and journalists, but the
survey results suggest that their poor understanding
of health economics is not entirely
their fault. First, the economic theorists
and the practicing physicians, two groups with
above-average ability and opportunity to absorb
the conclusions of the health economists,
did not show good command of the positive
questions. In my judgment the health economists
answered 80 percent correctly, but the
average theorist answered only 52 percent correctly
and the mean score for the physicians
was only 53 percent. The differences in the
distributions of scores is striking: 45 of the
46 health economists had more correct answers
than the average theorist or the average
physician.

A second possible reason for the poor understanding
of health economics displayed by
the politicians and the media in 1993-1994 is
the wide disagreement among health economists
over the policy-value questions. When
health economists interact with politicians and
journalists, their discussions probably focus on
the policy-value questions; in the absence of a
professional consensus on many of these questions,
it is not surprising that politicians and
journalists fall back on their own values to
shape their positions.

Returning to the question posed at the beginning
of this section about why economic
research failed to result in a more informed
and productive health care policy debate, the
survey results provide some provisional answers.
First, although health economists are
in substantial agreement about the positive
questions, they have major disagreements
about policy-value questions. Second, health
economists were not successful in getting
their conclusions on positive questions accepted
by the politicians or the media, and
even had difficulty in communicating their
results to economic theorists and practicing
physicians. Third, the health economists'
disagreements over policy probably reflect
differences in values, although it is clear that
there are many positive questions embedded
in the policy-value questions. In my judgment
the problem is not so much that the
health economists disagree about the embedded
questions as that they are uncertain
about them. In the face of such uncertainty,
they tend to let their values drive their policy
recommendations.

III. The Future

If values play such an important role in policy
disputes, what are the implications for
economics and economists? First, we should
endeavor to make explicit the differences in
values, and seek ways to resolve them. Value
differences can take many different forms.
Economists are most familiar with the distinction
between efficiency and distributional issues,
especially greater equality of income
versus greater total income.40 But comprehensive
changes in health policy can have other
important distributional effects. Even for individuals
at the same income level, the costs
and the benefits of care could change along
many dimensions: rural areas versus central
cities, the elderly versus the young, smokers
versus nonsmokers, savers versus nonsavers,
men versus women, and so on. Health economists
who are unanimous in approving gains
in efficiency might have very different views


### ---Economics-1996-0-17.txt---
regarding the desirability of the distributional
changes and might also differ in the weights
they give to the changes in efficiency versus
the distributional consequences.
Second, greater openness about value differences
should force economists to make explicit
the positive questions that are embedded
in most policy-value questions. This would
point the way to productive research. If the
embedded questions are identified and studied,
it should be possible to reduce the uncertainty
about them and thus provide a basis for narrowing
differences on policy-value questions.
A third agenda item for economists is to undertake
research on the formation of values,
especially insofar as they are the consequences
of policy. Economists are understandably reluctant
to prescribe values or to make normative
judgments about them. But when
economic policies affect values and preferences,
and these in turn affect behavior, it is
incumbent on economists to analyze the links
between policies and values, and to examine
the economic and social consequences of alternative
value systems. I believe there is an
analogy between the economics of values and
the economics of technology. Over the past
several decades some economists have begun
to treat technology as at least partly endogenous.
4" Now, a similar effort must be undertaken
for values (Henry J. Aaron, 1994;
Becker, 1996; Albert 0. Hirschman, 1986;
Assar Lindbeck, 1994).

Finally, economists must develop more selfawareness
of how our values color our judgment
about policy, and more candor in making
clear to others the respective roles of positive
research and of values in our policy recommendations.
Alice M. Rivlin, in her AEA presidential
address in December 1986, warned
economists against letting "their ideological
position cloud their judgment about the likely
effects of particular policies" (p. 4). She
urged us "... to be more careful to sort out, for
ourselves and others, what we really know
from our ideological biases" (p. 9). In my
view, there is a vast difference between a researcher
and a reformer, between an analyst
and a player in the policy arena. They are all
socially valuable occupations, and the same
individual may successfully wear different
hats at different times. What is not likely to
work well, either for economics or for policy,
is trying to wear two hats at the same time.
In the remainder of this paper I present a
summary of my policy recommendations for
health system reform. The use of the bully pulpit
by an AEA president to push personal policy
choices has ample precedent, but I also
want to use this opportunity to show how those
choices are shaped by the interaction between
my values and my understanding of health
economics. Finally, I identify aspects of my
policy recommendations that are problematic
and which would clearly benefit from additional
research.

My three major recommendations are:
(i) a broad-based tax earmarked for health
care to provide every American with a
voucher for participation in a basic plan;
(ii) provision of care through integrated health
systems that include hospitals, physician
services, and prescription drugs. These
systems would be led by physicians, would
be reimbursed by capitation plus modest
co-payment from patients at the time of
use, and would be required to offer a wide
variety of point-of-service options to be
paid for by patients with after-tax dollars;
(iii) a large private center for technology assessment
financed by a small industrywide
levy on all health care spending.
My desire to see all Americans insured for
a basic health plan is clearly driven in part by
values. Although medical care is often not a
crucial factor in health outcomes, it is nearly
always a source of utility through its caring
and validation functions. In my judgment, it
fully meets Adam Smith's 1776 definition of
a necessary: "By necessaries I understand not
only the commodities which are indispensably
necessary for the support of life but whatever
the custom of the country renders it indecent
for creditable people, even of the lowest order,
to be without" (1776; republished 1937 p.
821). To achieve universal coverage there
must be subsidization for those who are too
4' For example, Kenneth Arrow, Zvi Griliches, Ed
Mansfield, Richard Nelson, Nathan Rosenberg, and Jacob
Schmookler.


### ---Economics-1996-0-18.txt---
poor or too sick to acquire insurance, and there
must be compulsion for the "free riders" 42 to
pay their share.

There are only two ways to achieve systematic
universal coverage: a broad-based general
tax with implicit subsidies for the poor and the
sick, or a system of mandates with explicit
subsidies based on income. I prefer the former
because the latter are extremely expensive to
administer and seriously distort incentives;
they result in the near-poor facing marginal tax
rates that would be regarded as confiscatory if
levied on the affluent.43

Both theory and experience show that integrated
health care systems are usually the best
way to deliver cost-effective care. The primary
reason is the physician's central role in medical
decision-making. Under any approach to
care, it is the physician who admits patients to
hospitals, orders tests and other procedures,
and decides when to discharge. It is the physician
who prescribes drugs and who refers
patients to other physicians for consultation
and treatment. Thus physicians' decisions
are the major determinant of the cost of care.
Only in an integrated system, however, do
physicians have the incentive, the information,
and the infrastructure needed to make
these decisions in a cost-effective way. Integrated
systems also have an advantage in
avoiding excess capacity of high-cost equipment
and personnel.

Given the central importance of physicians
to medical care, I believe the integrated systems
should be led by them and other health
care professionals. At a minimum, health care
professionals should have a prominent place
in the govermance of the systems. One of the
greatest errors of health policy-makers today
is their assumption that market competition or
government regulation are the only instruments
available to control health care. There
is room for, indeed need for, a revitalization
of professional norms as a third instrument of
control.' The patient-physician relationship
often is highly personal and intimate, similar
in many ways to relationships within families
or between teachers and pupils or ministers
and congregants. This relationship is, in part,
what economist Kenneth Boulding (1968)
called an integrative system, one that depends
on mutual recognition and acceptance of rights
and responsibilities, enforced by traditional
norms as well as market pressures and government
regulations. As long as physicians control
the use of complex technology in life and
death situations, and as long as we expect them
to perform priestly functions, they must be endowed
with certain privileges and held to certain
standards of behavior different from those
assumed by models of market competition or
government regulation.45

Comprehensive government control of medical
care has not worked well in any setting.
The essence of good care is an informed patient
working cooperatively with a health professional
who provides personalized attention
and concern. The rules, regulations, and bureaucratic
controls that almost always accompany
governmental activities are inimical to
high-quality cost-effective care. It is revealing
that countries such as England and Sweden
with deep government involvement in the
financing of medical care have bent over backwards
to leave physicians with a great deal of
professional autonomy-indeed more autonomy
than is possessed by many American physicians
working in a "private" system.
Market competition also has its problems. It
assumes a preoccupation with the bottom line
and governance by a corporate mentality that
judges the success of each division by its profit
growth. Physician-led systems will also have


### ---Economics-1996-0-19.txt---
to pay attention to costs, and physicians will
also be interested in making a good income,
but there is a vast difference between a profitmaximizing
corporation and physicians who
strive to balance their obligations to patients,
the organization, and themselves.46
Reimbursement of these integrated systems
should be primarily by capitation, adjusted
for patient characteristics. In addition, patients
should be required to make modest copayments
at the time of use (e.g., $15 for each
visit and $5 for each prescription). Such payments
will generate some income but, more
important, will help to discourage wasteful use
of health care. The payments could be waived
for patients living below the poverty level,
and for essential preventive services such as
vaccination.

The earmarked tax would provide every
American with a voucher for a basic health
care plan. Each integrated system would be
required to offer the basic plan, plus a variety
of options. These options are not alternative
insurance plans; they are services to be paid
for at time of use with after-tax dollars.47 The
options could take many forms: a private room
in the hospital; a wider choice of physicians
and hospitals than is available through the basic
plan; or access to new experimental technologies
or older technologies not included in
the basic plan because they have a low benefitto-
cost ratio.48

These options would accommodate the demands
of patients with higher incomes or
those who choose to spend more of their income
on medical care. The options would not
constitute establishment of different plans. Everyone
would be in the same plan and most
persons would stick to the basic plan most of
the time. An option would be exercised only
when the patient desired and was willing to
pay for it. This is the quintessential American
approach to balancing equality and freedom.
On the one hand, this approach avoids the
egalitarianism of the English and Canadian
systems in which only a small elite have an
escape valve. On the other hand, it does not
create a separate plan for the poor while the
great majority of Americans obtain care from
a different system. The experience with Medicaid
shows that a separate system limited to
the poor is not likely to function well.
Where feasible, the integrated health care
system would engage in managed competition.
49 Having advocated policies similar to
such an approach to health care for more than
20 years (Fuchs, 1974a), I am not unmindful
of its virtues. We cannot, however, rely on
managed competition alone to contain costs.
In most rural areas, population density is too
low to support several health care systems.
Even in some urban areas, competition is impossible
or undesirable because of economies
of scale. For instance, only one hospital is
needed to serve a population of 100,000 efficiently.
Similar constraints apply to competition
in physician specialty care, especially if
the physicians work full time at their specialties.
A population of 1 million would probably
not justify enough independent maternity services
or open-heart surgery teams to create
competitive conditions. Moreover, the public
interest is not best served by insisting that
health professionals always maintain rigorous
arm' s-length competition with one another.
Patients can benefit from cooperation among
physicians and hospitals, both in reduced costs
and better service. Managed competition alone
will not be enough to contain costs; it must be
46 The effects on television network news departments
of the subordination of professional norms to the pursuit
of profits shows what could happen in medical care.
4 Readers whose values lead them to prefer a more
egalitarian system might consider how individuals now
have options to use their income to live in safer neighborhoods,
drive safer cars, avoid unhealthy occupations,
and make other choices that have larger and more predictable
effects on health than the options available in my
recommendation for health care.
48 Many advances in medicine do not spring full-blown
from the test tube. They require long periods of development
through trial and error and incremental improvements.
In my judgment it is desirable to have a system in
which technologic opportunities can be explored on a reasonably
large scale with the cQsts borne by those patients
who are most willing and able to pay for a chance at unproven
benefits. Government- or industry-financed randomized
clinical trials with small samples of selected
patients treated in selected environments are not always a
satisfactory substitute for larger scale efforts to establish
the effectiveness, and especially the cost-effectiveness of
a medical technology.

4 See Alain Enthoven (1986, 1988).


### ---Economics-1996-0-20.txt---
supplemented by constraints on the supply
side, especially with respect to technology and
the specialty mix of physicians.
In 1995 Americans spent about $1 trillion
for health care, broadly defined. If, during the
past 30 years, health care spending had grown
at the rate of the rest of the economy, the
health care bill in 1995 would have been only
a little more than $400 billion. What accounts
for this extraordinary excess of almost $600
billion in annual spending? There has been a
small increase in physician visits per capita,
but use of acute care hospitals has decreased
sharply. Patient-days per 1000 population are
less than three fifths the level of 30 years ago.
By far the most important factor accounting
for the increase in health care's share of the
GDP is the change in technology.50 Physician
visits and hospital-days cost more than they
used to because the content has changed-the
technologies used for diagnosis and treatment
are more expensive than in the past. Much of
this technological change is welcome; it contributes
to enhancing the length and quality of
life. Some of the change is less desirable because
it adds more to cost than to patient benefit.
Unfortunately, there is great uncertainty
regarding the merits of many technologies.
Moreover, even when the advantages and disadvantages
are known, there are often significant
barriers facing physicians who would
like to practice in a cost-effective manner.
To deal with this problem, I propose the creation
of a large, private center for technology
assessment. Financing for this center would
come from a small levy (less than one tenth of
1 percent) on all health care spending. A centralized
approach is necessary, because health
care is highly fragmented. Individual physicians
and health plans lack the incentive and
ability to commit the resources needed to assess
-new technologies. Even the largest insurance
companies individually account for only
a small percentage of the health care market;
they are, therefore, understandably reluctant to
pay for large-scale assessments that would
benefit all.51 Government agencies try to fill
the void, but the scale of effort is too small,
and a private center would be able to avoid the
political interference that often intrudes on
government-run agencies.52 Health care providers
would fund and set the agenda for the
center, much as the electric power companies
do for the Electric Power Research Institute.
This institute is financed by a small levy on
every public utility bill.

A health care technology assessment center
would have two primary functions. First, it
would help to develop and disseminate systematic
knowledge about the cost-effectiveness of
medical technology through support of research
and through a comprehensive program of
publications and conferences. The center would
have some intramural research capability, but
most of the research would be conducted extramurally
at medical schools, hospitals, and research
institutes throughout the country. It would
provide health professionals with essential information
to evaluate and improve their clinical
practices and offer a rational basis for deciding
what services should be included in the basic
plan.

The second important function would be to
provide legitimacy for the cost-effective practice
of medicine. Currently, many directors of
health plans and many individual physicians
know they could be practicing in a more costeffective
way, but they are inhibited from doing
so because they do not practice in a
vacuum. Physicians are influenced by peers
who have been trained in settings that emphasized
the use of the latest technologies regardless
of cost. Patients come with particular sets
of expectations based on what they read or
hear in the media and what their relatives and
friends tell them has been their experience.
The threat of malpractice suits lurks in the
background. A major function of the center


### ---Economics-1996-0-21.txt---
would be to give legitimacy and a stamp of
authority to physicians who practice in a more
cost-effective way.

My policy recommendations seek to achieve
a balance among the diverse values of efficiency,
justice, freedom, and security. The link
between the earmarked tax and the basic plan
would create a healthy tension between the desire
to increase benefits and the need to pay
for the increase in a responsible and equitable
manner. Competition among health care systems
in highly populated areas would widen
choice and foster cost-effective practice. The
private technology assessment center would
help to contain costs without the imposition of
controls or caps that might stifle innovation
and progress.

Are these recommendations politically saleable?
In the short run, certainly not. But neither
are any other proposals for comprehensive
reform. Indeed, for more than 20 years it has
been my view that the United States would not
enact comprehensive health care reform except
in the wake of a major war, a depression,
large-scale civil unrest, or some other event
that completely changed the political climate.
Why is the United States the only major
industrialized nation without national health
insurance? Many observers focus on the opposition
of "special interests," and that certainly
is a factor, but I do not find it a
completely satisfactory explanation. After all,
special interests are not unknown in Sweden,
England, Canada, and other countries that do
have national health insurance.
In 1976 I suggested four reasons for its absence
in the United States: distrust of government,
heterogeneity of the population, a weak
sense of noblesse oblige, and strong private
voluntary organizations such as nonprofit hospitals
and Blue Cross and Blue Shield plans
that carry out quasi-governmental functions
with respect to the financing and delivery of
health care (Fuchs, 1976). Upon revisiting
this question (Fuchs, 1991), I concluded that
the first three reasons were stronger than ever,
but the fourth had weakened considerably. It
is ironic that "the competition revolution"
(Fuchs, 1988b), which erodes the ability of
not-for-profit health care institutions to provide
a modicum of social insurance through
community rating and cost shifting, may in the
long run push the country toward national
health insurance.

My plan is certainly not a panacea; it would
be difficult to implement and others might
seek a different balance of values. Several aspects
require additional research. For example,
what should be the content of the basic plans?
How should the content change over time?
How should the plans be reimbursed from the
funds raised by the earmarked tax, and especially
how should reimbursement be risk adjusted
to take account of differences in plan
populations? Another problem is how to encourage
competition among plans where it is
feasible, while recognizing that a competitive
approach will not be desirable or possible in
areas of low population density. Considerable
research is needed on how the out-of-plan options
should be priced53 and how the providers
of such care should be reimbursed. Finally,
much thought should be given to how to reinvigorate
professional norms as a third instrument
of control, along with market competition
and government regulation.54

I conclude this tour of health economicspast,
present, and future-on a mildly optimistic
note. In the past three decades economics
has made a positive contribution to health
and medical care, and I believe that future contributions
will be even greater. Now that the
basic ideas of economics are gaining acceptance,
it will be more important than ever for
economists to master many of the intricacies
of health care institutions and technologies.
We will also have to consider the problems of
dissemination in order to insure that when we
agree on research results, these results are understood
and accepted by all relevant audiences
including the media, politicians, and
health professionals. Moreover, we must pay
more attention to values than we have in the
past. Through skillful analysis of the interactions
between values and the conclusions of
positive research, we will be able to contribute
more effectively to public policy debates. And,
13 For an interesting discussion of the "topping off"
problem, see Robert H. Frank (1996).
5 This would undoubtedly require research to uncover
the reasons for the erosion of professional control. See, for
example, Steven Brint (1994).


### ---Economics-1996-0-22.txt---
if health economists are successful in this demanding
assignment, we can lead the way toward
progress in areas such as child care and
education that face similar problems of reconciling
multiple goals and heterogeneity in
values. To be useful to our society while deriving
pleasure from our work-in the words
of the old Gershwin tune, "Who could ask for
anything more?"
 ## Economics-1997-0


### ---Economics-1997-0-02.txt---
The improvement in living standards, life
expectancy, and economic growth prospects in
developing countries ranks among the most
important economic success stories since the
Second World War. Growth in some has been
dramatic, and while progress has been far from
uniform, there are grounds for optimism that
future growth prospects can be even better
than performance to date.

One factor accounting for that success has
been improved understanding and adoption of
economic policies much more conducive to satisfactory
economic growth than was the case in
the 1950's and 1960's. That better understanding,
in turn, resulted from a combination and
interaction of research and experience with development
and development policy.

Ideas with regard to trade policy and economic
development are among those that have
changed radically. Then and now, it was recognized
that trade policy was central to the overall
design of policies for economic development.
But in the early days, there was a broad consensus
that trade policy for development should
be based on "import substitution." By this
was meant that domestic production of importcompeting
goods should be started and increased
to satisfy the domestic market under incentives
provided through whatever level of protection
against imports, or even import prohibition, was
necessary to achieve it. It was thought that import
substitution in manufactures would be synonymous
with industrialization, which in turn
was seen as the key to development.
The contrast with views today is striking. It
is now widely accepted that growth prospects
for developing countries are greatly enhanced
through an outer-oriented trade regime and
fairly uniform incentives (primarily through
the exchange rate) for production across exporting
and import-competing goods.' Some
countries have achieved high rates of growth
with outer-oriented trade strategies. Policy reform
efforts removing protection and shifting
to an outer-oriented trade strategy are under
way in a number of countries. It is generally
believed that import substitution at a minimum
outlived its usefulness and that liberalization
of trade and payments is crucial for both industrialization
and economic development.

While other policy changes also are necessary,
changing trade policy is among the essential
ingredients if there is to be hope for improved
economic performance.

And, while there are still some disagreements
over particular aspects of trade policy
both among academic researchers and policy
makers,2 the current consensus represents a
distinct advance over the old one, in terms
both of knowledge and of the prospects it
offers for rapid economic growth. While it
will no doubt be further refined in light of


### ---Economics-1997-0-03.txt---
experience, a changing world economy, and
research, there is no question of "going back"
to the earlier thinking and understanding of the
process.

A number of interesting questions arise
about this change in thought and policy.
How could it happen that a profession, for
which the principle of comparative advantage
was one of its key tenets, embraced such
protectionist policies? What was the contribution
of economic research to the sea
change in thinking, policy prescriptions, and
politicians' acceptance of the need for policy
reform? What sorts of economic research
best informed the policy process? In a nutshell,
how did we learn? And what was the
contribution of economists and their research
to the process?

Attempting to answer these questions is
the subject of this lecture. Even with a focus
limited to trade and development, analysis of
the role of research and its usefulness is at
least somewhat conjectural. The issue, however,
of what types of research inform good
policy is an important one. I suspect that the
tentative conclusions I draw here may be relevant
for other areas of research-informing
policy, but leave that to others to demonstrate
or refute.3

In what follows, I first sketch the initial approach
to trade policy in early development
research and thought. Next, consideration is
given to the evolution of thought, research,
and experience with respect to trade and development
over the next several decades, and
to the "conventional wisdom" of the 1990's.
Thereafter, I consider the role of research and
the sorts of research that proved most fruitful
in guiding policy and changing the consensus.
Before proceeding, two caveats are necessary.
First, it is very difficult to disentangle
views of the proper role for trade policy in
development from views about the appropriate
role for the state. Partly as a legacy of the
Great Depression, partly because of the belief
that the Soviet Union had succeeded in its developmental
and industrial aspirations through
central planning, and partly because of the perceived
success of wartime controls, there was
widespread agreement- in developed and developing
countries alike-that the state should
play a major role in economic activity, not
only in affecting aggregate demand, but also
in regulating private markets and indeed augmenting
or supplanting them with state-owned
enterprise production of manufactured and
other goods. Quite clearly, early views about
the necessity for a leading role for the state in
guiding resource allocation were incompatible
with an open trade policy or outer-oriented
trade strategy. Yet to attempt to consider the
evolution of both views is well beyond the
scope of this paper, and focus here is confined
to trade policy.

Second, to focus on research that influenced
thinking about economic policy is not to denigrate
the importance of research that does not
appear to have had immediate policy relevance.
First of all, basic research often informs more
applied research. Second, in some cases of research
that provided little of lasting value, that
outcome could not be known at the time. Perhaps
some of that research served to demonstrate
the infeasibility of certain policy paths,4 or to
demonstrate the futility of further explorations.
Nonetheless, ex post it is clear that some
lines of research served to hasten the day
when policy makers would accept the desirability
of removing high walls of protection,
while others were irrelevant or served largely
to reinforce prejudices and perpetuate the "old
wisdom." Perhaps that is inevitable in the
"marketplace of ideas" as new paradigms are
brought forth to replace old ones.


### ---Economics-1997-0-04.txt---
I. Evolution of Theory, Understanding,
and Policy

A. The Early Years

As developing countries gained independence
from their former colonial rulers, 5 their
leaders had a political mandate to achieve
higher living standards and rapid economic
growth.6 It is difficult in the 1990's to recall
the extent to which it was then plausible to
view the world economy as split into the industrialized
countries and the underdeveloped
countries, or "first world" and "third world,"
as they were often called. Underdeveloped
countries had markedly lower average educational
attainments (including a great deal of
illiteracy and a high fraction of the population
with no schooling), poor health conditions,
and very little infrastructure. They were heavily
specialized in the production and export of
primary commodities and imported most of
their manufactured goods. While differences
among the underdeveloped countries were acknowledged,
these seemed minor contrasted

with the overwhelming realities of their common
attributes and widespread poverty.
The new field of development economics
was regarded by many as covering underdevelopment
because "conventional economics"
did not apply (see Albert Hirschman, 1982).
Focus on how the developing countries should
shape policies for accelerating growth and
raising living standards was the central issue.7
B. Accepted Stylized Facts and Premises
Early trade and development theories and
policy prescriptions were based on some
widely accepted stylized facts and premises
about the underdeveloped countries. These
were a mixture of touristic impressions, halftruths,
and misapplied policy inferences. In
hindsight, it is surprising how some thenaccepted
stylized "facts" were so uncritically
accepted and held sway for so long. However,
it is not possible to understand what thinking
about trade and development was except in
light of those premises. Indeed, it can be argued
that improved understanding of trade and
development came about in large part through
research which effectively demonstrated the
falsity of these premises.

A first premise was based on the fact-then
certainly true-that developing economies'
production structures were heavily oriented toward
primary commodity production. The dependence
on foreign trade was believed to be
extreme, as there was virtually no production
capacity for manufactured goods outside a few
light mass-consumed commodities. However,
many observers went further and attributed the
low living standards in developing countries
to dependence on primary commodity production
and export.

A second "fact," or premise, was that if
developing countries adopted policies of free
trade, their comparative advantage would forever
lie in primary commodity production. It
followed that industrialization and, hence, development
would not take place if free trade
policies were adopted.

A third premise-termed "export pessimism"
-was that both the global income and
price elasticities of demand for primary commodities
were low. Consequently, it was anticipated
that export earnings would not grow
very rapidly, if at all.8

'Latin America and a few other countries (including
China, Thailand, and Turkey), then deemed "underdeveloped,
" were not formally colonies prior to the Second
World War. However, it was widely believed that they had
been "economically dependent." The leaders and elite in
most poor countries shared the perception that their economies
were "different" from industrialized countries and
like other developing countries. The G-77 (77 countries),
or nonaligned nations, were all developing countries
whose leaders perceived themselves to be in a similar economic
situation with similar goals of rapid growth and
improved living standards.

6 As was then conventional, I shall assume here that
higher living standards, more rapid growth, and economic
development were/are synonymous for purposes of analyzing
trade policy.

7 There was, to be sure, a growing literature on the contribution
of technical change and factor accumulation to
growth in the industrialized countries. But most development
economists saw that research as irrelevant for developing
countries.

8 Another widely held view, closely related to export
pessimism, was the proposition that the terms of trade had
inexorably deteriorated against primary commodities and
would continue to do so. Investigation of this proposition


### ---Economics-1997-0-05.txt---
A fourth premise was that the labor force
in developing countries, predominantly engaged
in agricultural activities as it was, had
a marginal product of labor that was "negligible,
zero, or even negative," to quote W.
Arthur Lewis (1954 p. 141). The stylized
"fact" that there was "surplus" labor, or
disguised unemployment in less developed
countries (LDCs) was widely accepted.9 In
many analytical formulations, it was explicitly
or implicitly assumed that labor was a
free good while capital was the scarce factor
of production.'0

Related to the fourth premise was a fifth
premise: that capital accumulation was crucial
for growth, and in early stages of development
it could occur only with the importation of
capital goods. Since it was expected that the
demand for capital goods imports, and imports
of other products used in the production
process, would grow rapidly while foreign
exchange earnings would not, it appeared that
growth could follow only if domestic production
of import-competing goods could expand
rapidly.

Yet a sixth widely accepted premise was
that there was very little response to price
incentives in developing countries: peasants
were "traditional" in their behavior, and
there were "structural" problems within the
economy." '

Based on these stylized facts and premises, it
was a straightforward step to believe that the
process of development was that of industrialization,
by which was essentially meant the
accumulation of capital for investment in manufacturing
industry and related infrastructure.
Moreover, since most manufactured goods were
imported, it seemed to follow logically that, as
stated by Chenery (1958 p. 463) among many
others: "Industrialization consists primarily in
the substitution of domestic production of manufactured
goods for imports."

C. Initial Policies

Policy prescriptions were derived from
these propositions, or stylized facts. Since it
was thought that industrialization was necessary
for development and that free trade would
leave underdeveloped countries specialized in
primary commodity production, it followed
that there had to be investment in new manufacturing
industries whose output would substitute
for imports. Further, it was widely
believed that new industries in poor countries
could not possibly compete with their established
counterparts in the developed world.
Therefore, industry would have to be protected
during its initial phase. Import-substitution
policies therefore became the hallmark of development
strategies for manufacturing and
the underlying rationale for trade policy."2
The case for import substitution was based
both on the premises outlined above and also
on received doctrine: the infant industry argument.
The notion that dynamic considerations
and externalities might imply that an
industry, although economic, would not be established
by private agents had been accepted
by economists as a legitimate exception to the
case for free trade since Hamilton and List."3
tended to demonstrate that at the very least the deterioration
had been much less than was believed. John Spraos
(1980) provided a classic review of the evidence.
9 A modem interpretation would be that there are many
people in developing countries with very low marginal
products of labor. While they are too poor to remain unemployed,
the process of development entails equipping
people with the capabilities (partly through education)
and opportunities to increase their productivity.
' To be sure, all analysts recognized the importance of
increased provision of education and health services. But
for purposes of analyzing trade policy, emphasis was almost
exclusively on investment.

" This gave rise to a great deal of literature based on
"structuralism." According to some, it was the absence
of responsiveness to price that made developing countries
"different." Structuralism was also used as an argument
that inflation was necessary in order to achieve growth.
See Hollis B. Chenery ( 1975) for a fuller description.
2 There were many important subthemes that are not
elaborated here, since they are not essential to the main
argument. It should, however, be noted that there were
many who believed that the situation of developing countries
was "structural" and that marginal changes would
not matter. It was then concluded that a "big push" was
needed, with many new investments simultaneously generating
additional demand and then becoming profitable.
Ragnar Nurkse's ( 1958) "balanced growth" prescription
reflected the same viewpoint.

'" See Robert E. Baldwin's (1969) classic analysis of
the argument, which not only sets up the conditions under
which there might be an infant industry, but also carefully


### ---Economics-1997-0-06.txt---
It was stipulated that a low-cost producer or
producers were already in operation abroad;
then, the argument proceeded, a potential entrant
in a developing country would be faced
with an initial period of high costs, but could
in the longer run compete. However, in the
presence of dynamic externalities (presumably
internal to the industry), it was believed
that no individual producer would find it profitable
to start production. In these circumstances,
the infant industry argument could
justify temporary intervention to make entry
into the new industry privately profitable provided
that, over the longer term, its costs
would decline below the imported cost by
enough to yield an economic return on the intervening
loss, which could be viewed as an
investment.

Although the infant industry argument was,
in a first-best world, an argument for a production
subsidy (which would presumably
equal the unit value of the externality and
might apply as well for production for exports
as for the domestic market), it was combined
with the appeal for import substitution14 to
yield a justification for protection of newly established
manufacturing industries in developing
countries.

However, combining the assumptions that
industrialization would have to take place
through substituting for imports, that there
were infant industries requiring initial intervention,
and that export earnings were unlikely
to increase, the stage was set for trade
and industrialization policies.
The premises underlying import-substitution
policies were so widely accepted that developing
country exceptions were even incorporated
into the General Agreement on Tariffs
and Trade (GATT) articles. Article XVIII
explicitly protected the developing countries
from the "obligations" of industrialized countries
and permitted them to adopt tariffs and
quantitative restrictions. They also were entitled
to "special and differential treatment" in
other regards under GATT. That the GATT,
the upholder of an open international trading
system, would accept an "exception" for
developing countries shows how deeply entrenched
the views supporting import substitution
were. It is arguable that the very
existence of this exception not only legitimized
developing countries' inner-oriented
trade policies, but also removed pressures
that might otherwise have been brought to
bear earlier for them to adopt trade and payments
regimes more conducive to economic
growth. "

D. Resulting Evolution of Policies
In one way or another, provision was made
in country after country that, once domestic
production became feasible, imports would be
restricted. In Brazil, a "Law of Similars" provided
that firms importing goods that were
similar to those available domestically would
lose their government privileges, which included
not only access to credit and tax treatment,
but also eligibility to bid on government
contracts and a variety of other valuable rights.
In India, imports were licensed, and in the
event that there was domestic production, any
would-be importer was required to obtain letters
from any supplier government officials
thought might be capable of producing the
good to the effect that the supplier could not
meet the specifications. In Turkey, goods were
removed from the list of items for which import
licenses could be granted once domestic
production capacity was available. Similar
provisions, or very high tariffs, were used to
encourage import substitution in most developing
countries."


### ---Economics-1997-0-07.txt---
In some countries and industries, the trade
regime was used as the key policy instrument
to provide incentives for import-substituting
investment and production by private firms. In
other circumstances, state-owned enterprises
were established, and investments were made
directly by the state sector in new manufacturing
activities. In that case, the trade regime
provided protection to the state-owned enterprises,
although their budget constraints were,
in any event, very soft. None of these policies,
as adopted, provided means of identifying
where dynamic externalities were largest, nor
was there any provision for reduction of protection
after an initial period. Indeed, protection
was virtually automatic for any new
import-substitution industry.

A final aspect of early policies also contributed
to high and indiscriminate levels of protection.
That is, as countries embarked on
ambitious development plans, inflation rates
rose to levels significantly above those in industrial
countries (although far below inflation
rates prevailing in many developing countries
today). Demand for foreign exchange was rising
rapidly in response to the development
plans, rising incomes, and domestic inflation.
Nonetheless, policy makers in most developing
countries chose to maintain their fixed nominal
exchange rates. In part, this reflected the perception,
noted above, that there was little response
to prices and that, indeed, maintaining
the nominal exchange rate "taxed" agriculture
while simultaneously subsidizing capital goods
imports. In part, exchange rates were held fixed
because it was believed that so doing made imports
of capital goods cheaper and thus increased
investment. The net result was, of
course, real appreciation of the exchange rate,
which further intensified ex ante payments
imbalances, reduced foreign exchange availability,
and induced greater restrictiveness in
import licensing.

It will be recalled that the 1950's and
1960's were a time of unprecedented economic
growth for the industrial countries and
for world trade. Buoyed in part by international
markets, and in part by the stimuli of
increased investment and other aspects of development
programs, the rates of growth of per
capita incomes rose markedly relative to historical
levels in most developing countries,
although they remained below those in industrial
countries with few exceptions. Even the
growth of industry itself was fairly rapid, as
the "easy" import-substitution opportunities
were by and large undertaken first.'7
However, with real exchange rate appreciation
and the pull of resources into newly
profitable, import-competing industries, the
growth of foreign exchange earnings inevitably
slowed. It is not widely appreciated that
developing countries, which had a 44 percent
share of world exports of agricultural commodities
in 1955, lost share to the point where
they had only 31 percent by 1970.18
With acceleration in the growth of demand
for foreign exchange, and deceleration in the
growth of supply, foreign exchange difficulties
were inevitable. The export pessimism premise
had been self-fulfilling, given the policies
that were followed. The drop in primary commodity
prices in the early 1950's accentuated
the phenomenon, but affected the timing more
than the actuality of the result. The initial
response by most policy makers was to impose
rationing of scarce foreign exchange
(and require the surrender of foreign exchange
from exports) on imports, and the resulting
system had little to do with encouraging infant
industries.

Although initial rationing of imports was
usually on a relatively uniform and across-theboard
procedure, controls over foreign trade
generally became more restrictive and complex
over the next two decades, both in
'7 See Raul Prebisch ( 1984) for the argument. It can be
argued that, with uniform incentives, import substitution
would have taken place first in those industries with least
comparative disadvantage. In fact, the use of import licensing
and prohibitions meant that rates of protection
were not uniform even across import-competing activities.
In addition, monopoly power in the domestic market was
conferred to domestic producers, so that profitability
hinged more on the price elasticity of the demand curve
than on producers' abilities to reduce costs and compete
with imports.

8 Agricultural protection in Japan, Europe, and the
United States may have contributed somewhat to this result.
But in most developing countries, the demand for
food was growing more rapidly than the supply (as producer
prices were suppressed relative to the prices of industrial
goods) and, thus, the supply (demand) curve for
exports (imports) of agricultural commodities was shifting
down (up) (see Krueger, 1990 p. 95).


### ---Economics-1997-0-08.txt---
response to growing "foreign exchange shortage,
" in reaction to the "unfairness" of the
undifferentiated controls, and in response to
evasion of the regimes."9 Periodic balance of
payments crises arose in reaction to overvaluation
of the real exchange rate, increased indebtedness,
and the failure of export earnings
to grow.

International Monetary Fund (IMF) "stabilization"
programs were undertaken, under
which import regimes were simplified and rationalized
(as import licensing was, in those
years, not abolished). The nominal exchange
rate was normally altered (but usually to a
new fixed exchange rate in the face of continuing
inflation).20 Even in IMF programs,
however, it was seldom intended that the
underlying trade policies related to import
substitution be changed: the intent, rather,
was to rationalize the trade regime and find
ways to induce more foreign exchange earnings
to finance the capital goods that would
be imported to undertake additional importsubstitution
investments. Growth proceeded

in "stop-go" fashion, as periods of foreign
exchange crisis were followed by tight(er)
monetary and fiscal policies, a consequent reduction
in excess demand for imports, and an
increase in foreign exchange earnings. When
the trade regime was again relaxed, growth
resumed and the demand for imports again
mushroomed until the next crisis.21
E. Research Directions and Contributions
Most research in the 1950's and 1960's was
based on the premises outlined above, and supported
the basic thrusts of policy. It needs only
brief mention here. Some focussed on the possible
existence of externalities and the need for
"balanced growth," as it was assumed that
expansion of any one industry alone would not
be feasible because of the limited size of the
market.22 This prescription, of course, was
based on the premise that development of
manufactured exports was not feasible. Another
line of supportive research focussed on
planning models, concentrating in large part
on interindustry flows and linkages.23 Empirical
research on pattems of development began,
focussing on the structure of economies
and their growth performance. For more than
a decade, the growing disparity between theory
and practice was all but ignored.
There was also research providing a rationale
for protection of new industries and import
substitution. These results demonstrated that
domestic distortions could warrant trade intervention
24 in a number of situations. Everett E.
Hagen (1958), in perhaps the best known of
these, set up a model assuming that urban
wages exceeded rural wages exogenously, and
demonstrated that a tariff could improve welfare
by inducing resources into the (artificially)
higher-cost urban industries.

Work also continued on structuralist models,
as a number of authors found reasons
why developing countries' economic structures
were "different" and why, therefore, the
usual economic analysis would not apply.25
Chenery and Michael Bruno (1962), Chenery
and Alan Strout (1966), and Chenery and
many other coauthors developed the "twogap"
model, using the stylized fact that
foreign exchange was "scarce" in developing
countries. In this model, export earnings
were exogenously given and growing more
slowly than the demand for foreign exchange.


### ---Economics-1997-0-09.txt---
Investment was limited by the more binding
of two linear constraints: the available savings
and the available foreign exchange. There
were thus two "gaps" -between savings and
investment, and between demand for, and
supply of, foreign exchange. Growth was
constrained either by savings or by foreign
exchange availability, and the model demonstrated
the high potential productivity of foreign
aid (in providing foreign exchange),
enabling otherwise redundant domestic savings
to be used in capital formation. The
model, reflecting the views of the day, had little
role for the price mechanism.26
An example of an analytical effort to clarify
circumstances under which one of the stylized
facts could be realized was Bhagwati's ( 1958)
and Harry G. Johnson's (1967) demonstration
of the possibility of "immiserizing growth,"
under which a country might increase its output,
only to find the price of exports falling so
much that the country was worse off. As
Bhagwati showed, the conditions under which
that might happen were fairly extreme.
An important development was the theory
of shadow pricing, which was an offshoot of
programming and planning models. It was initially
used to demonstrate how reliance on
market prices might yield an inappropriate resource
allocation. Quickly, however, analysts
pointed to the distortions between domestic
prices of import-competing and exportable
goods because of the trade regime. There is
little doubt that cost-benefit techniques improved
project selection and enabled improved
governmental decision-making with, inter alia,
the insistence on use of border prices. The
publication of the I. M. D. Little and James A.
Mirrlees (1969) volume marked a milestone,
after which there was almost no question about
the appropriateness of using border prices in
project evaluation.

In a related and important development, the
theory of effective protection was developed
by Johnson (1965a), W. M. Corden (1966),
Bela Balassa (1965), and others, providing a
framework for analyzing the protection accorded
to industries engaged in light processing
and much higher value-added activities on
a comparable basis. The notion of domestic
resource costs (Bruno, 1965; Krueger, 1966),
showing the uneven allocation of resources to
earning and saving a unit of foreign exchange
across activities, was developed to meet the
argument that market prices failed to reflect
opportunity cost. This research provided a tool
with which economists could measure the
wide disparities in protection accorded to different
import-competing industries.

Recognizing that these estimates were based
in part on partial equilibrium analysis,27 a
number of researchers began work on developing
techniques for computing general equilibrium
results. Based on newly developed
solution algorithms, techniques were developed
for models which endogenized prices,
and thus moved away from the linear models
earlier used for analysis.28

By the late 1960's and 1970's, there were significant
contributions which undermined some
of the premises on which import-substitution
strategies were based. At an analytical level,
one line of research focussed on whether the
stylized facts of "market failure" in fact warranted
the imposition of trade restrictions.
Bhagwati and V. K. Ramaswami (1963),
Johnson (1965b), Bhagwati (1969), and others
demonstrated that a trade instrument (tariff
or quota) was usually not a first-best, nor often
even second-best, instrument for achieving the
objectives in the name of which protection had
been granted. The equivalence of tariffs and
quotas, an old result in international economics,
was revised and refined, as quotas became
more frequently used.29

Research also began analyzing other aspects
of the ways in which protection actually
worked. Here, attention focussed on rentseeking
(Krueger, 1974) as a by-product of
protection (and, indeed, as a user of resources
26 See Ronald I. McKinnon (1966), who provided the
first demonstration of this important proposition at the
time.

27 They did not, in principle, have to be partial equilibrium
estimates if shadow prices were known and used
in calculations. In practice, however, that was seldom
feasible.

28 For an exposition of the development of these models
into the 1970's, see Kemal Dervis et al. (1982).
29 See the survey in Bhagwati (1969).


### ---Economics-1997-0-10.txt---
as lobbyists sought protection-see Bhagwati
and T. N. Srinivasan, 1980), as resources
were used to obtain valuable import licenses,
thereby incurring deadweight costs. This, in
turn, showed that protection was more costly
than earlier, area-under-the-triangle estimates
had indicated. It further enabled insights as to
the buildup of vested interests that is likely to
arise once any policy is undertaken. When policy
reforms were attempted, it was clear that
those administering earlier policies were in
the forefront of those opposing change, alongside
the beneficiaries of protection (or other
policies).

Related to work on rent-seeking and the tendency
for vested interests to spring up around
the policies that were adopted, others worked
on the theory of overinvoicing and underinvoicing
(see Bhagwati, 1974) and smuggling
(see Munir A. Sheikh, 1974; Mark Pitt, 1981),
again focussing on some of the flaws of the
system of protection as practiced in most developing
countries.

As trade regimes became more chaotic,
empirical work began to document these problems,
bolstered by the development of the
measurement tools embodied in the concepts
of effective rates of protection and domestic
resource costs. Researchers focussing on Pakistan
discovered that there was actually negative
value added in some circumstances,
suggesting that it would have been cheaper to
pay workers to stay home and import the final
product.3"

The Organization for Economic Cooperation
and Development (OECD) sponsored a
series of country studies on industrialization
led by Little et al. The three synthesized
(1970) the results and provided estimates of
effective rates of protection in a number of developing
countries. These showed how high
and indiscriminate protection levels were and
demonstrated the extent to which import substitution
had failed to achieve many of the objectives
set for it. A later series of country
studies undertaken under the auspices of the
National Bureau of Economic Research, synthesized
in works by Bhagwati (1978) and
Krueger (1978), provided further systematic
empirical evidence of the economic wastefulness
and irrationality of the inner-oriented
trade regimes.

F. East Asian Experience

At the same time as evidence of the high
costs of import-substitution regimes was accumulating,
another important development

occurred. Starting first in Taiwan, several East
Asian economies began growing rapidly under
policies diametrically opposite those prevalent
under import substitution. Interestingly, the
Taiwanese government seems to have listened
carefully to the views of S. C. Tsiang,3 1a professor
at Cornell University specializing in international
economics. Following the precepts
of comparative advantage, Tsiang advocated
growth through industrialization, but with industrialization
taking place through increased
capacity for exports, as well as for the domestic
market. Taiwan's transformation from a
high-inflation, inner-oriented, aid-dependent
economy to a major exporting economy is well
known.

Korea, whose initial conditions appeared, if
anything, even less conducive to growth than
those of Taiwan, followed the same pattern. In
the late 1950's, Korea's exports had averaged
only 3 percent of gross domestic product
(GDP) and were growing slowly, if at all,
while imports represented 13 percent of GDP.
The current account deficit was financed
largely by foreign aid, and the domestic savings
rate was virtually zero. Major policy reforms
took place in Korea in the early 1960's,
which greatly increased the return to exporters.
There were fairly uniform incentives to
all exporters and assurances that the real
exchange rate would not appreciate to their
detriment. Reforms also reduced the protection
to import-competing producers and permitted
exporters duty-free importation of needed
intermediate goods and raw materials.
The Korean economic performance was
transformed, as growth rates entered the
double-digit range and living standards


### ---Economics-1997-0-11.txt---
improved rapidly. Hong Kong and Singapore
also became part of the East Asian "miracle"
through policies designed to encourage exporting.
Growth rates exceeded those previously
thought to represent an upper bound on
attainable performance.32

It was not until the 1980's, however, that
the importance of the differences became
unarguable. After the second oil price increase
of 1979, the worldwide recession of 1980-
1982, and the accompanying "debt crisis,"
the East Asian net importing countries (NICs)
rapidly resumed growth, whereas other heavily
indebted countries were unable to service
their debts and were hard hit by events in the
international economy. Research undertaken
in attempting to understand the impact of the
debt crisis on the developing countries made
it abundantly evident that the debt-GDP ratios
were not significantly different between the
two groups of countries. What was significantly
different was the debt-export ratios, as
the East Asian countries were able to maintain
debt servicing and resume growth because of
the greater flexibility of their economies.33 It
also emerged that, even prior to the debt crisis,
the rates of growth of inner-oriented developing
countries had not increased despite substantial
increases in their savings rates.:
This is not the place to enter into the debate
as to the factors contributing to the success of
the East Asian "tigers." For, while there is
debate about whether government intervention
in "picking the winners" was a key component
of the growth strategy, 35 all recognize that
the reversal from an import-substitution strategy,
the opening up of the economy, and the
relative uniformity of incentives across the
board were necessary, if not sufficient, for success.
Indeed, there is an irony in the fact that
the East Asian experience has stimulated some
to attempt to identify the "dynamic" factors
in exporting that are absent from production
for the domestic market. Thus, we have a complete
turnaround: in the 1950's and 1960's, the
neoclassical argument for an open trade regime
was rejected on the grounds that it was
"static" and ignored "dynamic considerations"
; in the 1990's, there appears to be widespread
agreement that the benefits of an open
trade regime are largely "dynamic" in nature,
and go well beyond the gains from trade under
"static" models of an open economy. Just as
was the case with the infant industry argument,
however, there is a question as to how to identify
and measure these "dynamic" gains.
II. How Did Economists and Researchers
Go Wrong?

The "Washington consensus" is very different
from the policy consensus that led to the
adoption of import-substitution policies in the
1950's and 1960's. While there will no doubt
be refinements in that consensus with further
experience and research, it is highly unlikely
that the ideas of the 1950's and 1960's will be
revived.

One can raise three questions about the
change in viewpoints. First, how could it be
that the economics profession, whose consensus
on the principle of comparative advantage
was at least as great as that on any other policy
issue, endorsed a highly protectionist policy
stance?36 Second, what factors contributed to
32 Chenery and Strout ( 1966) actually had a third constraint,
"absorption," which restrained growth to 8 per-
cent of GDP or less, on the grounds that more rapid growth
would not be feasible.

33 See Jeffrey Sachs (1985) for an early development
of the argument.

34The World Bank (1983) documented that this phenomenon
of a greatly increased average savings rate with
no increase in the growth rate and, therefore, a presumed
relatively sharp increase in the incremental capital output
ratio, affected most developing countries.
35 It can be argued that this is a difference between
those who see the East Asian trade policies as "free trade"
and those who see them as intervention, but of a different
type, from that under import substitution. The critical difference
is probably between those who would stress uniformity
of incentives for earning or saving foreign
exchange (and, therefore, would argue that the East Asian
NICs were arbitrarily close to a free trade regime), and
those who believe the "dynamic externalities" earlier associated
with infant industry protection really call for the
"right kind" of intervention and argue that the trade strategy
was really one of "export substitution."
36 It can also be asked why it took so long for policy
makers in countries such as India to recognize that import
substitution (and other policies) as a strategy for development
was not delivering the hoped-for results and that
a preferable path existed. That is an important question
that is well beyond the scope of this paper.


### ---Economics-1997-0-12.txt---
changing the entrenched views of the 1950's
and 1960's? Finally, what types of research
were most (and least) productive in bringing
about better understanding of the role of trade
and trade policy in development? I address
these questions in turn.

The first is the issue of how the principle of
comparative advantage could have been so
blithely abandoned. With hindsight, it is almost
incredible that such a high fraction of
economists could have deviated so far from
the basic principles of international trade.
What led them to do so? Can any lessons be
drawn to avoid (or shorten the duration of)
similar mistakes in other applied fields when
new policy problems arise?

But, recall the stylized facts that were
widely accepted. People were thought not to
respond to incentives; exports earnings were
thought to be predetermined and slowly growing
at best; industrialization was necessary for
development; supply response was lacking;
and so on. These stylized facts, which were at
best simplistic and in most instances simply
wrong, permitted economists to conclude that
developing economies were "different."
However, it took theory to support these
conclusions. Here, one can distinguish several
failures. First, there was misapplication of
good theory. Second, there was what I shall
call the "theory of negative results," which
essentially could be used to provide a rationale
for virtually any trade intervention. Third,
there was good theory harnessed to erroneous
stylized facts.

A. Misapplication of Good Theory
Misapplication of good theory was significant.
37 The identification of comparative
advantage with the two-factor, two-good
model, and the assumption that free trade
would imply that developing countries would
forever specialize in primary commodities,
was an important misapplication. One of the
puzzling aspects of the evolution of thinking
about policy is the degree to which proponents
of open trade regimes failed to refute the allegation
that free trade would forever leave developing
countries specialized in production
of agricultural commodities.38
It was not until the 1970's (see Ronald W.
Jones, 197 ib; Krueger, 1977) that modelsmotivated
in part by the East Asian experiencewere
developed in which three factors of
production (land, labor, and capital) were allocated
among sectors, each of which could
produce many commodities. As the threefactor
models demonstrated, comparative advantage
lies within manufacturing and within
agriculture, and not between them. Thus, poor
unskilled, labor-abundant countries have a
comparative advantage in labor-intensive agricultural
and unskilled labor-intensive manufactured
commodities, while countries with
a much higher land-labor ratio have a comparative
advantage in more land-using agricultural
commodities and their comparative
advantage in manufacturing lies more in goods
with higher capital-unskilled labor ratios. In
these models, the overall trade balance in manufactures
is a function of the size of the manufacturing
sector, itself a function of past
capital accumulation and the land-man ratio.
A second serious misapplication of good
theory arose because of the nonoperational
nature of the theory itself, and the failure to
identify circumstances under which policy implementation
might be incentive compatible

and potentially increase welfare. A key culprit
in this case was the interpretation of the infant
industry argument. As I already discussed, it
was widely touted as a basis for import substitution,
and generally recognized as a "legitimate"
case for a departure from free trade.
One can hardly argue with the proposition
that the presence of a positive externality gives
rise to a basis for intervention; if the externality
is dynamic and temporary, then temporary


### ---Economics-1997-0-13.txt---
intervention, such as infant industry protection,
can be called for.

The problem with the argument, as a basis
for policy, is that it fails to provide any guidance
as to how to distinguish between an infant
that will grow up and a would-be producer
seeking protection because it is privately profitable.
It is not even clear how one could begin,
empirically, to identify the domain of the
externality. Moreover, even if there were a
producer or producers whose increased production
would generate dynamic externalities,
it does not follow that any level of protection
is warranted. And there is nothing in the infant
industry argument to provide guidance for
quantifying or estimating the likely magnitude
of the externality.

Indiscriminate protection in developing
countries was defended on infant industry
grounds with arguments of capital market failure,
labor market failure (as the costs of training,
presumably, would be borne by first
entrants into industries and then not recouped
as others hired workers away), costs of investments
in technology, and uncertainty all
used. It was not until Baldwin's (1969) seminal
article that it was demonstrated that, even
when the presumed imperfection existed, it
was unlikely that infant industry protection
would help correct it. As Baldwin cogently argued,
later entrants to an industry might speed
up their investments if protection made domestic
production more profitable, and the first
entrant might even be worse off! It was only
after critical examination of these circumstances
that the defenders of the infant industry
case for import substitution became less
vehement.

The infant industry argument also is an
excellent example of a theory that is nonoperational
because criteria for bureaucrats to identify
cases have not been put forward. Quite
aside from the unpredictability and immeasurability
of the future time path of costs in new
factories and the moral hazard associated with
asking individual entrepreneurs to indicate
how much protection they need, there is nothing
to my knowledge in the literature specifying
how the policy maker might instruct a
bureaucrat to identify (much less measure) a
dynamic externality if it were present, how an
incentive-compatible mechanism might be devised
for improving welfare, how the bureaucrat
might measure the height of warranted
protection, nor how policy makers might credibly
commit to temporary protection. Even ex
post, it is not entirely clear how one might
identify an industry as a successful infant: simply
because a firm became profitable and exported
does not prove that there was either an
externality or a dynamic process at work! 9
B. Negative Results

Much of the theorizing that took place was
concerned with what I call "negative results."
That is, analysts sought to find reasons why,
for example, an exception to free trade should
be made. Once the principle of comparative
advantage was laid down as a basis for policy,
there was little left for theorists to prove supporting
an open trading system, so the challenge
to theorists was to find conditions under
which the free trade precept did not hold. As
theory, these findings were significant, but for
policy they were unhelpful, and probably
served to perpetuate inappropriate policies.
In most real-world circumstances, one
strongly suspects that protection exists where
theoretical exceptions do not justify it, and that
moves to first-best policies would on average
lower, and not raise, protection. Judged by that
metric, research output relevant for policy
would consist more of attempts to measure the
costs of these excess levels of protection. In
practice, it would be interesting to review the
literature and ascertain how many articles, or
pages, or other measures of research output
were devoted to finding exceptions to the
proposition that comparative advantage should
form the basis for trade policy, contrasted with
those focussing on circumstances where protection
was too high! In undergraduate inter-
39 The same is true of the optimum tariff argument. In
the presence of many goods with varying degrees of monopoly
power, the formula becomes hopelessly complex.
It is certainly true that many tariff structures would lead
to lower, rather than higher, welfare in the presence of
monopoly power in trade. Yet, in practice, many policy
makers have been misled into thinking that they could
defend very high tariffs (sometimes even on goods that
their countries import in small quantities) on optimum tar-
iff grounds.


### ---Economics-1997-0-14.txt---
national economics courses, sections on trade
policy spend considerable time addressing national
defense exceptions, the optimum tariff
argument, the infant industry argument, secondbest
arguments, and other arguments for protection.
While attention is paid to the reasons
why these arguments may not be correct, focus
nonetheless centers on the exceptions to the
case for free trade, rather than on the reasons
for it. While this may be inevitable as a way
of reasoning, the temptation to draw inappropriate
inferences seems high.

An example will illuminate the argument.
Whereas theory suggests criteria for departures
from laissez-faire free trade which normally
would result in different levels of
protection for different industries, a widely
used prescription for policy makers is that, if
there is to be protection, a uniform tariff is
usually preferable to any alternative structVre.
This proposition rests on several considerations.
First, only a uniform tariff can generate
a uniform rate of effective protection in the
import-competing sectors and, if different
goods are subject to different rates of tariff,
the resulting differences in effective rates of
protection will lead to resource misallocation
even within the import-competing industries
and have no relation to underlying "dynamic"
or market-failure considerations. Second, a
uniform tariff simplifies customs administration,
making evasion and/or bribery of customs
officials more difficult than a varying rate
structure. Third, a uniform tariff greatly reduces
the opportunities for resource losses in
rent-seeking and lobbying. Fourth, given international
prices, international value added is
more likely to be maximized under a uniform
tariff structure than under a variable one.
None of these arguments is sufficient to
prove that a uniform tariff is optimal. And, indeed,
it is straightforward to develop models
in which a uniform tariff is nonoptimal, especially
in the presence of income-distribution
considerations. In theory, the costs of protection
can be minimized by imposing higher
tariffs or taxes on goods whose supply and demand
is relatively more price inelastic.
Those arguments, as put forward, are all
couched in terms of demonstrating the "falsity"
of the proposition that a uniform tariff is preferable
to variable tariff rates and that there is a
departure from uniformity that can potentially
improve welfare. But the difficulty with that formulation
is that it does not provide a criterion
for which departures from uniformity might
improve welfare, because a model considering,
for example, income-distribution considerations,
cannot simultaneously address issues of corruption
and administration. And, the fact that
income-distribution considerations can warrant a
nonuniform tariff structure does not prove that
any nonuniform tariff structure is preferable to
a uniform one! As such, a negative result gives
little or no guide for policy. Nonetheless, it arms
lobbyists and others with ammunition to discredit
technocrats' efforts to maintain a less irrational
structure of protection.

Some good theoretical papers would have
done less damage, or at least given less aid and
comfort to policy positions that were clearly
not those intended in the analyses, if the authors
had taken greater pains to note the limitations
to their analyses, and the other factors
that would have to be taken into account, before
their results were applied to policy.
In that regard, it is often overlooked that most
policy implementation is carried out by government
officials who cannot be expected to have
advanced degrees, and sometimes even undergraduate
degrees, in economics. In many instances
(including formulae for optimal tariff
differentiation), the degree of sophistication
needed to interpret research results is well beyond
that which most bureaucrats will have. As
pointed out by Johnson (1970 p. 101):
...The fundamental problem is that, as
with all second-best arguments, determination
of the conditions under which a
second-best policy actually leads to an
improvement in social welfare requires
detailed theoretical and empirical investigation
by a first-best economist ... it is
therefore very unlikely that a second-best
welfare optimum will result based on
second-best arguments.

C. Good Theory Assuming

Counterfactual Situations

The final abuse of theory was primarily a
fault of inappropriate stylized facts. Nonetheless,
in many instances, analysts assumed
signs of variables that were certainly questionable,


### ---Economics-1997-0-15.txt---
modelled the situation neatly, and then
drew policy conclusions that could hold only
if the posited signs were valid. Yet their claims
often went beyond the assertion that "if these
facts ... then" variety.

As an example to illustrate the point, I have
deliberately chosen a good, widely cited paper,
because the paper represents good theory, but
interprets it, for policy purposes, with dubious
"stylized facts." Sudhir Anand and Vijay Joshi
(1979) considered a world, such as that envisaged
by Hagen ( 1958), in which workers in the
advanced sector receive a higher wage than in
the rest of the economy due to unions or other
(presumably unalterable) circumstances. They
then asked whether maximizing international
value added for given employment of domestic
resources is an appropriate criterion when
income-distribution considerations cannot be separated
from productive-efficiency considerations.
In their setup, the clear answer is no, because
tradeables are produced by the advanced (presumably
unionized) sector, and hence maximizing
international value will pull more
resources into that sector at the cost of a deteriorating
income distribution. Interestingly,
they do not address the question of whether the
advanced sector is labor or capital intensive. If,
as is true for outer-oriented developing countries,
the exportables are labor intensive relative
to import-competing activity, removing protection
to induce a move of more workers to the
"advanced" high-wage sector would presumably
increase wages of those workers and also
those in the rest of the economy: a more equal
income distribution would be obtained at the
expense of lower real wages for all. Without
regard to factor intensity, however, Anand and
Joshi (1979 p. 350) conclude that:
The motivation behind the theory of distortions
has been to criticise and to guide
trade and industrialisation policies ...
Our analysis emphasises the need for
caution ... Departures from technical efficiency
may be called for as part of the
rational response by governments to the
limitations they face in carrying out desirable
income distribution policies ...40
Anand and Joshi ( 1979) assumed that moving
toward economic efficiency in tradeables
requires paying higher wages because of a distortion.
Yet, in fact, the evidence suggests that
it has been the highly protected, importcompeting
industries which have been able to
pay above-average wages; removing protection
has led to rapid expansion of employment
in labor-intensive industries. If the latter stylized
fact is correct, and if income-distribution considerations
are important, it would suggest that
the policy implications of the Anand-Joshi
analysis are the opposite of what they suggest-
namely, that policy makers should encourage,
even beyond the optimum, a shift of
resources out of protected industries (presumably
by removing protection) and into exportable
industries.4"

III. What Research Contributed
to Improved Policies

Policies that were not consistent with policy
makers' growth objectives were cloaked in respectability
in the 1950's and 1960's by theory
and stylized facts of the type I have already
described. I have so far discussed properties of
some theories that made them susceptible to
misapplication or misuse.

A second question is equally important,
however. That is, how did the change in economists'
policy prescriptions come about? What
led to the reversal to recognition of the importance
of an open economy after the conversion
to advocacy of import substitution in
the 1950's and 1960's? I can address this question
more rapidly because much of the answer
was implicit in the description of the evolution
of developing countries' trade policies.
40 Another example of the "negative results" research
arises from early findings (see Bhagwati and Srinivasan,
1973; Jones, 1971a) that the resource pulls associated with
raising an effective rate of protection did not necessarily
accord with those associated with increasing a nominal
rate of protection. These findings did not significantly affect
research efforts in part because the authors made clear
the relatively extreme conditions necessary to generate the
"perverse" resource pull, and partly because other researchers
were able to demonstrate that there seemed to
be few, if any, empirical counterparts to the perverse pull
cases.

4' See Pranab Bardhan (1996), tracing how the presumed
"efficiency-equity" trade-off has been shown to
be false in considerable measure.


### ---Economics-1997-0-16.txt---
Three sets of research efforts can be singled
out as having been particularly useful in informing
changes in policy, although others,
no doubt, also contributed.42 First, there was
research analyzing how import-substitution
policies were actually working. Second, and
not unrelated to the first, there was the refinement
and more appropriate interpretation of
theory. Third, there was research demonstrating
the feasibility of the alternative.
A. Challenging the Stylized Facts and
Understanding How Import-Substitution
Regimes Worked

Analyses of the evidence regarding the key
stylized facts were in hindsight important steps
in undermining the intellectual consensus.
Demonstration that there were significant responses
to incentives undermined the policy
case for ignoring prices. Proof that the terms
of trade had deteriorated very little, if at all,
began to undermine export pessimism.
Empirical work on the ways in which
import-substitution regimes functioned was
crucial. Comparative analyses such as those
of Little et al. (1970), Bhagwati (1978),
Krueger ( 1978, 1983 ), and Michael Michaely
et al. ( 1991 ) clearly contributed significantly
to awareness that the effects of importsubstitution
policies were not idiosyncratic to
individual countries. The comparative studies
provided a great deal of evidence as to the
shortcomings of reliance on import substitution.
Evidence that protection was not temporary,
that protection levels were high and
idiosyncratic, that there was very great discrimination
against exports, and that "foreign
exchange shortage" was a function of policies
and not an exogenously given datum, were all
important in challenging the protectionist trade
policies still prevailing in most developing
countries in the 1980's.

If one considered the evidence regarding the
workings of trade policies in any one country
taken alone, there were ample grounds for criticism
of inner-oriented trade policies, with the
monopoly positions they conferred on domestic
producers, the high costs of doing business,
rent-seeking low quality of products, and so
on. It was possible, however, to recognize that
and nonetheless conclude that policy makers
in that particular country had been inept, or
had simply failed to implement policies appropriately.
As evidence mounted across countries,
the similarity in the evolution of regimes
and their consequences was striking. It was increasingly
difficult to dismiss the evidence
from a particular country as being sui generis
or the failing only of the particulars of policy
execution in that country.

But, underpinning the analyses of individual
country situations, either in the comparative
studies or individually, were agreed-upon
measurement tools. The empirical studies
could not have had their impact without the
development and use of measurement tools.
As cost-benefit techniques were used, it became
increasingly difficult to justify some
highly uneconomic projects. And, as measurement
of effective rates of protection was undertaken
in country after country, the high and
erratic nature of protection became evident.
Techniques for cost-benefit analysis and measurement
of effective rates of protection were
important, first of all, in providing analysts
with tools with which to demonstrate the chaotic
nature of import-substitution policies. In
addition, even before the policy consensus
changed, there is little doubt that some of the
earlier extreme irrationalities of policy were
curbed through use of these tools. It became
extremely difficult to defend the high average
of, and wide variance in, effective rates of
protection.

At an empirical level, it seems clear that early
demonstrations of the great range of variation in
rates of effective protection were useful both in
demonstrating some of the problems with trade
regimes and also in preventing at least a few of
the worst excesses that might otherwise have occuffed.
More generally, recognition and reintroduction
of the proposition that there is a


### ---Economics-1997-0-17.txt---
response to incentives that cannot be overlooked
in policy formulation, combined with the evidence
on the erratic and arbitrary nature of incentives
provided by trade regimes, forced a
reexamination of the premises on which importsubstitution
policies were based.

Yet another contribution of empirical research
was to focus upon the actual workings
of policy implementation. In early policy prescriptions,
there had been something of a naive
tendency to assume that enunciating a desired
outcome was itself sufficient to achieve it.
This naivete was dispelled, as the theories regarding
bureaucratic behavior, rent-seeking,
smuggling, and overinvoicing and underinvoicing
all enabled observers to examine more
critically the ways in which alternative policy
prescriptions might have side effects that had
earlier been unanticipated.

B. Refinement and More Appropriate
Interpretation of Theory

As already seen, some of the intellectual underpinning
of import-substitution policies was
provided by inappropriate interpretation of
theory, or the failure of theory to take into account
key institutional or behavioral variables.
Analytical developments focussing on conditions
under which these interpretations were
valid, or examining the ways in which results
had to be modified to take into account these
institutional and behaviorial aspects, were
clearly important in improving understanding.
The entire literature on optimal interventions
in the presence of domestic distortions is
one important example of a demonstration that
earlier interpretations of theory had failed to
examine the relevant alternatives. It was invaluable
in demonstrating clearly that in most
circumstances, the presence of a distortion
warranted a first-best policy intervention other
than a tariff.43 For example, in the case of
Hagen's (1958) employment-generating case
for protection, the optimal intervention literature
demonstrated clearly that a first-best intervention
would be in the labor market, and
that a tariff or quota could not achieve a firstbest
outcome.

Similarly, developments showing that the
comparative advantage results were not the
simple "specialize forever in primary products"
precept proved significant in enabling
policy makers to contemplate alteration in
trade strategy. Baldwin's (1969) critical examination
of the infant industry argument provides
yet another example of an analytical
contribution that was important in making
those concerned with policy consider carefully
the effectiveness of the policies they had
adopted in achieving their desired goals.
Finally, there was theory that was developed
in response to the functioning of importsubstitution
regimes. Here again, the theory of
rent-seeking, as it pointed to the ways in which
bureaucrats and others made protection very
costly, was important. Further, when it was
recognized that bureaucrats, businessmen, and
others attempted to capture or thwart policy
initiatives not in their self-interest and that
they acquired an interest in maintaining the
system, once established, and that resources
were expended in operating the system, it had
to be recognized that changing the system
would be politically difficult.
Development of a better understanding of
the incentives for underinvoicing and overinvoicing
of exports and imports and for smuggling
under exchange-control regimes worked
in the same direction: not only could these activities
prove costly to the exchequer and in
terms of resource drains, but the very recognition
of their presence served to remind
policy makers of the limitations of their
instruments.

Finally, good analyses demonstrating how
individual import controls actually worked
contributed to understanding and made empirical
work more effective. The further
refinement of theory showing tariff-quota
equivalence has already been mentioned.
Rent-seeking again comes to mind. But, in
addition, individual mechanisms for encouraging
import substitution each had their
own, often idiosyncratic, incentive effects. A
good example is Gene Grossman's (1981)
classic analysis of domestic content regulations
and their effects.

" However, those advocating import substitution
seized upon the infeasibility of first-best policy as a defense
for following the policies they wished in any event
to follow.


### ---Economics-1997-0-18.txt---
C. Demonstration of the Viability of
Alternative Trade Policies

Research on the contrast between East
Asian and other developing countries and reasons
for it obviously turned out to be a major
contributing factor in influencing thinking
about policy. In a way, research on East Asian
experience provided a final blow to the earlier
uncritical acceptance of the stylized facts. For,
the East Asian experiences demonstrated, as
nothing else could have, the feasibility and viability
of alternative trade policies: it was no
longer possible to associate comparative advantage
with reliance on primary commodity
exports, and the East Asian experience certainly
put an end to the belief that developing
countries could not develop rapidly when relying
on integration with the international
economy.'

The experience of the East Asian exporters
did several things. Most important, it provided
concrete evidence that a developing country
could achieve industrialization without relying
on domestic markets to absorb almost all
additional output. That demonstrated the fallacy
of the earlier view that industrialization
could take place only through import substitution.
45 Also, the East Asian trade regimes offered
significant opportunities for empirical
research, and the evidence mounted that properties
formerly thought to be those of all developing
countries were, in fact, properties
resulting from inner-oriented trade and payments
regimes.

It cannot be said that either research results
or the contrast in economic performance alone
led to the change in policies in other developing
countries.46 Both research (especially
that which brought the sharply contrasting experiences
of the East Asian exporters and the
import-substituting countries into focus) and
experience contributed.

Whether one should regard the East Asian
experience as entirely separate from economic
theory, however, is an interesting question. As
already mentioned, Tsiang (1985) was himself
an international economist, and it was in
significant part his efforts that led the Taiwanese
authorities to abandon inner-oriented
policies and attempt to develop through exports.
The theory of comparative advantage
was, at least in that instance, a pillar on which
policy was built. And, while a variety of factors
no doubt contributed to the Korean adoption
of outer-oriented trade policies after 1960,
the favorable experience of Taiwan undoubtedly
facilitated the willingness of decision
makers to try the new approach.
The East Asian exporters put to rest the
mistaken belief that developing countries relying
on the international market would forever
be specialized in the production of primary
commodities. They also showed that rates
of growth well above those realized even in
the most rapidly growing import-substitution
countries such as Brazil and Turkey could be
realized.

IV. What Lessons Can Be Learned for Research
in New Applied Fields?

It is difficult to draw generalizations based
on the evolution of analysis, empirical research,
and policy in one applied field. Nonetheless,
in the hope that insights from other
applied areas may reinforce or amend the list,
the effort seems worthwhile.

Perhaps the most obvious generalization
from the various factors that have been discussed
is that empirical research which tests
for the presence and order of magnitude of
stylized facts which are used in modelling and


### ---Economics-1997-0-19.txt---
policy formulation can be invaluable. If the
right stylized facts can be used as a basis for
theory, and theorists have good indications of
the relative quantitative importance of various
phenomena, it is clearly far more likely that
the theory itself can make a useful contribution.
In the case of trade policy and development,
the demonstrations that there were responses
to incentives and that developing countries
could expand export earnings and did have
comparative advantage in other than primary
commodities, were clearly crucial to improved
understanding of the relationship of trade to
development.

For that reason, high marks must go to the
analytical research that pointed to measurement
techniques such as effective protection
and cost benefit, which enabled policy makers
and their analysts to obtain empirical quantification,
however rough, of the relevant
magnitudes.47

In like manner, the empirical demonstration
of the similarity of policy responses across developing
countries, and of the wide and largely
irrational variation in incentives for importcompeting
industries, increased understanding
of what was wrong with existing policies.
Overturning, or more accurately interpreting,
the accepted stylized facts, therefore, was
a first prerequisite for developing a better theory
of trade policy for development. But theory
was important in many ways, in addition
to pointing to appropriate measurement tools.
First of all, good policy-relevant theory provided
blueprints for those windows of opportunity
in which governments genuinely sought
to improve economic performance, as was the
case in Taiwan and Korea in the early 1960's,
and in Chile, Mexico, and India in later decades,
to name just a few.48 Having the "blueprints"
on hand from good theory is obviously
a major contribution. As already noted, however,
that theory is often relatively dull -such
as comparative advantage-rather than the
more exciting and refined results of complex
models.

Second, theory was invaluable when it
showed why simple interpretations of received
doctrine were in fact wrong. This was the case
with the theory of first-best intervention in the
case of domestic distortions, and in the case
with comparative advantage as interpreted to
mean developing countries would specialize in
the production of primary commodities, and
with the infant industry argument.
These considerations suggest that research
results, in order to be most likely to be amenable
to policy relevance, should be interpretable
into phenomena that are observable,
hopefully quantifiable, and recognizable by
the policy maker. A negative result, such as
that theory does not always tell us, can be
counterproductive precisely because the policy
maker is informed only that a certain generalization
(such as comparative advantage
and the value of free trade) is not without
exception; the generalization can then be
ignored.

A more general statement of the problems
inherent in theorems which show that major
propositions are "not generally true" would
encompass all of that theory which is cast in
terms of "anything can happen." While it is
certainly true that there are conditions under
which a wide range of outcomes (Paretoinferior,
a bad equilibrium, Pareto-superior,
etc.) are possible from the same policy instrument,
it would have challenged the skills of
even the most superb theorist to attempt to develop
a case for the sorts of chaotic policies
prevalent in Turkey in 1957, in Ghana in 1983,
and in Argentina in the late 1980's. It is far
too easy for analysts to ignore the fact that "an
exception" does not rationalize all possible
policy alternatives to free trade.
There is a criterion for efficient resource allocation,
equating domestic and international
marginal rates of transformation. Even if there
are "dynamic" factors which contravene part
of the static efficiency criterion, they too are
measurable. Yet the "anything can happen"
theories do not provide guides as to how the
4 There is another example from a related field. As is
well known, multilateral negotiations with regard to agricultural
protection were completely stalled until the
1980's. In the 1980's, economists at the OECD proposed
the use of a "producer subsidy equivalent" to measure
the degree of government intervention in various agricultural
commodities across countries. That tool permitted
negotiations to begin restricting and dismantling agricul-
tural protection.

48 See Arnold C. Harberger (1993) for a discussion of
the roles played by economists in some key policy reform
episodes.


### ---Economics-1997-0-20.txt---
phenomena under examination may be quantified,
and thus provide rationalizations (admittedly
for those who want them) for policies
that cannot by any realistic test pass muster.
Perhaps the lesson is that there is a significant
danger that economic theory will be misinterpreted
in the policy arena, and researchers
could productively take more pains to distance
themselves from policy conclusions that are
not warranted by their analysis. Theoretical
papers which end with "it has been shown
that, under conditions x and y, policy z may
no longer represent an optimum ... Therefore
policy should ..." are obviously overstepping
their bounds when the empirical relevance of
x and y are not yet established, and even more
so when conditions other than x and y also may
be important (as, for example, with rentseeking)
.

But many good theory papers are written
where the authors assume that their audience
will consist entirely of other theorists. In such
instances, good theory may be misused, and it
certainly will be in the self-interest of some to
harness it to their own ends. It behooves applied
economists, as well as the theorists, to
be careful to interpret the policy relevance of
results in ways which minimize the scope for
misinterpretation. This is as true for those
seeking to find "dynamic" aspects of exporting,
or endogenous aspects of a "big push,"
as it should have been for those developing the
infant industry or optimum tariff arguments.
Complex results, such as those noted by
Johnson (1970), are particularly suspect in
that they can be interpreted in whatever ways
suit the decision maker or lobbyist.
Finally, there is theory which provides no
guidance as to when or how to observe the
phenomenon. In such instances, it is difficult
to find policy implications that will not be captured.
One possible challenge for theorists
might well be to ask for at least one plausible
incentive-compatible mechanism under which
the inefficiencies they identify might be improved
upon by policy makers and bureaucrats.
The existence of infant industries, of
cases in which there are rents that might be
captured by appropriate strategic trade policy,
and of informational asymmetries and other
market imperfections cannot be doubted. But
until the magnitude of these phenomena can
somehow be measured, or incentive-compatible
mechanisms for correcting them can be devised,
theorists asserting their presence are
simply providing a carte blanche for policy
makers and bureaucrats to intervene in whatever
ways they like, and this will simultaneously
be seized upon by special interests to
bolster their causes.

No matter how careful economists are,
special interests always will seize their research
results in supporting their own objectives.
And, no matter how sophisticated and
careful research findings are, there always
will be politicians formulating, and noneconomists
administering, policies. Recognition
of these propositions could do much
to increase the degree to which economists'
research results can contribute (positively)
to policy formulation.

I
 ## Economics-1998-0


### ---Economics-1998-0-01.txt---
One of the great pleasures of belonging to
my generation of economists is that we were
able to witness the birth and the subsequent
evolution of the modern approach to the
analysis of economic growth. The centerpiece
of that approach is probably growth
accounting, but we should never forget that
growth accounting is firmly rooted in economic
theory.

My way of telling the story goes like this:
Many, maybe even most, economists expected
that increments of output would be explained
by increments of inputs, but when we took our
best shot we found that traditional inputs typically
fell far short of explaining the observed
output growth. Our best shot consisted in attributing
to each factor a marginal product
measured by its economic reward. Thus:
(1) ffAy = WAL + (p + 6)AK + R.
Here:

Ay = change in output (GDP);

AL = change in labor input;

p = initial general price level;
w = initial real wage;

T = initial real rate of return to capital;
8 = rate of real depreciation of capital;
AK = change in capital stock; and
R = "the residual" of growth unexplained
by increases in traditional inputs.
Many economists are probably more familiar
with a variant of (1)

(1') (Ay/y)-= (wL/py)(LL/L)

+ [(+ 6)Klfy ] (AK/K)

+ (Rly) = se(AL/L)

+ Sk(AK/K) + (Rly).

In whichever form, the measured residual
typically accounted for an important fraction
of the observed output growth, quite often half
or more.


### ---Economics-1998-0-02.txt---



### ---Economics-1998-0-03.txt---
This result came as a surprise to the profession,
though perhaps less so to those who
reached it, or something very like it, by an altemative
route. They were the people who
came at the problemn out of a tradition of measuring
labor productivity, and at some point
complemented output per worker with a measure
of output per unit of capital, and finally
joined the two to create a measure of total factor
productivity (TFP). The idea of total factor
productivity increasing through time was less
a shock to these people than the "growth residual"
was to those who approached its measurement
along the lines of equation ( 1) or
(1'). See Moses Abramovitz (1952, 1956)
and Solomon Fabricant (1954).

In any case, as the newly discovered residual
loomed large in our professional thinking, our
discussion centered on two potential explanations:
"human capital" and "technical advance.
" (See Robert M. Solow, 1957.) These
can be thought of as complementary explanations,
at least up to a point, with technical advance
representing truly new ways of doing
things, and the accumulation of "human capital"
representing increases in the "quality" of
the typical human agent. It was not long before
attempts were made to quantify the contribution
of improved labor quality. These came as part
of a general move toward disaggregation of the
two factors, which can be represented by:
(2) ffAy=ZiwViLi

+ z9(Pj + 6j)AKJ + R'.

Here the index i can vary over all sorts of education
and skill groups as well as categories
like gender, age, occupation, region, etc. All
these are items that may signal a different market
wage. In a similar vein, the index j would
appropriately vary over categories like the corporate,
noncorporate, and housing sectors
where, for tax if for no other reasons, different
(gross-of-tax) rates of return would presumably
prevail, even in a full equilibrium.
In an equation like (2), the presumed marginal
product of each category of labor is measured
by the wage wi . Average quality can be
measured by Q, = iwi Lil i L,, and the contribution
of change in quality to Ay, between
t and t + 1 can be calculated as Liw( i(A Q, + I /
Qt). Thus, the contribution of quality change
is already built into the first summation in (2),
but can be separately identified if we so
choose.

A focus on human capital could lead us to
a slightly different way of breaking down
1wiALi. Here we could choose some "basic
wage" w*, ideally the wage of some welldefined
category of relatively unskilled labor.
Then we could divide the remuneration wi of
any given category into a part w* which was
a reward for "raw labor" and another part
(wi - w *) which we would identify as the
reward to the human capital of a typical
worker of type i.

Using a framework like (2) has long been
the standard for careful professionals. Pioneered
by Zvi Griliches (1960, 1963), it was
utilized by Edward F. Denison (1967) and
John W. Kendrick (1973, 1976, 1977), among
others. This approach has been further developed
and carried to a high art by Dale W.
Jorgenson and Griliches ( 1967), Jorgenson et
al. (1987), and Jorgenson (1995).
The main point to be made here is that once
the residual is measured using a framework
like (2) or its equivalent, the direct, measured
contribution of human capital is captured in
the labor term 1wiALi. By direct contribution
I mean what people are paid for. Doctors earn
more than nurses, and engineers more than
draftsmen. These and similar differences are
captured in D;FiALi, which can be positive
even if XALi is zero, just from an upward reshuffling
of the same labor force. A truly accurate
measurement of type (2) would capture
all the subtle differences of quality that exist
in a modem labor force and would give each
a weight corresponding to the (gross-of-tax)
earnings that demanders are observed to pay.
We may do this imperfectly, but, in concept at
least, the residual R' as measured by (2) does
not contain any elements of quality change or
any direct contributions of human capital to
growth. This is a quite important point for it
permits us to zero in on the residual as representing
"technical change," "TFP improvement,
" and "real cost reduction."

There is no analytical reason to prefer one
of the above three terms over another, in referring
to the residual R'. But I am going a bit
out on a limb to say that a term like "technical
change" leads most economists to think of inventions,
of the products of research and development


### ---Economics-1998-0-04.txt---
(R&D), and of what we might call
technical innovations. On the other hand, TFP
improvement, once purged of the changes in
the quality of labor and/or the direct contributions
of human capital, makes one think of
externalities of different kinds -economies of
scale, spillovers, and systematic complementarities.
And finally, real cost reduction, to my
mind, makes one think like an entrepreneur or
a CEO, or a production manager.
I think it would be perfectly fair to characterize
my presentation today as a paean in
praise of "real cost reduction" as a standard
label for R'. Labels do not change the underlying
reality, but they may change the
way we look at it and the way we think about
it. They also can lead us to understand it better.
Thinking in terms of real cost reduction
has certainly done all this for me, as I have
tried to sort out the many puzzles and complexities
that surround the process of economic
growth.

Let me try to take you down the path I traveled.
In the first place, real cost reduction
(RCR) is probably on the mind of most business
executives, production managers, etc., at
some point or another in any given week, let
alone in any given month or year. It is a major
path to profit in good times, and a major defense
against adversity in bad times. Most U.S.
firms that have downsized in recent years did
so with RCR in mind. So, too, did the firms
that computerized their payrolls and other accounts.
And so also did those who shifted to
what they considered more modern management
techniques. I recall going through a
clothing plant in Central America, where the
owner informed me of a 20-percent reduction
in real costs, following upon his installation of
background music that played as the seamstresses
worked. And then there is the story of
two Chilean refrigerator firms that ended up as
parts of a single conglomerate at one point.
The new management reduced the number of
models from something like 24 to two, making
agreements to import other models while exporting
these two. The end result was that output
more than doubled, while the labor force
was cut to less than half, and even the capital
stock (at replacement cost) was significantly
reduced. This sounds like (alnd is really) economies
of scale, but they would not be detected
by our usual measures, as both labor force and
capital stock went down. And we all have seen
cases where, say, an office's real costs were
reduced when a martinet of a manager was replaced
by someone more reasonable. But we
have also seen cases where real costs were reduced
when a very lax manager was replaced
by someone more strict.

It has long been my song that there are at
least 1001 ways to reduce real costs and that
most of them are actually followed in one part
or other of any modem complex economy,
over any plausible period (say, a decade).
Once one accepts this proposition as true, the
question then arises: Why would anybody try
to settle on just one underlying cause of real
cost reduction? The answer, I think, is mindset-
the framework in which one is thinking
at the moment. The pioneer writings of the recent
endogenous growth literature can, I think,
be said to reflect a kind of annoyance at something
like R or R' being considered exogenous.
There was an urge to surmount that inelegance
by somehow making the residual endogenous.
And in a simple growth model that meant generating
a feedback from the rest of the model
to the residual. A 1001 feedbacks would be out
of the question, but one feedback would work
just fine. Thus Paul Romer (1986) focused on
a feedback through "knowledge," with the
stock of knowledge shifting production functions
all over the economy; Robert E. Lucas,
Jr. (1988) focused on "human capital," not
on its direct and remunerated productivity, but
on the externalities that each increase in the
stock of human capital were presumed to generate.
These single feedbacks achieved the
limited purpose of endogenizing R or R'
within a specified model, but they did not represent
very well the multifaceted nature of real
cost reduction as we observe it in actuality.
And, in point of fact, both the cited authors in
their more recent writings display a deep recognition
of the subtlety and complexity of the
growth process, not really capable of being
captured through a simple feedback mechanism.
(See Romer, 1990, 1994a, b; Lucas,
1993.)

So, real cost reduction is multifaceted and
everywhere around us. Where does that get us?
Or how can we get anywhere in the face of
such complexity? The next step is to recognize
that in spite of its complexity, real cost reduction
can be reduced to a single metric, and can


### ---Economics-1998-0-05.txt---
TABLE 1-GROWTH BREAKDOWN TREATING REAL COST REDUCTION As ADDITIVE
TFP growth Absolute amount

over period of real cost

(1.0 = 100 reduction Cum. sum Initial value Cum. sum
Industry percent) [(1) X (4)] of (2) added of (4)
(1) (2) (3) (4) (5)

1 0.800 $80b. $80b. $100b. $100b.
2 0.600 $120b. $200b. $200b. $300b.
3 0.500 $150b. $350b. $300b. $600b.
All the rest 0.107 $150b. $500b. $1,400b. $2,000b.
be made additive. For a quick appreciation of
this, assume that total factor productivity grew
by 80 percent in one industry over a decade,
by 60 percent in another industry, and by 50
percent in a third. If their initial value added
amounted to $100 billion, $200 billion, and
$300 billion, respectively, then the real cost
reduction of the first was $80 billion, that of
the second was $120 billion, and that of the
third $150 billion. So we can say that, measured
at initial prices, the real cost reduction
of the three together was $350 billion over the
decade in question. I truly think that the notion
of real cost reduction being additive in this
way came to my mind, and is easily seen by
others, just as a consequence of the label. The
idea of additivity does not follow nearly so
easily from the labels "technical advance"
and "total factor productivity."
Anyway, this vision of the growth process
opens up many new vistas and gives us many
new challenges. To me, it gives life to the residual,
viewed as real cost reduction, in a way
that remote macroeconomic externalities
never did. It gives the residual body, in the
sense that the number of dollars saved by real
cost reduction is a tangible and measurable
quantity. It gives the residual a name (real cost
reduction), an address (the firm), and a face
(the face of the entrepreneur, the CEO, the
production manager, etc.) And, finally, we
shall see that there can be vastly different expressions
on that face, even as we move from
firm to firm in a given industry, as the TFP
experience of a period moves from sharply
positive to devastatingly negative.
I. Yeast versus Mushrooms: Part I
Table 1 is based on the numerical example
just given, plus the information that the remaining
industries (say, in the economy) together
had an initial value added of $1,400
billion and experienced real cost reduction of
$150 billion over the period. Setting out data
in the format of Table 1 allows us to make
statements like "15 percent ($300 b./$2,000
b.) of the industries (measured by initial value
added) accounted for 40 percent ($200 b./
$500 b.) of the real cost reduction (RCR) of
the period" and "30 percent ($600 b./$2,000
b.) of the industries accounted for 70 percent
($350 b./$500 b.) of the period's RCR."
I stumbled on this way of presenting data on
real cost reduction in the course of writing a
background paper (Harberger, 1990) for the
World Bank's World Development Report of
1991. Once I saw it, I immediately embraced
it, because it helped me communicate to others
what I call the "yeast versus mushrooms" issue.
The analogy with yeast and mushrooms
comes from the fact that yeast causes bread to
expand very evenly, like a balloon being filled
with air, while mushrooms have the habit of
popping up, almost overnight, in a fashion that
is not easy to predict. I believe that a "yeast"
process fits best with very broad and general
externalities, like externalities linked to the
growth of the total stock of knowledge or of
human capital, or brought about by economies
of scale tied to the scale of the economy as a
whole. A "mushroom" process fits more
readily with a vision such as ours, of real cost


### ---Economics-1998-0-06.txt---
reductions stemming from 1001 different
causes, though I recognize that one can build
scenarios in which even 1001 causes could
work rather evenly over the whole economy.
Personally, I have always gravitated toward
the "mushrooms" side of this dichotomy. I
remember being impressed, when I first saw
some early industry estimates of TFP improvement,
by their tendency to industry concentration.
For years I told my students that the
1920's were the decade of cars and rubber
tires, the 1930's the decade of refrigerators,
the 1940's that of pharmaceuticals (especially
antibiotics), and the 1950's that of television,
with telecommunications anid computers taking
over in recent decades. But these were just
impressions, not based on any systematic approach.
My real turnaround came in the course
of writing my 1990 paper, where I presented
a series of tables based on Kendrick and Elliot
S. Grossman's (1980) work. Table 2 is an
example.

Table 2 has the same format as Table 1. Column
(1) presents the familiar measure of the
percentage by which TFP grew, or real costs
were reduced, during the period in question
(note that the percentages apply to the period
1958-1967 as a whole; they are not annual


### ---Economics-1998-0-07.txt---
rates). To turn these percentages into dollar
amounts of real cost saving over the period,
one multiplies them by base-period real GDP
[col. (4)]. The results are shown in colunm
(2). Columns (3) and (5) are the cumulative
sums of columns (2) and (4), respectively.
Working with these figures one can make
statements like those at the bottom of the
table-i.e., the top 10 percent of industries
accounted for 30 percent of total real cost reduction;
the top 22 percent of industries (measured
by initial value added) accounted for
more than half of total real cost reduction.
Readers will notice that at the foot of each
column in the table is an entry refering to 18
additional industries, which together accounted
for only 10 percent of the total TFP contribuition,
while their combined share of initial output was
almost 60 percent of the total.
Using the analogy with yeast and mushrooms,
the results of my calculations using the
Kendrick-Grossman data pointed very clearly to
a "nmushrooms" interpretation. Not only were
the contributions to RCR highly conicentrated in
a relatively few industries, these industries also
were very different as one shifted from decade
span to decade span. The top four branches in
percentage of real cost reduction during 1948-
1958 were Communications, Public Utilities,
Farming, and Miscellaneous Manufacturing. In
1958-1967 they were Lumber, Railroad Transport,
Textile Mills, and Electrical Machinery. In
1967-1976 they were Finance, Insurance &
Real Estate, Apparel, Communications, and
Chemicals. Only Communications appears twice
among these 12 listings.

Now to my mind, this already brings
evidence to bear on a number of possible hypotheses
concerning the nature of TFP

improvement. Certainly some ways of interpreting
a generalized externality due to improved
education would be hard to justify
using evidence like this. Strong links of the
residual term to R&D expenditures' would
suggest a high degree of persistence among the
leaders in TFP improvement. So also (probably)
would economies of scale associated with
the scale either of the firm or of the industry.
Such economies are not likely to jump wildly
around from one industry to the next, from period
to period. One would expect them to embody
characteristics of the productive process
that would be relatively stable over time;
hence they should show a reasonably high degree
of persistence, over time, in terms of the
TFP experience of particular industries.
No economist can look at Table 2 without
thinking of its close analogy with a Lorenz
curve. That, indeed, was the next step I took
in trying to represent the degree of concentration
of real cost reduction. Figure 1 (drawn
from Edgar Robles, 1997), shows the quasiLorenz
curves for a 20-industry breakdown of
the U.S. manufacturing sector over four successive
five-year periods.

What strikes one immediately about Figure
1 is the characteristic "overshooting." I have
marked withi the first vertical line the point
where the rising curve crosses 100 percent on
the vertical axis. The interpretation is that in
1970-1975 the cumulative real cost reduction
of just 25 percent of manufacturing industries
(measured by initial value added) was equal
to the total RCR for manufacturing as a whole.
After that there are other industries producing
another 40 percent of the total, but their contribution
is offset by still other industries with
negative RCR during the period.
Corresponding to the 25-percent figure for
1975, we have around 12 percent for 1975-
1980, 48 percent for 1980-1985, and 40 percent
for 1985-1991. These are the fractions of
manufacturing industry which by themselves
were able to account for the full amount of real
cost reduction during the respective period, in
manufacturing industry as a whole.
The second vertical line in each panel of
Figure 1 marks the maximum point of the
curve. The interpretation is that about 64 percent
of industries enjoyed real cost reduction
during 1970-1975, with the remaining 36 percent
suffering real cost increases (declining
TFP). For the subsequent periods, the corresponding
figures are 65(35) percent, 78(22)
percent, and 82 ( 18 ) percent. Here the first figure
is the percent of industries enjoying real
cost reductions; the figures in parentheses represent
those experiencing declining TFP.
Some interest attaches to the ordinate of the
maximum point on each curve. In the first period,
TFP growth ended up accounting for close
' For a review of the current status of analysis of R&D
expenditures and their impact on economic growth see
Griliches (1994).


### ---Economics-1998-0-08.txt---
to 170 percent of the RCR for total manufacturing.
In 1975-1980 this figure was about 240
percent, in 1980-1985 only about half that, and
in 1985-1991 a little more than 125 percent.
The trouble is that when the aggregate TFP contribution
is relatively small, the cumulative total
of the positive contributions is a large multiple
of that aggregate, while when the aggregate is
large, this multiple tends to be smaller. Thus, for
1970-1975 and for 1975-1980, the total RCR
in manufacturing as a whole was only about 2.3
percent of initial manufacturing value added. In
contrast, the total RCR for all manufacturing was
almost 10 percent of initial manufacturing value
added in 1980-1985, and about 7.5 percent in
1985-1991.

The problem obviously becomes greatly compounded
if the real cost reduction for the aggregate
(in this case total manufacturing) turns out
to be negative. Special conventions would have
to be established to make clear the interpretation
of Loreinz-like diagrams in such cases.
I believe I have hit on a felicitous way of
solving all these problems, and at the same
time creating an even better, clearer visual representation
of the degree of concentration or
dispersion of real cost reduction among the
components of an aggregate. The idea is simply
to relabel the vertical axis of the Lorenzlike
diagram, making it represent an annual
growth rate. For simplicity, think of a 30-
degree line as representing 1 percent per annum
of TFP growth. The rest of the vertical
axis would be calibrated accordingly. Thus, by
looking at the slope of a simple chord, we
could visually assess how rapid was the TFP
growth of the aggregate in question.
Figure 2 is presented simply for didactic
purposes. Here we have a hypothetical industrial
branch made up of four industries, A, B,


### ---Economics-1998-0-09.txt---
.01

Gum. Rate

of TFP

Cum. Sum G rowth

Real Cost

Reduction

/.005

0 0

0 1

Percentile of initial value added
.0105 - <-0

Cum. Rate

of TFP

Cum. Sum Growth

Real Cost

Reduction

_~~~~~~~~~~~~~~~~~~~ t .0025

0 1

Percentile oi initial value added
.005 - .005

Cum. Rate

ofiTF P

_ Growth

Cum. Sum

Real Cosa

Reduction

w ~~~~~~~~~~~~~~~~~~~~0

-005 - | -005

01

Percentile of initial value added
FIGURE 2. ILLUSTRATIVE TFP GROWTH PROFILES (SUNRISE-SUNSET DIAGRAMS)
C, and D. First, we order the industries in descending
order, according to their rates of TFP
increase in the period. Then we calculate cumulative
real cost reduction (a real dollar
amount) and plot it against cumulative initial
real value added. Then we scale the vertical
axis so as to comply with whatever metric we
have decided upon for the TFP growth rate (in
the example, a 30-degree line representing a
1-percent annual TFP growth rate), and the
horizontal axis so as to add up to 100 percent.
In the lower panel of Figure 2 1 give examples
to show how these diagrams cope with
the problems of a low TFP growth rate (the
overshoot for the case of 0.25-percent growth
would show up peaking at over 400 percent in


### ---Economics-1998-0-10.txt---



### ---Economics-1998-0-11.txt---
a Lorenz-type diagram) and of negative TFP
growth (where it is hard to even conceptualize
a Lorenz-type picture).

I first presented these diagrams before a
large audience at the Western Economic Association
meetings in Seattle (July 1997), and
for that presentation coined the label of "sunrise
diagrams" on their analogy with the sun
rising over a hill. That same evening Yoram
Barzel suggested that where the aggregate
slope is negative, we apply the term "sunset
diagrams," which I immediately accepted.
Figure 3 presents a set of sunrise-sunset
diagrams based on Jorgenson et al. (1987 pp.
188-90). These cover 32 industrial sectors
(their 35 minus Agriculture, Trade, and
Government Enterprises). I think the utility
of sunrise-sunset diagrams needs no further
championing once these pictures are examined
and digested. Practically all variants are
represented in these real-world cases: low
TFP growth with a huge overshoot (1953-
1957 and 1969-1973); negative growth
with large and moderate overshoots (1966-
1969 and 1973-1979); moderate growth
with small (1979-1985), medium (1960-
1966), and large (1948-1953) overshoots.
One striking fact that emerges from this set
of pictures is how variable across periods is
the negative contribution of the losers. If the
losers had only contributed zero change in
TFP, we would have had cumulative TFP contributions
of about 0.8 percent per annum in
1948-1953, in 1957-1960, and in 1960-
1966. And the other periods would not have
been much different: about 0.7 percent in
1953-1957 and in 1969-1973, 0.6 percent in
1966-1969, and 0.5 percent in 1973-1979
and 1979-1985. Instead of this narrow range
of cumulative contributions, we have an actual
distribution that goes from -0.9 percent in
1973- 1979 through around 0.1 percent in
1953-1957 and 1969-1973 to over 0.5
percent in 1960-1966 and 1979-1985.
Does this not suggest that we make a major
research push trying to improve our understanding
of the phenomenon of negative TFP
growth? What syndromes characterize the firms
and industries experiencing it? How much of it
stems from external shocks like international
prices? How much of it from competition
within the industry? How much of it represents
firms struggling to survive, yet experiencing
output levels well below their previous peaks
(and presumably below installed capacity)?
How much of it represents things like "labor
hoarding" as firms go through periods of
adversity?

IL. Yeast versus Mushrooms: Part II
I hope that in the previous section I have
made a convincing case concerning: (a) the
usefulness of sunrise-sunset diagrams, (b) the
aptness of the "yeast versus mushrooms" dichotomy,
and (c) the pervasiveness with
which the mushroom side of that dichotomy
seems to come out ahead when the GDP is
broken down into industries or industrial
branches for TFP analysis. The grand design
that emerges from the studies reported here,
and from just about all the other industry
breakdowns that I recall having seen, is that:
(i) a small-to-modest fraction of industries
can account for 100 percent of aggregate real
cost reduction in a period; (ii) the complementary
fraction of industries contains winners
and losers, the TFP contributions of
which cancel each other; (iii) the losers are
a very important part of the picture most of
the time, and contribute greatly to the variations
we observe in aggregate TFP performance;
and (iv) there is little evidence of
persistence from period to period of the leaders
in TFP performance.

The above results are, I think, very interesting
(in the sense of piquing our curiosity)
, very strong (in terms of their implications
about the nature of the growth
process), and very robust (in the sense that
they have wide applicability over different
data sets analyzed by different authors using
at least somewhat different methods). But
these results, so far, are quite compatible
with what I might call an "industry view"
of the TFP story. This is the way I, myself,
looked at the growth process until quite
recently-a vision that was reflected in my
stories about rubber tires and autos in the
1920's, refrigerators and other household
appliances in the 1930's, pharmaceuticals in
the 1940's, etc. The image that I had in
mind was one of yeast within each industry
and mushrooms between industries-a
commonality of TFP experience by firms within'
an industry, depending on that industry's


### ---Economics-1998-0-12.txt---
luck in the technological draw, side by side
with highly diverse experience between industries
because the distribution of technical advances
had wide dispersion, even for periods
as long as a decade.

Getting access to data at the firm level permits
one to explore whether this view is compatible
with the actual experiences of firms and
industries. We are just in the early stages of this
exploration, but I think the result is quite clear
already; namely, the "mushrooms" story prevails
just as much among firms within an industry
as it does among industries within a
sector or broader aggregate. I will present here
only a taste of the evidence from the United
States (on which our systematic work just recently
got started). Our massive evidence
comes from the Mexican manufacturing sector,
for which Leonardo Torre ( 1997) has analyzed
data from a sample of over 2,000 firms. A small
fraction of these firms were lost owing to missing
data, but some 1,900 firms remained in the
sample that Torre finally worked with. These
firms were divided into 44 branches of industry,
so that on average we have about 43 firms per
branch.

There are really too many ways to present
such a mass of information as is contained in
Torre's study. What I will do here is give the
aggregate picture in Figure 4, and then show
in Figures 5A-C three fast-growing branches,
three of around median growth, and three from
among the slowest-growing branches.
To complement these figures, I finally present,
in Figures 6A-D, certain summary statistics
from the sunrise-sunset diagrams of the 44
branches that Torre studied. Here Figure 6A
gives the distribution of average rates of TFP
growth among the 44 industries. Figure 6B
shows the distribution of peak cumulative contributions,
i.e., what the TFP contribution
would have been had all the negatives been
zeros. Figure 6C displays the percentile of
firms (by initial value added) marking the borderline
between positive and negative TFP
growth. And finally, Figure 6D shows, for
branches with positive TFP growth, the percentile
of firms which, by themselves, account
for 100 percent of the industry's TFP growth.
This evidence almost seems to replicate, for
firms within an industry, what was found in
the previous section for industries within the


### ---Economics-1998-0-13.txt---
MexAcan Manufacturing: Soft Drinks [66 Establishments]
.06 -

Cum.

Rate .03- _ .0302

of TFP -.0204

Growth 0

-.03

-.06 -

0 .2 .4 .6 .8 1

Percentile of Initial Value Added
Mexican Manufacturing: Cement [79 Establishments]
.06 - Cum.

Rate ______________ _- .0337

of TFP-03 -

Growth .0159

0 '- 0

-.03

-.06

Percentile of Initial Value Added
Mexdcan Manufacturing: Other Wood Prods. [43 Establishments]
I I I I ~ ~~~~~~~ I

.06 - Cum. Rate

of TFP.03 - .0179

Growth - = =. . 0026

O- ~ ~ ~ ~ ~~ ___________ .002
-.03

-.06 5A. T T L F W N E A R 8

6 .2 ~Percentile of InitalIau AddA
FIGURE, 5A. TFP GROWTH PROFILES FOR FAST-GROWING BRANCHES (MEXICAN MANUFACTURING, 1984- 1994)
economy-rampant overshooting of sunrisesunset
diagrams, great influence of firms with
negative TFP growth in determining the TFP
outcome for an industry, and a small or moderate
fraction of firms accounting for 100 percent
of the TFP growth of an industry (when
that growth is positive), with the complementary
fraction being winners and losers whose
efforts end up just offsetting each other. It remains
to try to give some interpretation to
those results.

III. "Just Errors" or "It's a Jungle
Out There?"

The first question that will enter the mind
of many economists on looking at the evidence
presented so far is: how much of what


### ---Economics-1998-0-14.txt---
we have seen and emphasized might simply
be the result of errors of observations? This
is by no means a frivolous question. For one
can actually create frequency distributions
of rates of TFP increase which contain exactly
the same information as the sunrisesunset
diagrams previously presented. The
only trick is to count as the unit of frequency
not one firm (out of an industry aggregate)
or one industry (out of some larger aggregate)
but, instead, say, 1 percent of the total
value added of the aggregate. Thus a firm
with 20 percent of the value added of an industry
would appear with 10 times the
weight of a firm accounting for 2 percent of
the value added of that industry. In such a
chart, the cumulative frequency (say, 68 percent)
above ATFP = 0 would represent the
projection on the horizontal axis of the
maximum point on a sunrise diagram. Its


### ---Economics-1998-0-15.txt---
MexAcan Manufacturing: Spinning (87 Establishments]
.06 -

Cum.

Rate 03

of TFP

Growth .. ... L .006

0- 0

-.03- .= -.0395

-.06 - d F Percentile of InitialA'aue AddA
Mexcan Manufacturing: Wearing Apparel [90 Establishments]
.06 -

Cum.

Rate .03-

of TFP .0086

Growth

0 0

-.03

-.038

-.06

Percentile of initia.'alue Added
Mexican ManufactLring: S nthetic Resins [35 Establishments]
.06

Cum.

Rate .03 -

of TFP

Growth .- e ............... ....... -.0059
0 0

-.03-

-.0377

-.06RE SC. TPRW PRIEFRL -Percentile of lnitiaNH(aluE Addd
FIGURE 5C. TFP GROWTH PROFILES FOR SLOW-GROWING BRANCHES (MEXICAN MANUFACTURING, 1984-1994)
complement (32 percent) would represent
the initial value added associated with negative
TFP performance during the period.
If, then, all the information could be generated
by a properly designed frequency distribution
of rates of TFP growth, could it not
all be the result of chance alone-more specifically,
of errors of measurements? I really
think not-my favorite quip on this is that
"white noise does not sing a tune." That is, if
we can rationalize what we see in terms of an
analytical framework which embodies wellestablished
economic principles and sensible
presumptions about underlying relationships
and facts, this is itself strong evidence against
the white noise hypothesis.

Nonetheless, we have to face the fact that
errors of observation of some magnitude certainly
do exist, and we must recognize that
they can cloud our perceptions and bias our
results. What I am going to do here is consider
frequency distributions of firms. TFP is measured


### ---Economics-1998-0-16.txt---
in two ways-one using value added by
a set of firms on the one hand, and the other
using "output" by those same firms, measured
through dividing value added by separate
estimated firm-by-firm price (of value
added) indexes pj. For these purposes we can
conveniently think in terms of logarithms, so
let:

V = observed value added of firm;
pj = estimated firm-level price index;
y, = v, - p, = estimated output;
Vj = Tj + ej [Tj =true value added];
p= = 7rj + Uj [7r, = true price index]; and
qj = T- 7r, [true output of firm].
We would like to have data on cj and its
variance

2 2 +

If we simply work with observed value added
as our quantity variable, we get
a?2 = a2 + a? (assuming at0 = ?)
If we worked with the measured yj, we get
2= ao + a? + a2 + u2- 2at

(assuming a and e to be strictly random).
My presumptions are as follows:
(i) We can estimate value added quite accurately
at the firm level. Hence the presumption
that ao is small.

(ii) In most industries, there is considerable
variety among the firms and their products.
Hence, except in cases of industries
with very homogeneous products, we
should not expect a2 to be small. Hence,
I expect a > a?

(iii) Finally, we have the presumption that, at
least at the level of firms within an industry,
o7 < 0. We know that firms

choose to operate in regions of the demand
curve where they consider the elasticity
facing them to be greater than one.
But also, in an analysis of the growth
process, one would expect the big gains
in value added to accrue to those firms
in an industry which were passing along
to consumers some of the fruits of current
or past real cost reductions.

These three presumptions lead me to the conclusion
that ao is likely to understate the true
variance of output ao (because - 2JT > 0 and
ao2 > sa?), and that ao is likely to overstate
0o2 (only the covariance terms with e and u,
which were assumed to be zero, could make it
otherwise). And since Torre worked with real


### ---Economics-1998-0-17.txt---
24-

20-

16-

U

or a,~~~~~~~~~~~

m~

LL 1

6

4

-6 -4.5 -3 -1.5 0 1.5 3 4.5 6

TFP Growth Rate

FIGURE 6B. MAXIMUM AVERAGE ANNUAL TFP GROWTH RATE (MAX. ORDINATE OF TFP PROFILE):
MEXICAN MANUFACTURING, 1984-1994
FREQUENcy DISTRIBUTION, 44 INDUSTRIAL BRANCHES
value added as his quantity variable, this suggests
that, if anything, the substitution of the
"true quantity variable" q for observed value
added v would have given results with
greater dispersion of TFP, and consequently
greater overshooting in his sunrise-sunset
diagrams.

The above demonstration should be taken as
merely suggestive. It is not important to me
that Torre' s results underestimate the variability
of the different firms' TFP experience. It is
only important that measurement error should
not be the principle determinant of those results.
On this I feel very confident. In my view,
it really is "a jungle out there," with winners
and losers in every period-good as well as
bad.

As I have noted earlier, we are only just beginning
a systematic study of TFP among U.S.
firms, so I can offer no display comparable to
Torre' s.

However, Robles (1997) did examine the
experience of 12 firms in the U.S. oil industry.
His results are summarized in Figure 7. But
Robles tells basically the same story as Torre.
Three firms out of the 12 were more than sufficient
to generate the real cost reduction experienced
by the total group. Half (or almost
half) of the firms had negative TFP growth in
each period. And the cumulated amount of this
negative TFP growth was sizeable when measured
against the total TFP performance of the
industry.

What I see in TFP performance is quite analogous
to what I see in the stock market pages of
the newspaper. There are winners and losers
eveiy day, every month, and every year. The
gains and losses come from all sorts of causes.
World price shocks can drive firms into negative
TFP performance if the consequent output reductions
are greater than the reductions of inputs.
So, too, can cyclical or secular declines in
demand, including those caused by the successful
actions of competitors.

When firms are under stress, they typically
fight to stay alive. Maybe they fight for too
long in some cases, in the sense that less of
society's resources would be wasted if they


### ---Economics-1998-0-18.txt---
12 -

10 -

8

C-)

CT

U-

4

2

0

0 12.5 25 37.5 50 62.5 75 87.5 iQO
Percentiles

FIGURE 6C. PERCENTAGE WITH POSITIVE TFP GROWTH: MEXICAN MANUFACTURING 1984-1994
FREQUENCY DISTRIBUTION OF PERCENTILES, 44 INDUSTRIAL BRANCHES
were to quit earlier in response to a challenge
that turns out to be deadly. But they do not
recognize the challenge as deadly, so they
keep struggling to survive. I believe this is part
of the nature of entrepreneurs, CEOs, and
business leaders in general. They would not be
where they are, doing what they are doing, if
they were ready to quit at the first sign of a
challenge. They are fighters by nature, and
they probably would not have achieved success
if they were not.

Firms with negative TFP growth may even
be innovators. New challenges come and different
firms think of different ways to respond
to them. Some (like Intel and Microsoft) end
up winners; others (Montgomery Ward and
Apple?) end up losing. But it may not be that
they just waited passively and tried to fight to
survive in the face of negative shocks. They
may have had quite innovative ideas, with decent
prior probabilities of success, but in the
end success did not come. Thus, negative TFP
performance can, and I believe often does,
come simply from "backing the wrong horse."
To me, Joseph A. Schlumpeter's vision
(1934) of "creative destruction" captures
much of the story. What he is saying is, yes,
it's a jungle out there, but the processes of that
jungle are at the core of the dynamics of a
market-oriented economy. They are what got
us to where we are, and they hold the best
promise for further progress in the future.2
In my opinion, Schumpeter saw through to
the essence of the problerrm, but it is not wise
for us to be fatalistic in accepting his vision.
We cannot lose by making a major effort to
understand the process of TFP improvement
where it happens-at the level of the firm.
This is all the more true because of the


### ---Economics-1998-0-19.txt---
12

8-

U-

4

3

2

1 --- --- --- ---

25 50 75 100

Percentiles

FIGURE 6D. PERCENTILE WHERE CUM. SUM TFP CONTRIBUTION = 100 PERCENT:
MEXICAN MANUFACTURING 1984-1994
pervasiveness of negative as well as positive
TFP performance among the components of
almost any aggregate. By learning more about
this aspect of the aggregate picture, we may
stumble upon ways to "accentuate the positive
and eliminate the negative" parts of the TFP
story. But that is too quixotic a goal to take as
the point of focus right now. To me, the present
task is simply to get hold of the huge mass
of information that is available at the firm
level and squeeze it hard enough to wring out
as much understanding and as much insight as
we can.

IV. Some Observations on Methods
and Research

What I am about to say in this section is not
meant to consist of direct implications of what
has gone before. Instead, I think of the earlier
parts of this paper as building a case for a certain
vision of the economy, and of how the
forces of growth work within it. This vision in
turn leads one to think in different ways not
only about the growth process itself but about
how we, as economists, might best advance
our study and understanding of it, and how
policies might be molded so as better to promote
it.

(a) It is always wise to study the components
of growth separately. The rate of investment,
the rate of return on capital, the rate
of growth of the labor force in numbers or in
hours worked, the contribution of human capital
or of the increment in average quality of
labor, and the residual representing real cost
reduction-all these are sufficiently different,
and potentially sufficiently disjoint from each
other, to merit their being treated separately. I
would give special emphasis to the following
three points.

(i) The worthwhileness of measuring the
rate of return and emphasizing its role in
the growth process.

(ii) The importance of focusing on investment
rather than saving in studying the


### ---Economics-1998-0-20.txt---



### ---Economics-1998-0-21.txt---
process of growth. Saving is an interesting
topic in its own right, but the more
''open economy" is the situation being
studied, the less saving has to do with
investment. Saving takes on great importance
in closed-economy models focused
on aggregate growth, in which

case it is equal to investment. It gets to
be almost meaningless as one focuses on
the growth of cities and regions, or on
firms and industries.

(iii) The importance of viewing the residual
as an umbrella covering real cost reductions
of all kinds, and of recognizing that
we are closer to home thinking that RCR
takes 1001 forms than that it can be well
represented by one or two or three
aggregate-style variables.

(b) In principle, the accumulation of human
capital by the laborforce should be represented
in the labor contribution of the
growth equation, or in a bifurcation of this
contribution into one due to raw labor, the
other to human capital. It is in a term like
iwiALi that one captures the shifting skill
composition of the labor force. In particular,
we capture here the higher wages that are the
fruits of investment in education and training,
which are the benefits that the workers themselves
perceive. These should be kept separate
from any externalities education might
have.

It is important to try to keep this internalized
part of the story out of the residual, so that we
can straightforwardly interpret the residual as
real cost reduction.

(c) To study externalities due to education,
training, or human capital, we should not be
content with broad generalizations such as
"TFP growth is higher in entities with lots of
human capital per worker." We should try to
figure out how this externality works. Is it
higher forfirms with high incidence of human
capital? Is it higher for industries or sectors?
Or are human capital externalities more spatial
in nature, making more efficient the economic
life of the cities, provinces, states, or nations
which have high concentrations of human capital?
And if this is a fruitful trail to pursue, at
what type and size of geographical units do
these externalities typically work?
(d) The same goes for economies of scale.
We should not be satisfied with vague attributions
of economies of scale, say, at the level
of the national economy. Instead, we should
pursue the matter. If the economies of scale
are national, through what channels do they
work, and what evidence do we have to look
at to see them in operation? In particular, what
is their connection to real cost reductions
where they really happen-i.e., at the level of
the firm? Economies of scale at the levels of
the firm and the industiy are easier to visualize.
Here, too, however, the task is to check
them out-to see if the real cost reductions of
firms are linked to the initial sizes of those
firrns themselves, or of the industries in which
they operate, and of the direction (up or down)
in which output is moving.

(e) Perhaps most important of all, we
should really try to take full advantage of evidence
at the firm level. I think particularly of
identifying considerable numbers of outstanding
cases of TFP improvements and TFP decline,
and studying them one by one to try to
ferret out the sources of their big real cost reductions
and real cost increases. You can be
pretty sure, if there have been big real cost
reductions in a firm, some people in that firm
have a pretty good idea of where those reductions
came from and how they were accomplished.
By capturing this grassroots evidence,
we can put some added discipline into our ruminations
about the nature of TFP at the aggregate
level. We must follow up on the sort
of work pioneered by Jacob Schmookler
(1966) and Edwin Mansfield (1995). In general,
our aggregate story should be compatible
with, and comfortably contain, what we see at
the grassroots level. In particular our overall
picture of TFP improvement should comfortably
accept the overwhelming evidence of the
"'mushroomss" rather than "yeast" nature of
the process.

(f ) Special urgency applies to the study of
declining total factor productivity at both the
firm and the industry levels. The pervasiveness
of declining TFP is perhaps the most profound
conclusion to emerge from the empirical links
that I have reported here. As a profession, we
obviously have been aware of its existence at
the industry level for virtually all studies that


### ---Economics-1998-0-22.txt---
give a breakdown by industry reveal it. Yet to
my knowledge, we have barely scratched the
surface in studying it. I find it hard to think of
more fertile soil for future research on the process
of economic growth.

(g) I do not think that we gain much by
trying to express the relation between policies
and economic growth by a series of regressions.
Cross-country growth regressions seem
hopelessly naive to longtime observers of the
growth process like myself. To us, there is too
mutch to question in regression lines that draw
much of their slope from the differences between
Sudan and Switzerland, between Bangladesh
and Brazil, or between Ceylon and
Canada. In contrast, it seems much more sensible
to look at episodes within individual
countries and to search for common elements
that characterize the passage from bad to good
growth experiences within each of the number
of countries, and for those elements that seem
to describe the good growth experiences on the
one hand and the bad ones on the other. I think
we can reach in this way a good appreciation
of the nature of the growth process, without
resorting to the straitjacket of regression lines
that seem to draw from comparisons among
very disparate countries, lessons that are supposed
to be meaningful for countries like
Bangladesh, Ceylon, and the Sudan-as well
as others at different levels-as each strives
to take the next upward step in the climb toward
modernization.

My view of cross-country growth regressions
is somewhat less negative to the extent
that they focus on the components of growth
(rate of investment, rate of return, and real
cost reduction in particular) rather than on the
overall growth rate. There is also a subtle distinction
to be drawn between two ways of
presenting cross-country regressions- (i) as
"explaining" why and how some countries
grow faster than others (not recommended),
and (ii) as simply summarizing a series of
"stylized facts" describing the experience we
observe (far preferable, and not just for its being
more modest in its claims).

V. Some Policy Implications

In approaching the question of the influence
of policy on real cost reduction in particular,
and, to a degree even on economic growth in
general, I believe that the key words are "obstructing"
and "enabling." We know from

sad experience how easy it is for governments
to adopt policies that get in the way of economic
growth and even turn it negative. We
know, too, that there is no "silver bullet," no
single magic key that by itself opens the door
to a paradise of prosperity and growth.
Broadly speaking, the easiest starting point for
a successful growth experience is a onceprosperous
economy that has suffered from
bad policies. Releasing that economy from its
trammels, correcting an accumulation of past
mistakes, can sometimes set in motion a prolonged
episode of astounding growth. A shift
from policies that obstruct to policies that enable
growth seems to lie at the heart of growth
"miracles" like those of Taiwan, Spain, Korea,
Brazil, Indonesia, Malaysia, and China
(among others).

The springboard for the following listing of
policy implications is the interpretation of the
growth residual as representing real cost reduction
and the ready acceptance that in the
real world RCR comes in 1001 different
forms.

(a) The first key observation is that people
must perceive real costs in order to reduce
them. Hence, policies that impede the accurate
perception of real costs are ipso facto inimical
to growth. Inflation is the most obvious, probably
the most pervasive, and almost certainly
the most noxious of such policies. If I have any
expertise based on experience in economics, it
has to be in the first-hand observation of processes
of serious inflation. So I ask you to
take my word for it: the most serious cost of
inflation is not a triangle or a trapezoid under
the demand curve for real cash balances, nor
is it the inflation tax. The most serious cost
of inflation is the blurring of economic
agents' perceptions of relative prices. This
happens because individual prices adjust in
different ways and at very different rates. A
high product price and a low input cost normally
is an invitation clamoring for new investments
to be made. This is not so during a
serious inflation, when such a signal can easily
turn out to be "here today, gone next
month" as both product and input prices continue
on their separate paths of adjustment to


### ---Economics-1998-0-23.txt---
the ongoing inflation. Without exception, in
my own observations, the higher the rate of
inflation, the worse is its effect in blurring
agents' perceptions of relative prices. In an
inflation at, say, 20 percent to 50 percent per
year, people see prices as in a morning haze;
in one of 20 percent to 50 percent per month,
they see them as in a London fog. Many empirical
studies exist showing that serious inflations
are seriously inimical to growth. (See
William Easterly, 1996.) The clouding of
perceptions of relative prices is an important
reason why-for it gets in the way of successful
real cost reductions at the level of the
individual firm.

Inflation also inhibits growth in other, perhaps
more obvious ways:

(i) by diverting energies from more productive
activities to the search for mechanisms
of inflation protection;

(ii) by reducing (often very drastically) people'
s real monetary balances, thus impacting
negatively on the real amount of
credit the banking system provides to the
productive sector; and

(iii) somewhat related to both (a) and (b),
by causing people (both "here"' and
abroad) to invest abroad some of the
funds they would otherwise have invested
"here," or (what is very close
to the same thing) by accumulating
hoards of hard currencies as an inflation
hedge.

(b) A second policy implication is, in the
words of my friend and longtime collaborator
Ernesto Fontaine, avoid "prices that lie"
(precios mentirosos). Talking about inflation,
we focus on the blurring of the signals
that the price system gives; here we focus on
its giving wrong signals due to distortions
that have been introduced, usually as a direct
consequence of government policies. No
good can or did come, in terms of economic
efficiency, from tariffs of 50 percent and 100
percent and more, giving effective protection
often of 200 percent and 300 percent
and more. Nor can growth be fostered by
heavy-handed price controls and interventions
in credit markets.

I am not being a religious purist here-just
as big distortions have big costs, small distortions
typically have small costs, and all economies
are distorted to some degree. The
message here is that economies have to pay
the price for the level of distortions they
choose to have, and that one of the important
components of that price is that distortions
create situations where what is truly a saving
of private costs is not a genuine saving of costs
from the point of view of the economy as a
whole.

(c) Just as bad, and often even worse than
direct distortions, are the excess costs imposed
on an economy by ill-conceived regulations
and bureaucratic hurdles. Hermando DeSoto
(1989) has made the exposure of these trammels
in Peru into what has become virtually
his life's work. Clear rules of the game are
an essential and integral part of a wellfunctioning
market economy, but all too easily
these get supplemented by others that make
investment, production, marketing, sales, new
product development, etc., more costly. Labor
laws have been particularly troublesome, often
adding artificially to the cost of labor and giving
firms a strong incentive to avoid hiring
new workers, simply because of the high costs
associated with any later dismissal of them.
But there are loads of other items-the need
for approvals, sometimes a dozen or more,
before undertaking some investment or some
new venture; regulations that one way or another
impede new entry, so as to protect strong
vested interests ( small retailers being protected
against supermarkets in many countries)
; and the complexity of tax codes and
their enforcement, which imposes large compliance
costs on business firms and individuals.
Somehow, countries interested in
promoting growth should find ways of paring
their regulatory frameworks down to those
rules and requirements that are really justifiable
in terms of their costs and benefits to the
economy and society at large.

(d) Although international trade distortions
(tariffs, quotas, licenses, prohibitions,
etc.) might be subsumed under points (b) and
(c), their importance merits a separate heading.
The move to openness (from a protectionism
that sometimes bordered on autarchy)
has been one of the main hallmarks of the
growth miracles of the past half-century [see


### ---Economics-1998-0-24.txt---
Sebastian Edwards (1993) and Anne 0.
Krueger (1985, 1997)]. Just as inflation has
costs beyond the area under the demand curve
for real cash balances, so protectionism seems
to have burdens that go beyond the standard
triangle-trapezoid-rectangle measures of the
costs of trade distortions. There are at least two
quite natural explanations: first, that openness
helps grease the wheels of the international
transfer of more modern technologies, and
second, that firms that may once have relaxed
in ease and comfort behind high protective
barriers end up having to sink or swim once
forced to compete in a much more openeconomy
setting. Under either explanation,
trade liberalization opens up new paths of real
cost reduction, thus providing additional impetus
to economic growth.

(e) The recent wave of privatizations
among both developed and developing economies
may have important effects in enabling
real cost reductions that otherwise might have
been delayed, or not have happened at all. It
is, I believe, fair to say that in most countries
state-owned enterprises operate under a series
of constraints that seriously get in the way of
real cost minimization in a comparative-static
sense and of real cost reduction in a dynamic
sense. These constraints sometimes limit the
salaries of executives, sometimes impose
onerous conditions on the firm as it employs
lower-skilled workers, often limit the capacity
of the firm to shut down inefficient lines of
production, and almost always make it difficult
to fire workers, etc. To my mind, however,
perhaps the worst attribute of state-owned enterprises
is the ethos that often evolves inside
of them-an ethos where middle managers
are well advised to "leave well enough
alone," "not rock the boat," and "not invite
trouble." This ethos flies in the face of a vision
of the growth process that gives a huge role to
the search for real cost reductions at the grassroots
level, and that recognizes the tumult that
accompanies "creative destruction" in all its
forms. I thus must applaud the contemporary
trend toward privatization. If I harbor any
qualms in this connection, they concern the
degree to which many privatizations have been
carried out in too much haste and with too little
care, often motivated by purely fiscal considerations
rather than by a general search for
economic efficiency. This nlay have led to gratuitous
transfers of wealth in some instances
and to the planting of newly private enterprises
in soil that was not properly prepared (e.g.,
still lacking a sound regulatory framework for
electricity rates, or intelligent rules promoting
competitiveness in at least some aspects of
telecommunications, etc.)

(f) One cannot complete a list like this
without mentioning something that most of us
simply take for granted-a sound legal and
institutional framework in which individuals
are protected against arbitrary incursions on
their property and other economic rights. This
very basic point-recently much emphasized
by Douglass C. North ( 1990), Robert J.
Barrow and Xavier Sala-i-Martin (1994),
Mancuir Olson, Jr. ( 1996), and Barro ( 1997 )
is at least potentially a vital element for a sustained
process of successful economic growth.
If it is true that spurts of growth have sometimes
occurred in the absence of such a framework,
it is also true that most cases of
sustained growth over long periods of time
have benefitted from a sound institutional and
legal environment.

(g) Somewhat related to the above is the
element of political consensus concerning the
broad outlines of economic policy. We have
learned from experience that very admirable
policy reforms can take place, yet end up having
little effect. This can happen because a
new government comes in and reverses the reform.
But it can also happen because people
fear that a new government will come in and
reverse the reforms later on. At the moment,
the Chilean economy is one of the jewels of
economic growth (and general economic success)
in Latin America. Many people point to
the thoroughness and pervasiveness of Chile's
economic reforms over the last two decades or
so. But not so many point to the fact that the
reform package has remained essentially intact
through several changes of ministers, and even
more i-mportant, through two presidential elections
in which the winners came from the opposite
side of the political fence from the
government that initiated the reforms. The
confidence in the economic order of things instilled
by this sequential endorsement of the
basic framework of economic policy has to be


### ---Economics-1998-0-25.txt---
one of the important reasons for Chile's continued,
very impressive economic performance.
And it is important, also, in the context
of this paper. Living in a world in which real
cost reductions are a key dynamic force producing
economic growth, we must look to the
motivations and preoccupations of those who
take the critical decisions at the level of the
firm. For these decisions, it is not only important
that the policy framework be good now;
the expectation that it will stay good in the
future is also important. Otherwise, investments
will tend to be limited to those with
short horizons and payment periods, and much
soil, fertile with longer-term economic opportunities,
will go unplowed.

VI. A Vision of the Growth Process
Let me now try to summarize my own vision
of the growth process-the major elements
of which have been presented here. In
the first place we have the five standard pillars
of growth-the rate of increase in the labor
force, the rate of increase in the stock of human
capital, the increase in the capital stock
(net investment as a fraction of value added),
the rate of return which that investment will
yield (or can be expected to yield) and, last
but not least, real cost reductions stemming
from 1001 different sources.

Commenting on these in turn, I would note
that increases in the labor force have taken on
new meaning in many countries as labor force
participation rates (particularly those of
women) have increased. Whereas with a constant
participation rate, the growth rate of the
labor force is just a proxy for the growth rate
of population, important increases in labor
force participation can lead, just by themselves,
to significant increases in measured
real income per head.

Concerning increases in the stock of human
capital, my conviction is that most of their
contribution to growth is, on the whole, well
measured by market wages, as in the expression
XiwiALi. This does not deny the existence
of externalities due to an increased
human capital stock; it simply judges their influence
on the growth rate to be modest in
comparison with the effects of education,
training, learning-by-doing, etc., that can be
(and typically are) internalized by those who
receive them. We therefore look for the effects
of human capital accumulation mainly in the
term YjwjALj, and only (via externalities) as
one of many elements underlying the growth
residual R'.

The rate of investment is a veteran on the
stage of growth analysis. What I would emphasize
here is the importance of maintaining
a clear separation between the rate of investment
and the rate of saving. Models (like
those of the representative consumer) in
which saving and investment are always
equal are not much use even for analysis at
the national level in our modern, interdependent
world. They are even less useful as
one goes down to smaller geographical
regions, and simply cease to make sense as
one studies the growth process at the levels
of the industry and the firm.

The rate of return to investment has in
many ways been the orphan of our growth
analysis, having been masked from view by
our typical representation of capital's contribution
to the growth rate as Sk (AK/K). Here
the rate of return (p + 6) is totally hidden
from view. I deeply urge that more of us get
into the habit of representing this same term
as (p + 6)(AKIy). I want to see more attention
paid to the rate of return because it plays
such a central role in the motivation of economic
agents, and also because changes in it
are such an important element in understanding
and explaining the growth residual, R'.
Table 3 helps explain why I feel this way.
This table is adapted from Harald Beyer's
(1996) work. He carried out an analysis of
the growth experiences of 32 countries ranging
from Sri Lanka to the United States on
the income scale, and from Iceland to Australia
on the geographic scale. In Table 3 we
present results for his ten countries with the
highest and for his ten countries with the lowest
GDP growth rates from 1971-1991. In the
second column the calculated average annual
rate of return is shown. In the third column
we have capital's contribution to the growth
rate [z (p + 6) (AKly) ], and in the final column
the estimated average annual rate of TFP
growth, all over the same time period.
Table 3 shows an unequivocal tendency for
fast-growing countnes to be experiencing high
rates of return as well as high capital contributions
and high rates of TFP improvement.


### ---Economics-1998-0-26.txt---
This is all the more interesting because in the
calculation of TFP a higher level of the rate of
return operates to reduce the calculated TFP
(i.e., Ap is a positive component of R' and
should presumably be positively correlated
with it,3 but R' is found by subtracting p AK
from Ay; hence, in a sense, the level of p
should presumably be negatively correlated
with R'). What we are seeing here, in my
opinion, is a genuine syndrome in which all
sorts of good things go together. Strong real


### ---Economics-1998-0-27.txt---
cost reductions and high rates of return create
attractive investment opportunities which,
when acted upon, bring about a high capital
contribution to growth. It should be no surprise
that under such circumstances the GDP growth
rate itself tends to be high. It should likewise
be no surprise that the opposite syndrome
with weak real cost reductions and low rates
of return producing fewer interesting investment
opportunities-should end up being associated
with a low capital contribution and a
low GDP growth rate.

Finally, we come to the residual R' itself.
To me, the biggest message here is to recognize
the multiplicity of sources from which it
can (and I believe does) come, and the additivity
that nevertheless is its attribute. I think
that the term real cost reduction very neatly
captures both these aspects in a way that renders
it preferable to terms like TFP improvement
and technical advance-preferable not
in the sense of a mechanical definition (for
which all three are equally good), but in the
sense of better conveying the underlying nature
of the process to one's listeners or readers.
The next step is to recognize that of the five
main pillars, at least three (the rate of investment,
the rate of return, and real cost reduction)
are key foci of decision-making
processes at the level of the firm. I cannot escape
the conclusion that the great bulk of the
action associated with the growth process
takes place at the level of the firm. Hence, I
feel we should focus much more study than
we have in the past on what happens at this
level. And when we are not working at the firm
level, we should pay a lot of attention to what
happens at lesser levels of disaggregation like
industries and industrial branches.
Key insights flow from taking this kind of
focus. Few economists are aware of the pervasiveness
with which sunrise-sunset diagrams
are characterized by overshooting, or of
the importance that firms or industries with
real cost increases (i.e., reductions in TFP)
play in determining the aggregate rates of real
cost reduction that we see in such diagrams.
Here we have only scratched the surface in
digesting the evidence. But I find impressive
the degree to which the data of Table 3 seem
to point to a growth syndrome in which high
rates of return, high rates of investment, high
rates of real cost reduction, and high rates of
output growth all go together. I see in this result
the likelihood that real cost reductions are
the big driving force, generating high rates of
return and calling forth high rates of investment
and high output growth. This interpretation
is compatible with many exercises that
I have perforned over the years in which I
have tried to contrast high-growth with lowgrowth
experiences. In such exercises, as in
Table 3, the difference in rates of real cost reduction
has typically been a major "source"
of the difference in growth rates.
Also impressive in the analysis of the
Jorgenson data is the degree to which the varying
experiences of U.S. manufacturing in different
decades derives from different degrees
of bad experience (real cost increases) rather
than different degrees of good experience (real
cost reduction). It is as if the creative part of
Schumpeter's "creative destruction" was
more steady (for these decades in U.S. manufacturing)
than the destructive part, whetting
our (or at least my) appetite to look deeper,
inquiring into why this was so.
The Mexican data at the firm level were
somewhat more recalcitrant than the Jorgenson
data by industrial branch, but they nonetheless
give us a clear picture of lots of winners and
lots of losers, with the losers being strongly
characterized by falling real value added and/
or by falling real rates of return.4
4 Of Torre's observations with negative TFP growth,
over three-fourths had negative growth in real value
added, and over one-half had falling real rates of return.
Less than 15 percent showed increases both in real value
added and in their real rate of return. Many of Torre's
"anomalies" of negative TFP improvement together with
positive real output growth stem from very high rates of
return (p) being imputed to the observed increases (AK)
in the capital stock.

This points to a problem that extends to all (or nearly
all) growth-accounting frameworks. Implicitly, they assign
to new investment a marginal product based on the
average observed gross rate of return (p + 6) or average
observed capital share Sk. This makes little sense in cases
where the observed p is far above or far below the going
market rates. Firms earning 50 percent real return in their
historical investments are very unlikely to be "requiring"
such a high expected yield on new investments. Similarly,
firms going through periods of actual accounting losses
may often still be investing, but it is absurd to think they
are expecting (or typically getting) negative returns on
their new investments (AK). There are, I think, good reasons
for us to experiment with alternative ways of selectThe


### ---Economics-1998-0-28.txt---
role of policy in this vision of the
growth process is an "enabling" one. By creating
the circumstances where firms can
quickly and accurately predict opportunities
for real cost reduction and act on them, governments
can guide the economy toward an
enhanced contribution of RCR to growth. By
rationalizing and/or eliminating barriers and
controls, they may also lead to an increased
pace of investment and increased rates of return.
In this view, the connection between
good policy and growth is not mechanical, and
thus cannot easily be captured in regressiontype
analysis, but that does not stop it from
being of vital importance.

I want to give special weight to another role
of "growth-enabling" policy actions, which
has to do with how policies, the effects of
which might typically be considered to be
comparative static, can nonetheless turn out to
influence economic growth rates over extended
periods of time.

This story begins with a recognition that
most developing countries have typically used
production techniques that were "backward"
in relation to those used by the advanced economies.
One way to verify this is to imagine
integrally replicating, say, a U.S. factory in India,
and manning it there with Indian workers
equivalent in skill to their U.S. counterparts.
The combination of lower construction costs
and much lower operating costs (mainly
wages) would permit this hypothetical new Indian
firm to undercut the prices of both the
U.S. firm of which it was a copy and the typical
Indian firm currently active in the same
industry. This says that the typical Indian firm
is operating on an "inferior" production function.
If, as I believe, the difference in efficiency
between U.S. and developing-country
firms is typically large, there is much room for
quite rapid improvements in the developing
countries as they learn how to "adopt and
adapt" already-known techniques from the
advanced countries.

I would assume that the incentives for "convergence"
are always present, but that they
have typically run into barriers and trammels
of many kinds in the poor countries of the
world. Reducing the barrfers and loosening the
trammels permits the more rapid convergence
to techniques that are closer to the frontier of
knowledge.

The way I see the influence of policy in
growth, it is simply not true that implementing
enabling policies typically permits a quantum
jump from the old to the truly modem. It is
more accurate to describe it as speeding up
what will in any case be a very lengthy process.
Personally, I like the analogy to a hydraulic
system in which a large vessel with a
high water level and lots of water is connected
to a much smaller and narrower vessel with a
much lower level of water. Physical laws dictate
a tendency for the water levels to equalize
in the end. But this can take a very long time
if the tube connecting the two vessels is tiny,
or is clogged by extraneous matter.
The policies that we consider good for
growth have the attribute, in this analogy, of
removing the extraneous matter and/or enlarging
the connecting tube. But even with the best
modernizing policies, the tube remains small
enough so that it takes many decades for a
country to pass from poor to middle income or
from middle income to rich. If somehow the
hydraulic connection could be made so large
as to bring about an almost instantaneous full
adjustment of the water level, then we would
say that good policies mainly represent level
adjustments. But observing even the best of
real-world growth experiences, I think we
have to conclude that the adjustment is going
to be extended over a lengthy period in any
event, thus causing the big observable result
of better policies to be a higher growth rate
over an extended period rather than a discrete
jump to a totally different level.
This way of looking at the world also leads
to some observations on the current literature
dealing with convergence. I have long been
mystified by allusions to an ultimate convergence
of growth rates among countries, or an
ultimate convergence of levels of output per
head. To me, the natural convergence is product
by product, not country by country. And


### ---Economics-1998-0-29.txt---
among products there may be some where
current techniques will never be further
improved. Those products will have no real
cost reductions or TFP improvements in the
future, while others will enjoy huge advances
of productivity. My guess is that unlucky
countries (Bhutan, Nepal, Mongolia?) may
always lag way behind the pack, while luckier
ones (Taiwan, Argentina, Brazil?) may one
day hope to be among the world's leaders. So
convergence comes through as a general tendency,
and quite surely a general possibility,
for the production techniques used in making
any given product to improve as enterprises
using "backward" techniques learn of better
ones, and even more important, learn about
how to put them into practice. I do not believe
that much more than this can be said of convergence
as a real-world force. Wage rates for
given types of labor tend toward a rough
equality across regions in a country because
of the ease of migration in response to perceived
wage differences. The forces at work
internationally are both weaker and more
complex, but the big message here is that
the improvement of technique in any one industry
does nothing to improve the wages of
labor in just that industry. After an improvement,
the industry may end up hiring more or
less labor, but will presumably choose the
amount of labor so as to bring marginal productivity
into correspondence with market
wages. So technological improvement has an
effect on wages via supply and demand in the
national labor market, not through any direct
link from technical improvement to wages
(for given skills, etc.) at the level of the industry
or the firm.

As my final point, let me return to the
thought that the justification for perfecting the
functioning of the market system does not lie
only in reducing the efficiency costs associated
with each period's operation of the economy.
Perfecting a country's economic policy does
not only cause it to move from a path at
around, say, 90 percent of its potential output
to another equal to 95 percent of potential,
with the time path of potential output being
somehow given in advance. That gain would
certainly be a worthwhile gain, and it would
amply justify a lot of hard work involved in
achieving it. But that gain is still fundamentally
comparative static in nature.

What I hope to have evoked in this paper is
a sense that the perfecting of economic processes
can also in nearly all cases be justified
as greasing the wheels of the constant search
for new avenues of real cost reduction. To the
extent that economic reforms do so, they become
vehicles for bringing an economy to a
point where, year after year, new, cheaper, and
better ways are found of doing things, not just
in so-called "production" but also in such
mundane areas as merchandising, sales, finance,
insurance, and many more.

Some years ago, in a book that I edited called
World Economic Growth (1984), I wrote an
essay called "Economic Policy and Economic
Growth," in which I listed "13 lessons" that
I thought followed from the papers presented
in the volume, recounting the growth experiences
of countries as disparate as Ghana and
Taiwan, or Japan and Sweden. These lessonsbasically
focused on thinking about policies in
terms of their economic costs and benefitscould
easily be read as a reprise of the old
comparative-static story. But they were not
meanit as such-it is quite relevant that they
appeared in an essay concerned with world economic
growth. The point was that these sensible
policies emerged as part of a consensus of serious
economists, each an expert in his particular
country's history, focusing attention on the
process of economic growth.

A few years later, and as the theme of a
different concept, John Williamson (1990)
coined the term, the "Washington consensus.
" Williamson listed ten points, covering
territory very similar to my 13 lessons. He also
produced a pithy summary that captures the
essential thrust of both his and my listings:
"Macroeconomic prudence, outward orientation,
and domestic liberalization." He, too,
and the members of the Washington professional
establishment whose apparent consensus
led to Williamson's list, were not just
thinking of comparative-static gains as they
reached their conclusions about policy. They
were thinking about ways to move economies
from slow growth, stagnation, and even in
some cases negative growth, to a healthy,
prosperous flowering of economic progress.
Similar views were more recently affirmed by
Stanley Fischer (1993).

To me, the dynamics of real cost reduction
are at the very least an important piece of what


### ---Economics-1998-0-30.txt---
people have in mind when they list efficiencyoriented
policies as essential ingredients of a
program promoting economic growth. It is
policies of this type that give the right signals
to the CEOs and the managers down the line,
that take away trammels that impede their
quest for real cost reductions, and that create
an environment in which Schumpeter's process
of "creative destruction" can work its
wonders.

APPENDIX ON METHODOLOGY

The vision of the growth process presented
in this paper leads one almost inevitably to
some methodological twists-twists which, if
they are not new, at least differ in significant
ways from what I take to be the most common
practice in breaking economic growth down
into components.

(a) To measure the real rate of return to
capital, one must express the numerator (real
dollars of retum) and the denomiinator (the
capital stock) in the same units. The most efficient
way to do this is to measure both output
(value added) and the capital stock in units of
the GDP deflator. That way one is sure that the
outputs of all the subaggregates in the economy
add up to the GDP, and one also satisfies
the requirement that the capital and return be
measured in the same units. This is also the
way cash flows would be deflated in a standard,
ex post project evaluation. When this is
done at the aggregate level the contribution of
capital to growth is (p + 5) AK where p is an
aggregate rate of return to capital, 6 the depreciation
rate (including obsolescence), and AK
the net increment to the capital stock in the
period in question. At a subaggregate level, the
contribution of capital to growth in activity j
is (pj + bj) AKj. At both levels, we find that
high rates of return are an important component
of most successful growth episodes.
(b) To capture the great diversity of the labor
factor, we would like to have a very fine
breakdown of labor into categories (indexed
by i). The labor contribution is then liwiALi,
where wi represents the real wage of category
i and ALi the change in hours worked by category
i. Since the number of relevant categories
of labor is huge, any such breakdown is
difficult, and gets more difficult as one disaggregates
from economy to sector to branch to
industry and to firm. This Gordian knot can be
cut by a simple assumption, similar to what is
done in most countries to convert residential
construction to real terms. There, one builds a
price index of a "standard house" p*, and
then obtains a quantum of construction C* by
dividing total construction outlays by the price
of the standard house. In the resulting aggregate,
each individual residence (i) gets attributed
a quantum of housing equal to Pi Ip */ In
this work I define a standard wage w*, which
I assign to "standard labor" or "raw labor."
The excess of anybody's actual wage over w *
is attributed to human capital. The returns to
natural ability, as well as to formal education,
training, and experience belong there, under
this interpretation. High returns due to a distorted
wage structure are not appropriately
attributable to human capital, but the methodology
would nonetheless be correct in attributing
to the affected labor a marginal
productivity that is measured by the distorted
high wage.

The "labor contribution" as measured by
w*AL* is equal to w* (Q S(wi /w *) AL1 +
iLi A(w /w*). The second term will be zero
if the structure of relative wages remains constant,
or even if the weighted average premium
does not change. Any changes in the weighted
average premium will cause the calculated residual
to be different from those calculated by
other methods.

The "two-deflator method" is characterized
by the use of a single numeraire-deflator (say,
the GDP deflator), by the treatment of the
quantum of output as value added divided by
the numeraire-deflator, and by the use of a
standard wage w * and a quantum of labor L *
equal to the wages bill divided by w *. This is
the method used by Beyer (1996), Robles
(1997), and Torre (1997) in their work reported
in this paper.

It goes without saying that the two-deflator
method is rough. But it is also tremendously
robust and easily applied. I think of it as being
really designed for use at the firm level, where
very commonly we can get data in value
added, on gross investment, and on the wages
bill, but know nothing (from standard sources)
about the quantum of output or about the number
of total hours worked (or even the total
number of employees used). This opening of
wide new vistas, of huge new data sets, is what


### ---Economics-1998-0-31.txt---
I consider the strongest argumnent for the twodeflator
approach.

But there are other pluses as well. First, at
the aggregate economy level, the two-deflator
approach comes very close to the traditional
approach. In rate-of-growth terms, we have
(R */y) = gy S- skgk * SL gL *, compared with
(Rly) =g - Skgk -SLgN, where g refers to
growth rate, and y to GDP. K* differs from K
in being built up from gross investment deflated
by the GDP deflator, while K is built up
from gross investment deflated by the investment
deflator of GDP. L * is in principle much
more refined than N (number of workers), but
its measurement can be influenced by a widening
or narrowing skill premium. (R */y)
likewise differs from Jorgenson's residual
(R'/y) = gy - Yjskjgkj - 1ise gf mainly in his
use of different capital deflators for different
categories of capital. (The implicit labor
breakdown in L * is much finer even than
Jorgenson's, but Jorgenson does not have to
contend with the possibilities of widening or
narrowing skill premiums -at least not
among the labor categories he works with,
which are typically broken down by gender,
age, education, occupation, industry, and employmnent
status.)

The bottom line is that when Beyer compares
his two-deflator results, at the national
aggregate level, with those of others using different
methods, he finds on the whole only
modest differences. [See Beyer (1996);
Harberger (1998).]

When one uses the two-deflator method at
the industry level, one often has the possibility
to adjust the quantity variable so as to make it
correspond with that of the more traditional
approach. Thus, we may start by using dy)e (=
pjdyj + yjdpj) as the quantity variable, and calculate
a residual Rj* using that concept. Then
we may get an adjusted residual by taking
R yjdpj. This is easy to do so long as we
have decent data on dpj, the relative price of
j, which we often do at the branch or industry
level. When Jorgenson's residuals are compared
with the two-deflator residuals, with and
without price adjustment, I find the differences
without adjustment to be small enough to be
quite acceptable. With adjustment, the degree
of agreement between the two approaches is
quite notable (85 percent of differences less
than one percentage point of per annium
growth). [See Robles ( 1997); Ilarberger
(1998).]

When one gets down to the firm level, the
two-deflator method is in its element. Rarely
do we have decent time series on the price index
of a firm' s output. Treating many firrns in
one industry, one might then give all of them
the same price index-that of the industry. At
that point the distribution of adjusted TFP residuals
among the firms would end up differing
from the original two-deflator distribution
only by a constant. When expressed in percentage
terms we vould have (R)' ly)) = g4i -
*kj j ge*, for each fim j without adjustment,
while with adjustment we would have
R]*Iy1 = the same expression minus go, the
rate of growth of the industry's relative ptice
index (the same for all firms).
So in the great bulk of cases one ends up
with something quite like the two-deflator
method when working at the individual firm
level. The consolation is that the residual
terms of individual firms, calculated for the
whole economy, add up to a residual term
for the aggregate-in the sense that outputs
sum to GDP, the L]* sum to L * for the economy,
the Kj* to K* for the economy, etc. For
more details on methodology, see Harberger
(1998).
 ## Economics-1999-0


### ---Economics-1999-0-01.txt---
In his Presidential Address five years ago,
Zvi Griliches ( 1994) called attention to the severe
difficulties that beset current attempts to
measure the growth of labor productivity in
the American economy. Because of these difficulties,
it is likely that the true rate of economic
growth is substantially underestimated.
The root of the problem is the difficulty in
measuring output in the service sector which
now represents two-thirds of the economy. In
such sectors as health care and information
services, the contribution to gross domestic
product (GDP) is measured by inputs rather
than outputs, a procedure that makes it impossible
to gauge accurately improvements in the
quality of output. Thus, in the case of computers,
which are transforming American society,
economists have been unable, so far, to
find a measurable contribution of computers to
the rise in labor productivity-an astonishing
paradox.

I want to follow up on this problem of mismeasurements.
My thesis is that the profession
is lagging behind the economy more than it
has to. We are, to some extent, entangled in
concepts of the economy and in analytical
techniques that were developed during the first
third or so of the century, when economics
emerged as a modem discipline. The range of
the discipline did not expand greatly during
the middle decades of the century, due partly
to a concentration on the reformulation of the
previous analytical concepts and techniques in
more sophisticated and more general mathematical
models. Although the dividends from
these efforts were high and have contributed
to the flexibility and capacity of economics,
they did not encourage a reconsideration of
some of the received assumptions about the
scope and focus of economic analysis. There
has been a significant broadening of the scope
of economics during recent decades, with the
emergence of such fields as the new household
economics, the new institutional economics,
the economics of aging, and medical economics,
but much remains to be done.

The balance of this address is divided into
four sections. I begin with the inadequate
attention to the accelerating rate of technological
change, the implications of this acceleration
for the restructuring of the economy,
and its transforming effect on human beings.
I then consider the neglect of the nonmarket
sector of the economy, the implication of that
neglect for the measurement of consumption,
and for the analysis of economic growth. The
third section deals with the need to shift the
focus of economic analysis from crosssectional
to life-cycle and intergenerational
data sets, especially in connection with forecasting.
The final section points to the impact
of cultural lag in the treatment of material inequality,
and the neglect of the more severe
problem of spiritual inequality. I use the word
spiritual not in its religious sense but as a reference
to commodities that lack material form.
Spiritual or immaterial commodities make up
most of consumption in the United States and
other rich countries today.


### ---Economics-1999-0-03.txt---
6000- Genome Project PCs

Man on Moon \ /

\ Man on MoonNuclear Energy

High-Speed Computers \ Discovery of DNA
5000- War on Malaria Penicillin
Invention of Airplane

4000- Discovery of New World Invention of Automobile
Population 01 Black Plague~

(millions) 0 Peak of Rome

3000- 0A Peak of Greece:

'V~~~~ & 10 ~~~~~~Invention of Telephone
\$w '/ Germ T eory

6% Os 1ttttt>,% s ' ', . BeginningofRailroads
1000- Invention of WattEngine

4; e$;tNtttH ' ~~~~~~~Beginning of Industrial Revolution
%V. ~~~~~Beginning ofRalod

YIL..YY Y _ 82nd Agricultural Revolution
0 --7 t 1. ...r TT T -- - - - _
-9000 -6000 -4000 -2000 0 2000
-5000 -3000 -1000 1000

Time (years)

FIGURE 1. THE GROWTH OF THE WORLD POPULATION AND SOME MAJOR EVENTS IN THE HISTORY OF TECHNOLOGY
Notes: There is usually a lag between the invention of a process or a machine and its general application to production.
"Beginning" means the earliest stage of this diffusion process.
Sources: Carl W. Bishop, 1936; T. K. Derry and T. I. William, 1960; Graham Clark, 1961; B. H. Slicher von Bath,
1963; Stuart Piggott, 1965; Glenn T. Trewartha, 1969; William McNeill, 1971; Jacob Bronowski, 1973; Carlo M.
Cipolla, 1974; B. M. Fagan, 1977. See also E. A. Wrigley, 1987; Robert C. Allen, 1992, 1994.
I. Some Implications of Technophysio Evolution
The rapid acceleration in technological
change is apparent in the decline in mortality
rates during the twentieth century. Study of the
causes of this decline point to the existence of
a synergism between technological and physiological
improvements that has produced a
form of human evolution that is biological but
not genetic, rapid, culturally transmitted, and
not necessarily stable. This process, which is
still ongoing in both rich and developing
countries, has been called "technophysio
evolution."

Unlike the genetic theory of evolution
through natural selection, which applies to the
whole history of life on earth, technophysio
evolution applies only to the last 300 years of
human history, and particularly to the last century.
Despite its limited scope, technophysio
evolution appears to be relevant to forecasting
likely trends over the next century or so in longevity,
the age of onset of chronic diseases,
body size, and the efficiency and durability of
vital organ systems. It also has a bearing on
such pressing issues of public policy as the
growth in population, in pension costs, and in
health-care costs (Fogel and Dora L. Costa,
1997).

Technophysio evolution implies that human
beings now have so great a degree of control
over their environment that they are set apart
not only from all other species, but also from
all previous generations of Homo sapiens.


### ---Economics-1999-0-04.txt---
This new degree of control has enabled Homo
sapiens to increase its average body size by
over 50 percent, to increase its average longevity
by more than 100 percent, and to improve
greatly the robustness and capacity of
vital organ systems.'

Figure 1 helps to point out how dramatic the
change in the control of environment after
1700 has been, and it highlights the astounding
acceleration of technological change over the
past two centuries. The advances in the technology
of food production after the Second
Agricultural. Revolution (which began about
1.700 A.D.) were far more dramatic than those
associated with the First Agricultural Revolution
since they permitted population to increase
at so high a rate that the line of
population appears to explode, rising almnost
vertically. The new technological breakthroughs
in manufacturing, transportation,
trade, communication, energy production,
leisure-time services, and medical services
were in many respects even. more striking than
those in agriculture. Figure 1. emphasizes that
prior to 1600, centuries elapsed between major
technological advances and the process of diffusion
was even more extended, continuing
over several millennia. Studies of the origin
and diffusion of the plow, for example, show
how little improvement there was in its design
between its original development in the Mesopotamian
valley around 4000 B.C. and its
diffusion across the Mediterranean Sea and
northward in Europe down to the beginning of
the second millennium (Bishop, 1936; E.
Cecil Curwen, 1953).

To my mind nothing better illustrates this
amazing acceleration in technological change
than the realization in the twentieth century of
humankind's ancient desire to fly. The first
successful motor-driven flight took place in
1903, but the fragile aircraft of Wilbur and
Orville Wright traveled only a few hundred
feet. Just 66 years later, an astronaut was
standing on the moon, talking to another astronaut
on earth, and hundreds of millions of


people around the world overheard and
watched that conversation.

Worldwide, the most important aspect of
technophysio evolution is the continuing conquest
of chronic malnutrition, which was virtually
universal three centuries ago.2 Table 1
shows that in rich countries today some 1,800
to 2,000 or more kilocalories (kcal) of energy
are available for work daily per equivalent
adult male, aged 20-39.3 At the beginning of
the eighteenth century, however, France produced
less than one-fifth of the current U.S.
amount of energy available for work. And England
was not much better off. Only the United
States provided potential energy for work
equal to or greater than late-twentieth-century
levels during the eighteenth and early nineteenth
centuries, although some of that energy


### ---Economics-1999-0-05.txt---
was wasted due to the prevalence of diarrhea
and other conditions that undermined the
body's capacity to utilize nutrients (Fogel and
Floud, 1999).

One implication of these estimates of caloric
availability is that mature adults of the eighteenth
and much of the nineteenth century
must have been very small by current standards
and less physically active. Today the average
American male in his early thirties is
about 177 cm (70 inches) tall and weighs
about 78 kg (172 pounds). Such a male requires
daily about 1,800 kcal for basal metabolism
and a total of 2,300 kcal for baseline
maintenance. If either the British or the French
had been that large during the eighteenth century,
virtually all of the energy produced by
their food supplies would have been required
for personal maintenance, with little available
to sustain work. To have the energy necessary
to produce the national products of these two
countries circa, 1700, the typical adult male
must have been quite short and very light
(Fogel, 1997).

Recent studies have established the predic
tive power of height and weight at early ages
with respect to onset of chronic diseases and
premature mortality at middle and late ages.
Variations in height and weight appear to be
associated with variations in the chemical
composition of the tissues that make uip vital
organs, in the quality of the electrical transmission
across membranes, and in the functioning
of the endocrine system and other vital
systems. Nutnitional status thus appears to be
a critical link connecting improvements in
technology to improvements in human physiology
(Fogel and Costa, 1997).

So far I have focused on the contribution of
technological change to physiol-ogical improvements.
The process has been synergistic,
however, with improvement in. nutrition and
physiology contributi-ng significantly to economic
growth and technological progress in a
manner described elsewhere (Fogel., 2000).
Here I merely want to point out the main conclusion.
Technophysio evolution appears to
account for about half of British economic
growth over the past two centuries. Much of
this gain was due to the improvement in human
thermodynamic efficiency. The rate of
converting human energy input into work output
appears to have increased by about 50 percent
since 1790 (cf., Partha Dasgupta, 1993).
Technophysio evolution calls into question
a number of easy and frequent assu-mptions in
economic analyses such as: tastes are fixed;
needs are fixed or exogenously determined;
existing life tables are adequate to forecast future
pension costs in. 2030 or 2075; and the
rate of aging is genetically controlled and unchanging
from one generation to another.
Technophysio evolution also calls into
question such frequently used theoretical assumptions
as fixed utility functions, fixed rates
of time preference over the life cycle, and the
related assumption that, except for risk differentials,
economic phenoomena should generally
be subject to the same rate of discount.
Yet the rate of discount varies over the life
cycle; children, after all, have much higher
time preferences than adults. It may also vary
by religion. Some individuals are prepared to
wait for their reward in heaven while others
want it here and now. Moreover, these differences
may be evolving and multiplying more
rapidly than is now presunmed, as is indicated
by the debates over the impact of greenhoiuse
gases. Rethinking of the issue is now underway
(see William D. Nordhaus, 1997; cf.,
Peter Koslowski, 1992; Nazli Choucri, 1.993;
Marc Fleurbaey and Philippe Michel, 1994;
Michael Toman, 1994) but the profession can
benefit from an increased allocation of resources
to this problem.

Technophysio evolution requires not just
marginal adjustments, but majtor leaps in economic
theory. We are slow in pondering such
grand questions as the inmplications of the Human
Genome Project, which is now nearing
completion, and the emergence of molecular
medicine for the future of economic life. We
have entered an era in which p-urposeful intervention
in evolutionary processes is passing
beyond plant and animal breeding. The new
growth economics needs to incorporate at least
some aspects of directed, rapid human evolution.
Endogenous technological change needs
to extend to the fundamentals of human behavior.
Theorists also need to grapple with the
ethical implications of technological changes
that, whatever their positive aspects, threaten
to undermine the mystery of human life by
transforming people into "material" that is


### ---Economics-1999-0-06.txt---
transplanted, cloned, arbitrarily altered in external
appearance, artificially changed in personality
and intelligence, and otherwise
manufactured in ways that challenge the definition
of a human being (cf., Zbigniew Brzezinski,
1996).

It. The Growth of the Nonmarket Sector'
One aspect of technophysio evolution has
been a change in the structure of consumption
and in the division of discretionary time between
work and leisure. Perhaps the best index
of the growth of the nonmarket sector is the
change in the use of time. Changes in hours of
work and in the average division of the day
have paralleled the changes in the structure of
consumption. Table 2 shows the remarkable
reduction in the workyear that has occurred for
males in the U.S. labor force over the past century.
Sleep, meals, and essential hygienie,
which are biologically determined, required
about 10 hours of the day in 1880, as they do
today. The remaining 14 hours represent "discretionary"
time.

The most notable feature of Table 2 is the
large increase in leisure available to the typical
male worker. His leisure time has tripled over
the past century, as his workyear declined
from about 3,100 hours to about 1,730 hours
today. Table 2 also forecasts the division of
the average day in 2040, indicating that by that
date more than half of the discretionary day
will be devoted to leisure activities. The forecast
is for a reduction of the workyear from
the current average of about 1,730 hours to just
1,400 hours, with the average workweek down
to 30 hours.

The pattern of chainge anmong women was
similar to that among men (cf., Claudia
Goldin, 1990; Stanley Lebergott, 1993). Th'e
workday of wotnen in 1.880 was somnewhat
longer, and in some respects may have bexen
more arduous, than that of men. In households
of working farmners, artisans, and manual laborers,
wives rose before their husbainds and
continued working until bedtimle at 10 or I1
P.M. That routine suggests a workday that


may have run about 15 niinutes longer than
that of males, implying an annual workyear of
perhaps 3,200 hours.

As a result of the mechanization of the
household, smaller families per household,
and the marketing of prepared foods, the typical
nofiemployed mnarried woman today
spends about 3.4 hours per day engaged in
housework; and if she is enmployed the figure
for housework drops to 2.2 hours. However,
women in the labor force average about 4.4
hours per day as emnployees. Hence comlbining
"4work" with " chores," men and women
work roughly equal amounts per day, and both
enjoy much miore leisure than they used to.
The principal difference is that the gains of
women have comrie exclusively from the reduction
in hours of housework, while the gains
of men have come from the reduction in the
hours of employed work (cf., John P.
Robinson and Geoffrey Godbey, 1997).
I have so far retained the common distinction
between work and leisure, although these
terms are already inaccurate and may soon be
obsolete. The distinction was invented when
most people were eingaged in manual labor for
60 or 70 hours per week and was intended to
conitrast with the highly regarded activities of
the English gentry or their American equivalent,
Thorsten Veblen's (l1899) "leisure
class." However, it should not be assumed
that members of the leisure class were indolent.
In their youth they were students and athletes.
In young adult years they were warriors.
In middle and later ages they were judges,


### ---Economics-1999-0-07.txt---
TABLE 3-ESTIMATED TREND IN THE LIFETIME
DISTRIBUTfION OF DiSCRETIONARY TIME
C. 1880 c. 1995 c. 2040

Lifetime discretionary

hours 225.900 298&500 321,900

Lifetinle earnwork

hours 182,100 122,400 75,900

Lifetime volwork

hours 43,800 176,100 246,000

bishops, merchant princes, and patrons of the
arts. Whatever they did was for the pleasure it
gave them since they were so rich that earning
money was not their concern.

Hence, leisure is not a synonym for indolence,
but a reference to desirable forms of effort
or work. As George Bernard Shaw (1928)
put it, "labor is doing what we must; leisure
is doing what we like; and rest is doing nothing
whilst our bodies and our minds are recovering
from their fatigue." In order to avoid confusion,
in the balance of this address I reserve
the word "work" for use in its physiological
sense, an activity that requires energy, over
and above the basal metabolic rate (BMR).
Activity aimed primarily at earning a living I
will call " earnwork. " Purely voluntary activity,
even if it incidentally carries some payment
with it, I will call "volwork "
Why have hours of earnwork declined so
much in recent years? The answer to that question
is suggested by the fact that it is not just
daily and weekly hours of earnwork that have
declined. The share of lifetime discretionary
hours spent in earnwork has declined even
more rapidly. Table 2 only dealt with the hours
of earnwork of persons in the labor force. It
did not reflect the fact that the average age of
entering the labor force is about five years later
today than it was in 1880, or that the average
period of retirement for those who live to age
50 is about 11 years longer today than it was
in 1880 (cf., Lee, 1996). A century ago only
one out of five males aged 65 and older were
retired. Today, six out of seven are retired.
All in all the lifetime discretionary hours
spent earning a living have declined by about
one-third over the past century (see Table 3)
despite the large increase in the total of lifetime
discretionary time. In 1880, four-fifths of
discretionary time was spent eamning a living.
Today, the lion's share (59 percent) is spent
doing what we like. Moreover, it appears probable
that by 2040 over three-quarters of discretionary
time will be spent doing what we
like, despite a fuLrther substantial increase in
discretionary time due to the continuing extension
of the life span (cf., Jesse H. Ausubel and
A. Griibler, 1995).

Why this deep desire for volwork? Why do
so many people want to forgo earnwork which
would allow them to buy more food, clothing,
housing, and other goods'? The answer tumrs
partly on the extraordinary technological
change of the past century, which has not only
greatly reduced the nunmber of hours of labor
the average individual needs to obtain his or
her -food supply, but has also made housing,
clothing, and a vast array of consurner durables
so cheap in real terms that the totality of
material consumption requires miuch fewer
hours of labor today than was required over a
lifetine for food alone in 1880.
Indeed, we have become so rich that we are
approaching saturation' in the consumption
not only of necessities, but of goods recently
thought to be luxuries, or which were only
dreams or science fiction during the first third
of the twentieth century. T'oday there is an average
of two cars per household in the United
States. Nearly every household with a person
competent to drive a car has one. On some
items such as radios, we seem to have reachied
supersaturation, since there is now more than
one radio per ear (5.6 per household). The
point is not merely that we are reaching saturation
in commodities that once defined a high
standard of living aiid quality of life but also
that the hours of labor required to obtain those
comnmodities have drastically declined. The
typical household in 1875 required 1,800
hours of labor in the marketplace to acquire
the annual food supply, but today it takes jus.t
260 hours. All in all, the comfrmodities that
used to account for over 80 percent of household
consumption can now be obtained in
greater abundance than previously, with less
"Saturation" in this context tneans that niost purchases
are for replacement rather than for new use.


### ---Economics-1999-0-08.txt---
than a third of either the market or the household
labor once required (Fogel, 2000).
Table 4 shows how sharply the U.S. distribution
of consumption has changed over the
past 120 years. Food, clothing, and shelter,
which accounted for about three-quarters of
consumption in 1875, accounted for just 12
percent in 1995. Leisure, on the other hand,
has risen from 18 percent of consumption to
67 percent.6 As Table 4 shows, the long-term
income elasticities of the demand for food and
clothing are below 0.5 and the elasticity of the
demand for shelter is closer to, but still below,
1.0. On the other hand, the long-term income
elasticities for leisure, education, and for medical
services are over 1.O.7

Table 4 differs from current official tabulations
of household consumption in two principal
respects. Official tabulations, with minor
exceptions, are limited to the out-of-pocket expenditures
of households. Table 4, however,
adds expenditures for education and health
care consumed by households but paid by government,
employers, and other third parties.
My procedure does little to change the distribution
of consumption in 1875 but significantly
increases the education and health-care
shares of consumption in 1995.8
Table 4 also differs from official tabulations
by adding to the value of out-of-pocket expenditures
on leisure, the value of the time devoted
to leisure (volwork). That time is priced
at the average hourly rate of compensation of
labor. The procedure increases the share of leisure
in consumption in both 1875 and 1995.
While leisure (volwork) remains a relatively
small share of expanded consumption in 1875,




it accounts for two-thirds of expanded consumption
in 1995.'

The failure to include the value of volwork
in the national income and product accounts is
perhaps the most glaring example of the cultural
lag to which I referred at the beginning
of this address. That omission also leads to a
significant underestimate of the long-term
growth rate of per capita income. Before adjustments
for the increased quality of volwork,
the growth rate is increased by eight-tenths of
a point (from 1.8 to 2.6 percent per annum).
But adjustments for improvements in the quality
of volwork might substantially increase
that figure, since leisure-time activities in 1875
were limited largely to church on Sundays and
carousing in bars during the rest of the week
(cf., Fogel, 2000). Despite some important
contributions to the development of an economics
of leisure (cf., Gary S. Becker, 1991;
Costa, 1998a, c; Daniel S. Hamermesh, 1998;
John Pencavel, 1998), the profession has far
to go before it catches up with the economy.
I1I. Shifting to Life-Cycle Data Sets for
Successive Cohorts and to Intergenerational
Data Sets

It is no secret that cross-sectional data sets
are cheaper to construct and more abundant


### ---Economics-1999-0-09.txt---
than longitudinal data sets. But whatever their
usefulness for other problems, cross-sectional
data are often highly misleading guides to
trends on such critical economic issues of the
new millennium as the prevalence rates of
chronic disabilities, expenditures on health
care, and pension costs.

Research initiated during the past decade
and a half has called into question previous
views on the length and fixity of the life span,
on the shape of the Gompertz curve (which
relates the log of the age-specific probability
of dying to age), on the theory of the epidemiological
transition, 10 and on the related
proposition that longer life expectancy implies
worse health among the survivors. It appears
that some of the earlier propositions were the
consequence of attempts to infer life-cycle behavior
from cross-sectional data sets. Such efforts
were thwarted by changes in the
sampling design of successive cross sections
and by changes in technology that led to earlier
diagnosis of preexisting conditions (Timothy
Waidmann et al., 1995).

The new research also accumulated evidence
on the outward movement of the survivorship
curve. Vaino Kannisto ( 1994) (cf.,
Kannisto, 1996) has shown that, in 14 countries
for which the data are adequate, mortality
over age 80 has been declining by about 1 percent
per annum for about half a century with
a significant acceleration in recent decades.
John R. Wilmoth (1995, 1997), who examined
extreme longevity in five countries,
concluded that the right-hand end of the survivorship
curve has been shifting outward for
two centuries. Moreover, there is additional
evidence that the Gompertz curve either levels
off or declines at old old ages (James W.
Vaupel, 1997; cf., S. Jay Olshansky and Bruce
A. Cames, 1997).

Another development is the continued accumulation
of evidence linking events early in
life, and reflected in height, weight, and body
mass index (BMI), to the onset of chronic con-
ditions. " A number of longitudinal studies that
were launched in the 1950's and 1960's have
recently been extended to cover the entire period
of human growth and early adulthood,
through follow-up studies. These have confirined
the persistence of central nervous systemr
defects induced by malnutrition in early
childhood (Nevin S. Scrimshaw, 1995). Numerous
other follow-up studies have shown
that malnutrition and smoking in adolescence
and in middle ages are risk factors for the early
onset of chronic conditions and premature
mortality, especially due to coronary heart disease,
non-insulin-dependent diabetes, and respiratory
diseases (Avita Must et al., 1992;
Vincent J. Carey et al., 1997; N. K. Chin et
al., 1997; Joan M. Dom et al., 1997; K. Kotani
et al., 1997; Dan S. Sharp et al., 1997; Ralf
Bender et al., 1998). These relationships have
been established in American, Asian, Australian,
European, and Latin American populations-
rich and poor. Economists have also
discovered a close link between later productivity
and height, BMI, and protein consumption
(after controlling for caloric intake)
(John Strauss and Duncan Thomas, 1995;
Thomas and Strauss, 1997).

There has also been an expansion of research
into the connection between intrauterine
and infant growth and the onset of chronic
diseases (or premature mortality). The strongest
evidence for such a link that has emerged
thus far is with respect to hypertension, coronary
heart disease (CHD), and non-insulindependent
diabetes. A review of 32 papers
dealing with the relationship between birth
weight and hypertension by Catherine M. Law
and Alistair W. Shiell (1996) showed a tendency
for middle-aged blood pressure to increase
as birth weight declined. Evidence of a
connection between birth size and later coronary
heart disease has been found in England,
Wales, Sweden, India, and Finland.12
"? This theory holds that the prevalence of chronic diseases
is unrelated to the prevalence of acute infectious
diseases but depends only on the natural process of senescence
that precedes mortality.

" BMI is measured by the ratio of weight in kilograms
to height in square meters (BMI = kg/m2).
i2 See Stephen Frankel et al., 1996; Law and Shiell,
1996; C. E. Stein et al., 1996; Swen-Olef Andersson et al.,
1997; D. J. Barker, 1997; Barker and C. N. Martyn, 1997;
J. L. Cresswell et al., 1997; T. Forsen et al., 1997; Jan A.
Henry et al., 1997; Ilona Koupilova et al., 1997; ScrimThe


### ---Economics-1999-0-10.txt---
accumulation of historical and current
biomedical studies on the trends in health, longevity,
and human physiology, combined with
controlled studies of animal populations, are
leading some evolutionary biologists to place
increasing emphasis on plasticity in human aging
(Michael R. Rose, 1991; Caleb E. Finch,
1997; Hillard Kaplan, 1997). The term "plasticity"
refers to the widely varying pattern of
aging within species and the ability of the
length of the life span to be affected by environmental
factors, some of which may operate
over several generations (cf., Kenneth W.
Wachter and Finch, 1997).

The search for better ways to forecast longterm
trends in health-care costs and in pension
costs has made it essential for economists to
understand the underlying physiological, nutritional,
and epidemiological factors that affect
the changing demand for health care and
retirement. Forecasts about health-care costs
are obviously related to assumptions about
changes in both life expectancy and agespecific
morbidity rates. If current morbidity
rates at older ages are presumed to remain constant
or to increase over the next 30 to 50
years, and if life expectancy is also presumed
to increase, then national health-care costs become
astronomic. However, if one takes account
of the recent declines in age-specific
disability rates, then the share of health-care
costs in GDP might remain constant (Kenneth
G. Manton et al., 1997b; Manton et al., 1998),
provided the demand for health services
(given disability rates) does not increase. If,
however, the income elasticity of the demand
for health is greater than one, then as income
increases, the share of income that is spent on
health care could increase even if disability
rates decline (see Fogel, 1997). It is quite possible
for the demand for medical interventions
to increase, even though age-specific morbidity
rates decline, for one or more of the following
reasons: shifts in the age structure of
the population toward ages with high morbidity
rates; improvements in inedical interventions
for a wide range of chronic conditions
which alleviate symptoms but do not cure; the
replacement of current disease-specific standards
of care by new ones based on improved technologies
that are much more expensive than
those replaced; the elimination of current age restrictions
on such expensive procedures as organ
replacements (cf., David M. Cutler and Ellen
Meara, 1998; Alan M. Garber et al., 1998).
The complexity of the underlying issues in
the economics and biodemography of aging
has led to a significant shift away from crosssectional
data sets and to a concentration on
longitudinal data sets. The earliest of the major
longitudinal studies sponsored by the National
Institutes of Health (NIH), the Framingham
Heart Study, began in 1950 and was focused
on the causes and treatment of coronary heart
disease rather than on the economics and biodemography
of aging. The Retirement History
Survey (RHS) followed a cohort of men and
women aged 58 to 63 from 1969 to 1979 and
then was suspended. Although not initially
planned as a longitudinal study, it became the
mainstay of retirement research during the
1980's. Linked to the social security file on
covered earnings, it provided important insights
into the labor-force and retirement behavior
of older workers.

In 1990, the National Institute of Aging
launched a new Health and Retirement Survey
(HRS) focused on the cohort born between
1931 and 1941, and aimed at collecting a much
wider set of variables than covered by the
older RHS. The new HRS collected numerous
measures of health (including cognitive ability)
, taxable earnings, job characteristics,
hours of work, job turnover, family characteristics,
pensions, health insurance, and ownership
of an array of assets including real estate,


### ---Economics-1999-0-11.txt---
consumer durables, and financial securities.
The most ambitious social science project ever
undertaken, the first wave of HRS cost
$14,000,000 and the second wave was budgeted
for $17,000,000. A third wave is currently
under way collecting information on the birth
cohorts of 1942-1947 (F. Thomas Juster and
Richard Suzman, 1995).

NIA launched a second longitudinal survey
aimed at cohorts born in 1924 or earlier years.
This sample, which is called Asset and Health
Dynamics. Among the Oldest Old (AHEAD),
consists of persons aged 70 years and older in
1994, when the first wave was undertaken. Although
the questions in AHEAD overlap with
HRS, functional ability is investigated intensively
and labor-market activity is covered
lightly.

Both HRS and AHEAD have made it possible
to examne a wide range of issues with
more reliable and more detailed evidence than
previously available. James P. Smith and
Raynard Kington (1997), for example, have
used AHEAD to examine disparities in functional
status and to relate them to socioeconomic
status (SES variables). They

discovered strong feedback effects not only
from health to SES variables but from SES
variables to health. To disentangle these effects
they broke household income and wealth
into various categories made possible by the
data and examined the effects by age. They
were also able to make use of a range of variables
bearing on the health and socioeconomnic
status of relatives in three generations (parents,
siblings, and children). Their analysis indicated
that cuffent-period health and income
are attributes of past, concurent, and future
generations. Measuring the full extent of these
influences, the direction of causation, and
complex interactions requires the study of lifecycle
histories from birth to very old age. It
would be inappropriate, they concluded, to use
SES variables to explain variations in late-age
health without taking account of the feedback
mechanisms or identifying within-period innovations
in the stock of health (cf. John
Bound et al., 1998).

Such findings point to the usefulness of creating
a prospective life-cycle sanple for an extinct
cohort (the veteratns of the Union Army)
by utilizing military, pension, and census records
in archives. This procedure not only creates a
longitudinal data base in a small fraction of the
time required to trace a living cohor, but cm be
done at less than a tenth of the normnal cost. Such
a project was launched by NIA in 1996 (cf,
Clayne L. Pope and Larry T. Wimmer, 1998).
Based on 11 different data sets, the fully
linked life histories contain over 10,000 variables
on each recruit, including socioeconomic,
ecological, and health variables. Called "Early
Idicators of Later Work Levels, Disease, d
Death," this project provides a conmprehensive
data set on the life course of over 39,000 Union
Amy veterans. Bor mainly between 1835 and
1845, these men represented the first cohor to
reach age 65 in the twentieth cent , d can
be compared with the veterans of World Wa IL1.
The preliminary comparisons revealed that at
the same ages the prevalence of chronic diseases
was much higher among elderly Uniion Any
veterans tha among veteras of World War IL
Musculoskeletal and respiratoy diseases were
16 times as prevalent, heart diseases were 2.9
times as prevyent, and digestive diseases were
4.7 times as prevalent among elderly veterans in
1910 than in the niid-1980's (cf., Sven E.
Wilson and Louis L. Nguyen, 1998). Moreover,
young adults bom during the second quaer of
the nineteenth centr who survived the deadly
contagious diseases of childhood and early adolescence
were not freer of degenerative diseases
than persons of the samne ages today, as
propounded by the theory of epidemiological
transition, but were more afflicted. Heria rates
at ages 35-39, for exanmple, were more than
three times as prevalent in the 1860's as in the
1980's.

Although the analysis of the data in the Un
ion Army sample is still in progress, some of
the initial findings have a bearing on forecasts
of long-term trends in health status. Costa
( 1998b) has reported that early-age socioeconomiic
and biomedical stress had a substantial
impact on the likelihood that Union Army vet
erans would have disabling chronic health
conditions by age 60. Thus, veterans raised in
a county with high mortality rates were, half a
century later, at elevated risks of suffering
from disabling respiratory disease, circulatory
disease, and musculoskeletal problems (cf.,
Lee, 1997). Episodes of acute diseases experienced
as youtng adults, such as respiratory


### ---Economics-1999-0-12.txt---
infections, work-related injuries, and extended
bouts of diarrhea, also increased the odds of
suffering from chronic disabilities by age 60.
Costa concludes that about 15 percent of the
decline in the prevalence of joint problems and
75 percent of the prevalence of back problems
between 1910 and 1980 was due to shifts in
the occupational structure from manual to
white-collar jobs. Moreover, a comparison of
rates of decline in disabilities before and after
1980 indicates that disabilities are declining at
an accelerating rate (cf., Manton et al., 1997a;
Cutler and Elizabeth Richardson, 1998).
The mounting evidence of substantial interactions
over the life cycle that influence the
process of aging, the acceleration of technological
change which has profoundly affected
the context in which aging occurs, and the increasing
evidence that environmental influences
on the aging process begin in utero, has
led to the initiation of a new life-cycle project
called "Fetal, Infant, and Later Aging Markers,
Cohort b. 1910-35." The acronym for
this project is FILAM.

The central objective of FILAM is the creation
of a life-cycle sample of persons born
between 1910 and 1935 that would make it
possible to compare changes in the aging process
over the period of 70 years that separate
this new sample from the aging experiences of
the Union Army cohort. The FILAM cohort is
important not only because it studies individuals
currently between ages 63 and 89, but also
because of the dramatic environmental and
early life-style changes they experienced.
These changes include the rise and partial decline
of smoking, the decline and the new rise
of alcohol consumption, the replacement of
horses by internal combustion engines as the
main source of urban vehicular power, the
cleaning up of the water and milk supplies, and
the emergence of a wide range of effective
medical interventions.

The sample will be drawn from the birth
records of 10,000 men and women of differing
ethnicities and races who were born in Boston,
New York City, Baltimore, Chicago, Iowa
City, and San Francisco. Approximately half
of the neonates will not have survived to the
present day and these men and women will be
linked to their death certificates. The survivors
will be traced and interviewed to determine the
presence of chronic conditions, socioeconomic
status, and family and own health history.
Survivors will be linked to social
security, census, military, and tax records, subject
to their written consent.

FILAM will make it possible to investigate
the predictive power of fetal, neonatal, and
early childhood measures of retarded growth
such as weight for gestational age, ratio of placental
weight to birth weight, infant weight
gain, thinness at birth, arid shortness at birth
relative to head size, on the risk of developing
specific chronic conditions at mid-adult and
late ages. Among the chronic conditions that
will be examined are coronary heart disease,
hypertension, stroke, obstructive lung disease,
non-insulin-dependent diabetes, and autoimmune
thyroiditis. FILAM will also make it
possible to investigate how the interaction of
fetal and infant developments with various risk
factors at later childhood, young adult, and
middle ages may intensify or moderate risks
of chronic conditions and early death after age
65. Another objective is the analysis of differences
in the health histories, occupational histories,
and retirement pattern between FILAM
and the Union Army sanmple, with special emphasis
on the similarities and differences of
predictors of later work levels, morbidity, and
waiting time until death.

The HRS, AHEAD, Union Army sample,
and FILAM all contain information on the
parents and children of the individuals under
study. Hence, they make it possible to address
some factors that may be transmitted
intergenerationally. An effort to utilize archival
data to study intergenerational processes
is also under way. Pope (1992) is
linking a large sample of genealogies to the
life-cycle sample of Union Army recruits.
There are at least 60,000 published family
histories that contain information on over
100,000,000 people who ever lived in North
America. Pope is drawing a subsample of
these histories containing 10,000 men of
military age at the time of the Civil War. It
is estimated that 40 percent of these men
served in the Union Army. This new sample
(called ILAS, for intergenerationally linked
aging sample) will be linked to the military,
pension, medical, and census data sets previously
discussed.


### ---Economics-1999-0-13.txt---
WlAS will make it possible to control for the
effect of wartime stress by comparing subsequent
morbidity and mortality among those who
served in the Union Arny with relatives who
did not. It will also be possible to measure family
effects on aging and mortality experience. This
will be done by including parents' occupation,
wealth, residential history, number of children,
place or region of birth, and migration histoiy.
Brothers in ILAS share a common prewar environment
as well as a common genetic heritage.
They may also share a common war experience
(see D. S. Lauderdale and P. J. Rathouz, 1998).
This common genetic heritage and environment
is not fully captured by the aforementioned intergenerational
variables because heritability is
composed of the many different dimensions included
in genetics and environment. However,
conmmon family effects may be measured by using
independent variables to "sweep out" the
effects of observed variables on death age. The
covariance of the errors in that regression with
brothers' estimated death ages is then a measure
of the common family effect on death age. As
an alternative to the residual-covariance approach,
a kiindred-frallty model (Vaupel, 1990)
could be used where brothers share a level of
frailty. Then, with assumptions about the structure
of the frailty distribution, the parameters of
a hazard function and a frailty distribution could
be estimated (cf., James J. Heckman and
Christopher R. Taber, 1994).

IV. The Production and Distribution
of Spiritual Assets

There is, finally, the issue of spiritual or immaterial
assets. A good place to begin is with
Socrates' question: What is the good life? That
was a critical question not only for the sons of
rich Athenians but for sons of the landed rich
throughout history. Freed of the need to work
in order to satisfy their material needs, they
sought self-realization in public service, military
adventures, philanthropy, the arts, theology,
ethics, and moral philosophy. Their
preoccupation with immaterial commodities
led Adam Smith to argue that the landed aristocracy
ignored their property and lacked interest
in advancing methods of cultivation.
"The situation of such a person," he wrote,
"naturally disposes him to attend rather to ornamenit
which please his fancy, than to profit
for which he has so little occasion" (Smith,
1937 pp. 364, 891-892).

In a world in which all but a small percenitage
are lacking in adequate nutrition and other
necessities of life, self-realization may indeed
seem like a mere ornanent, but not in a country
where even the poor are rich by past or
Third World standards. That is the case in
America today since the poverty line is at a
level of real income that was attained by only
those in the top 10 percent of the income distribution
a century ago'3 (Fogel, 2000). Technophysio
evolution has made it possible to
extend the quest for self-realization from a
minute fraction of the population to almost the
whole of it. Although those who are retired
will have more time to pursue self-realization,
even those still in the labor force will have
sufficient leisure to seek it either within their
professional occupations or outside of them
(Peter Laslett, 1991; Hans Lenk, 1994),
Some proponents of egalitarianism insist
on characterizing the material level of the
poor today as being harsh. They confound
current and past conditions of living. Failure
to recognize the enormous material gains
over the last century, even for the poor, impedes
rather than advances the struggle
against chronic poverty in rich nations, the
principal characteristic of which is spiritual
estrangement from the mainstream society.
Although material assistance is an important
element in the struggle to overcome spiritual
estrangement, such assistance will not be
properly targeted if one assumes that iinprovement
in material conditions naturally
leads to spiritual improvement.
That proposition, so widely emnbraced by the
nore secular of the economic reformers of the
twentieth century, did more to promote the
consumerism of the 1920's and 1930's than to
produce spiritual regeneration. The middle and
working classes became preoccupied with the
acquisition of automobiles and those household
appliances made possible by electricity:
" This calculation neglects a variety of goods and services,
such as heart bypass operations, that are available
today through Medicaid, but which were unavailable at
any income a century ago.


### ---Economics-1999-0-14.txt---
irons, lamps, telephones, toasters, refrigerators,
radios, and washing machines. It was this
consumerism that led such progressive critics
of the era as Vernon Louis Parrington, pioneer
in the development of intellectual history, to
decry the "cash-register" mentality of modern
urban life (Parrington, 1930 p. 81).
The economist's traditional measures of income
inequality are inadequate measures of
both egalitarian gains and egalitarian failures
(cf., Amartya K. Sen, 1996). They focus on a
variable- money income-that currently accounts
for less than half of real consumption
and which in a generation may slip to just a
quarter of real consumption. The most serious
threats to egalitarian progress-certainly the
most intractable forms of poverty-are related
to the unequal distribution of spiritual (immaterial)
resources (cf., William Julius
Wilson, 1996).

Realization of the potential of an individual
is not something that can be legislated by the
state, nor can it be provided to the weak by the
strong. It is something that has to develop
within each individual. Moreover, which aspect
of one's potential an individual chooses
to develop most fully, such as choosing a profession,
is purely an aesthetic consideration.
John Dewey and one of his chief disciples,
Richard Rorty of the University of Virginia,
contend that in a democracy self-realization is
"4a particularized creative project of individual
growth" (Richard Shusterman, 1994 pp. 396-
97). The emphasis on individual choice does
not mean that other individuals and institutions
play no role in shaping those choices. Quite
the contrary, the quality of the choices and the
range of opportunity depends critically on how
well endowed an individual is with spiritual
resources. But the spiritual resources needed
for personal development are unequally distributed
among young and old, among men
and women, among various ethnic groups, and
among rich and poor. Those rich who are continuously
preoccupied with sensual gratifications
are as likely to fail in self-realization as
the poor who share that preoccupation. Although
the rich may have the wealth to buy
the treatment needed from highly trained professionals,
inexpensive character and religious
counseling may serve as well, if not better, for
the addicts of all classes.

The full list of maldistributed spiritual resources
is too long to discuss adequately here
but I have in mind such vital assets as a vision
of opportunity and a work ethic. A common
characteristic of such assets is that they are
transferred from one individual to another
mainly very early in the life of the recipient.
Self-esteem and a sense of family solidarity
begin to be transferred to children along with
mother's milk and with pabulum. Other spiritual
resources begin to be transferred during
the toddler and toilet-training stages, including
a sense of discipline, a capacity to resist or
control impulses, and a sense of community.
Telling nursery rhymes such as "TThis little
piggy went to market," recounting the autobiographies
of the mother and father, and family
histories going back two or three
generations convey such spiritual resources as
a work ethic, a sense of the mainstream of
work and life, an ethic of benevolence, a vision
of opportunity, and a thirst for knowledge.
Although these early transfers of spiritual
resources are enriched and expanded by primary,
secondary, and college education, and
by occupational and other later-life experiences,
the salience of these later transfers depends
in no small measure on what happens at
home before formnal education begins. It is,
therefore, necessary to remedy the maldistribution
of spiritual resources early in life, because
the most spiritually deprived infants will
often be born to single, teenaged mothers who
are themselves spiritually deprived.
Some young mothers are too deprived, or
too young, to call on their own life experiences
to transmit a sense of discipline and of opportunity,
a work ethic, a family ethic, a sense of
self-esteem, and a knowledge of the mainstream
of work and life. The deprivation can
be addressed by promoting a system of mentoring,
taking advantage of the increasingly
large number of retired men and women who
have abundant -spiritual resources. Such mentoring
programs would be useful, not only for
the toddlers and their mothers and fathers, but
also for the elderly who are looking for ways
to enrich their retirement years.
Despite the improvements in their material
conditions of life, including comfortable
stocks of consumer durables, the elderly today
suffer from a maldistribution of immaterial


### ---Economics-1999-0-15.txt---
resources that traces back to the conditions of
their youth. Persons aged 80 today were borm
in 1918 or 1919. Only 43 percent of that cohort
graduated from high school and less than 15
percent entered college. Even among the
youngest cohort of the elderly, those born in
1933 and 1934, only half graduated from high
school and about 20 percent entered college
(U.S. Bureau of the Census, 1975 p. 379).
These cohorts also suffered from high infant
death rates, poor nutrition in early infancy, and
early onset of chronic diseases, as compared
with cohorts born since World War II.
Hence, despite their relatively high levels of
income and stocks of consumer durables, the
maldistribution of spiritual resources is substantial.
Depression, alienation, and substance
abuse are common (S. C. Samuels, 1997).
Those who are most afflicted are lonely, have
few communal contacts, live in retirement
homes rather than in their own households,
and sense a loss of control over their personal
lives (W. L. Fletcher and R. 0. Hansson,
1991; K. Pahkala et al. 1992; K. Yamashita
et al., 1993). Recent studies also indicate that
those who lacked immaterial resources early
in life have difficulty in attaining selfrealization
after retirement (J. C. HIenretta,
1997; J. E. Mutchler et al., 1997).
I have emphasized the level of education because
recent studies indicate that the capacity
of the elderly to engage effectively in physical
activity was strongly correlated with education
early in life. Education also affects cognitive
ability and the rate of illness (J. W. Rowe and
R. L. Kahn, 1997). Consequently, individuals
who were deprived of adequate education in
youth are, for that reason among others, relatively
deprived of both physiological and spiritual
resources in late life.

Despite the long reach of youthful deprivation,
there are enough other factors affecting
the quality of elderly life to pennit redistributions
that compensate for previous deficits.
On the physiological side, for example, there
are effective medical interventions that can increase
the quality of life and longevity. Although
the elderly are eligible for Medicare to
pay for treatment, the quality of treatment is
variable, and many individuals may be
shunted to low-quality care. Moreover, some
interventions are denied, or are more reluc-
tantly ordered, for the elderly than for the middle
aged.

Because spiritual resources are so unequally
distributed among the elderly, different
programs are needed for different
strata. The mninority of the current elderly
who are well educated, the 14 percent with
at least bachelor's degrees, most of whom
had professional careers, have developed
some innovative programs (U.S. Bureau of
the Census, 1997 p. 160). In Great Britain
one of these is called the "University of the
Third Age." This educational program is not
aimed at providing credentials for those
about to embark upon new careers, but at
satisfying the thirst for knowledge. It is
based on the proposition that education, and
the acquired knowledge and skills, are a
source of self-satisfaction, even if they do
not enhance an individual's employability.
As Laslett (1991 pp. 171-71) put it:
Reading in a literature, mastering a language,
unraveling a point in logic or philosophy,
understanding the objectives

set for themselves by poets, painters,
novelists or architects, these things extend
your appreciation and your miastery
of your world, your objective and your
subjective world as well. They are fulfilling,
and adding to other people's

knowledge is the most fulfilling of all.
Programs su'h as the University of the
Third Age will become increasingly important
as the baby boomers and others of the more
highly educated cohorts of the post-World
War :I era begin to retire. However, today and
for the next decade, the bulk of the elderly
lacks the skills to create and participate in such
high-level programs as the University of the
Third Age. A recent survey of adult literacy
revealed that more than half of the elderly population
suffers from functional illiteracy.
These individuals may be able to sign their
name or read very simple material, but they
cannot follow instructions for taking medicines
or cope with a variety of documents encountered
in daily living (R. Boling, 1,998).
Those who suffer from low levels of literacy
are educable. Engaging them in intellectual activities
has a significant influence on their
physiological perormance. Recent studies reveal


### ---Economics-1999-0-16.txt---
more physiological plasticity than was
previously suspected. The capacity for selfimprovement
continues into old age and appropriately
designed programs can return

diminished individuals to earlier levels of
functioning (Rowe and Kahn, 1997; cf.,
Wachter and Finch, 1997).

Peer tutoring has a two-way effect, since
it is beneficial both to the learner and to the
tutor. Both gain from involvement in social
networks that enhance mood, combat depression,
and reduce the risk of suicide. For
widowed men, the benefits are physiological
as well as psychological. Men in situations
that provide higher social support have significantly
lower losses of cortisol, epinephrine,
and norepinephrine (hormones that
reduce pain, stimulate the functioning of the
heart, and improve electrical transmission
across cells). Statistical analysis indicates a
positive relationship for both men and
women between social support and physical
performance. For the tutors, being active in
such productive and emotionally rewarding
activities serves to retain a sense of relative
youthfulness. Thus volwork, because it is effective,
because it is emotionally rewarding,
and because it is what the tutors want to do,
adds significantly to national product.
Use of fiscal policy to correct the maldistribution
of income is based, explicitly or
implicitly, on the ethical proposition that
those households at the top of the income
distribution have more income than they
ought to have. What about the case of spiritual
redistributions? Are spiritual resources
maldistributed because virtue is too heavily
concentrated? Government cannot legislate
the transfer of virtue as it does with money
income. Even if they desired to do so, those
rich in virtue or in the family ethic, or in
benevolence, could not transfer spiritual resources
by writing out checks denominated
in virtue, benevolence, or family solidarity.
Those poor in these spiritual resources acquire
more of them only through the process
of self-realization, through a concerted effort
to develop as fully as possible the virtuous
aspects of their nature.

Those rich in spiritual resources can help
those who are spiritually deprived by counseling
them, by providing spiritual companionship
and moral support, by informing and
teaching those who are deprived about existing
opportunities and procedures, and by helping
to raise their self-esteem. But this process of
correcting the maldistribution of spiritual resources
not only leaves those who are deprived
better off, it also increases the spiritual resources
of those who have virtue in abundance.
In contrast to income redistribution,
spiritual redistribution is not a fixed-sum game
in which some people can become better off
only if other people are made worse off. It is
a game in which total resources increase and
the share of the deprived in this larger total
may also increase without in any way diminishing
those who have a superabundance of
spiritual resources.

Some economists may be astonished by my
claim that in some respects the discipline has
fallen seriously behind the economy. After all,
if it were so, who would know about it before
them? The answer lies in the subtext of this
address. To understand where the economy is
and how it is evolving one needs to study not
only the present but the past. In the 1940's,
Kuznets ( 1941 ), musing about some of the analytical
mistakes made in the aftermath of the
Great Depression said: "A broader historical
background might have prevented some economists
from ignoring the dependence of their
generalizations upon transient historical conditions.
"' That advice is as good today as it
was a half century ago.
 ## Economics-2000-0


### ---Economics-2000-0-01.txt---
People today have more adequate nutrition
than ever before and acquire that nutrition at the
lowest cost in all human history, while the
world has more people than ever before-not by
a little but by a lot. This is an achievement that
many have argued could not be realized.
Throughout history there have been those who
believed that food shortages and famine were
the fate of humanity and that the world's population
was restricted not by human decisions
on fertility but by limitations imposed by nature.
Unfortunately for nearly all of human history
and for the vast majority of the world's
people, this pessimism was justified. In the last
two centuries, and especially in the twentieth
century, all has changed to a remarkable degree.
The twentieth century can be remembered as the
century in which hunger could have been eliminated
and, to a significant extent, has been.
I. Food and Population Growth

Thomas Robert Malthus, publishing the first
edition of his famous An Essay on the Principles
of Population in 1798, is usually credited
with the pessimistic view that population had a
tendency to outrun the available food supply
and was held in check by vice and misery--war,
disease, or starvation-but he was not the originator
of the idea. At least two millennia earlier
it was written in the Bible: "When goods increase,
those who eat them increase." (Ecclesiastes
5.)

Quintus Septimus Florence Tertillianus wrote:
Indeed it is certain, it is clear to see, that
the earth itself is currently more cultivated
and developed than in earlier times.
Now all places are accessible, all are docurnented,
all are full of business. The

most charming farms obliterate empty
places, ploughed fields vanquish forests,
herds drive out wild beasts, sandy places
are planted with crops, stones are fixed,
swamps drained, and there are such great
cities where formerly hardly a hut ... everywhere
there is a dwelling, everywhere
a multitude, everywhere a government,
everywhere there is life. The greatest evidence
of the large number of people: we
are burdensome to the world, the resources
are scarcely adequate to us; and
our needs straiten us and complaints are
everywhere while already nature does not
sustain us. Truly, pestilence and hunger
and war and flood must be consider as a
remedy for nations, like a pruning back of
the human race becoming excessive in
numbers. (Bart K. Holland, 1993 pp.
328-29.)

This was written about 200 A.D. when the
world's population was approximately 200 million.
Note that the quotation includes nearly all
the modern complaints about the effects of excessive
population on the environment-deforestation,
loss of biological diversity, farming
unsuitable land, drainage of the natural refuges
for wildlife-as well as the massing of people in
cities.

While Malthus was neither the first, nor the
last, to claim population growth carried with it
the seeds of disaster for humanity, he may have
been the first to significantly modify his view
that population growth would inevitably press
against the food supply. Five years after the
gloomy first edition, in the second edition of An
Essay on the Principles of Population he significantly
modified his major conclusion. After
noting the recent growth of European population,
he wrote:

.. fewer famines and fewer diseases arising
from want have prevailed in the last
century than in those that preceded it. On
the whole, therefore, though our future
prospects respecting the mitigation of the


### ---Economics-2000-0-03.txt---
evils arising from the principle of population
may not be so bright as we could
wish, yet they are far from entirely disheartening
and by no means preclude

gradual and progressive improvement in
human society which, before the late wild
speculations on the subject, was the object
of rational expectations. (Malthus, 1992
pp. 330-31.)

Unfortunately, the Malthus of the first edition
provided a model of population growth that
accurately depicted the experience of nearly all
of human history and was generally valid up to
the time he wrote. But he was also correct in his
view that during the century following the publication
of the essay that there would be gradual
improvement in the well-being of people in the
part of the world where he lived, namely Europe.
However, the progress was neither uniform
nor without interruption, as witness the
Irish famine of the 1840's and other famines
and food shortages that occurred in several European
countries during the nineteenth century.
In the second edition he recognized a third
factor that affected population growth in addition
to vice and misery-namely, the desire for
self-improvement. In other words, families
were willing and capable of influencing the
number of children by changing the age of
marriage, for example.

What made it possible for the world to escape
from what could be called the Malthusian trap?
The answer is simple: the creation of knowledge.
1 While there had been improvements in
agriculture for many millennia through knowledge
gained from practical experience-learning
by doing-there was an explosion of
knowledge over the past two centuries that
made possible an unparalleled increase in per
capita well-being, not just in terms of food but
in all aspects of life. Fundamentally, new technologies
have been developed at a rate unprecedented
by historical standards.

Consider the following (Angus Maddison,
1995):

1. The increase in the world's population in the
decade of the 1980's of 844 million was
nearly as large as the world's total population
in 1800 of 900 million.

2. During the decade of the 1980's the increase
in the world's gross domestic product per
capita equaled the estimated per capita gross
domestic product in 1820 (Maddison, 1995
p. 228).

Measured in 1990$, in the 1980's the per
capita GDP increased by $661; the per capita
figure for 1820 was $651.

3. The physical world-the land, the water, the
air, the sun-was basically the same in the
1980's as it was in 1820 or 1020 or 109000
years ago. Some might argue that the physical
world was less valuable than in the past.
The magnitude of the increase in the world's
output since 1820 is much greater than is directly
implied by the comparability of the increase
in GDP during the 1980's and in all
history up to 1820. The increase in real world
output during the 1980's was more than 10
times the output in 1820 and the world output in
1990 was 40 times that of 1820. How could
these enormous changes have occurred? They
occurred because we have found ways to offset
the limitations that natural resources imposed
on the world's output in times past as well as
improving greatly the amount and productivity
of human capital. We have not found how to
repeal the principle of diminishing marginal
returns. But we have found low cost and abundant
substitutes for natural resources important
in the production process.

As I will show later, the improvement in
well-being of the world's population goes far
beyond the enormous increase in the value of
the world's output. The improvements are evident
in fewer famines, increased caloric intakes,
reduced child and infant mortality, increased
life expectancy, great reductions in time
worked, and greatly increased percentage of the
population that is literate.


### ---Economics-2000-0-04.txt---
This lecture proceeds in the following way. I
show that for most of human history, life was
both short and difficult for the vast majority of
the world's people, that food supply was a major
factor affecting population size, and that
consumption-nonfood as well as food-was
very limited. I shall then turn to the question of
how the developed world escaped from the
Malthusian trap during the nineteenth century
and how the developing world did so more than
a century later.

I emphasize three major factors that I consider
responsible for the remarkable period of
economic growth that has occurred over the past
two or three centuries that permitted breaking
free from the limits imposed by the food supply.
The first factor is the significant advances in
agricultural productivity in the eighteenth and
nineteenth centuries. The increase in agricultural
productivity made possible the development
of cities as the major focus of further
economic development and growth. The second
factor is the enormous increase in knowledge
over the past two centuries made possible by
increasing population and rising real per capita
incomes resulting from the economic growth
from the mid-eighteenth century. The increase
in real incomes permitted the allocation of substantial
resources to the creation of knowledge.
This reallocation was associated with the rapid
development of two institutions- universities
and research institutes. The third factor, contrary
to what is often assumed, is that the response
of families to the removal of restraints
on their well-being imposed by limited food
supplies was not significantly increased fertility;
population growth resulted primarily from
mortality declines. Population growth was not
limited by the supply of food but by the decisions
of families.

The three reasons do not fully explain why
population growth did not spoil everything for
the developed world in the nineteenth century
and for the developing world more recently.
One reason the growth of food output in the
nineteenth century may not have been overwhelmed
by population growth was that knowledge
and technology required for the rapid
reduction of mortality did not become generally
available until near the end of the nineteenth
century and, further, the rapid increase in the
population of cities limited the decline in mortality.
The decline in mortality in the developing
world in the twentieth century was far more
rapid and resulted in a much higher rate of
population growth than the experience of the
nineteenth century even though fertility declined
significantly.

II. Agriculture and Food Before the
Nineteenth Century

Agriculture is a relatively recent inventionthe
transformation from hunting and gathering
to planting and growing crops and domesticating
animals probably occurred about 10 millennia
ago. At that time the world's population was
about 4 million and a large fraction of all resources
were devoted to obtaining food, and a
very poor lot it was.

As of 1800 it is estimated that '75 to 80
percent of the working population in the developed
world was engaged in agriculture (Bairoch,
1988 p. 287). In the rest of the world, with
nearly 80 percent of the world's population, the
percentage of workers engaged in agriculture
was certainly higher-of the order of 85 to 90
percent. In 1891, 90 percent of the population of
India was rural (Adna Ferrin Weber, 1899 p.
124) and as late as 1949, 89 percent of China's
population was rural.2 Unfortunately we do not
have evidence that permits -us to directly determine
the amount of food available in ancient
times. But if life expectancy in Roman times
were 25 years (Donald J. Bogue, 1969 p. 566),
it is highly probable that the available food per
capita was very limited. Fogel has estimated the
2 To say that a high percentage of the working population
was engaged in agriculture does not mean that this
was the share of labor time devoted to the production of
agricultural products. Prior to the nineteenth century,
farm families were largely self-sufficient and had to
devote a significant percentage of their labor to provide
for their housing, to collect fuel, and to make their own
clothing, bedding and furniture, and most of the simple
tools and equipment that they used on the farm and in the
house. In temperate climatic zones much of this work
may have occurred during the colder periods when little
or no farm work could be done, other than caring for
animals. The percentage of the labor force engaged in
agriculture is a rough inverse indicator of the percentage
of their food production that they could sell to nonfarmers.
If 80 percent of the population were engaged in
agriculture, they could sell or trade approximately a fifth
of the food they produced.


### ---Economics-2000-0-05.txt---
daily caloric supplies at the beginning of the
eighteenth century of 2,095 for Great Britain
and 1,657 for France (Fogel, 1996 p. 10). Life
expectancy for England in 1725 is estimated to
be 32 years and in France in 1750, 26 years.
Over the next century per capita calories increased
by approximately 10 percent-to 2,237
in Great Britain and to 1,846 in France and by
1800 life expectancy in England was 36 years
and in France 32 years (Fogel, 1996 p. 2).
Obviously other factors had a role in the increase
in life expectancy, but it is unlikely that
these increases and those that followed would
have occurred in the absence of improved nutritional
intake.

A life expectancy of between 25 and 30 years
was probably the fate of most of humanity
throughout recorded history until about 1650
(Bogue, 1969 p. 566). It was not until the seventeenth
century that there is evidence that life
expectancy increased significantly beyond what
it was in Roman or earlier times. As noted,
England and France, two of the wealthiest nations
of the world, had life expectancies at the
beginning of the eighteenth century that were
not much above what had prevailed throughout
human history.

It is probable that the per capita calorie supplies
for the world prior to the seventeenth
century were in the range found in England and
France at the beginning of the eighteenth century-
perhaps from 1,650 to less than 2,000.
These are in the range of calorie intakes in many
developing countries in 1934-1938, the earliest
date for which we have estimates for several
countries. Calorie intakes in India, the Philippines,
Peru, Colombia, and Mexico were in the
range of 1,800 to 2,000 calories (M. K. Bennett,
1976 p. 199). By 1934-1938 these countries
had significant population growth rates and at
earlier times consumption was probably rather
less.

Important evidence that the productivity of
agricultural resources in Europe remained relatively
constant and low was that in Europe,
excluding Russia, there was almost no change
in the percentage of the total urban population
between 1300 and 1800. Bairoch (1988 pp. 177,
216) estimates that in 1300 the urban population
of Europe was 10.4 percent of the total; five
centuries later in 1800 it was only 12.1 percent
and most of this increase occurred in England in
the eighteenth century. The nineteenth century
saw a major increase in urbanization; by the end
of the nineteenth century the urban population
was 37.9 percent of Europe's population, Russia
excluded. The urban population increased
almost five times in the nineteenth century after
little more than doubling in the previous five
centuries (Bairoch, 1988 pp. 177, 216). The
development of cities as a significant share of
the total population became possible only after
farmers increased production relative to their
own consumption.

Further evidence that per capita output of
agriculture increased very little throughout history
was the slow growth of world population
until nearly the beginning of the nineteenth century.
During the first millennium of the current
era, the annual rate of growth was 0.04 percent,
a doubling time of 1700 years. In the 700 years
ending in 1700, the rate of population growth
was 0.12 percent, a doubling time of about 580
years. The rate of population growth did increase
in the eighteenth century-to 0.41 percent
annually. But even at that rate it would take
179 years to double.

Europe's population during the eighteenth
century increased from 102 million to 154 million
and this increase was made possible by a
significant increase in food production occurred.
However, except in England, there was
no increase in urbanization so it is reasonable to
infer that in the rest of Europe the growth of
food production increased at approximately the
same rate as population during the eighteenth
century.

I[I. Agriculture and the Industrial Revolution
What was agriculture's contribution to the
Industrial Revolution? The Industrial Revolution
is generally considered to have started


### ---Economics-2000-0-06.txt---
about 1750 in England and up to a century later
in the rest of Europe. As I have noted, the share
of cities in Europe's population had remained
nearly constant for the previous five centuries,
and that this meant that the available food supply
had not increased significantly faster than
population and, equally important, that the productivity
of labor in agriculture also had not
increased enough to permit labor to transfer out
of agriculture and migrate to cities. The midpoint
of the eighteenth century marks a striking
dividing point in the demographic and agricultural
history of England. In the century up to
1750 England's population was static. It actually
declined in some periods and increased at
an annual rate of only 0.1 percent or by 10
percent in the entire century (E. A. Wrigley and
R. S. Schofield, 1981 pp. 528-29). Life expectancy
may have actually declined. But between
1751 and 1801 its population grew at an annual
rate of 0.81 percent and the total increased by
50 percent. In the next half century the population
nearly doubled. Nearly all of the increase in
the United Kingdom's population in the nineteenth
century was urban (Bairoch, 1988
p. 290).4

What increased productivity in agriculture so
strikingly after 1750? Exact causes are unknown,
but many changes were involved, including
the spread of two high-yielding crops
from the Americas-corn (maize) and potatoes,
the enclosure movement, the elimination of fallow,
improved drainage, and increased availability
of animal manure made possible by the
cultivation of turnips as feed for cattle (David S.
Landes, 1969 p. 76). There was a major increase
in England and the Netherlands in the grain-toseed
ratio to more than ten in 1750-1820 compared
to seven in the two centuries prior to 1700
(B. H. Slicher van Bath, 1963).5 Such a large
increase in yield was almost certainly associated
with a significant increase in labor productivity.
While urbanization increased very little in England
during the last half of the eighteenth century,
there was a significant expansion of
industrial activity in rural areas, especially in
the production of textiles.

Significant increases in per capita food production
and in labor productivity in agriculture
were necessary conditions for the Industrial
Revolution which was associated with, and may
well have been advanced by, rapid population
growth. The increase in food production was
necessary to sustain the rapid population
growth; the growth in agricultural labor productivity
was required to permit a reduction in the
share of labor devoted to farming and to permit
the transfer of labor to the cities. I do not argue
that the improvements in food supply and labor
productivity were sufficient conditions for the
Industrial Revolution. It is quite probable that
the agricultural and industrial revolutions had
the same sources and each was affected by
developments in the other.

IV. The Mechanical Revolution

The improvements in labor productivity in
agriculture occurring in the eighteenth century
and the early years of the nineteenth century
were insignificant compared to the changes that
occurred in the rest of the century. Throughout
history for most of the world's population the
4During the nineteenth century the United Kingdom was
transformed from a rural to an urban economy. In 1800 it
was 19.4 percent urban and in 1900, 67.6 urban. The population
increased from 16 million in 1800 to 41 million in
1900, an increase of 25 million, while the urban population
increased by 24 million. Continental Europe was only 32.9
percent urban in 1900 (Bairoch, 1988 p. 290).
5 It is not clear whether the increase in food production
in Western Europe in the eighteenth century after 1725 was
due primarily to productivity improvements or to a lengthy
period of good weather. Ronald Maxwell Hartwell (1971 p.
283) speculates that good weather might have been a sig-
nificant factor:

An important, neglected and completely exogenous
factor (perhaps the only one that is really exogenous)
, affecting eighteenth-century growth was the
beginning after 1730 of more than two centuries of
"good" weather (relative to the "bad" weather of the
previous more than two centuries). The coincidence
in time of the beginnings of the industrial revolution
and a long period of good weather suggests the
possibility of a theory of weather-induced growth:
better weather enabling an increase in food supplies,
increasing real incomes, and also an increasing pop-
ulation.

The weather, through its effects on food production, could
have been a factor in the slow growth of England's population
of 0.13 percent annually from 1621 to 1721 compared
to the much more rapid annual growth rate of 0.77 percent
in the following century (Wrigley and Schofield, 1981 pp.
528-29).


### ---Economics-2000-0-07.txt---
major source of calories has been grain-of the
order of 75 to 80 percent (Bennett, 1976 p. 206).
Until the early nineteenth century the serious
bottleneck in the production of grain was harvesting.
The plow was introduced several millennia
before, and it saved labor, but at a time of
the year when labor was not scarce. Plowing
could be done over an extended period of time,
but harvesting in most areas had to be done in a
brief period to prevent the crop being harmed or
destroyed by wind, rain, or frost.
At the beginning of the nineteenth century
grain was harvested by the same methods as in
the fourteenth century and probably much earlier-
the sickle, the scythe, and the cradle. T'he
invention and introduction of the reaper in
America in the second quarter of the nineteenth
century changed all that. The reaper was soon
followed by the binder, which was a reaper with
an attachment that brought the grain straw together
in a bundle and tied it with twine. The
binder was complemented by the thresher that
saved a great deal of labor, though at a time less
critical than the savings made possible by the
reaper and binder. In turn the binder and
thresher were largely replaced by the combine,
but not until well into the twentieth century.
Many other machines and tools were part of
the mechanical revolution. Very important
was harnessing the internal combustion engine
to create the tractor. The labor savings of
the mechanical revolution were enormous. It
is estimated that the direct labor input used to
produce a ton of grain in the United States
declined by 70 percent in the nineteenth century
(Martin R. Cooper et al., 1947). Consequently
in the developed countries after the
mid-nineteenth century the transfer of labor
from agriculture to nonagricultural pursuits
was more likely limited by the rate of growth
of nonagricultural employment than by the
labor requirements of agriculture.
V. Land Was Not the Scarce Resource
While today many give emphasis to the limited
supply of land of good quality as a major
impediment to further increases in food production,
throughout nearly all of human history
land has not been an important factor limiting
production. It had to have been something else
when the world's population was 500 million,
as it was in the sixteenth century, or perhaps
even when the population first reached one billion,
early in the nineteenth century. Given the
state of knowledge that existed until quite recently,
the primary limiting factor was labor.
Labor limited the amount of food that could be
produced by a family and, as noted, for much of
human history, farm families were barely able
to produce enough for their own consumption
with little surplus for trade with others. Until
quite recently this surplus was hardly more than
a quarter or a fifth of what they produced. A
good indicator that land was not the limiting
factor is that until the beginning of the nineteenth
century yields were calculated per unit of
seed, not per unit of land (Slicher van Bath,
1963).

Ester Boserup (1965) makes a convincing
case that labor and not land was the limiting
factor in agricultural output until quite recently.
She showed how farmers adapted to increasing
population by modifying the ways that land was
utilized, shifting from slash and burn and long
fallow to shorter periods of fallow and in Western
Europe eliminating fallow entirely. They
found ways other than fallow to maintain the
fertility of the soil-the use of manure and
legumes, for example. These changes were the
result of new knowledge, knowledge largely
derived from experience of the farmers themselves
and were a response to the growing population
and the need to expand food production.
VI. The Increased Role of Knowledge
As noted, the enormous increase in the
world's output over the past two centuries has
been due in large part to the advancement of
knowledge combined with the increase in human
resources, both in number and capabilities,
and savings translated into physical capital. We
do not have more natural resources than existed
in the distant past, yet output has increased
many fold. What is the source of the increased
knowledge? Two factors have been importantone
is simply the growth of population and the
other is that rising real per capita incomes have
made it possible for specialization in the production
of knowledge and for devoting a significant
share of our resources to that effort.
Michael Kremer (1993) makes a convincing
case for the conclusion that a larger population


### ---Economics-2000-0-08.txt---
leads to greater creation of knowledge. First, the
larger the population, the greater the benefit
from a given improvement in productivity resulting
from new knowledge. Second, with a
larger population, there are more individuals
capable of making a significant discovery or
adding to knowledge. It is not that today we are
smarter or more intelligent than populations a
century ago, two centuries ago, or a millennium
ago. Presumably the distribution of talents or
intelligence is the same today as at any past
time. But there are many, many more of us and
if the distribution of talents has not changed,
there are many more individuals capable of
advancing knowledge.

But it is not only that there are more of us
available to add to the world's knowledge, but
with the improvements in agricultural productivity,
the expansion of the cities, and the very
large increases in real per capita incomes that
have occurred over the past two centuries, institutions
have been created specifically to advance
and transmit knowledge. I refer to
universities and research institutes, including
both public and private ones. It was not that
prior to the nineteenth and twentieth centuries
that there were no individuals who had the
intelligence, time, curiosity, and energy for the
creation of knowledge. But their numbers were
limited. Our lives, however, are greatly influenced
by those who developed the reaper and
the binder, the internal combustion engine, the
steam engine, the railroad, electricity, the telephone,
and by those who discovered the small
pox vaccine and the germ theory of disease. But
by the beginning of the twentieth century their
effects on the lives of individuals were limited
compared to the effects of the recent increases
in knowledge and their applications.
When as many as 80 to 85 percent of the
world's labor force was engaged in farming, a
small percentage of a much smaller world population
had the time and resources to devote to
producing nonfood products, such as clothing,
tools, roads, and housing, let alone acquiring
new knowledge and technology. In 1990 in the
developed world no more than 10 percent of its
labor force was engaged in agriculture and in
the developing world approximately 60 percent
(World Bank, 1997 p. 220). Not only are there
about seven times as many people as there were
in 1800, but a significant percentage of this
much larger population specializes in the creation
of knowledge compared to the very small
number who could do so just two centuries ago.
The modern university, with many faculty devoting
their time to research in science and
graduate education, is a very recent creationsuch
institutions hardly existed before the middle
of the nineteenth century. German
universities dominated the world' s graduate education
in the nineteenth century. Yet as of
1900 in all the German universities there were
only 38,000 students and 1,830 faculty
(Friedrich Paulsen, 1908 p. 193); these are totals
for all colleges and universities, not just those
engaged in graduate education.
In 1869-1970, only a single Ph.D. was
awarded in the United States (U.S. Bureau of
the Census, Department of Commerce, 1960).
The development of colleges and universities
after 1869-1970 was quite remarkable. In that
year there were 563 colleges and universities
with a total faculty of 5,553 and 52,000 students.
The contribution to new knowledge had
to be limited; there were approximately ten faculty
members per institution, including, I assume,
the president who probably spent much
of the available time trying to find enough financial
resources to keep the institution open.
Sixty years later, for example, there were
82,000 faculty, 1.1 million students, and 2,299
doctorates awarded. Further rapid expansion of
higher education came after World War II and
by 1994/1995 there were an estimated 915,000
faculty, 14.3 million students, and 43,000 doctorates
awarded (Thomas I). Snyder, 1993).
In part, as a result of World War II, the
governmental support of research in universities
and federal research laboratories was greatly
expanded and many private research institutes
were created and developed. Prior to World
War II federal support of research was largely
concentrated in agriculture and the military.6
6One of the great achievements of the presidency of
Abraham Lincoln was the creation of a land grant college in
each state. These colleges were to specialize in agriculture
and engineering and to provide e-ducation that emphasized
the practical as well as the theoretical. Approximately two
decades later the agricultural experiment stations were established.
The establishment of the agricultural experiment
stations were especially noteworthy. Agriculture was a competitive
industry, which meant that no individual farmer
could devote any significant amount of resources to


### ---Economics-2000-0-09.txt---
As the twentieth century ends, both the share
and the absolute amount of the world's resources
devoted to the development of new
knowledge are vastly greater than at the beginning
of the century. Equally important is that
the share of resources devoted to the wide distribution
of that knowledge has also increased
greatly.

VII. Population Growth-Roles of Fertility
and Mortality

My third point is that population growth in
the developed countries in the nineteenth century
and in the developing countries in the twentieth
century was due almost entirely to
mortality declines and not to fertility increases.
In other words, the response of men and women
to improved circumstances-improved nutrition
and higher incomes-was not to increase
fertility significantly. For the nineteenth century
the picture is clear. For the three European
countries, both fertility and mortality declined.
In England and Sweden there was an increase in
fertility during the latter half of the eighteenth
century but the increase was small (less than 10
percent) and lasted less than 50 years before
declining throughout the nineteenth century.
The increases in fertility had little or no effect
on population growth in Europe, with a modest
positive effect in Sweden and England from
perhaps 1750 to 1800, but with declines
throughout the nineteenth century.
The total fertility rate (average number of
children per woman) for Sweden increased from
4.21 in 1750 to 4.68 in 1800 but then declined
continuously throughout the nineteenth century,
reaching a level of 1.90 in 1990 (Massimo LiviBacci,
1992 p. 122). Life expectancy in the last
20 years of the eighteenth century was 34 years,
increasing to 39 years by 1835 and to 54 years
in the first decade of the twentieth century
(Nathan Keyfitz and Wilhelm Flieger, 1968 pp.
36-37).

Total fertility rates for England increased
from 5.28 in 1750 to 5.87 in 1775 and then
declined to 1.96 in 1900 (Livi-Bacci, 1989 p.
122). In the early period of the Industrial Revolution
there seems to have been a small positive
response in fertility to the improved
circumstances that lasted less than half a century
and had only a modest effect on the rate of
population growth. In England life expectancy
was 32 years in the last fifth of the seventeenth
century and remained at that level in the years
before 1750. It increased to 36 years by the end
of the eighteenth century and to 41 years by the
middle of the nineteenth century (Wrigley and
Schofield, 1981 pp. 528-29) and continued to
increase thereafter. In France the fertility and
mortality trends are very clear-the total fertility
rate was low in 1825 at 3.42 and fell continuously
reaching 2.14 in 1900 (Livi-Bacci,
1992 p. 122). Life expectancy increased from
about 28 years in 1760 to 40 years in 1840 and
to 46 years at the end of the century (Wrigley,
1987 pp. 274).

The data on fertility and mortality available
for the developing countries since 1960 prove
that the source of the rapid population growth in
these countries was the decline in mortality
rather than an increase in fertility. In fact, both
mortality and fertility fell much more rapidly in
the developing countries in the twentieth century
than in the developed countries in the nineteenth
century prior to 1875.7 Excluding China,
which has had coercive restraints on fertility,
the decline in fertility in the 31 lowest-income
countries from 1960 to 1995 was 38 percent
(United Nations Development Program
[UNDP], 1998). Over the same period of time,
life expectancy increased from 42 years to 59
years for the same countries. But the fertility
decline lagged behind the mortality decline by a
decade or more and high rates of population
growth occurred in the 1960's and the 1970's.
For example, between 1960 and 1978, in the 38


### ---Economics-2000-0-10.txt---
low-income economies the crude death rate declined
31.5 percent while the crude birth rate
declined 14.4 percent (China excluded) (World
Bank, 1980). The annual population growth rate
for the same countries was 2.5 percent for
1960-1970 and 2.2 percent for 1970-1978
(World Bank, 1980).

Why has fertility declined as real per capita
incomes have increased? At low levels of income,
with agriculture as the major occupation,
children have a positive benefit in increasing the
level of income of the parents as well as providing
security against illness and old age. Children,
and their growth and development, enter
into the utility functions of parents positively.
Parents realize satisfaction both in terms of the
quantity and quality of their children (Becker,
1991). As real per capita incomes increase, the
structure of the benefits from children change.
The direct contribution of children to the income
and material welfare of their parents diminishes
and in urban communities becomes
negative; even in agricultural communities
where incomes and the level of mechanization
are high, children make modest contributions to
current incomes and fertility in rural and urban
areas in the United States are now the same.
However, the utility that parents derive from
their children's growth and development increases
as their incomes increase and the emphasis
on the quality is reflected in increased
investment in their children. Children have increasingly
become a consumption good as real
per capita incomes have increased. Thus the
desired number of children is negatively related
to real income and as contraceptive knowledge
and technology have improved, families now
have the ability to achieve the number of children
desired to a greater degree and at lower
cost than in decades past.

VIII. Why the Nineteenth Century Had Lower
Population Growth

The differences in the population growth
rates between Europe in the nineteenth century
and in the developing countries in the twentieth
century are very great. During the first half of
the nineteenth century, the annual growth rate
for Europe (excluding Russia) was 0.55 percent;
from 1850 to 1880, 0.60, and for the last two
decades, 0.80 percent. The annual rate of increase
for the developing regions for 1950 to
1995 was 2.0 percent. In 1900 Europe's population
(excluding Russia) was 285 million. If
Europe's population during the nineteenth century
had grown at the developing countries' rate
from 1950 to 1995, its population would have
exceeded a billion in 1900, more than three
times its actual population. Could Europe have
accommodated such a large population in 1900
without a significant reduction in its real per
capita income at the end of the century? Obviously
we will never know, but it seems very
unlikely that it could have unless many of the
technological developments of the twentieth
century had occurred much, much earlier. While
it was true that the rate of economic growth, as
measured by changes in real GDP per capita,
was much slower in Europe in the nineteenth
century than in the developing countries in the
twentieth century (Maddison, 1995), it is worth
exploring why population growth was relatively
slow in Europe.

One factor responsible for the slow growth of
population in Europe was the significant increase
in the percentage of the population living
in cities-in 1800 the percentage was 12.1 and
in 1900 it was 37.9. Throughout the nineteenth
century, cities had much higher rates of mortality
than rural areas. Migration from rural areas
was the source of the growth of cities. Not only
did death rates in cities exceed birth rates, but
their death rates were significantly higher than
in rural areas.

Bairoch (1988 p. 230) reports that in Western
Europe throughout the nineteenth century infant
mortality rates in urban areas exceeded the rates
in rural areas by 30 to 60 percent. In Sweden in
the early nineteenth century infant deaths accounted
for approximately 25 percent of all
deaths (Keyfitz and Flieger, 1968). But the difference
in mortality was not confined to infants;
life expectancy at age 15 in Sweden was more
than four years higher in rural than in urban
areas in 1881-1890 (Bairoch, 1988 p. 235).8
8 The higher rates of infant mortality and mortality in
general in urban than rural areas has long been well known
to demographers. Wrigley and Schofield (1981 p. 415), for
example, wrote as follows:

... the absence of any improvement in mortality in
the second and third quarters of the nineteenth


### ---Economics-2000-0-11.txt---
The difference between population growth rates
in the nineteenth and twentieth centuries was
due primarily to the advancement of knowledge
and technology that permitted much more rapid
reductions in mortality in the twentieth century,
even in the world's poorest countries. There
was rather limited progress in reducing death
rates until the midpoint of the nineteenth century
in Sweden and significantly later for England.
The basic environmental problems of
unclean water, inadequate sanitation, and childhood
infectious diseases still took a major toll
until the early years of the twentieth century.
The infant mortality rate in New York City in
1890 was 264 per 1,000 births, more than double
the rate of 121 in rural areas (Weber, 1899).
The rates of decline in fertility in the developing
world were greater than in the developed
world for the periods under consideration. However,
the crude birth rates in the developing
world started their decline from a much higher
level than existed in the developed world in the
eighteenth and nineteenth centuries. The crude
birth rates in Sweden and England in the eighteenth
and early nineteenth centuries were in the
range of 35 to 37 per thousand population while
the averages for low-income countries in 1960
was 48 and the average for middle-income
countries was 40 (World Bank, 1980). Consequently,
the birth rate in the low-income countries
needed to fall by a third just to reach the
level prevailing in Western Europe at the beginning
of the Industrial Revolution. The more
rapid growth of population in today's developing
world was not due to increases in fertility
but to the combined effects of high initial rates
of fertility and rapid declines in mortality.
IX. Wide Distribution of Benefits of Knowledge
In recent years a great deal of concern has
been expressed about the lack of convergence of
per capita income among countries and increasing
inequality within countries. The emphasis
on increased income inequality has left the impression
that most measures of well-being have
become much more unequal as well. The use of
differences in per capita incomes as measures of
either satisfaction or well-being assumes that
these measures are proportional to income, a
conclusion that cannot be supported.
Contrary to views that are widely held, for
several important measures of well-being there
has been great improvement, both absolutely
and relatively, in the lives of the people of the
low-income developing countries.9 The benefits
of the growth of knowledge have not been restricted
to the countries responsible for the advances
in knowledge but have spread

throughout most of the world. And they would
have spread more quickly and more widely if
the policies of many governments had been
more supportive of economic growth and
development.

Improvements in the conditions of life in
terms of nutrition, infant mortality, and life expectancy,
have occurred at a much faster pace in
the developing countries in the twentieth century
than in the developed countries in the nineteenth
century. These improvements have
occurred with much larger populations and
greater population densities. The population of
the developing countries at the end of the twentieth
century is 4.84 billion, an increase of 350
percent in a century. This compares to the increase
in Europe in the nineteenth century of 85
percent. But not only did the improvements in
well-being occur more rapidly during similar
periods of economic development, but with respect
to several very important variables the
gaps narrowed significantly during the twentieth
century and especially during the last half of
that century.


### ---Economics-2000-0-12.txt---
A striking difference between the developing
countries at the end of the twentieth century and
the developed countries at the end of the nineteenth
century is that the lowest-income countries
have achieved rates of infant mortality and
life expectancy that are significantly superior to
those attained at the end of the nineteenth century
in the developed countries.

The infant mortality rate for 30 low-income
developing countries, including China, in
1960 was 157 per thousand births and it declined
by 62 percent to 62 in 1996 (IJNDP,
1998). The infant mortality rate in 1900 in
nine European countries ranged from a low of
121 in Denmark to a high of 216 in Austria
(Bairoch, 1988 p. 231). The rate in the United
States was 160. The teeming cities of the
developing world are often viewed negatively
by observers from the developed world. A
recent publication has the title "The Poverty
of Cities in the Developing World" (Martin
Brockerhoff and Ellen Brennan, 1997). Yet
the study revealed that cities with a population
of a million or more included in their
sample had an infant mortality rate of 60 per
thousand births in the 1990's (Brockerhoff
and Brennan, 1997 p. 24). Two comparisons
are relevant. Estimates of infant mortality for
the first decade of the twentieth century were
500 to 600 for Bombay and 350 to 400 in
Singapore (Bairoch, 1988 p. 450). The infant
mortality rate in New York City at the beginning
of the twentieth century was 264 (Weber,
1899 p. 346).

While infant mortality rates in cities in the
developed world at the beginning of the twentieth
century were higher than in rural areas, in
the developing world the rates in the cities are
now below those in rural areas (Brockerhoff and
Brennan, 1997 p. 24). These data indicate both
the large magnitude of the declines and the
extent to which the improvements have been
widely shared, even among many of the lowestincome
families in the world.

Significant increases in life expectancy were
achieved between 1900 and mid-century in the
developing countries, though much greater absolute
increases came later. The best available
long-term data are for India, at least one benefit
of being a British colony. Life expectancy in
India in 1900 was 23 years, increasing to 32
years in the 1940's (Bogue, 1969 p. 572) and to
43 years in 1960 and 62 years in 1996.10 Life
expectancy increased by 170 percent in a century-
-it is now almost three times what it was a
hundred years ago. Life expectancy in the
world's poorest countries has increased since
1940 at a far more rapid rate than achieved in
any country in the developed world in the nineteenth
century. This is an area where there has
been convergence between the rich and the poor
countries over the past several decades. At the
end of the nineteenth century life expectancy in
seven industrial countries ranged from 46 to 51
(Bogue, 1969). In 33 low-income countries in
1996 life expectancy was 64 years, compared to
44 years in 1960, an absolute increase of 20
years (World Bank, 1998). Furthermore, the
improvements in infant mortality and life expectancy
have been achieved at lower levels of
real per capita incomes than those prevailing in
the developed countries at the beginning of the
twentieth century (Maddison, 1995). The
knowledge about the benefits of clean water and
sanitation has been widely distributed and investments
have been made to make those benefits
widely available."

Since the late 1940's there has been greater
improvement in the world's availability of food
than had occurred in all previous history. The
evidence is the increase in per capita food supplies
that occurred in the developing countries,
with nearly 80 percent of the world's population.
There are reasonably reliable estimates of
daily per capita supply of calories for 1961-
10 Other data on life expectancies indicate that in 1896-
1997 it was 32 years in Russia and in 1900, 35 years in
Spain (Bogue, 1969 pp. 576-77). The earliest estimates for
several African countries are for the 1940's and 1950's and
range from 27 to 40 years.

"1 The area of the world that appears to have participated
least in the recent improvements in well-being is SubSaharan
Africa. Per capita income measures show that in the
majority of the countries real incomes have fallen in the last
three decades. However, John Sender (1999) has shown that
there have been significant improvements in infant mortality
and life expectancy in the region. He estimates that in 1950
life expectancy was about 30 years, whereas a girl born in
1995 had a life expectancy of 54 years (Sender, 1999 p. 91).
The unweighted child mortality rate (per thousand) for the
18 countries was 254 in 1960 and it declined to 139 in 1995,
a decline of 45 percent. Sender shows that by a number of
other measures, such as female enrollment rates in schools,
the number of tractors, average grain yields, and the number
of radio and television sets, substantial improvements oc-
cuffed.


### ---Economics-2000-0-13.txt---
1963 when the daily per capita supply was
1,940 k/cals. In 1994-1996 the supply was
2,580 (Nikos Alexandratos, 1999 p. 5908)-a
remarkable increase of 33 percent, given that
population doubled during the period. Based on
the increase in per capita grain production from
1948-1952 to 1961-1963, it can be estimated
that the per capita calorie supply was about
1,700 in 1948-1952. The increase in calories
per capita available in the developing countries
from 1948-1952 to 1994-1996 was of the order
of 50 percent.12 Since developing countries
produce at least 90 percent of the food they
consume, this means that food production almost
trebled in four decades! This could not
have happened prior to the last half of the twentieth
century; the knowledge that made it possible
did not then exist.

Alexandratos (1999 p. 5908) provides a picture
of the improvement in food supplies that
adds another dimension to the increase in the
per capita availability in the developing countries:
" ... the part of the world population living
in countries where per capita food supplies are
still very low (under 2,200 k/cal/day) decreased
considerably to only 10% in the mid-1990s,
down from 56% 30 years earlier." Note that the
2,200 calories per day that is now defined as
very low is somewhat higher than was available
in England and significantly higher than in
France in 1800, just two centuries ago. True, the
average citizen of England and France as of
1800 was stunted and/or wasted-as short in
height and low in weight-just as the average
person in the developing countries with significantly
less than 2,200 calories per day would
be similarly designated today. What is important
is that this level of consumption applies to
countries with only a tenth of the world's population
while 200 years ago it applied to nearly
all of the world's population.
It is estimated that in 1990 approximately
780 million persons (19 percent of the developing-
country population, down from 36 percent
in 1969-1971) were malnourished (Alexandratos,
1995 p. 33). There is an adequate quantity
of food now produced to provide these people
with sufficient calories. However, as Adam
Smith taught us, policies are an important factor
determining how well a nation utilizes its resources.
The majority of the malnourished people
live in rural areas and most of them live in
countries that have had policies that discriminated
against agriculture and rural people for all
or part of the last three decades. The most
effective means to eliminate such malnutrition
is to increase the incomes of farm people in
these countries, something that would occur
with more appropriate policies (Johnson, 1999
p. 52).

In my opening sentence I stated that not only
are people better fed than ever before, but they
acquire their food at the lowest cost in all history.
There is no way to prove that the real cost
per calorie is the lowest in all history, but we do
know that nearly all people are now devoting a
smaller percentage of their consumption expenditures
to the acquisition of food than was true
at times past. As recently as 1955 families in the
United States allocated 23 percent of their consumption
expenditure to food; today it is about
10 percent. For Japan the reduction was from 54
to 20 percent and for South Korea from 50 to 36
percent. Between 1960 and 1990 the food share
in Thailand declined from 47 to 23 percent
(United Nations, National Account Statistics). It
is not unreasonable to assume that at the beginning
of the nineteenth century in the developed
countries that 70 percent or more of the consumption
expenditures went for food and that
the percentage was even higher at the beginning
of the twentieth century for the developing
countries. 13


### ---Economics-2000-0-14.txt---
X. Concluding Comments

During the last two centuries, and especially
in the twentieth century, there has been an enormous
increase in knowledge that has been transformed
into technology and ways of utilizing
resources more efficiently. It is not only that
knowledge has increased rapidly but the means
of communicating that knowledge in an effective
way have been markedly improved and the
knowledge has become much more accessible
throughout the world.

The rapid growth of knowledge has resulted
both from the growth of the world's
population and the increase in the percentage
of that population that is now able to devote
time and energy to the creation of knowledge.
It was not so long ago that farmers accounted
for 80 percent of the world's labor force and
they barely produced enough for themselves
with little left for exchange. As productivity
in agriculture increased, the rapid growth of
cities occurred and the growth in real per
capita incomes exceeded what had ever been
achieved before. The fact that during the
1980's the increase in the world's output was
ten times what world output was in 1820
illustrates how great has been the growth of
output in a very brief period of time.
But perhaps the greatest achievement of the
twentieth century is that the majority of the
poor people of the world have shared in the
improvements in well-being made possible by
the advancement of knowledge. Three measures
show how great these improvements
have been-infant mortality rates, life expectancy,
and per capita food supplies. The large
cities of the developing world now have infant
mortality rates about a quarter of those of
New York City in 1890. True, there is much
more that can be done to share more fully the
benefits of the knowledge base. And I am
confident that whoever speaks from this platform
just 25 years from now could point to
further dramatic reductions in worldwide inequalities
in well-being.

pollution, do have an adverse effect on life expectancy, but
these effects are probably measured in months rather than
years. But those of us who live in high-income countries
often consider problems from the standpoint of our environment,
not the environment facing the majority of the
world's population. For example, with respect to air pollution,
the World Health Organization has estimated that in
the world in 1996 there were 2.7 million deaths from air
pollution (UNDP, 1998 p. 70). However, less than 7 percent
were in industrial countries. Of the 2.5 million deaths in
developing countries, almost 2.2 million deaths were attributed
to indoor air pollution, not to outdoor pollution, and
most were in rural areas. As real incomes in the developing
countries increase, leading to improvements in the quality
of housing and the replacement of indigenous fuels by
modem fuels, the adverse environmental effects of air pollution
will be significantly reduced.
 ## Economics-2001-0


### ---Economics-2001-0-03.txt---
The resurgence of the American economy
since 1995 has outrun all but the most optimistic
expectations. Economic forecasting models
have been seriously off track and growth projections
have been revised to reflect a more
sanguine outlook only recently.! It is not surprising
that the unusual combination of more
rapid growth and slower inflation in the 1990's
has touched off a strenuous debate among economists
about whether improvements in America'
s economic performance can be sustained.
The starting point for the economic debate is
the thesis that the 1990's are a mirror image of
the 1970's, when an unfavorable series of "supply
shocks" led to stagflation-slower growth
and higher inflation.2 In this view, the development
of information technology (IT) is one of a
series of positive, but temporary, shocks. The
competing perspective is that IT has produced a
fundamental change in the U.S. economy, leading
to a permanent improvement in growth
prospects.

The relentless decline in the prices of information
technology equipment has steadily enhanced
the role of IT investment as a source of American
economic growth. Productivity growth in ITproducing
industries has gradually risen in importance
and a productivity revival is now under way
in the rest of the economy. Despite differences in
methodology and data sources, a consensus is
building that the remarkable behavior of IT prices
provides the key to the surge in economic growth.
In the following section I show that the foundation
for the American growth resurgence is
the development and deployment of semiconductors.
The decline in IT prices is rooted in
developments in semiconductor technology that
are widely understood by technologists and
economists. This technology has found its
broadest applications in computing and communications
equipment, but has reduced the cost of
a wide variety of other products.
A substantial acceleration in the IT price decline
occurred in 1995, triggered by a much
sharper acceleration in the price decline of
semiconductors in 1994. Although the decline
in semiconductor prices has been projected to
continue for at least another decade, the recent
acceleration could be temporary. This can be
traced to a shift in the product cycle for semiconductors
from three years to two years that
took place in 1995 as the consequence of intensifying
competition in markets for semiconductor
products.

In Section II I outline a framework for analyzing
the role of information technology in the
American growth resurgence. Constant quality
price indexes separate the change in the performance
of IT equipment from the change in price
for a given level of performance. Accurate and
timely computer prices have been part of the
U.S. National Income and Product Accounts
(NIPA) since 1985. Unfortunately, important
information gaps remain, especially on trends in
prices for closely related investments, such as
software and communications equipment.
The cost of capital is an essential concept for
capturing the economic impact of information
technology prices. Swiftly falling prices provide
powerful economic incentives for the substitution
of IT equipment for other forms of capital and for
labor services. The rate of the IT price decline is a


### ---Economics-2001-0-04.txt---
key component of the cost of capital, required for
assessing the impacts of rapidly growing stocks
of computers, communications equipment, and
software.

In Section III I analyze the impact of the 1995
acceleration in the information technology price
decline on U.S. economic growth. I introduce a
production possibility frontier that encompasses
substitutions between outputs of consumption
and investment goods, as well as inputs of capital
and labor services. This frontier treats IT
equipment as part of investment goods output
and the capital services from this equipment as
a component of capital input.

Capital input has been the most important
source of U.S. economic growth throughout the
postwar period. More rapid substitution toward
information technology has given much additional
weight to components of capital input
with higher marginal products. The vaulting
contribution of capital input since 1995 has
boosted growth by nearly a full percentage
point. The contribution of IT accounts for more
than half of this increase. Computers have been
the predominant impetus to faster growth, but
communications equipment and software have
made important contributions as well.
The accelerated information technology price
decline signals faster productivity growth in
IT-producing industries. In fact, these industries
have been the source of most of aggregate productivity
growth throughout the 1990's. Before
1995 this was due to the decline of productivity
growth elsewhere in the economy. The ITproducing
industries have accounted for about
half the surge in productivity growth since 1995,
but faster growth is not limited to these industries.
I conclude that the decline in IT prices will
continue for some time. This will provide incentives
for the ongoing substitution of IT for
other productive inputs. Falling IT prices also
serve as an indicator of rapid productivity
growth in IT-producing industries. However, it
would be premature to extrapolate the recent
acceleration in productivity growth in these industries
into the indefinite future, since this depends
on the persistence of a two-year product
cycle for semiconductors.

In Section IV I outline research opportunities
created by the development and diffusion of
information technology. A voluminous and rapidly
expanding business literature is testimony
to the massive impact of IT on firms and product
markets. Highest priority must be given to a
better understanding of the markets for semiconductors.
Although several models of the
market for semiconductors already exist, none
explains the shift from a three-year to a twoyear
product cycle.

The dramatic effects of information technology
on capital and labor markets have already
generated a substantial and growing economic
literature, but many important issues remain to
be resolved. For capital markets the relationship
between equity valuations and growth prospects
merits much further study. For labor markets
more research is needed on investment in information
technology and substitution among different
types of labor.

I. The Information Age

The development and deployment of information
technology is the foundation of the
American growth resurgence. A mantra of the
"new economy"-faster, better, cheaper-captures
the speed of technological change and
product improvement in semiconductors and
the precipitous and continuing fall in semiconductor
prices. The price decline has been transmitted
to the prices of products that rely heavily
on semiconductor technology, like computers
and telecommunications equipment. This technology
has also helped to reduce the cost of
aircraft, automobiles, scientific instruments, and
a host of other products.

Modem information technology begins with
the invention of the transistor, a semiconductor
device that acts as an electrical switch and encodes
information in binary form. A binary digit
or bit takes the values zero and one, corresponding
to the off and on positions of a switch. The
first transistor, made of the semiconductor germanium,
was constructed at Bell Labs in 1947
and won the Nobel Prize in Physics in 1956 for
the inventors-John Bardeen, Walter Brattain,
and William Shockley.4

The next major milestone in information technology
was the coinvention of the integrated circuit
by Jack Kilby of Texas Instruments in 1958
4 On Bardeen, Brattain, and Shockley, see: http://www.
nobel.se/physics/laureates/l956/.


### ---Economics-2001-0-05.txt---
and Robert Noyce of Fairchild Semiconductor in
1959. An integrated circuit consists of many, even
millions, of transistors that store and manipulate
data in binary form. Integrated circuits were originally
developed for data storage and retrieval and
semiconductor storage devices became known as
memory chips.5

The first patent for the integrated circuit was
granted to Noyce. This resulted in a decade of
litigation over the intellectual property rights.
The litigation and its outcome demonstrate the
critical importance of intellectual property in
the development of information technology.
Kilby was awarded the Nobel Prize in Physics
in 2000 for discovery of the integrated circuit;
regrettably, Noyce died in 1990.6
A. Moore's Law

In 1965 Gordon E. Moore, then Research
Director at Fairchild Semiconductor, made a
prescient observation, later known as Moore's
Law.7 Plotting data on memory chips, he observed
that each new chip contained roughly
twice as many transistors as the previous chip
and was released within 18-24 months of its
predecessor. This implied exponential growth
of chip capacity at 35-45 percent per year!
Moore's prediction, made in the infancy of the
semiconductor industry, has tracked chip capacity
for 35 years. He recently extrapolated this
trend for at least another decade.8
In 1968 Moore and Noyce founded Intel Corporation
to speed the commercialization of
memory chips.9 Integrated circuits gave rise to
microprocessors with functions that can be programmed
by software, known as logic chips.
Intel's first general purpose microprocessor was
developed for a calculator produced by Busicom,
a Japanese firm. Intel retained the intellectual
property rights and released the device
commercially in 1971.

The rapidly rising trends in the capacity of
microprocessors and storage devices illustrate the
exponential growth predicted by Moore's Law.
The first logic chip in 1971 had 2,300 transistors,
while the Pentium 4 released on November 20,
2000, had 42 million! Over this 29-year period the
number of transistors increased by 34 percent per
year. The rate of productivity growth for the U.S.
economy during this period was slower by two
orders of magnitude.

B. Semiconductor Prices

Moore's Law captures the fact that successive
generations of semiconductors are faster and better.
The economics of semiconductors begins with
the closely related observation that semiconductors
have become cheaper at a truly staggering
rate! Figure 1 gives semiconductor price indexes
constructed by Bruce T. Grimm (1998) of the U.S.
Bureau of Economic Analysis (BEA) and employed
in the U.S. National Income and Product
Accounts since 1996. These are divided between
memory chips and logic chips. The underlying
detail includes seven types of memory chips and
two types of logic chips.

Between 1974 and 1996 prices of memory
chips decreased by a factor of 27,270 times or at
40.9 percent per year, while the implicit deflator
for the gross domestic product (GDP) increased
by almost 2.7 times or 4.6 percent per year! Prices
of logic chips, available for the shorter period
1985 to 1996, decreased by a factor of 1,938 or
54.1 percent per year, while the GDP deflator
increased by 1.3 times or 2.6 percent per year!
Semiconductor price declines closely parallel
Moore's Law on the growth of chip capacity,
setting semiconductors apart from other products.
Figure 1 also reveals a sharp acceleration in
the decline of semiconductor prices in 1994 and
1995. The microprocessor price decline leapt to
more than 90 percent per year as the semiconductor
industry shifted from a three-year product
cycle to a greatly accelerated two-year
cycle. This is reflected in the 2000 Update of
the International Technology Road Map for
Semiconductors,'0 prepared by a consortium of
industry associations.


### ---Economics-2001-0-06.txt---
100,000.0

10,000.0

1,000.0

_0E

i 100.0

0

1.0

0.1

0.0

1959 1964 1969 1974 1979 1984 1989 1994 1999
-u-Computers - Memory- Logic

FIGURE 1. RELATIVE PRICES OF COMPUTERS AND SEMICONDUCTORS, 1959-1999
Note: All price indexes are divided by the output price index.
C. Constant Quality Price Indexes
The behavior of semiconductor prices is a
severe test for the methods used in the official
price statistics. The challenge is to separate observed
price changes between changes in semiconductor
performance and changes in price
that hold performance constant. Achieving this
objective has required a detailed understanding
of the technology, the development of sophisticated
measurement techniques, and the introduction
of novel methods for assembling the
requisite information.

Ellen R. Dulberger (1993) of IBM introduced
a "matched model" index for semiconductor
prices. A matched model index combines price
relatives for products with the same performance
at different points of time. Dulberger
presented constant quality price indexes based
on index number formulas, including the
[Irving] Fisher (1922) ideal index used in the
U.S. national accounts. 1 1 The Fisher index is the
geometric average of the familiar Laspeyres and
Paasche indexes.

W. Erwin Diewert (1976) defined a superlative
index number as an index that exactly
replicates aflexible representation of the underlying
technology (or preferences). A flexible
representation provides a second-order approximation
to an arbitrary technology (or preferences)
. A. A. Konus and S. S. Byushgens
(1926) first showed that the Fisher ideal index is
superlative in this sense. Laspeyres and Paasche
indexes are not superlative and fail to capture
substitutions among products in response to
price changes accurately.

Grimm (1998) combined matched model
techniques with hedonic methods, based on an
econometric model of semiconductor prices at
different points of time. A hedonic model gives
the price of a semiconductor product as a function
of the characteristics that determine performance,
such as speed of processing and storage
capacity. A constant quality price index isolates
the price change by holding these characteristics
of semiconductors fixed.

Beginning in 1997, the U.S. Bureau of Labor
" See J. Steven Landefeld and Robert P. Parker (1997).


### ---Economics-2001-0-07.txt---
Statistics (BLS) incorporated a matched model
price index for semiconductors into the Producer
Price Index (PPI) and since then the national
accounts have relied on data from the
PPI. Reflecting long-standing BLS policy, historical
data were not revised backward. Semiconductor
prices reported in the PPI prior to
1997 do not hold quality constant, failing to
capture the rapid semiconductor price decline
and the acceleration in 1994.

D. Computers

The introduction of the Personal Computer
(PC) by IBM in 1981 was a watershed event
in the deployment of information technology.
The sale of Intel's 8086-8088 microprocessor
to IBM in 1978 for incorporation into the PC
was a major business breakthrough for Intel.
12 In 1981 IBM licensed the MS-DOS
operating system from the Microsoft Corporation,
founded by Bill Gates and Paul Allen
in 1975. The PC established an Intel/
Microsoft relationship that has continued up
to the present. In 1985 Microsoft released the
first version of Windows, its signature operating
system for the PC, giving rise to the
Wintel (Windows-Intel) nomenclature for this
ongoing collaboration.

Mainframe computers, as well as PC's, have
come to rely heavily on logic chips for central
processing and memory chips for main memory.
However, semiconductors account for less
than half of computer costs and computer prices
have fallen much less rapidly than semiconductor
prices. Precise measures of computer prices
that hold product quality constant were introduced
into the NIPA in 1985 and the PPI during
the 1990's. The national accounts now rely on
PPI data, but historical data on computers from
the PPI, like the PPI data on semiconductors, do
not hold quality constant.

Gregory C. Chow (1967) pioneered the use of
hedonic techniques for constructing a constant
quality index of computer prices in research
conducted at IBM. Chow documented price declines
at more than 20 percent per year during
1960-1965, providing an initial glimpse of the
n'k ~~~~~~~~~13

remarkable behavior of computer prices. In
1985 the Bureau of Economic Analysis incorporated
constant quality price indexes for computers
and peripheral equipment constructed by
Rosanne Cole et al. (1986) of IBM into the
NIPA. Triplett (1986) discussed the economic
interpretation of these indexes, bringing the
rapid decline of computer prices to the attention
of a very broad audience.

The BEA-IBM constant quality price index
for computers provoked a heated exchange between
BEA and Edward F. Denison (1989), one
of the founders of national accounting methodology
in the 1950's and head of the national
accounts at BEA from 1979 to 1982. Denison
sharply attacked the BEA-IBM methodology
and argued vigorously against the introduction
of constant quality price indexes into the national
accounts.14 Allan Young (1989), then Director
of BEA, reiterated BEA's rationale for
introducing constant quality price indexes.
Dulberger (1989) presented a more detailed
report on her research on the prices of computer
processors for the BEA-IBM project. Speed of
processing and main memory played central
roles in her model. Triplett (1989) provided an
exhaustive survey of research on hedonic price
indexes for computers. Robert J. Gordon (1989,
1990) gave an alternative model of computer
prices and identified computers and communications
equipment, along with commercial aircraft,
as assets with the highest rates of price
decline.

Figure 2 gives BEA's constant quality index
of prices of computers and peripheral equipment
and its components, including mainframes,
PC's, storage devices, other peripheral
equipment, and terminals. The decline in computer
prices follows the behavior of semiconductor
prices presented in Figure 1, but in much
attenuated form. The 1995 acceleration in the
computer price decline parallels the acceleration
in the semiconductor price decline that
resulted from the changeover from a three-year
product cycle to a two-year cycle in 1995.


### ---Economics-2001-0-08.txt---
10,000 -

1,000 X

100

10 -

1948 1953 1958 1963 1968 1973 1978 1983 1988 1993 1998
- Computers Communications Software Services
FIGURE 2. RELATIVE PRICES OF COMPUTERS, COMMUNICATIONS, SOFTWARE, AND SERVICES, 1948-1999
Note: All price indexes are divided by the output price index.
E. Communications Equipment and Software
Communications technology is crucial for the
rapid development and diffusion of the Internet,
perhaps the most striking manifestation of information
technology in the American economy.
15 Kenneth Flamm (1989) was the first to
compare the behavior of computer prices and
the prices of communications equipment. He
concluded that the communications equipment
prices fell only a little more slowly than computer
prices. Gordon (1990) compared Flamm's
results with the official price indexes, revealing
substantial bias in the official indexes.
Communications equipment is an important
market for semiconductors, but constant quality
price indexes cover only a portion of this equipment.
Switching and terminal equipment rely
heavily on semiconductor technology, so that
product development reflects improvements in
semiconductors. Grimm's (1997) constant quality
price index for digital telephone switching
equipment, given in Figure 3, was incorporated
into the national accounts in 1996. The output
of communications services in the NIPA also
incorporates a constant quality price index for
cellular phones.

Much communications investment takes the
form of the transmission gear, connecting data,
voice, and video terminals to switching equipment.
Technologies such as fiber optics, microwave
broadcasting, and communications satellites
have progressed at rates that outrun even the dramatic
pace of semiconductor development. An
example is dense wavelength division multiplexing
(DWDM), a technology that sends multiple
signals over an optical fiber simultaneously. Installation
of DWDM equipment, beginning in
1997, has doubled the transmission capacity of
fiber-optic cables every 6-12 months.'6
15 A general reference on the Internet is Soon-Yong Choi
and Andrew B. Whinston (2000). On Internet indicators,
see: http://www.internetindicators.com/.
16 Rick Rashad (2000) characterizes this as the "demise"
of Moore's Law. Jeff Hecht (1999) describes DWDM technology
and provides a general reference on fiber optics.


### ---Economics-2001-0-09.txt---
Both software and hardware are essential for
information technology and this is reflected in
the large volume of software expenditures. The
eleventh comprehensive revision of the national
accounts, released by BEA on October 27,
1999, reclassified computer software as investment.
17 Before this important advance, business
expenditures on software were treated as current
outlays, while personal and government expenditures
were treated as purchases of nondurable
goods. Software investment is growing rapidly
and is now much more important than investment
in computer hardware.

Robert P. Parker and Grimm (2000) describe
the new estimates of investment in software. BEA
distinguishes among three types of softwareprepackaged,
custom, and own-account software.
Prepackaged software is sold or licensed in standardized
form and is delivered in packages or
electronic files downloaded from the Internet.
Custom software is tailored to the specific application
of the user and is delivered along with
analysis, design, and programming services required
for customization. Own-account software
consists of software created for a specific application.
However, only price indexes for prepackaged
software hold performance constant.
Parker and Grimm (2000) present a constant
quality price index for prepackaged software,
given in Figure 3. This combines a hedonic
model of prices for business applications software
and a matched model index for spreadsheet
and word-processing programs developed
by Steven D. Oliner and Daniel E. Sichel
(1994). Prepackaged software prices decline at
more than 10 percent per year over the period
1962-1998. Since 1998 the BEA has relied on a
matched model price index for all prepackaged
software from the PPI; prior to 1998 the PPI
data do not hold quality constant.
BEA's prices for own-account software are
based on programmer wage rates. This implicitly
assumes no change in the productivity of
computer programmers, even with growing investment
in hardware and software to support
the creation of new software. Custom software


### ---Economics-2001-0-10.txt---
prices are a weighted average of prepackaged
and own-account software prices with arbitrary
weights of 75 percent for own-account and 25
percent for prepackaged software. These price
indexes do not hold the software performance
constant and present a distorted picture of software
prices, as well as software output and
investment.

F. Research Opportunities

The official price indexes for computers and
semiconductors provide the paradigm for economic
measurement. These indexes capture the
steady decline in IT prices and the recent acceleration
in this decline. The official price indexes
for central office switching equipment and prepackaged
software also hold quality constant.
BEA and BLS, the leading statistical agencies
in price research, have carried out much of the
best work in this area. However, a critical role
has been played by price research at IBM, long
the dominant firm in information technology.'8
It is important to emphasize that information
technology is not limited to applications of
semiconductors. Switching and terminal equipment
for voice, data, and video communications
has come to rely on semiconductor technology
and the empirical evidence on prices of this
equipment reflects this fact. Transmission gear
employs technologies with rates of progress that
far outstrip those of semiconductors. This important
gap in our official price statistics can
only be filled by constant quality price indexes
for all types of communications equipment.
Investment in software is more important
than investment in hardware. This was essentially
invisible until BEA introduced new measures
of prepackaged, custom, and own-account
software investment into the national accounts
in 1999. This is a crucial step in understanding
the role of information technology in the American
economy. Unfortunately, software prices
are another statistical blind spot, with only
prices of prepackaged software adequately represented
in the official system of price statistics.
The daunting challenge that lies ahead is to
construct constant quality price indexes for custom
and own-account software.

II. The Role of Information Technology
At the aggregate level IT is identified with the
outputs of computers, communications equipment,
and software. These products appear in
the GDP as investments by businesses, households,
and governments along with net exports
to the rest of the world. The GDP also includes
the services of IT products consumed by households
and governments. A methodology for analyzing
economic growth must capture the
substitution of IT outputs for other outputs of
goods and services.

While semiconductor technology is the driving
force behind the spread of IT, the impact of
the relentless decline in semiconductor prices is
transmitted through falling IT prices. Only net
exports of semiconductors, defined as the difference
between U.S. exports to the rest of the
world and U.S. imports, appear in the GDP.
Sales of semiconductors to domestic manufacturers
of IT products are precisely offset by
purchases of semiconductors and are excluded
from the GDP.

Constant quality price indexes, like those reviewed
in the previous section, are a key component
of the methodology for analyzing the
American growth resurgence. Computer prices
were incorporated into the NIPA in 1985 and
are now part of the PPI as well. Much more
recently, semiconductor prices have been included
in the NIPA and the PPI. Unfortunately,
evidence on the prices of communications
equipment and software is seriously incomplete,
so that the official price indexes are seriously
misleading.

A. Output

The output data in Table 1 are based on the
most recent benchmark revision of the national
accounts, updated through 1999.19 The output
concept is similar, but not identical, to the concept
of gross domestic product used by the
BEA. Both measures include final outputs purchased
by businesses, governments, households,
and the rest of the world. Unlike the BEA
concept, the output measure in Table 1 also
18 See Alfred D. Chandler, Jr. (2000 Table 1.1 p. 26).
19 See Jorgenson and Kevin J. Stiroh (2000b Appendix
A) for details on the estimates of output.


### ---Economics-2001-0-11.txt---



### ---Economics-2001-0-12.txt---
TABLE 2-GROwTH RATES OF OUTPUTS AND INPUTS
1990-1995 1995-1999

Prices Quantities Prices Quantities
Outputs

Gross domestic product 1.99 2.36 1.62 4.08
Information technology -4.42 12.15 -9.74 20.75
Computers -15.77 21.71 -32.09 38.87
Software -1.62 11.86 -2.43 20.80
Communications equipment -1.77 7.01 -2.90 11.42
Information technology services -2.95 12.19 -11.76 18.24
Noninformation technology investment 2.15 1.22 2.20 4.21
Noninformation technology consumption 2.35 2.06 2.31 2.79
Inputs

Gross domestic income 2.23 2.13 2.36 3.33
Information technology capital services -2.70 11.51 -10.46 19.41
Computer capital services -11.71 20.27 -24.81 36.36
Software capital services -1.83 12.67 -2.04 16.30
Communications equipment capital services 2.18 5.45 -5.90 8.07
Noninformation technology capital services 1.53 1.72 2.48 2.94
Labor services 3.02 1.70 3.39 2.18
Note: Average annual percentage rates of growth.
includes imputations for the service flows from
durable goods, including IT products, employed
in the household and government sectors.
The imputations for services of IT equipment
are based on the cost of capital for IT described
in more detail below. The cost of capital is
multiplied by the nominal value of IT capital
stock to obtain the imputed service flow from IT
products. In the business sector this accrues as
capital income to the firms that employ these
products as inputs. In the household and government
sectors the flow of capital income must
be imputed. This same type of imputation is
used for housing in the NIPA. The rental value
of renter-occupied housing accrues to real estate
firms as capital income, while the rental
value of owner-occupied housing is imputed to
households.

Current dollar GDP in Table 1 is $9.8 trillions
in 1999, including imputations, and real
output growth averaged 3.46 percent for the
period 1948-1999. These magnitudes can be
compared to the current dollar value of $9.3
trillions in 1999 and the average real growth
rate of 3.40 percent for the period 1948-1999
for the official GDP. Table 1 presents the current
dollar value and price indexes of the GDP
and IT output. This includes outputs of investment
goods in the form of computers, software,
communications equipment, and non-IT investment
goods. It also includes outputs of non-IT
consumption goods and services as well as imputed
IT capital service flows from households
and governments.

The most striking feature of the data in Table
1 is the rapid price decline for computer investment,
17.1 percent per year from 1959 to 1995.
Since 1995 this decline has almost doubled to
32.1 percent per year. By contrast the relative
price of software has been flat for much of
the period and began to fall only in the late
1980's. The price of communications equipment
behaves similarly to the software price,
while the consumption of capital services from
computers and software by households and governments
shows price declines similar to computer
investment.

The top panel of Table 2 summarizes the
growth rates of prices and quantities for major
output categories for 1990-1995 and 1995-
1999. Business investments in computers,
software, and communications equipment are
the largest categories of IT spending. Households
and governments have also spent sizable
amounts on computers, software,
communications equipment and the services
of information technology. Figure 4 shows
that the output of software is the largest IT


### ---Economics-2001-0-13.txt---
category as a share of GDP, followed by the
outputs of computers and communications
equipment.

B. Capital Services

This subsection presents capital estimates for
the U.S. economy for the period 1948 to 1999.20
These begin with BEA investment data; the
perpetual inventory method generates estimates
of capital stocks and these are aggregated, using
service prices as weights. This approach, originated
by Jorgenson and Zvi Griliches (1996), is
based on the identification of service prices with
marginal products of different types of capital.
The service price estimates incorporate the cost
of capital.21

The cost of capital is an annualization factor
that transforms the price of an asset into the
price of the corresponding capital input.22 This
includes the nominal rate of return, the rate of
depreciation, and the rate of capital loss due to
declining prices. The cost of capital is an essential
concept for the economics of information
technology,23 due to the astonishing decline of
IT prices given in Table 1.

The cost of capital is important in many areas
of economics, especially in modeling producer
behavior, productivity measurement, and the
economics of taxation.24 Many of the important
issues in measuring the cost of capital have been
debated for decades. The first of these is incorporation
of the rate of decline of asset prices
into the cost of capital. The assumption of perfect
foresight or rational expectations quickly
emerged as the most appropriate formulation


### ---Economics-2001-0-14.txt---
and has been used in almost all applications of
the cost of capital.25

The second empirical issue is the measurement
of economic depreciation. The stability of
patterns of depreciation in the face of changes in
tax policy and price shocks has been carefully
documented. The depreciation rates presented
by Jorgenson and Stiroh (2000b) summarize a
large body of empirical research on the behavior
of asset prices.2 A third empirical issue is the
description of the tax structure for capital income.
This depends on the tax laws prevailing
at each point of time. The resolution of these
issues has cleared the way for detailed measurements
of the cost of capital for all assets that
appear in the national accounts, including information
technology.27

The definition of capital includes all tangible
assets in the U.S. economy, equipment and
structures, as well as consumers' and government
durables, land, and inventories. The capital
service flows from durable goods employed
by households and governments enter measures
of both output and input. A steadily rising proportion
of these service flows are associated
with investments in IT. Investments in IT by
business, household, and government sectors
must be included in the GDP, along with household
and government IT capital services, in
order to capture the full impact of IT on the U.S.
economy.

Table 3 gives capital stocks from 1948 to
1999, as well as price indexes for total domestic
tangible assets and IT assets-computers, software,
and communications equipment. The estimate
of domestic tangible capital stock in
Table 3 is $35.4 trillions in 1999, considerably
greater than the $27.9 trillions in fixed capital
estimated by Shelby W. Herman (2000) of
BEA. The most important differences reflect the
inclusion of inventories and land in Table 3.
Business IT investments, as well as purchases
of computers, software, and communications
equipment by households and governments,
have grown spectacularly in recent years, but
remain relatively small. The stocks of all IT
assets combined account for only 4.35 percent
of domestic tangible capital stock in 1999. Table
4 presents estimates of the flow of capital
services and corresponding price indexes for
1948-1999.

The difference between growth in capital services
and capital stock is the improvement in
capital quality. This represents the substitution
towards assets with higher marginal products.
The shift toward IT increases the quality of
capital, since computers, software, and communications
equipment have relatively high marginal
products. Capital stock estimates fail to
account for this increase in quality and substantially
underestimate the impact of IT investment
on growth.

The growth of capital quality is slightly less
than 20 percent of capital input growth for the
period 1948-1995. However, improvements in
capital quality have increased steadily in relative
importance. These improvements jumped
to 44.9 percent of total growth in capital input
during the period 1995-1999, reflecting very
rapid restructuring of capital to take advantage
of the sharp acceleration in the IT price decline.
Capital stock has become progressively less accurate
as a measure of capital input and is now
seriously deficient.

Figure 5 gives the IT capital service flows as
a share of gross domestic income. The second
panel of Table 2 summarizes the growth rates of
prices and quantities of capital inputs for 1990-
1995 and 1995-1999. Growth of IT capital services
jumps from 11.51 percent per year in
1990-1995 to 19.41 percent in 1995-1999,
while growth of non-IT capital services increases
from 1.72 percent to 2.94 percent. This
reverses the trend toward slower capital growth
through 1995.

C. Labor Services

This subsection presents estimates of labor
input for the U.S. economy from 1948 to 1999.
These incorporate individual data from the Censuses
of Population for 1970, 1980, and 1990,
as well as the annual Current Population Surveys.
Constant quality indexes for the price and
25 See, for example, Jorgenson et al. (1987 pp. 40-49)
and Jorgenson and Griliches (1996).
26 Jorgenson and Stiroh (2000b Table B4 pp. 196-97)
give the depreciation rates employed in this study. Barbara
M. Fraumeni (1997) describes depreciation rates used in the
NIPA. Jorgenson (2000) surveys empirical studies of depre-
ciation.

27 See Jorgenson and Yun (2001). Diewert and Denis A.
Lawrence (2000) survey measures of the price and quantity
of capital input.


### ---Economics-2001-0-15.txt---



### ---Economics-2001-0-16.txt---
TABLE 4- INFORMATION TECHNOLOGY CAPITAL SERVICES AND GROSS DOMESTIC INCOME
Gross domestic

Computer Software Communications Total IT income
Year Value Price Value Price Value Price Value Price Value Price
1948 1.7 1.20 1.7 4.31 307.7 0.14
1949 1.3 0.79 1.3 2.83 297.0 0.14
1950 1.8 0.91 1.8 3.27 339.0 0.15
1951 2.1 0.90 2.1 3.21 370.6 0.15
1952 2.6 0.94 2.6 3.36 387.4 0.15
1953 3.2 0.96 3.2 3.46 418.2 0.15
1954 2.7 0.70 2.7 2.49 418.3 0.15
1955 3.6 0.85 3.6 3.05 461.3 0.16
1956 4.2 0.87 4.2 3.12 484.7 0.17
1957 3.7 0.68 3.7 2.44 503.6 0.17
1958 4.1 0.68 4.1 2.45 507.2 0.17
1959 0.2 444.36 0.1 0.63 5.2 0.80 5.5 2.87 551.9 0.18
1960 0.2 433.59 0.1 0.62 5.4 0.75 5.6 2.68 564.9 0.18
1961 0.3 637.21 0.1 0.58 5.6 0.71 6.0 2.59 581.8 0.18
1962 0.4 508.68 0.2 0.62 6.6 0.76 7.2 2.71 623.3 0.19
1963 0.6 311.81 0.3 0.58 6.5 0.67 7.3 2.34 666.9 0.20
1964 0.8 211.28 0.4 0.60 7.1 0.67 8.3 2.26 726.5 0.21
1965 1.3 182.17 0.6 0.59 9.1 0.78 11.0 2.52 795.1 0.22
1966 2.2 173.57 1.0 0.64 9.6 0.73 12.8 2.40 871.3 0.23
1967 2.3 110.97 1.1 0.50 9.8 0.66 13.2 2.01 918.2 0.23
1968 2.6 87.05 1.6 0.60 10.2 0.61 14.5 1.86 973.0 0.24
1969 2.8 68.23 1.7 0.52 11.3 0.61 15.8 1.76 1,045.8 0.25
1970 3.6 65.38 2.3 0.56 13.3 0.65 19.1 1.83 1,105.2 0.26
1971 5.2 72.48 3.7 0.77 14.9 0.67 23.9 1.99 1,178.8 0.27
1972 4.9 48.57 4.0 0.71 16.6 0.69 25.4 1.85 1,336.2 0.30
1973 4.4 33.06 4.5 0.71 22.8 0.88 31.7 2.04 1,502.5 0.32
1974 6.6 38.82 5.1 0.70 20.3 0.72 32.0 1.84 1,605.9 0.34
1975 5.9 28.43 6.7 0.80 23.2 0.77 35.7 1.85 1,785.8 0.37
1976 6.6 26.07 7.7 0.81 25.0 0.78 39.2 1.84 2,017.5 0.41
1977 7.0 20.69 8.4 0.82 41.8 1.20 57.2 2.40 2,235.7 0.44
1978 11.8 22.49 9.7 0.86 35.5 0.93 57.0 2.07 2,517.7 0.47
1979 11.6 13.33 11.6 0.90 47.9 1.14 71.1 2.15 2,834.9 0.51
1980 16.6 11.81 13.6 0.91 42.0 0.90 72.2 1.82 2,964.5 0.53
1981 17.7 7.89 15.5 0.90 40.5 0.79 73.6 1.53 3,285.2 0.58
1982 19.6 5.93 17.6 0.89 43.1 0.77 80.3 1.41 3,445.4 0.60
1983 26.4 5.46 20.6 0.91 49.4 0.82 96.4 1.43 3,798.8 0.66
1984 36.1 4.87 25.4 0.96 54.3 0.83 115.7 1.41 4,288.1 0.71
1985 39.6 3.70 30.6 0.99 63.1 0.89 133.3 1.35 4,542.6 0.73
1986 43.1 3.04 35.3 0.99 69.3 0.89 147.6 1.27 4,657.4 0.73
1987 53.4 2.93 42.1 1.04 86.5 1.02 181.9 1.36 5,078.1 0.77
1988 52.7 2.31 50.5 1.10 104.1 1.14 207.3 1.36 5,652.0 0.81
1989 57.6 2.08 60.4 1.13 105.8 1.07 223.8 1.29 5,988.8 0.84
1990 64.7 2.01 67.2 1.08 109.8 1.04 241.7 1.25 6,284.9 0.86
1991 64.2 1.76 70.8 1.00 104.2 0.93 239.2 1.12 6,403.3 0.88
1992 71.7 1.66 89.9 1.11 112.2 0.96 273.7 1.16 6,709.9 0.91
1993 77.8 1.45 90.4 0.98 126.9 1.03 295.1 1.11 6,988.8 0.92
1994 80.1 1.19 109.5 1.05 142.4 1.10 331.9 1.10 7,503.9 0.96
1995 99.3 1.12 115.5 0.99 160.7 1.16 375.6 1.09 7,815.3 0.96
1996 123.6 1.00 131.9 1.00 149.0 1.00 404.5 1.00 8,339.0 1.00
1997 134.7 0.76 156.2 1.02 157.1 0.98 448.1 0.92 9,009.4 1.04
1998 152.5 0.59 178.2 0.97 162.0 0.93 492.6 0.82 9,331.1 1.04
1999 157.7 0.42 204.4 0.91 175.3 0.91 537.4 0.72 9,817.4 1.06
Notes: Values are in billions of current dollars. Prices are normalized to one in 1996.


### ---Economics-2001-0-17.txt---
quantity of labor input account for the heterogeneity
of the workforce across sex, employment
class, age, and education levels. This
follows the approach of Jorgenson et al. (1987).
The estimates have been revised and updated by
Mun S. Ho and Jorgenson (2000).28
The distinction between labor input and labor
hours is analogous to the distinction between
capital services and capital stock. The growth in
labor quality is the difference between the
growth in labor input and hours worked. Labor
quality reflects the substitution of workers with
high marginal products for those with low marginal
products. Table 5 presents estimates of
labor input, hours worked, and labor quality.
The value of labor expenditures in Table 5 is
$5.8 trillions in 1999, 59.3 percent of the value
of output. This share accurately reflects the concept
of gross domestic income, including imputations
for the value of capital services in
household and government sectors. As shown in
Table 2, the growth rate of labor input accelerated
to 2.18 percent for 1995-1999 from 1.70
percent for 1990-1995. This is primarily due to
the growth of hours worked, which rose from
1.17 percent for 1990-1995 to 1.98 percent for
1995-1999, as labor-force participation increased
and unemployment rates plummeted.
The growth of labor quality has declined considerably
in the late 1990's, dropping from 0.53
percent for 1990-1995 to 0.20 percent for
1995-1999. This slowdown captures wellknown
demographic trends in the composition
of the workforce, as well as exhaustion of the
pool of available workers. Growth in hours
worked does not capture these changes in laborquality
growth and is a seriously misleading
measure of labor input.

III. The American Growth Resurgence
The American economy has undergone a remarkable
resurgence since the mid-1990's with
accelerating growth in output, labor productivity,
and total factor productivity. The purpose of
this section is to quantify the sources of growth
for 1948-1999 and various subperiods. An


### ---Economics-2001-0-18.txt---
TABLE 5-LABOR SERVICES

Labor services Weekly Hourly Hours
Year Price Quantity Value Quality Employment hours compensation worked
1948 0.08 1,924.6 156.1 0.75 61,536 39.1 1.2 125,127
1949 0.09 1,860.0 171.5 0.75 60,437 38.5 1.4 121,088
1950 0.09 1,961.0 179.2 0.76 62,424 38.5 1.4 125,144
1951 0.10 2,133.0 214.4 0.78 66,169 38.7 1.6 133,145
1952 0.10 2,197.2 227.2 0.79 67,407 38.5 1.7 135,067
1953 0.11 2,254.3 241.8 0.80 68,471 38.3 1.8 136,331
1954 0.11 2,190.3 243.9 0.81 66,843 37.8 1.9 131,477
1955 0.11 2,254.9 256.7 0.81 68,367 37.8 1.9 134,523
1956 0.12 2,305.0 275.0 0.82 69,968 37.5 2.0 136,502
1957 0.13 2,305.1 295.5 0.83 70,262 37.0 2.2 135,189
1958 0.14 2,245.3 309.1 0.83 68,578 36.7 2.4 130,886
1959 0.14 2,322.1 320.1 0.84 70,149 36.8 2.4 134,396
1960 0.15 2,352.2 344.1 0.84 71,128 36.5 2.5 135,171
1961 0.15 2,378.5 355.0 0.86 71,183 36.3 2.6 134,451
1962 0.15 2,474.1 376.7 0.87 72,673 36.4 2.7 137,612
1963 0.15 2,511.4 386.2 0.88 73,413 36.4 2.8 139,050
1964 0.16 2,578.1 417.6 0.88 74,990 36.3 3.0 141,447
1965 0.17 2,670.6 451.9 0.89 77,239 36.3 3.1 145,865
1966 0.18 2,788.5 500.3 0.89 80,802 36.0 3.3 151,448
1967 0.19 2,842.4 525.5 0.90 82,645 35.7 3.4 153,345
1968 0.20 2,917.0 588.3 0.91 84,733 35.5 3.8 156,329
1969 0.22 2,992.1 646.6 0.91 87,071 35.4 4.0 160,174
1970 0.23 2,938.6 687.3 0.91 86,867 34.9 4.4 157,488
1971 0.26 2,924.9 744.5 0.90 86,715 34.8 4.7 156,924
1972 0.27 3,011.7 817.6 0.91 88,838 34.8 5.1 160,873
1973 0.29 3,135.0 909.4 0.91 92,542 34.8 5.4 167,271
1974 0.31 3,148.2 988.5 0.91 94,121 34.2 5.9 167,425
1975 0.35 3,082.9 1,063.9 0.92 92,575 33.8 6.5 162,879
1976 0.38 3,174.4 1,194.0 0.92 94,922 33.9 7.1 167,169
1977 0.41 3,277.4 1,334.5 0.92 98,202 33.8 7.7 172,780
1978 0.44 3,430.3 1,504.2 0.92 102,931 33.8 8.3 180,842
1979 0.47 3,554.7 1,673.2 0.92 106,463 33.7 9.0 186,791
1980 0.52 3,535.7 1,827.9 0.92 107,061 33.3 9.9 185,591
1981 0.55 3,563.8 1,968.8 0.93 108,050 33.2 10.6 186,257
1982 0.60 3,519.7 2,096.3 0.93 106,749 32.9 11.5 182,772
1983 0.63 3,586.7 2,269.8 0.94 107,810 33.1 12.2 185,457
1984 0.66 3,786.7 2,499.1 0.94 112,604 33.2 12.9 194,555
1985 0.69 3,882.9 2,679.0 0.95 115,205 33.1 13.5 198,445
1986 0.75 3,926.3 2,931.1 0.95 117,171 32.9 14.6 200,242
1987 0.74 4,075.1 3,019.7 0.96 120,474 32.9 14.6 206,312
1988 0.75 4,207.7 3,172.2 0.96 123,927 32.9 15.0 211,918
1989 0.80 4,348.4 3,457.8 0.97 126,755 33.0 15.9 217,651
1990 0.84 4,381.5 3,680.8 0.97 128,341 32.9 16.8 219,306
1991 0.88 4,322.0 3,800.2 0.98 127,080 32.5 17.7 214,994
1992 0.94 4,353.9 4,086.9 0.98 127,238 32.6 19.0 215,477
1993 0.96 4,497.4 4,297.7 0.99 129,770 32.8 19.5 221,003
1994 0.96 4,628.3 4,453.1 0.99 132,799 32.9 19.6 226,975
1995 0.98 4,770.7 4,660.5 1.00 135,672 33.0 20.0 232,545
1996 1.00 4,861.7 4,861.7 1.00 138,018 32.8 20.6 235,798
1997 1.03 4,987.9 5,122.0 1.00 141,184 33.0 21.1 242,160
1998 1.08 5,108.8 5,491.5 1.00 144,305 33.0 22.2 247,783
1999 1.12 5,204.8 5,823.4 1.00 147,036 32.9 23.1 251,683
Notes: Value is in billions of current dollars. Quantity is in billions of 1996 dollars. Price and quality are normalized to one
in 1996. Employment is in thousands of workers. Weekly hours is hours per worker, divided by 52. Hourly compensation is
in current dollars. Hours worked are in millions of hours.


### ---Economics-2001-0-19.txt---
important objective is to account for the sharp
acceleration in the level of economic activity
since 1995 and, in particular, to document the
role of information technology.
The appropriate framework for analyzing
the impact of information technology is the
production possibility frontier, giving outputs
of IT investment goods as well as inputs of IT
capital services. An important advantage of
this framework is that prices of IT outputs and
inputs are linked through the price of IT capital
services. This framework successfully
captures the substitutions among outputs and
inputs in response to the rapid deployment of
IT. It also encompasses costs of adjustment,
while allowing financial markets to be modeled
independently.

As a consequence of the swift advance of
information technology, a number of the most
familiar concepts in growth economics have
been superseded. The aggregate production
function heads this list. Capital stock as a measure
of capital input is no longer adequate to
capture the rising importance of IT. This completely
obscures the restructuring of capital input
that is such an important wellspring of the
growth resurgence. Finally, hours worked must
be replaced as a measure of labor input.
A. Production Possibility Frontier
The production possibility frontier describes
efficient combinations of outputs and inputs for
the economy as a whole.29 Aggregate output Y
consists of outputs of investment goods and
consumption goods. These outputs are produced
from aggregate input X, consisting of
capital services and labor services. Productivity
is a "Hicks-neutral" augmentation of aggregate
input.

The production possibility frontier takes the
form:

(1) Y(In 9 IC , Is, It,9 Cn 9 Cc)
where the outputs include non-IT investment
goods I,, and investments in computers Ic, software
IS' and communications equipment It, as
well as non-IT consumption goods and services
Cn and IT capital services to households and
governments Cc. Inputs include non-IT capital
services Kn and the services of computers Kc,
software Ks, and telecommunications equipment
K, as well as labor input L.30 Totalfactor
productivity (TFP) is denoted by A.
The most important advantage of the production
possibility frontier is the explicit role that it
provides for constant quality prices of IT products.
These are used as deflators for nominal
expenditures on IT investments to obtain the
quantities of IT outputs. Investments in IT are
cumulated into stocks of IT capital. The flow of
IT capital services is an aggregate of these
stocks with service prices as weights. Similarly,
constant quality prices of IT capital services are
used in deflating the nominal values of consumption
of these services.

Another important advantage of the production
possibility frontier is the incorporation of costs of
adjustment. For example, an increase in the output
of IT investment goods requires forgoing part of
the output of consumption goods and non-IT investment
goods, so that adjusting the rate of investment
in IT is costly. However, costs of
adjustment are external to the producing unit and
are fully reflected in If prices. These prices incorporate
forward-looking expectations of the future
prices of If capital services.
B. Aggregate Production Function
The aggregate production function employed
by Robert M. Solow (1957, 1960) and, more recently,
by Jeremy Greenwood et al. (1997, 2000),
Arnold C. Harberger (1998), and Hercowitz
(1998) is a competing methodology. The production
function gives a single output as a function of
capital and labor inputs. There is no role for separate
prices of investment and consumption goods
and, hence, no place for constant quality IF price
indexes for outputs of IF investment goods.
Greenwood et al. employ a price index for
consumption to deflate the output of all


### ---Economics-2001-0-20.txt---
investment goods, including information technology.
Confronted by the fact that constant
quality prices of investment goods differ from
consumption goods prices, they borrow the concept
of embodiment from Solow (1960) in order
to convert investment goods output into an appropriate
form for measuring capital stock.31
Investment has two prices, one used in the measuring
output and the other used in measuring
capital stock. This inconsistency can be removed
by simply distinguishing between outputs
of consumption and investment goods, as
in the national accounts and equation (1). The
concept of embodiment can then be dropped.
Perhaps inadvertently, Greenwood et al. have
revisited the controversy accompanying the introduction
of a constant quality price index for
computers into the national accounts. They have
revived Denison's (1993) proposal to use a consumption
price index to deflate investment in
the NIPA. Denison found this appealing as a
means of avoiding the introduction of constant
quality price indexes for computers. Denison's
approach leads to a serious underestimate of
GDP growth and an overestimate of inflation.
Another limitation of the aggregate production
function is that it fails to incorporate costs
of adjustment. Robert E. Lucas, Jr. (1967) presented
a production model with internal costs of
adjustment. Fumio Hayashi (2000) shows how
to identify these adjustment costs from James
Tobin's (1969) Q-ratio, the ratio of the stock
market value of the producing unit to the market
value of the unit's assets. Implementation of
this approach requires simultaneous modeling
of production and asset valuation. If costs of
adjustment are external, as in the production
possibility frontier (1), asset valuation can be
modeled separately from production.32
C. Sources of Growth

Under the assumption that product and factor
markets are competitive, producer equilibrium
implies that the share-weighted growth
of outputs is the sum of the share-weighted
growth of inputs and growth in total factor
productivity:

(2) wi,,,,A In I,, + wi',cA In IC + WiP,sA In Is
+ wP,t/A In It + wc,A In C,,

+Wc,cAIn Cc

- VK,nA In K,, + VK,CA In KC

+ VK,sA In Ks + VK,tA In Kt

+ VLA In L + A In A

where wP and -v denote average value shares. The
shares of outputs and inputs add to one under
the additional assumption of constant returns,
WI,n + WI,c + WI's + WI,t + WC,1n + WC,c
VK,n + VK,c + VK,s + VK,t + VL 1.
Equation (2) makes it possible to identify the
contributions of outputs as well as inputs to U.S.
economic growth. The growth rate of output is
a weighted average of growth rates of investment
and consumption goods outputs. The contribution
of each output is its weighted growth
rate. Similarly, the growth rate of input is a
weighted average of growth rates of capital and
labor services and the contribution of each input
is its weighted growth rate. The contribution of
TFP, the growth rate of the augmentation factor
A in equation (2), is the difference between
growth rates of output and input.
Table 6 presents results of a growth accounting
decomposition, based on equation (2), for
the period 1948-1999 and various subperiods,
following Jorgenson and Stiroh (1999, 2000b).
Economic growth is broken down by output and
input categories, quantifying the contribution of
information technology to investment and consumption
outputs, as well as capital inputs.
These estimates identify computers, software,
and communications equipment as distinct
types of information technology.
Rearranging equation (2), the results can be
presented in terms of average labor productivity
(ALP), defined as y = YIH, the ratio of
output Y to hours worked H, and k = KIH is
the ratio of capital services K to hours worked:
(3) A Iny== VKA Ink

+ VL(A InL - A In H) + A In A.
3' Karl Whelan (1999) also employs Solow's concept of
embodiment.

32 See, for example, John Y. Campbell and Robert J.
Shiller (1998).


### ---Economics-2001-0-21.txt---
Equation (3) allocates ALP growth among three
sources. The first is capital deepening, the
growth in capital input per hour worked, and
reflects the capital-labor substitution. The second
is improvement in labor quality and captures
the rising proportion of hours by workers
with higher marginal products. The third is TFP
growth, which contributes point-for-point to
ALP growth.

D. Contributions of IT Investment
Figure 5 depicts the rapid increase in the
importance of IT services, reflecting the accelerating
pace of IT price declines. In 1995-1999
the capital service price for computers fell 24.81
percent per year, compared to an increase of
36.36 percent in capital input from computers.
As a consequence, the value of computer services
grew substantially. However, the current
dollar value of computers was only 1.6 percent
of gross domestic income in 1999.
The rapid accumulation of software appears
to have different sources. The price of software
services has declined only 2.04 percent per year
for 1995-1999. Nonetheless, firms have been
accumulating software very rapidly, with real
capital services growing 16.30 percent per year.
A possible explanation is that firms respond to
computer price declines by investing in complementary
inputs like software. However, a more
plausible explanation is that the price indexes
used to deflate software investment fail to hold
quality constant. This leads to an overstatement
of inflation and an understatement of growth.
Although the price decline for communications
equipment during the period 1995-1999 is
comparable to that of software, investment in
this equipment is more in line with prices. However,
prices of communications equipment also
fail to hold quality constant. The technology of
switching equipment, for example, is similar to
that of computers; investment in this category is
deflated by a constant-quality price index developed
by BEA. Conventional price deflators are
employed for transmission gear, such as fiberoptic
cables. This leads to an underestimate of
the growth rates of investment, capital stock,
capital services, and the GDP, as well as an
overestimate of the rate of inflation.
Figures 6 and 7 highlight the rising contributions
IT outputs to U.S. economic growth. Figure
6 shows the breakdown between IT and
non-IT outputs for subperiods from 1948 to
1999, while Figure 7 decomposes the contribution
of IT into its components. Although the


### ---Economics-2001-0-22.txt---
4.5 .

..

...:,%,.'....

.,". .

... .. - .... .. 1,

......... ...'.

. .......i

.-.. . i, . .. - "'. .... Rm.

... .:

...-, ..' . :.::. ''

%.-....:. - - -

... ...',%.. ''... i.- , '':, ;-
- -

''"

'' ".''.:.. -

.:.' ... ....................................,- ........: -'- :- ...,:.:,..."...........:..,....
... .1

'

...-.-......,..,..,...-.........,....,..::.,........, ...'........ .........:...'.:
.

---'---.--'-:'-'

''" ""' ,,,,,,

... .....: - --::-: .. - :

%. . ..

...

.. ' '' . ... .:-'-.-

... ...-I.. :.:.

'%. . ..

."..",,'..',..".',.,".,.

..... ... ..:.!'.:-.-

...:,., -'.'-.''-"":.-- ...........-..........--........
........::'......

.. -- ::. ...".. ......... ...........'..
.... .:. -'.--.-..'.'.-'-..'-'.-'.-'. -..-'.'--.".-.-..'-'..-.'.-.
...-...-......-.---...-.-..-.--.-..'.-.-.--............................-..
.

':

.:.:.

.....-..... .,.:,.""'""',-"""'...... .::
-:------'-.--' .. " ". - .,.,.:, ... .:..:..'
'

.11. ..

....

'.". ' '.' :..

4 .0 - '' %. I"'.

...

.. ... -'

....... %.

"....

......' ....

- '

I

%...

.

.

..

. . . .

.

""""'"

.

. . .. :

""' '"' I.......: '- ."--, ..' -' .,-.,: ..... - - .
,

..."..". "........." -1-:'-1 .:..:.
- .:.-... -- -. - - .. ... .

..'..-'..-.".'..-.-. .

. .

. . .

.

'.. - - , -', ,,

: ..

3.5 - ..... .

,-." -" - , -, ..' -'-

.

-..

.........

- -............

. . .

- -

... '

- -

:%.'. .

,,

..'.:

.... . ... ....

1

. .... .

. .

' ""

-

- - - ....

....-,.- :' -'? .... . . - . ... .. ......... ... ......:
' '

: .... ......

'"

". . ....1

......,.. ....%

.. .:- . -.:. - - - '-.'.....-..'...-..-..'.'.....,..,.,,-.....,."-....,.,.,,.."-,..:.'-.......
-.- : '.'. -, ....% ..

'----.--------1- - ::.,...,.......,..,.....'..'...-."...,...,...'.,'..'.....'...'..".."..'....'.."......-...'.....'...--.-:--.-.--.-.., "...... .:. ...... ...... .:--.-: :: ...' ':. , -'.
3.0 -

.. . ..:% ...... ......

.... .. - -. .".'.

. .. ...,..,...,.,..,,.

.,...,,,..,.,..,,..,,.,,.,,.,,.,...,..,,.,.,..,.,,.,..,,.,..,.,"..,,.,..- ". ' "'':.,
... .....................................-....-.......-...........................-.-
, ,............................................
.........:,,....,..,.....,,....,.::....,..,..........,...-........,............,.
.... ..... .. ... - .. .. .... ..
...... ... ....... - ..... ....
--:-:%%:'--.------'.'....... .. ...........
......

...............

.....

......

.....

...... :'

....:-' ' .'.'.'.-"'.'..."".-.'..'..""..'.".'."-'.'...'-.'..'.-'..'L- .......:... :- ..,...,.....:.-.".,.".."
:.... ...........

..''...:..--

.,. .:....%

...- . . .. ..... ... .. . .. ..
. .- ':: -.'.

.

2 .5 - '--' %. . .

.....--.- ..-...-... -.'

, ,

..%-

'I -"

-'---- - .:----..:..:. . . .%

. - - .

.. . . .... .. . . . .

' '

.. . . % -

.

"---'-.'---':'-'.- .. .

....

'

.. . . . . . ..... ..:

-:% .. ........,

-

'...-.,",..i..,..,...-...'.'..'...'..'..,."......'
%%...... ... .. . . ....... .- ... ...
' . .

'

.

' '

' ' "

-'.. 1. .. " ..'. . : . .

% ' "

... ...

,.-:' .... .'..."....% ...

-.......'.' ::. % '% ..-.. : -,-
'

2 .0 f '......'.."..'...".."

1-.... ----,-

..'.... . ..,. ...

.... ..'i.'..'.,.'....,:..,.,.l......'..'......'.'.
,:".-%:-'-,-::- -, "...... ....
--.':.'-'. ..... ....... .... .... .. :%. .%......
' '

.

"

. %

%

-

..: ..

.. i:

.

. .

- ''

.-1-'':::--'.-'---:-'-.-:-'.- ..il -.%
.: .. .,.- -;------.--% .-:---.-: .:-.. ' :.::......
.. ............................ ... .. --;;'.---'--- .. . --: ..' %. ..'. ;.'-'.--.-;,-- ..
1.5 -

i.,...".."".,.".",.,"."'."".,'.
...-?. ..................................
.. . . .....

.. %': ..,.,..,,.,.,,.,,.

.... .......................

:'.... %::..,...,..::"."..,...".,.,,..
-- %.." ' ",.'',.",..."....

.- ..

.

:

... ... ...- .....

%

.

.0 -:, .

- .

.. '-.' ..

.':...:.. :...:.,.%....:: .-..... ..
"-'--'--'-'..%... .:...%. -..... ': :...'. :
..

.

.

'

.

.-.-I

,...,.::.... "'%.

,

.

..-. ..

,

. .

.- -' "....---:....-%.'-..-

' ,

%... , :

'-' : -.".'-L-- ...: .:

.

.'

.. ......:.

0.5 . ..

.%

-

.

. .. %..

.-'

.. .

. ..

-

.. %

"-

.".'

...

..

": ....

'' '

.%:..:..

,

...

- -

'

..

-

..

0.0 .

1948-73 1973-W

SNon-ITConsumpbon MITConsumpd

FiGuRE 6. OUTPUT CONTRIBUTIO

Note: Output contributions are the average annual (pe
2.00 ,

.

.

.

.

.

.

.

.

..''....

.

,,

.... -::"

- . .

. . .

-

-' -'.' -, :' . .

. .

. . . . .

. ' .

,

. . .

. .

. : .. .

.

. . .. .

. .

.: -...::. .

.6 0 - . ... %. .. %. % : .

.

.

,...-.

...,. . ....-: .

'.. - : -:-:-:- ...'... .':.. - !..'... ..': "
,'

'

,,......

'''


### ---Economics-2001-0-23.txt---



### ---Economics-2001-0-24.txt---
importance of IT has steadily increased, Figure
6 shows that the recent investment and consumption
surge nearly doubled the output contribution
of IT. Figure 7 shows that computer
investment is the largest single IT contributor in
the late 1990's, but that investments in software
and communications equipment are becoming
increasingly important.

Figures 8 and 9 present a similar decomposition
of IT inputs into production. The contribution
of these inputs is rising even more
dramatically. Figure 8 shows that the contribution
of IT now accounts for more than 48.1
percent of the total contribution of capital input.
Figure 9 shows that computer hardware is the
largest IT contributor on the input side, reflecting
the growing share and accelerating growth
rate of computer investment in the late 1990's.
Private business investment predominates in
the output of IT, as shown by Jorgenson and
Stiroh (1999, 2000b).33 Household purchases of
IT equipment and services are next in importance.
Government purchases of IT equipment
and services, as well as net exports of IT products,
must be included in order to provide a
complete picture. Firms, consumers, governments,
and purchasers of U.S. exports are responding
to relative price changes, increasing
the contributions of computers, software, and
communications equipment.

Table 2 shows that the price of computer
investment fell by more than 32 percent per
year, the price of software 2.4 percent, the price
of communications equipment 2.9 percent, and
the price of IT services 11.8 percent during the
period 1995-1999, while non-IT prices rose 2.2
percent. In response to these price changes,
firms, households, and governments have accumulated
computers, software, and communications
equipment much more rapidly than other
forms of capital.

E. Total Factor Productivity

The price or "dual" approach to productivity
measurement makes it possible to identify the
role of IT production as a source of productivity
growth at the industry level.34 The rate of productivity
growth is measured as the decline in
the price of output, plus a weighted average of
the growth rates of input prices with value
shares of the inputs as weights. For the computer
industry this expression is dominated by
two terms: the decline in the price of computers
and the contribution of the price of semiconductors.
For the semiconductor industry the expression
is dominated by the decline in the price of
semiconductors.35

Jorgenson et al. (1987) have employed
Domar's (1961) model to trace aggregate productivity
growth to its sources at the level of
individual industries.36 More recently, Harberger
(1998), William Gullickson and Michael
J. Harper (1999), and Jorgenson and Stiroh
(2000a, 2000b) have used the model for similar
purposes. Productivity growth for each industry
is weighted by the ratio of the gross output of
the industry to GDP to estimate the industry
contribution to aggregate TFP growth.
If semiconductor output were only used to
produce computers, then its contribution to
computer-industry productivity growth, weighted
by computer-industry output, would precisely
cancel its independent contribution to aggregate
TFP growth. This is the ratio of the value of
semiconductor output to GDP, multiplied by the
rate of semiconductor price decline. In fact, semiconductors
are used to produce telecommunications
equipment and many other products.
However, the value of semiconductor output is
dominated by inputs into IT production.
The Domar aggregation folmula can be approximated
by expressing the declines in prices of
computers, communications equipment, and software
relative to the price of gross domestic income,
an aggregate of the prices of capital and
labor services. The rates of relative IT price decline
are weighted by ratios of the outputs of IT
products to the GDP. Table 7 reports details of this
TFP decomposition for 1948-1999; the IT and
non-IT contributions are presented in Figure
33 Bosworth and Triplett (2000) compare the results of
Jorgenson and Stiroh (2000b) with those of Oliner and
Sichel (2000).

34 The dual approach is presented by Jorgenson et al.
(1987 pp. 53-63).

35 Dulberger (1993), Triplett (1996), and Oliner and
Sichel (2000) present models of the relationships between
computer and semiconductor industries. These are special
cases of the Evsey Domar (1961) aggregation scheme.
36 See Jorgenson et al. (1987 pp. 63-66, 301-22).


### ---Economics-2001-0-25.txt---
10. The IT products contribute 0.50 percentage
points to TFP growth for 1995-1999, compared to
0.25 percentage points for 1990-1995. This reflects
the accelerating decline in relative price
changes resulting from shortening the product cycle
for semiconductors.

F. Output Growth

This subsection presents the sources of GDP
growth for the entire period 1948 to 1999. Capital
services contribute 1.70 percentage points,
labor services 1.14 percentage points, and TFP
growth only 0.61 percentage points. Input
growth is the source of nearly 82.3 percent of
U.S. growth over the past half century, while
TFP has accounted for 17.7 percent. Figure
11 shows the relatively modest contributions of
TFP in all subperiods.

More than three-quarters of the contribution
of capital reflects the accumulation of capital
stock, while improvement in the quality of capital
accounts for about one-quarter. Similarly,
increased labor hours account for 80 percent of
labor's contribution; the remainder is due to
improvements in labor quality. Substitutions
among capital and labor inputs in response to
price changes are essential components of the
sources of economic growth.

A look at the U.S. economy before and after
1973 reveals familiar features of the historical
record. After strong output and TFP growth in
the 1950's, 1960's, and early 1970's, the U.S.
economy slowed markedly through 1990, with
output growth falling from 3.99 percent to 2.86
percent and TFP growth declining from 0.92
percent to 0.25 percent. Growth in capital inputs
also slowed from 4.64 percent for 1948-1973 to
3.57 percent for 1973-1990. This contributed to
sluggish ALP growth-2.82 percent for 1948-
1973 and 1.26 percent for 1973-1990.
Relative to the early 1990's, output growth increased
by 1.72 percent in 1995-1999. The contribution
of IT production almost doubled, relative
to 1990-1995, but still accounted for only 28.9
percent of the increased growth of output. Although
the contribution of IT has increased
steadily throughout the period 1948-1999, there
has been a sharp response to the acceleration in the
IT price decline in 1995. Nonetheless, more than
70 percent of the increased output growth can be
attributed to non-IT products.
Between 1990-1995 and 1995-1999 the contribution
of capital input jumped by 0.95


### ---Economics-2001-0-26.txt---
1.00

0.80.II IE

0.60

0.40

0.20

0.00

-0.20 --

1948-i3 197340 1990405 1995-99
ONon-IT P cMl arT Produ

FIGURE 10. CONTRIBUTIONS OF INFORMATION TEcHNoLoGY To TOTAL FACTOR PRODucTIviTy GRowTH
Note: Contributions are average annual (percentage) relative price changes, weighted by average nominal output shares from
Table 7.

percentage points, the contribution of labor input
rose by only 0.24 percent, and TFP accelerated by
0.51 percent. Growth in ALP rose 0.92 as more
rapid capital deepening and growth in TFP offset
slower improvement in labor quality. Growth in
hours worked accelerated as unemployment fell to
a 30-year low. Labor markets have tightened considerably,
even as labor-force participation rates
increased.37

The contribution of capital input reflects the
investment boom of the late 1990's as businesses,
households, and governments poured
resources into plant and equipment, especially
computers, software, and communications
equipment. The contribution of capital, predominantly
IT, is considerably more important than
the contribution of labor. The contribution of IT
capital services has grown steadily throughout
the period 1948-1999, but Figure 9 reflects the
impact of the accelerating decline in IT prices.
After maintaining an average rate of 0.25 percent
for the period 1973-1990, TFP growth fell to
0.24 percent for 1990-1995 and then vaulted to
0.75 percent per year for 1995-1999. This is a
major source of growth in output and ALP for the
U.S. economy (Figures 11 and 12). While TFP
growth for 1995-1999 is lower than the rate of
1948-1973, the U.S. economy is recuperating
from the anemic productivity growth of the past
two decades. Although only half of the acceleration
in TFP from 1990-1995 to 1995-1999 can be
attributed to IT production, this is far greater than
the 4.26 percent share of IT in the GDP.
G. Average Labor Productivity

Output growth is the sum of growth in hours
and average labor productivity. Table 8 shows
the breakdown between growth in hours and
ALP for the same periods as in Table 6. For the
period 1948-1999, ALP growth predominated
in output growth, increasing just over 2 percent
per year for 1948-1999, while hours increased
about 1.4 percent per year. As shown in equation
(3), ALP growth depends on capital deepening,
a labor-quality effect, and TFP growth.
37 Katz and Krueger (1999) analyze the recent performance
of the U.S. labor market.


### ---Economics-2001-0-27.txt---
Figure 12 reveals the well-known productivity
slowdown of the 1970's and 1980's, emphasizing
the acceleration in labor productivity growth in the
late 1990's. The slowdown through 1990 reflects
reduced capital deepening, declining labor-quality
growth, and decelerating growth in TFP. The
growth of ALP slipped further during the early
1990's with a slump in capital deepening only
partly offset by a revival in labor quality growth
and an up-tick in TFP growth. A slowdown in


### ---Economics-2001-0-28.txt---
3.50

300

2.50

200

1.50

0.00

1948-73 1973-90 1990-95 1995-09
*Labor Quality 8 Non-IT Capital Deepening EIT Captal Deepening El Non-IT Productivlty OIT Productivity
FIGURE 12. SOURCES OF AVERAGE LABOR PRODUCTIVITY GROWTH
Note: Contributions are from Table 8.
hours combined with slowing ALP growth during
1990-1995 to produce a further slide in the
growth of output. In previous cyclical recoveries
during the postwar period, output growth accelerated
during the recovery, powered by more rapid
growth of hours and ALP.

Accelerating output growth during 1995-
1999 reflects growth in labor hours and ALP
almost equally.38 Comparing 1990-1995 to
1995-1999, the rate of output growth jumped
by 1.72 percent-due to an increase in hours
worked of 0.81 percent and another increase in
ALP growth of 0.92 percent. Figure 12 shows
the acceleration in ALP growth is due to capital
deepening as well as faster TFP growth. Capital
deepening contributed 0.60 percentage points,
offsetting a negative contribution of labor quality
of 0.20 percent. The acceleration in TFP
added 0.51 percentage points.

H. Research Opportunities

The use of computers, software, and communications
equipment must be carefully distinguished
from the production of IT.39 Massive
increases in computing power, like those experienced
by the U.S. economy, have two effects
on growth. First, as IT producers become more
efficient, more IT equipment and software is
produced from the same inputs. This raises productivity
in IT-producing industries and contributes
to TFP growth for the economy as a
whole. Labor productivity also grows at both
industry and aggregate levels.
Second, investment in information technology
leads to growth of productive capacity in
IT-using industries. Since labor is working with
more and better equipment, this increases ALP
through capital deepening. If the contributions
to aggregate output are captured by capital
deepening, aggregate TFP growth is unaffected.
40 Increasing deployment of IT affects
TFP growth only if there are spillovers from
IT-producing industries to IT-using industries.
Top priority must be given to identifying the
impact of investment in IT at the industry level.
Stiroh (1998) has shown that this is concen-
38 Stiroh (2000) shows that ALP growth is concentrated
in IT-producing and IT-using industries.
39 Economics and Statistics Administration (2000 Table
3.1 p. 23) lists IT-producing industries.
40 Martin N. Baily and Robert J. Gordon (1988).


### ---Economics-2001-0-29.txt---
trated in a small number of IT-using industries,
while Stiroh (2000) shows that aggregate ALP
growth can be attributed to productivity growth
in IT-producing and IT-using industries. The
next priority is to trace the increase in aggregate
TFP growth to its sources in individual industries.
Jorgenson and Stiroh (2000a, 2000b)
present the appropriate methodology and preliminary
results.

IV. Economics on Internet Time
The steadily rising importance of information
technology has created new research opportunities
in all areas of economics. Economic historians,
led by Chandler (2000) and Paul A. David
(2000),41 have placed the information age in
historical context. The Solow (1987) Paradox,
that we see computers everywhere but in the
productivity statistics,42 has provided a point of
departure. Since computers have now left an
indelible imprint on the productivity statistics,
the remaining issue is: Does the breathtaking
speed of technological change in semiconductors
differentiate this resurgence from previous
periods of rapid growth?

Capital and labor markets have been severely
impacted by information technology. Enormous
uncertainty surrounds the relationship between
equity valuations and future growth prospects
of the American economy.43 One theory attributes
rising valuations of equities since the
growth acceleration began in 1995 to the accumulation
of intangible assets, such as intellectual
property and organizational capital. An
alternative theory treats the high valuations of
technology stocks as a bubble that burst during
the year 2000.

The behavior of labor markets also poses
important puzzles. Widening wage differentials
between workers with more and less education
has been attributed to computerization of the
workplace. A possible explanation could be that
high-skilled workers are complementary to IT,
while low-skilled workers are substitutable. An
alternative explanation is that technical change
associated with IT is skill biased and increases
the wages of high-skilled workers relative to
low-skilled workers.44

Finally, information technology is altering
product markets and business organizations, as
attested by the large and growing business literature,
45 but a fully satisfactory model of the
semiconductor industry remains to be developed.
46 Such a model would derive the demand
for semiconductors from investment in information
technology in response to rapidly falling IT
prices. An important objective is to determine
the product cycle for successive generations of
new semiconductors endogenously.
The semiconductor industry and the information
technology industries are global in their
scope with an elaborate international division of
labor.47 This poses important questions about
the American growth resurgence. Where is the
evidence of a new economy in other leading
industrialized countries? An important explanation
is the absence of constant quality price
indexes for semiconductors and information
technology in national accounting systems outside
the U.S.48 Another conundrum is that
several important participants-Korea, Malaysia,
Singapore, and Taiwan-are "newly industrializing"
economies. What does this portend
for developing countries like China and India?
As policy makers attempt to fill the widening
gaps between the information required for
sound policy and the available data, the


### ---Economics-2001-0-30.txt---
traditional division of labor between statistical
agencies and policy-making bodies is breaking
down. In the mean time, monetary policy makers
must set policies without accurate measures
of price change. Similarly, fiscal policy makers
confront ongoing revisions of growth projections
that drastically affect the outlook for future
tax revenues and government spending.
The stagflation of the 1970's greatly undermined
the Keynesian Revolution, leading to a
New Classical Counterrevolution led by Lucas
(1981) that has transformed macroeconomics.
The unanticipated American growth revival of
the 1990's has similar potential for altering economic
perspectives. In fact, this is already foreshadowed
in a steady stream of excellent books
on the economics of information technology.49
We are the fortunate beneficiaries of a new
agenda for economic research that could refresh
our thinking and revitalize our discipline.
 ## Economics-2002-0


### ---Economics-2002-0-02.txt---
Diversity is the staff of economic life. Interpersonal
differences in tastes and talents,
whether naturally endowed or environmentally
produced, give us the unique "propensity to
truck, barter, and trade" that improves our standards
of living. Every elementary economics
student knows how different endowments in an
exchange economy create potential gains from
trade, and how competitive markets efficiently
intermediate and exhaust those gains. In production
activities, work is organized in highly
specialized ways to use our human resources to
the fullest.

All this is so elementary that economists
largely take it for granted, yet much of the
complexity of modem economic life is built
upon these foundations. The variety of choices
that confront us is astonishing. No consumer
buys more than a tiny fraction of goods that are
available to be purchased. The average person is
unable to identify by name more than a handful
of goods because most are irrelevant to anyone'
s personal economic behavior. And in work
activities, each of us masters hardly any of the
immense varieties of skill required in a modem
economy. Out of the totality of what is known
by society at large, a single person knows practically
nothing, no matter how well educated or
how brilliant! Work and production knowledge
are even more specialized than consumption
choices and activities.

How do decentralized markets accommodate
the diversity of choices, tastes, and productivities
that are so important in economic affairs?
The choice of quantities (the intensive margin)
by a typical buyer or seller has dominated neoclassical
economics. Most of price theory focuses
on the determination of price and
quantities of already-defined goods, but does
little to examine the extensive margin by which
the nature of the goods is chosen. Price theory
does not provide a rich enough structure to
analyze these issues, which require a framework
that takes heterogeneity and diversity as a fundamental,
primitive construct. Location or spatial
theory is specifically designed for that task.
It is a theory of choice based on interpersonal
differences in willingness to pay for differentiated
objects perhaps best known from Adam
Smith's theory of equalizing differences.' Differentiated
products are valued according to
their various qualities and product characteristics.
Comparing reservation prices with market
prices determines specific choices of buyers and
sellers out of the vast varieties that are available.
Many successful examples have clarified
how spatial models can be applied to the economics
of variety and diversity.

Much of my research reflects my attempts
over the years to work out some of the economic
issues associated with diversity and the
implications of heterogeneity for markets and
prices.2 There are three main themes: the determination
of value in the presence of diversity,
the sorting or allocation of diverse buyers to
diverse sellers, and the effects of heterogeneity
and sorting on inequality.

I. Value, Assignment, and Inequality
How do markets accommodate inherent differences
in goods, jobs, and productive talents
of people? How are these things valued? Just as
the value of land depends on its location, it is
often possible to think of goods, jobs, and people
in terms of their addresses in a map of
productive attributes or characteristics. Some
addresses are more desirable than others and
market prices equate the supply and demand for


### ---Economics-2002-0-03.txt---
the latent characteristics of goods at each location
on the map. Thinking spatially proves especially
useful when there are many more
varieties of goods than attributes that each contains.
Because there are fewer attributes than
goods, the dimensionality of the problem is
greatly reduced and analysis can proceed on
conventional cost-benefit terms.
Examples of applications include hedonic indexes
of quality change needed to correct price
indexes for changing product characteristics;
among such products as automobiles and computers;
regression analysis of housing prices on
house characteristics, used for real estate assessments
in urban economics; the capital asset
pricing model, where asset characteristics are
the means and covariances of their rate of return
distributions; studying how labor markets evaluate
jobs of varying quality, useful for estimating
the social value of safety and environmental
goods that are not directly traded; and how and
why labor markets sustain enormous differences
in rewards and rents among people of
different talents. All are manifestations of almost
exactly the same basic spatial problem:
valuing diversity.

Market values are an important part of the
story. The allocation of diversity in the economy
is another. How are buyers and sellers
matched or assigned to each other in market
equilibrium? These marriage-type questions
bear importantly on the economic consequences
of diversity because stratification of agents is
inherent in spatial equilibrium. Certain kinds of
buyers come to be associated with certain kinds
of sellers, even when there are no externalities
and social influences in preferences. Rich people
tend to ride in a better class of automobiles
than poor people. They are more likely to live
on the lakeshore and in other high-rent districts,
to eat in fancy restaurants, wear designer
clothes, send their children to better schools,
and work in more pleasant environments. But
there are many other manifestations of the same
basic sorting or assignment issues in the presence
of diversity. Widows and orphans tend to
hold their wealth in relatively safe assets. People
from similar ethnic groups tend to live together
in city enclaves, more talented students
are apt to be found in colleges and graduate
schools that have more talented teachers,
higher-quality lawyers work on the largest legal
claims, and people exposed to higher unemployment
risks tend to live in the same
neighborhoods.

The address analogy in spatial equilibrium
often extends to these kinds of matches or assignments:
goods with special attributes appeal
to buyers with specific kinds of tastes. In product
markets, sellers design their goods to cater
to specific types of customers. And in labor
markets, each of us in our career choices and
work activities seeks a niche in the incredibly
complex machinery of modern production and
the division of labor. The number of people
seeking these slots and the nature of the technologies
that affect the personal scale of operations
affects the distribution of rewards in
society.

Markets accommodate diversity by establishing
prices that tend to make different things
relatively close substitutes at the margin. Adam
Smith's insight that market prices tend to equalize
their net advantages is fundamental to these
problems. If one good has more desirable characteristics
than another, the less preferred variety
must compensate for its disadvantages by
selling at a lower price. Supply is inelastic and
exogenous in geographic spatial theory, but in
many applications both sides of the market must
be considered jointly. Sellers choose their varieties
by comparing prices with costs. Higherquality
goods are more costly to produce and
must be offered at a higher price. In equilibrium
these extra costs can only be supported if their
incremental value to some customers is at least
as large as their incremental costs. Thus diversity
creates inequality in prices and values. The
reverse statement is also true. Certain kinds of
inequality are necessary to sustain diverse outcomes.
For example, if higher-quality goods
were not more expensive to produce than lowerquality
ones and if all consumers had the same
relative ranking on the quality of two different
goods, then only the higher-quality good would
survive in the market. The lower-quality good is
driven out by the existence of a superior one
that can be produced at equal cost.
What is less obvious is that there are social
incentives to create inequality, even when
agents are initially identical in every conceivable
way. This is a third theme of the essay. The
basic idea also derives from Smith, who argued
that personal investments in skill acquisition,


### ---Economics-2002-0-04.txt---
not inherited differences in natural abilities, are
the principal causes of wage inequality in society.
Since labor-market skills are costly to learn,
in market equilibrium their costs must be reimbursed
by offering larger expected earnings to
potential entrants, otherwise students would not
have the proper economic incentives to study
them. Since much of the cost of education and
learning are in time and opportunities forgone,
the force of interest weighs heavily in these
decisions and can cause remarkably large differences
in observed earnings, as equilibrium
phenomena.

But once a skill has been acquired, its economic
return is greatest if it is used as intensively
as possible. That the costs of acquiring
most skills are to some extent independent of
how intensively they are utilized makes it efficient
for people to specialize their skills and
trade with each other. There are huge economies
of scale in skills. Once acquired, a skill can
be used over and over again without diminishing
its stock. Indeed, the reverse may be true,
which provides incentives for students to acquire
skills early and to increase their work
hours after having become skilled.
Furthermore, individuals have different talents
and are better suited to some productive
activities than others. The principle of comparative
advantage holds true for human-capital
production as well as for international trade. It
accounts for why work is so specialized and
why each person knows such a small amount of
what is known in total. It even holds if people
are identical ex ante. Similar ideas have received
lots of attention lately in the fields of
industrial organization and international trade,
but are just as important, if not more so, for the
organization of work.

The cost basis that supports induced or "voluntary"
inequality has other interesting consequences.
Unequal rewards motivate people to
strive for superior performance and influence
their decisions to acquire skills. The two interact
because new generations of workers replace
older generations: the assignment of people to
jobs changes over the life cycle. A large share
of the growth in personal earnings over managerial
and other careers occurs at discrete promotion
points to higher-ranking positions.
Competition for promotions, to acquire greater
skill, show one's stuff, and get more powerful
and higher-paying positions, plays an important
role both in the internal dynamics of organizations
and in the overall economy. Uncertainty of
outcomes and the statistical aspects of promotion
and job assignments guarantee that competition
for superior positions occurs in every
form of economic organization. The need to use
the record of past performance to assess prospects
for other positions automatically gives
people incentives to try to influence the measures
that will put them in a superior category.
The strength of these incentives depends on
how much of a difference-in money, prestige
and perquisites-it makes to achieve a better
grade and a higher classification.
II. Valuing Diversity

Much of my research consists of applications
of the problem of analyzing markets for differentiated
products when the measure of differentiation
is naturally ordered from best to worst.
Market prices reflect both the costs and values
of the underlying attributes of goods. Agents
implicitly use cost-benefit analysis to choose
locations in the product spectrum, with buyers
comparing the market prices of alternative varieties
with their relative values in use and with
sellers comparing market prices with their relative
costs. Equality between demand and supply
for each variety sustains the market
equilibrium price-quality structure.
Consider a commodity that comes in two
different varieties. For example, there are highand
low-quality cars, better and worse houses
(or schools or neighborhoods), good jobs and
bad ones, fast and slow computers, and so on.
Let c represent all other goods consumed and let
Zh and z1 measure the high- and low-quality
characteristics of the goods in question. The
relative prices of the two varieties in terms of
other goods are Ph and Pl In the situation considered,
which is typical of many markets, individual
buyers and sellers are small compared
to the overall market and individually have no
market power. Suppose that customers purchase
either one unit of the differentiated product or
none. This is a leading case. Most people live in
exactly one neighborhood, hold one job, and
drive one car. Preferences are given by a utility
function u(c,z) of the usual kind. The choice set
consists of three distinct points in the (p,z)


### ---Economics-2002-0-05.txt---
Plp'(z) 92(z)

P, Af0'(z(

00(Z)~~~~~~Z

A I

0,0 Zi Zh z

FIGURE 1. SPATIAL EQUILIBRIUM

plane, as in Figure 1. A consumer lives at point
A, (0,0), if neither variety is purchased, at point
B, (pl,zl), if the low-quality variety is most
preferred, and at point C, (Ph,Zh), if high quality
is chosen. High-quality goods must sell for
higher prices than low-quality goods, or else
low quality is dominated and disappears from
the market. Thus, higher z is always associated
with larger p if both Zh and z, are actually
traded. Given that some variety is purchased, a
consumer chooses Zh if the benefits of its additional
quality exceed its additional cost. Once
having decided on the choice of variety, the
consumer decides whether to purchase it or do
without.

The first part of the problem is equivalent to
choosing the maximum between u(y - Ph, Zh)
and u(y - pl, zl), where y is income. The
added cost of high quality is l\p = Ph - Pl- Its
added benefit is how much more the buyer is
willing to pay for it, A\O, and is defined as a
compensating variation:

(1) U(y - pI - AO, Zh) = U(y - pi, Z1)-
AXO is the money premium a person would pay
for Zh when the lower-quality item z, is available
at price p,. The optimal choice is Zh if
AXO > Ph - p, and z, otherwise. The second part of
the problem, whether or not to purchase the
good at all, is another cost-benefit comparison.
Define 0 as the compensating variation that
equates the utility of not purchasing anything to
that obtained from the best possible variety:
(2)

u(y,O)

= max{u(y - 0 - AO, Zh), U(y - 0, Z1)}.
The consumer does not purchase any z if both
0 < p, and 0 + AO < PhThis

can be neatly described diagrammatically
with a spatial bidfunction 0 (z), defined as
the amount a person with income y will pay for
various varieties at some constant utility index:
(3) u(y - 0(z), z) = constant.
0(z) is an indifference curve between money
and the measure of quality z. From (3), its
derivative a 0/az = uzJuC, is the marginal rate
of substitution between z and c. This is positive
and 0 (z) is upward sloping. Diminishing
marginal rate of substitution implies that
0"(z) < 0: the marginal willingness to pay
for additional quality is decreasing. The
curves labeled 0' in Figure 1 depict indifference
curves for three different kinds of
buyers. Consumers whose tastes look like 00 do
not purchase the differentiated product at all
because the indifference curve through (0,0) lies
above the available price-quality combinations
for z. Analogously, those whose tastes look like
01 purchase z, and those whose tastes are more
like 02 buy Zh- Choices of differentiated varieties
are nicely ordered by the intensity of preferences
for quality in this example.

Supply decisions of sellers are similar. The
benefit of selling a variety is its market price.
Production is profitable if price exceeds production
costs of at least some sellers. Parallel with
bid functions, these choices are depicted by a
spatial offer function p( z)-the locus of pricequality
pairs that result in the same profit. 'p( c)
is the supply price of quality z for that seller.
Since higher-quality goods are more costly to
produce, 'p'(z) > 0. Offer curves are increasing
and convex functions of z. Producers either
specialize their production in distinct varieties
or produce several of them in a product line.
Costs and production conditions, indivisibilities,
the nature of competition, and competitors'
costs factor into these outcomes. Figure 1 depicts
a case of specialization. The offer function
labeled Sp1 refers to a seller with comparative


### ---Economics-2002-0-06.txt---
advantage at producing the lower-quality variety
and the curve labeled 92 refers to a seller
whose production conditions are better suited to
high quality. Since (pl lies everywhere above
(Zh, Ph), seller 1 cannot offer the high-quality
variety at a profit level higher than that obtained
by offering the low-quality variety. Conversely,
since (P2 lies everywhere above (zl, Pl), seller 2
cannot offer the low-quality variety at a profit
level higher than that obtained by offering the
high-quality variety. Sellers whose minimum
acceptable profit-offer curve cover both prices
produce both objects. An example of such a
seller is one with offer function Sp*, who is
indifferent between selling the two varieties.
Some auto manufacturers produce a full
product line and others specialize in niche markets.
Research universities cater to students
with superior high-school records and achievements,
and would not be very cost effective as
junior colleges. Climatic and geographic endowments
give some California vineyards advantages
in producing high-quality wine that is
much harder to produce in New York and Michigan,
yet some California vintners produce both
higher- and lower-quality brands. The prestigious
law firms handling large legal claims are
careful about which cases they take on and
whom they admit as partners. Production activities
in which workers directly interact require
personnel who complement each other's personal
productivity and efficiency characteristics.
Because of direct complementarity in most
production settings, more interpersonal variation
in efficiency within production units can be
tolerated by transacting through the market
rather than directly in teams. Impersonal
transactions are the equivalent of intermediate
product markets and reduce the adverse consequences
that occur when less efficient workers
pull down the productivity of more efficient
ones.

Figure 1 shows how the market sustains diversity
as an equilibrium phenomenon. Different
kinds of buyers purchase different kinds of
goods. Consumers with tastes that correspond to
O'(z) buy z1 at price Pi and are supplied by
sellers with bid functions that correspond to
pl(z). Consumers with tastes that correspond
to 02( z) buy z, at price Ph and are supplied by
sellers with bid functions that correspond to
(p2( z). Neither seller has any incentive to try to
sell to consumers of the other type, nor does any
buyer have any incentive to purchase from the
other type of seller. All four types of agents can
do no better, given the opportunities available.
A. Interpreting the Implicit Value of
Characteristics

Empirical investigations of product and labor
differentiation use cross-section data to relate
prices p with attributes z. The hedonic regression
method regresses product prices on product
characteristics. It was initially developed to
study real cost reductions in auto and other
durable goods manufacturing that were concealed
by product design changes and quality
improvements. In labor economics, wages of
workers are regressed on their personal productivity
and job characteristics. In land and housing
markets, site and structure prices are
regressed on house attributes (size, architectural
style, and age) and onsite characteristics (neighborhood,
location, and public services). Labor
and land market studies are useful for imputing
the social value of certain intangible goods, like
safety and clean environments. An important
use in goods markets is to construct price and
quantity indexes that control for changes in the
quality of goods over time.

B. Cross-Section Values

Environmental and safety concerns are at the
forefront of public policy today. The rhetoric
and passions they arouse make it easy to forget
that these goods are costly to produce and that
rational decisions require comparing their benefits
with their costs. Assessing the costs of
these kinds of public policies is like finding the
costs of any other investment. Assessing benefits
requires estimating the willingness of consumers
to pay for more safety and better
environments. Practice is tricky because there
exist no explicit markets where safety and clean
air are directly traded, and from which demand
values can be directly inferred. Instead, safety
and environmental quality often are by-products
of other transactions and their valuations must
be imputed from the observed packages in
which they play a part.

Exposure to risk and pollution are affected by
work and residential choices. Some jobs are


### ---Economics-2002-0-07.txt---
more hazardous than others and the social and
physical environment varies greatly among
neighborhoods. Private cost-benefit considerations
underlying such choices are the basis for
imputing the required values. Housing in crimefree
neighborhoods is expensive because people
are willing to pay more for greater personal
safety and protection of their property and because
crime-free neighborhoods are scarce.
Wages on hazardous jobs must be higher in
order to induce workers to expose themselves to
greater risk of life and limb. Observed price
differences across jobs and locations are the
implicit prices of characteristics, as in Figure
1, with z interpreted as job safety or neighborhood
safety.

Valuations generally vary among agents.
People with different tastes and incomes have
different bid and offer functions, but more is
involved. In most types of economic exchange,
the Law of One Price implies that marginal
valuations across buyers are identical and that
differences in tastes manifest themselves only in
differences in quantities consumed. For example,
the "last" loaf of bread is worth about the
same-its market price-to a person who consumes
one loaf per week as to a person who
buys one loaf per day.

The Law of One Price always applies to the
specific houses, jobs, or goods markets that
embody intangible characteristics, but not necessarily
to the intangible characteristics themselves.
Whether there is a unique market price
of a characteristic depends on whether or not the
characteristics embodied in existing varieties
can be recombined or remanufactured by buyers
into different varieties. The leading example of
such "combinability" is asset and portfolio
management. Risk and return of any single asset
are relevant only insofar as they affect the risk
and return on one's total portfolio. A portfolio is
a linear combination of various assets, so covariance
of risk on one asset compared to others
is the key risk component. The implied linear
restrictions (or no-arbitrage conditions) imply a
unique market price for risk.

But the fact is that combinability of characteristics
across varieties is not possible for most
other goods. If it is expensive to untie bundles
after they have been manufactured, sellers must
design their goods for specific tastes and assemble
packages of characteristics that appeal to
specific market segments. This generally results
in differing attribute prices across packages.
One cannot buy another unit of comfort for a
sports car in an independent "comfort" market.
Instead, a larger car must be purchased. A
worker on a risky job cannot subcontract little
bits of the risk to others in a secondary market.
It is all-or-nothing. The only way less risk can
be chosen is by finding a safer job. Workers
come prepackaged with various combinations
of skills and traits, some productive and others
counterproductive. Employers cannot detach
the less desirable ones from any single worker.
Rather, an entirely different person has to be
hired. In these cases the market equilibrium
price function p( z) usually is nonlinear, and the
gradient p'(z) generally depends on z itself.
Since there is no single market price for z,
different people have different valuations of it at
the margin. Type 0' and 02 buyers in Figure
1 serve as an example. In principle an average
of the two slopes at z1 and Zh is appropriate for
assessing the (marginal) benefits of a small independent
public project affecting z in some
application, with the average weighted by benefit
incidence of the project among different
types of people.

So many factors determine market prices of
goods in practice that the best chance empirically
for isolating the implicit value of safety
and environment is to examine specific goods or
jobs where these aspects dominate other considerations
of choice. For instance, my labormarket
study with Richard Thaler (1 976) on the
value of life was one of the first to systematically
examine wage premiums on very risky
jobs, where risk was measured as excess insurance
premiums charged by private companies
on worker compensation policies.
The revealed preference argument in Figure
1 applies directly to that problem. Here z represents
job safety and p( z) is estimated by
statistically comparing the smaller market wage
that workers are paid on safer jobs compared to
higher wages paid on riskier ones, other things
equal. Examining risky jobs empirically confines
the study to jobs with relatively low values
of safety, e.g., to those in the neighborhood of z,
in the figure. Since most workers hold relatively
safe jobs (located closer to a point like z,, in the
figure), people holding risky jobs surely place
smaller values on safety than the median person,


### ---Economics-2002-0-08.txt---
much like the difference between people of
types 01 and &2 in the figure. Workers choosing
safe jobs are willing to pay at least as much for
safety as people choosing risky jobs, so the
wage premium on risky jobs is likely to be a
lower bound on the average person's willingness
to pay for safety.

Thaler and I estimated a "value of life" of
about $800,000 in dollars of Year 2000 purchasing
power. Other labor-market studies using
data on a wider range of risks have found
much larger values. The broader range of risks
in these studies is the most probable reason
for the larger estimates: they include more
between-variation (e.g., the differences between
z1 and Zh), whereas our study was largely confined
to within-variation (around zl). Similar
considerations apply to housing and land market
studies that impute values for crime, climate,
and pollution. Here, too, revealed
preference implies that estimated pollution and
crime gradients are likely to be lower bounds
for the average citizen because they ignore important
sorting considerations.

C. Time-Series Imputations

The main practical difficulty in assessing
changing standards of living is that goods
change their character over time. The prices of
tractors and automobiles today differ from 30
years ago, not only because manufacturing productivity
and input prices have changed, but
also because the products themselves have improved.
The nature of the problem is starkest
when entirely new goods appear on the market.
If their introduction is successful and they supplant
older varieties, how should they be linked
into a price index?

Conceptually, the only possibility is to think
in terms of the costs of providing ultimate services.
Technical changes reduce the cost of services.
Autos were successful because they were
a superior way of producing transport services
compared to animals. Electricity produced heat
and light more efficiently than steam and lantems,
and radio, television, and movies reduced
the cost of entertainment services relative to live
performances. In these examples, technical
changes should be factored into price indexes
for transport services, power, and entertainment
services. In practice it is extremely difficult to
link entirely new goods to old goods in this
way. Adjusting for quality improvements of existing
goods is more manageable.

Rising incomes naturally cause product quality
to improve over time, because the income
elasticity of demand for quality is positive.
Even when the prices per unit quality of goods
do not change, rising incomes increase the demand
for quality and raise average transactions
prices over time. Offsetting the rise in demand
for quality is that some aspects of quality (like
ornate detail on early twentieth-century houses)
are labor intensive, which causes their prices to
rise as incomes rise. As long as the cost factor is
not dominant, there is upward bias in measured
prices and downward bias in measured real living
standards from a change in the composition
of goods toward more expensive varieties.
Transactions prices rise not because cost has
increased, but because the quality of what is
being referred to as a particular good has increased.
Income effects are represented by
movements along a given p( z) locus in Figure
1. The same envelope of offer functions define
p(z), but bid functions shift up and to the right
as income rises because the willingness to pay
for increments of z increases with income. 01
and 0& in Figure 1 could represent two people
with the same underlying utility functions but
different incomes. With constant returns to
scale in production, prices of each variety do
not change as income rises, but average quality
purchased and average transactions prices both
increase. We should not confuse movements
along the p( z) locus with changes in the cost of
living. Most consumption decisions change as
income rises, and these are part and parcel of
the same general phenomenon. Standardized
comparisons that control for the changing composition
of goods eliminate this kind of bias.
To assess changes in the real costs of living,
we should account for shifts in the price-quality
locus or technological changes that extend the
real range of choice. Such shifts can cause
quality-specific prices either to rise or fall. For
example, p( z) and the cost of living go up in the
income-increasing experiment above if goods
are supplied inelastically. For example, rising
real incomes increase the demand for amenities
associated with geographic location, raising
site values and the costs of housing services
in the preferred locations. On the other hand,


### ---Economics-2002-0-09.txt---
PG _ 2(z)

Ph2t ./

phI B I/0

A F 0'(z) H _(

Ph2 D .............O E

P12 7 O"(Z)

4 4 ~ ~I I I I I

ZI Z1 2 Zb I ZLI Zh2' z

FIGURE 2. TIME-SERIES EFFECTS

innovations that extend the quality range of
goods tend to reduce the real costs of services
and reduce living costs for those who buy them.
Productivity-improving quality changes generally
reduce the real prices of goods. Prices and
available qualities are always changing. Some
sellers are innovating and attempting to increase
profits by extending the desirable characteristics
in their goods and reducing prices below those
of competitors. The prototypical example appears
in Figure 2. The first-period equilibrium is
the same as before, where goods are labeled zll
and Zh I and equilibria are found at points A and
B. In the second period the attributes of goods
have changed to Z12 and Zh2 so the price-quality
locus has moved down, and the average price
per unit quality has declined. (A ray from the
origin through A is steeper than one through D
and similarly for B and E.) Quality-adjusted
price indexes require measuring the distance
between the two price-quality loci, shown in
Figure 2 as the curve that connects A and B and
the one that connects D and E. The distance
between F and D is a measure of the qualityadjusted
change in price at quality level z,12
When technical change is so large that z could
not be produced in period 1 (a 32-bit computer
chip did not exist in 1980) shown, for example,
by Zh2', direct price comparisons of the improved
product are impossible. In principle this
can be overcome with sufficient structural
knowledge of bid functions. In this case, the
requirement amounts to knowing the exact
shape of 02( z) and of 02'( z), which represents
different indifference curves for the same consumer.
The conceptually appropriate measure of
distance is the difference between actual price
in period 2 and what type 2 buyers would have
been willing to pay for Zh2' in period 1 had it
been available, shown in Figure 2 as the distance
between G and H. Generally such detailed
structural knowledge is unavailable, so distance
must be measured by using overlapping varieties
whose quality is more or less comparable
across adjacent periods. For example, top-ofthe-
line goods in period 1 might be compared
with bottom-of-the-line goods in period 2.
Longer period comparisons are made by linking
or chaining indexes with common components
over the years.

Figure 2 contains a graphic example of why
these adjustments are important. Consider a
new good Zh2' not previously available, whose
current price, Ph2', exceeds that of the best prior
models. Unadjusted indexes would show rising
prices simply because Ph2' exceeds Ph 1. A more
accurate distance-based quality-adjusted index
would correctly show prices declining because
consumers would have been willing to pay more
than its period 2 cost in period 1, had it been
available. Note that there is a consumer whose
bid function, in this case 03( z), reflects higher
satisfaction at (Zh2', Ph2') than at any other
available (z, p) combination.

This example is transparent, but very relevant.
Until a few years ago, the National Income
and Product Accounts priced computers by the
box-by the package in which producers delivered
them. In the early years of the computer
revolution boxes were getting bigger and more
powerful, and their prices were increasing. It
was as if soap were suddenly produced in bigger
bars and the index used the price per bar, not the
price per unit volume or weight. Computer productivity
was grossly distorted in the official
indexes, though the aggregate consequences of
the error were limited because investment in
computers was not such a large share of total
investment as is true today. But even when box
prices were falling, as has been true for many
years, unadjusted price indexes distort productivity.
Prices are really falling much faster than
appears because products are improving so
much. The mainframe of yesterday is the laptop
of today. Considering that computers currently
make up almost half of gross business investment,
eliminating these biases is important for


### ---Economics-2002-0-10.txt---
getting an accurate assessment of national income.
The recent report of The Presidential
Commission on Price Indexes shows that more
widespread failures to make quality adjustments
of goods in price indexes have serious negative
biases for assessing real wage and productivity
growth, and overstate inflation and the social
security and other entitlements that are indexed
to them.

III. Sorting and Stratification
In any market equilibrium, interpersonal differences
in tastes and technologies affect the
locations chosen by buyers and sellers on the
attributes map. Some of these differences are
inexplicable: sometimes there really is no accounting
for different tastes. Others have more
proximate causes, but might just as well be
summarized stochastically. For instance, the
preferences for material goods of a person from
humble origins that subsequently makes lots of
money often seem to differ from those of her
children. Families that are contemplating having
children are likely to prefer larger cars to
smaller ones and suburban to downtown dwelling
units. In the labor market, a person's supply
price for physically demanding or dangerous
work depends on age, physical condition, and
number of dependents, among other things. Different
endowments affect productivities in different
pursuits. Musicians cannot be tone-deaf;
football players tend to be large; while lawyers,
and many economists, have a propensity to talk.
What matters for economic allocations in all
of these cases are the direct manifestations of
tastes. Here, the reservation prices themselves,
not the specific causes that make preferences,
differ case by case. The distributions of bid and
offer functions are sufficient (in the statistical
sense) for ascertaining the demand, supply, and
equilibrium price of each variety. But when
reservation prices are systematically associated
with observable traits of buyers and sellers,
these markets become stratified in many ways.
Stratification and sorting are ubiquitous in spatial
equilibrium and have many interesting consequences.
Neighborhood, bandwagon, status,
and other social externalities are often invoked
to account for stratification, but most stratification
occurs without them.

Sorting by income is one of the most important
forms of stratification in goods, land, and
labor markets. As noted above, people who buy
high-quality goods, and live in better residences
and neighborhoods tend to be richer than other
people. Similarly, high-wage people use their
higher earning capacity to purchase more job
amenities and better working conditions. Stratification
of job quality by the skill characteristics
of workers can make it difficult to estimate
equalizing wage differences for the implicit attributes
of jobs. Sometimes the data on job and
worker characteristics are so colinear that the
ceteris paribus conditions required for identifying
taste parameters alone are overwhelmed by
income stratification and cannot be observed in
the data. Yet it is incorrect to take this as evidence
of failure of the equalizing or compensation
principle in the labor market.
Stratification and colinearity is itself an implication
of the economic theory of preferences
when income effects are important. High-wage
jobs generally are the good jobs, not the bad
ones. They are staffed by skillful workers
whose higher incomes make job amenities more
valuable and who buy more of them. There is no
logical contradiction here. The identifying restrictions
for backing out implicit valuations in
the data just are not satisfied. After all, sometimes
there is not the right kind of variation in
the data to estimate a conventional demand
curve, but that does not imply that a demand
curve does not exist. These difficulties are less
often encountered in imputing implicit prices of
characteristics from goods and land market
prices. Prices and attributes of goods and land
are almost always measured independently of
the characteristics of buyers and sellers. Of
course many aspects of product attributes may
be highly correlated, making it difficult to extract
the value of any single one of them. The
stratification problem more often arises in labor
markets, because observed wages always reflect
the total bundle of both job and workerproductivity
attributes. The correlation between
them can be too large to separate the components.
In sum, potential for stratification places
limits on the empirical applicability of the hedonic
method. Sometimes it cannot be used, but
the test is always empirical.

Colinearity affects the precision with which
effects can be estimated, but sometimes there is
bias in estimating the importance of equalizing


### ---Economics-2002-0-11.txt---
Wage

\ -0~~~~~2

N . \

(p1 (p

yp2

Pension

FIGuRE 3. ACTUAL VERSUS OBSERVED FUNCTIONS
differences. It is not that coefficients are noisy;
it is rather that they have the wrong sign. This
frequently occurs in the labor-market setting
when one estimates compensating differences
in wage functions. A good example involves the
trade-off between pensions and wages. Other
things equal, jobs with higher pension benefits
should have lower wages because the total
amount that employers are willing to pay for a
given worker should not vary with the composition
of the compensation package and because
workers are willing to trade wages for pensions.
In Figure 3, the observed trade-off is shown by
the locus AB, which is the estimated market
relation of wages to pensions. Note that it is
positively sloped, but this is not because workers
do not view both wages and pension benefits
as goods. Instead, it reflects the fact that more
productive workers, who are richer, take some of
their income in the form of wages and some in the
form of pensions. The (pl offer function reflects
the bid by firms to low-productivity workers and
the (p2 function reflects the bid by firms to highproductivity
workers. Even if all workers had the
same preferences, as shown by the dotted offer
functions (the one to the northeast reflecting
higher utility), the market equilibrium would select
points A and B. For any given worker, the
trade-off would be negative because then the (p or
0 curves would be relevant. But the market observes
the AB line, because key productivity factors
cannot be held constant. The AB line might
well be estimated with precision, depending on
the amount and nature of the data, but it identifies
neither a firm's nor worker's willingness to trade
off pensions for wages.

IV. Diversity and Specialization
Specialization, division of labor, and the organization
of work socially create much of the
diversity observed in economics. In his compelling
discussion, Smith identified scale economies
as the principal cause of specialization.
Instead of working alone and doing everything
by oneself, it is productive for a worker to join
teams and assign individual members to a few
mutually exclusive tasks. The division of labor
is one of the most important bits of economic
analysis at the extensive margin. How are productive
activities packaged and bundled together
into jobs, and who works on them?
For the economy as a whole, the most important
reason by far for specialization and division
of labor are scale economies in utilizing acquired
skills. The returns to investing in a particular
skill are proportional to the frequency of
its subsequent use. This makes it efficient to use
th'e skills one has already acquired as intensively
as possible and not to spread oneself too
thinly over a highly diversified portfolio. These
same connections between capacity and utilization
apply to all capital goods, not only to
human-capital varieties. The costs of constructing
an office building depend on its size; but
once it is built, it is efficient to keep the offices
as fully occupied as possible because the marginal
cost of an unoccupied office is much
smaller than its average cost.
Specialization is optimal if learning new
skills involves significant fixed costs that are
only loosely linked to the intensity of subsequent
utilization. Then it is best to learn a few
skills very well and use them all the time. These
basic forces produce enormous social gains
from trade. We could all build our own houses
and educate our own children. Instead, we use
markets to buy new houses built by expert
house builders and purchase educational services
for our children from expert teachers. The
houses are better and the children learn more.
Specialization and trade are important causes of
economic diversity among people in society.
Many aspects of economic diversity and its
manifestations in economic inequality serve
valuable social purposes. The fact is that substantial
inequality is necessary for decentralized
societies to function. Many components of variance
of earnings among persons are sustained


### ---Economics-2002-0-12.txt---
by personal activities that would characterize
the most free and open societies. They are necessary
to sustain both human-capital investment
incentives and work incentives.
For example, in almost every society doctors
earn more than other people. These wage differentials
persist in equilibrium in order to compensate
prospective medical students for their
arduous and costly training. The number and
quality of doctors would fall if earnings were
artificially compressed and the rate of return to
medical education was reduced. A society with
few doctors would score more egalitarian points
on a Gini coefficient, but it would not be a better
society. This example is trivial, but the point is
far more general and often not so obvious.
Much inequality in the overall distribution of
earnings is attributable to substantial differences
in mean earnings among workers of different
ages and educational attainments that
are associated with occupational choices and
human-capital investments over the life cycle.
They reflect rational personal choices that
change a person's economic status and current
incomes over a lifetime.

Rising earnings over working life as well as
earnings differences between occupational
and educational categories largely reflect returns
to human-capital investments. A clear
distinction between human-capital (lifetime)
wealth and current earnings is needed because
learning always involves choosing prospects
with smaller earnings now and higher earnings
in the future. Observing that a person has
low current earnings is no signal of lifetime
poverty if future earnings will be high. Similarly,
high current earnings are no signal of
excess wealth if a person paid the price in low
previous earnings. The point is that the distribution
of current income is far more unequal
than the distribution of human-capital
wealth. Inequality indexes based on current
data alone give a very misleading impression
of true inequality.

The reasoning is most easily understood in
terms of an example. Consider a completely
equal society in which all workers have exactly
the same age-earnings profiles over their careers.
Then the distribution of annual earnings is
a deterministic function of the age distribution
of workers. Societies with more variation in
worker age show more inequality, but that is not
a very interesting aspect of inequality. In fact,
the age distribution makes an enormous difference
to measured inequality indexes: earnings
data used to assess inequality that are not age
standardized are practically worthless for assessing
inequality. Remarkably, such adjustments
are seldom made. Standardizing current
earnings data for education presents more difficult
conceptual problems because family backgrounds
and personal financial barriers on
educational choices distort investment margins
and make wage differentials not fully equalizing
on costs. Nonetheless, some portion of educational
wage differences-and, judging from
the remarkable uniformity of estimated rates of
return to education in different countries, perhaps
a major portion-are equalizing on their
costs. They represent productive, socially manufactured
diversity and inequality that we cannot
live very well without.

V. Personal Productivity and the Distribution of
Earnings

Of course, not all differences are willfully
created. Many are caused by inherited interpersonal
differences in talents and tastes. The hedonic
or characteristics approach to labor
markets bears similarities to statistical factoranalytic
ideas in accounting for earnings differences
across individuals. Factor analysis
partitions observed variance into a small number
of unobserved, latent "causes" or factors. A
leading example is intelligence testing, where
test scores are thought to reflect the quantitative,
analytical, verbal, and mechanical abilities of
subjects. Similarly, personal productivity and
earning power are ultimately determined by
such things as strength, intelligence, dexterity,
and attention to detail. Think of a model in
which a person's earning power is "scored" as
the sum of the amounts of each attribute possessed,
times their market prices. If the number
of factors is small, important proximate causes
of the distribution of earnings are reduced to
small dimensions. The economic rationale for
compacting the determinants of earnings into a
small set of universal factors and prices turns on
the existence of unique economywide prices
("loadings") on the factors. Since these price
weights are parameters in any factor representation
of earnings across persons, dimensionality


### ---Economics-2002-0-13.txt---
is reduced only if the same prices apply to all
persons.

The implausibility of invariance in any market
equilibrium is transparent in the goods market.
Is it possible that the marginal value of a
unit of comfort in an automobile should be the
same as a unit of comfort in an easy chair? Not
at all. These commodities represent different,
imperfectly substitutable services that cannot be
unbundled into such components. We do not see
easy chairs in autos and bucket seats in living
rooms. No "arbitrage" opportunities are available
for trading comfort in one kind of good
with comfort in an entirely different class of
goods, because those characteristics cannot be
untied from the larger bundle of attributes for
which the good was designed. Again, the Law
of One Price does not apply to characteristics,
and their implicit value usually differs among
categories of goods. It would be the same only
by accident.

Similarly, why should the value of another
unit of strength to, say, an accountant be the
same as its value to an athlete? It would be the
same if there were aggregate markets for such
things as strength and intelligence and if a unit
of "accounting strength" was a perfect substitute
for units of "athletic strength" in all productive
activities. The images of accountants on
the 50-yard line and linemen running interference
against the IRS is not, however, reassuring
in this regard. Once again the bundling of personal
productivity characteristics and the impossibility
of untying bundles and repackaging
them into something else is crucial. The marginal
products of underlying factors vary across
activities-strength is more important to professional
athletes than to accountants-and the
shadow prices of these factors vary across activities
as well. Thus, athletes are stronger than
accountants and accountants have more quantitative
skills than athletes. This is another important
manifestation of sorting and stratification in
spatial equilibria.

Activity-specific prices generate comparative
advantage. Just as differences in the relative
abundance of sunshine to rainfall in Portugal
and England give each country its comparative
advantage in wine or cloth, so too does strength
give people who have it a comparative advantage
in some forms of athletics, while arithmetic
skills and attention to detail give other people
comparative advantage in accounting. Comparative
advantage has interesting consequences.
For one, people observed in various job and
educational categories tend to be selected and
stratified by the personal attributes that give
them a competitive edge in a specific field. A
person's financial self-interest is served by selecting
the occupation that maximizes expected
earnings. If this is a major consideration in
occupational choice (though certainly not the
only one), observed earnings of individuals who
voluntarily chose an occupation might be a poor
estimate of what the earnings of people who
avoided that occupation would have been.
Obviously, the earnings of successful athletes
or actresses are not representative of the average
person's prospects in those fields. But the point
is more subtle in other important applications,
for instance in interpreting income differences
between people with more and less education.
As seen above, if all people were identical, the
education-earnings premium would be sustained
by the supply conditions that pay must
compensate for incurring the costs of investment
to equalize net advantages across trades.
When people differ, things are more complicated.
Those who expect the largest returns on
their educational investments are more likely to
make them. Ability rents persist in equilibrium.
There are two main reasons why rates of
return to human-capital investments and human
wealth differ among people. First, natural talents
complement occupation-specific humancapital
investments in different ways. Verbal
ability is indispensable for lawyers and quantitative
ability for engineers and scientists, for
example. People with greater endowments of
such traits have better prospects for success in
those activities. Another way of saying it is that
there are "ability" rents in occupational choices.
Wage differences are not fully equalizing on the
costs of acquiring skills because natural endowments
and premarket investments cause these
costs to differ among people. Second, there are
substantial financial barriers to educational investments
because human capital is not legal
collateral for investment loans. The main manifestation
of this is traditionally seen in high
stratification of educational attainments of children
according to parental wealth. Financing
difficulties cause inefficiencies and inequities in
human-capital investment decisions because


### ---Economics-2002-0-14.txt---
some socially desirable investment opportunities
are not available to poor people. Here, too,
earnings differences are not fully equalizing on
educational costs. They manifest the effects of a
form of financial "noncompeting groups," as
well as the effects of true differences in costs
and talents.

The effects of equal educational opportunities
on the distribution of earnings depends on
interpersonal differences in talents, abilities,
and motivation on the one hand, and on the
importance of noncompeting groups and financial
barriers on the other. Econometricians have
assessed the "ability-bias" in estimated rates of
return to higher education. This work is best
interpreted in terms of a one-factor representation
of skill where individuals are essentially
rank ordered from most able to least able, or
according to absolute advantage. Then, if financial
barriers are not too negatively correlated
with ability, people with more education tend to
be more able than those with less, and the wage
difference between college and high-school
graduates is an upwardly biased estimate of
what noncollege graduates would have earned
had they gone to college. As a practical matter,
the estimates of bias typically are rather small.
The characteristics model enriches analysis by
allowing selection by comparative advantage
rather than absolute advantage. Here abilities
and talents have different values in different
labor-market pursuits.

For example, Robert Willis and I (1979)
modeled educational choice by combining traditional
human-capital ideas with the theory of
comparative advantage, and developed a
method to estimate the behavior determinants of
actual choices observed in the data. We found
that high-school and college graduates do indeed
have different comparative advantages
across skills that dominate the selection process.
Detailed analysis of the earnings patterns of
World War II veterans showed the usual result
that high-school graduates would not have
earned as much as those who actually chose to
get their college degrees. But we also discovered
that persons who subsequently gained college
degrees probably would have earned
relatively less as high-school graduates than
those who voluntarily discontinued their education
after high-school graduation. Positive selection
at both levels of school is inconsistent
with a simple rank-order interpretation or
single-factor model of ability. It is only consistent
with two or more dimensions of ability that
are negatively correlated across people. Comparative
advantage also accounts for why most
estimates of ability-bias interpreted as a single
factor (absolute advantage) are so small. Those
who leave school early do relatively well in
their pursuits so that simple cross-sectional estimates
are not much different from abilitycorrected
ones.

VI. Manufactured Inequality

The production of income is not deterministic.
Sometimes, random forces play an important
role in assigning earnings to individuals.
For example, some occupations are risky. The
arts come to mind, where only a few individuals
can support themselves on the earnings from
their trade. Musicians have very skewed earnings
distributions, but most young music students
who choose to enter the field understand
that there is only a small chance that they will
end up at the desired end of the distribution.
Somewhat counterintuitively, perhaps, the
variance in outcomes that is introduced by this
randomness can improve welfare. The idea is
rooted in early work by Milton Friedman (1953)
and examined again in a different context by
Theodore Bergstrom (1986). Indivisibilities
play a key role here. Most people live in one
and only one house. Area amenities enjoyed
depend on the location of the house and on
individuals' choices on location that are correlated
with income. For example, rich people
may choose to live in New York City rather
than in Kankakee, Illinois because there are
more ways to spend income in New York than
there are in Kankakee. Conversely, the life of a
poor person in Manhattan is difficult because
the amenities are expensive and tend to be targeted
toward high-income people. There is
complementarity in the utility function between
the quality of housing, in this case proxied by
urban amenities, and the level of consumption
of other goods. The situation is illustrated in
Figure 4.

A consumer may choose to live in a highquality
house, Zh, or a low-quality house, zl,
with corresponding house prices Ph and Pl- Because
of the complementarity in consumption


### ---Economics-2002-0-15.txt---
Utility

U(Y-Ph,Zh)

As>_ ~~~~U(Y-pi,ZI)

Y1 YO Yk Y2 Income

FIGURE 4. MANUFACTURED INEQUALITY
between other goods and housing, those whose
incomes are below Yk derive higher utility from
living in the low-quality house in a low-amenity
location and those whose incomes are above Yk
derive higher utility from living in the highquality
house in a high-amenity location. But
consumers with incomes between YI and Y2 can
improve their expected utility by entering into
gambles, which would offer them expected utility
that lies along the line that connects A and B.
For example, an individual with income yo who
chooses to live in a low-quality house offering
utility U(yo - Pl, zl) would prefer a fair gamble
that paid income Yi half the time and income
Y2 half the time. Were he to lose the gamble, he
would live in the low-quality house and derive
utility U(y, - pl, Z,). Were he to win, he would
live in the high-quality house and derive utility
U(Y2 - Ph, Zh). His expected utility would be at
point C, which yields more utility than the certain
utility obtained at point D.

Occupational lotteries of this sort manufacture
inequality but make the individuals who
enter the occupation better off. Because the
variance in outcomes gives individuals a shot at
a much better standard of living, they willingly
bear the risk that results in observed ex post
inequality. Going to a high-stakes law firm in
New York may turn out to be a good option,
offering a partnership, high income, and an entertaining
city in which to spend income. Those
who lose the law-firm lottery accept jobs as
corporate counsel in Kankakee, buy a less expensive
house, and enjoy fewer amenities. Although
it is better to win the lottery than to lose
it, the existence of the risky occupation makes
all individuals who enter better off in an expected
utility sense, primarily because the inequality
manufactured by this occupation
allows different combinations of income and
amenities that are complementary.
VI. Conclusion

Despite the importance of diversity in economic
life, only a small part of economic theory
is devoted to analyzing differences. Competitive
markets value diversity and sort out complex
patterns of tastes and technologies that
translate into supply of and demand for an enormous
variety of products and factors of production.
Markets accommodate diversity by
establishing values that make differentiated
items relatively close substitutes at the margin.
The markets match buyers and sellers in
marriage-type equilibria where agents sort according
to their talents in response to market
prices. To bring about the appropriate sorting,
markets must create inequality in values. Thus, in
labor markets, large differences in earnings can
result even when individuals are ex ante identical.
The theory of diversity applies universally
and is manifest in many economic problems. In
addition to issues involving earnings inequality,
occupational choice, and the differentiation of
products, risk analysis of environmental safety
concerns and price index problems are analyzable
using standard economic approaches to
diversity. In such problems, sorting is key, so
market valuations understate the average individual'
s distaste for disagreeable attributes and
overstate the average individual's preference for
agreeable ones. Sorting plays a role in price index
problems, where we seek to ascertain shifts in
prices as a result of technology shifts, not movements
along a price line that results, say, from
increases in real income. Thus, price might appear
to rise because richer people buy new varieties of
a commodity that has superior attributes that are
not captured in standard measurement.
Although tastes may differ, it is the reservation
prices themselves and not the causes of the
differences that have consequence for economic
allocation. Differences in talents are behind
much of occupational choice where the theory
of comparative advantage is central. Individuals
specialize in skills because the fixed costs of
skill acquisition is only loosely linked to the
levels of utilization. It pays for a worker to learn


### ---Economics-2002-0-16.txt---
one thing well and exploit it over and over
again. Talents are multidimensional in general,
so those who go on to college are better at
college jobs than those individuals who do not
go on to college. But the converse is also true:
Those who opt against college are better at their
jobs than those who complete college would be
at noncollege jobs. Rather than strict hierarchy,
comparative and even some absolute advantage
play important roles.

This essay has explored three themes: Markets
value diversity, markets sort buyers and
sellers appropriately to take advantage of heterogeneous
talents and tastes, and sorting and
choice create income inequality.
Value is determined in diverse markets in the
standard way, equating supply with demand.
The difference is that there are more margins on
which to operate. Not only is quantity a choice
variable, but consumers and producers can substitute
along varying dimensions of quality.
Equilibrium is established when no seller can be
made better off by altering the quality of his
product and offering it to different buyers and
when no buyer can be made better off by substituting
a different quality good for the one that
he currently consumes.

Just as value is determined by market equilibrium,
so too is the allocation of buyers to
sellers set by the market. Sellers who have a
comparative advantage in producing highquality
products sell to consumers who are willing
to pay a sufficient premium for additional
quality. Conversely, sellers who have a comparative
advantage in producing low-quality goods
cheaply cater to consumers who prefer to substitute
away from high quality so that they can
spend the saved dollars on other goods.
Finally, income inequality results from heterogeneity.
Some of this is determined by nature
as individuals are born with different abilities,
but inequality is also manufactured by actions
that individuals take. The most obvious actions
involve investments in human capital, either
through formal schooling or work experience.
Such investments create inequality, but are beneficial
to individuals and society as a whole
because they improve the overall standard of
living. A more subtle variant involves gambles
that individuals take as they choose to enter
risky occupations or make risky investments.
Because of indivisibilities, risky payoffs allow
individuals to couple the consumption of large
amounts of some goods with high-quality versions
of others, such as living in expensive
cities when income turns out to be high. Losers
of occupational lotteries combine their lower
consumption with lower-quality indivisible
goods, consuming less and living in less expensive
cities. Expected utility is higher than it
would be, absent this type of inequality.
Markets value diversity. Individuals, using
their respective talents and different preferences,
respond to these valuations and create
important induced differentiation in consumption
patterns, earnings, and occupational choices.
 ## Economics-2003-0


### ---Economics-2003-0-02.txt---
Macroeconomic Prioritiest

By ROBERT E. LUCAS, JR.*

Macroeconomics was born as a distinct field
in the 1940's, as a part of the intellectual response
to the Great Depression. The term then
referred to the body of knowledge and expertise
that we hoped would prevent the recurrence of
that economic disaster. My thesis in this lecture
is that macroeconomics in this original sense
has succeeded: Its central problem of depression
prevention has been solved, for all practical
purposes, and has in fact been solved for many
decades. There remain important gains in welfare
from better fiscal policies, but I argue that
these are gains from providing people with better
incentives to work and to save, not from
better fine-tuning of spending flows. Taking
U.S. performance over the past 50 years as a
benchmark, the potential for welfare gains from
better long-run, supply-side policies exceeds by
far the potential from further improvements in
short-run demand management.

My plan is to review the theory and evidence
leading to this conclusion. Section I outlines the
general logic of quantitative welfare analysis, in
which policy comparisons are reduced to differences
perceived and valued by individuals. It
also provides a brief review of some examples-
examples that will be familiar to
many-of changes in long-run monetary and
fiscal policies that consumers would view as
equivalent to increases of 5-15 percent in their
overall consumption levels.

Section II describes a thought-experiment in
which a single consumer is magically relieved
of all consumption variability about trend. How
much average consumption would he be willing
t Presidential Address delivered at the one-hundred fif-
teenth meeting of the American Economic Association,
January 4, 2003, Washington, DC.
* Department of Economics, University of Chicago,
1126 East 59th Street, Chicago, IL 60637. I am grateful for
discussions with Fernando Alvarez, Gadi Barlevy, Lars
Hansen, Per Krusell, Ellen McGrattan, Chris Phelan,
Edward Prescott, Esteban Rossi-Hansberg, Tom Sargent,
Matthew Shapiro, Tony Smith, Nancy Stokey, Kjetil Storesletten,
and Tom Tallarini, and for the able assistance of
Adrian Kats and Mikhail Golosov.
to give up in return? About one-half of onetenth
of a percent, I calculate. I will defend this
estimate as giving the right order of magnitude
of the potential gain to society from improved
stabilization policies, but to do this, many questions
need to be addressed.

How much of aggregate consumption variability
should be viewed as pathological? How
much can or should be removed by monetary
and fiscal means? Section III reviews evidence
bearing on these questions. Section IV considers
attitudes toward risk: How much do people
dislike consumption uncertainty? How much
would they pay to have it reduced? We also
know that business-cycle risk is not evenly distributed
or easily diversified, so welfare cost
estimates that ignore this fact may badly understate
the costs of fluctuations. Section V reviews
recently developed models that let us explore
this possibility systematically. These are hard
questions, and definitive answers are too much
to ask for. But I argue in the end that, based on
what we know now, it is unrealistic to hope for
gains larger than a tenth of a percent from better
countercyclical policies.

I. Welfare Analysis of Public Policies:
Logic and Results

Suppose we want to compare the effects of
two policies, A and B say, on a single consumer.
Under policy A the consumer's welfare is
U(CA), where CA is the consumption level he
enjoys under that policy, and under policy B it
is U(CB). Suppose that he prefers cB: U(CA) <
U(csB). Let A > 0 solve

U((l + A)CA) = U(CB).

We call this number A-in units of a percentage
of all consumption goods-the welfare gain of
a change in policy from A to B. To evaluate the
effects of policy change on many different consumers,
we can calculate welfare gains (perhaps
losses, for some) for all of them, one at a time,
and add the needed compensations to obtain the
1


### ---Economics-2003-0-03.txt---
THE AMERICAN ECONOMIC REVIEW

welfare gain for the group. We can also specify
the compensation in terms of one or a subset
of goods, rather than all of them: There is no
single, right way to carry these comparisons out.
However it is done, we obtain a method for
evaluating policies that has comprehensible units
and is built up from individual preferences.
There is a great tradition of quantitative public
finance that applies this general framework
using well-chosen Taylor expansions to calculate
estimates of the compensation parameter A,
"welfare triangles" as Arnold C. Harberger
called them. Today we use numerical simulation
of general-equilibrium models, often dynamic
and subject to unpredictable shocks, to
carry out welfare analysis with the general logic
that I have just sketched. Some examples will, I
hope, convey the applicability of this approach
and some of the estimates that have emerged.
Martin J. Bailey's (1956) thought-experiment
of a perfectly predictable inflation at a constant
rate, induced by sustained growth in the money
supply, was a pioneering example of the quantitative
evaluation of policy. In a replication of
the Bailey study, I estimated the welfare gain
from reducing the annual inflation rate from 10
to 0 percent to be a perpetual consumption flow
of 1 percent of income.1 Some economists take
estimates like this to imply that inflation is a
relatively modest problem, but 1 percent of income
is a serious amount of money, and in any
case, the gain depends on how much inflation
there is. The gain from eliminating a 200-
percent annual inflation-well within the range
of recent experience in several South American
economies-is about 7 percent of income.
The development of growth theory, in which
the evolution of an economy over time is traced
to its sources in consumer preferences, technology,
and government policies, opened the way
for extending general-equilibrium policy analysis
to a much wider class of dynamic settings. In
the 1980's, a number of economists used versions
of neoclassical growth theory to examine
the effects of taxation on the total stock of
capital, not just the composition of that stock.2
The models used in t

ophe P. Chamley (1981), Lawdetails,
but all were variations on a one-good
growth model in which consumers (either an
infinitely lived dynasty or a succession of generations)
maximize the utility of consumption
and leisure over time, firms maximize profit,
and markets are continuously cleared.
In general, these studies found that reducing
capital income taxation from its current U.S.
level to zero (using other taxes to support an
unchanged rate of government spending) would
increase the balanced-growth capital stock by
30 to 60 percent. With a capital share of around
0.3, these numbers imply an increase of consumption
along a balanced growth path of 7.5 to
15 percent. Of course, reaching such a balanced
path involves a period of high investment rates
and low consumption. Taking these transition
costs into account, overall welfare gains amount
to perhaps 2 to 4 percent of annual consumption,
in perpetuity.

Production per adult in France is about 70
percent of production per adult in the United
States. Edward C. Prescott (2002) observes that
hours worked per adult in France, measured as
a fraction of available hours, are also about 70
percent of the comparable U.S. figure. Using
estimates for France and the United States of the
ratio (1 + Tr)/( - Th) that equals the marginal
rate of substitution between consumption
and leisure in the neoclassical growth model, he
shows that tax differences can account for the
entire difference in hours worked and, amplified
by the indirect effect on capital accumulation,
for the entire difference in production. The
steady-state welfare gain to French households
of adopting American tax rates on labor and
consumption would be the equivalent of a consumption
increase of about 20 percent. The conclusion
is not simply that if the French were to
work American hours, they could produce as
much as Americans do. It is that the utility
consequences of doing so would be equivalent
to a 20-percent increase in consumption with no
increase in work effort!

The gain from reducing French taxes to U.S.
levels can in part be viewed as the gain from
adopting a flat tax


### ---Economics-2003-0-04.txt---
LUCAS: MACROECONOMIC PRIORITIES
ful that all of it can be obtained simply by
rearranging the tax structure. It entails a reduction
in government spending as well, which
Prescott interprets as a reduction in the level of
transfer payments, or in the government provision
of goods that most people would buy anyway,
financed by distorting taxes. Think of
elementary schooling or day care. The gains
from eliminating such fiscal "cross-hauling" (as
Sherwin Rosen [1996] called the Swedish daycare
system) involve more than eliminating "excess
burden," but they may well be large.
The stakes in choosing the right monetary
and fiscal policies are high. Sustained inflation,
tax structures that penalize capital accumulation
and work effort, and tax-financed government
provision of private goods all have uncompensated
costs amounting to sizeable fractions of
income. We can see these costs in differences in
economic performance across different countries
and time periods. Even in the United States,
which visibly benefits from the lowest excess
burdens in the modem world, economic analysis
has identified large potential gains from further
improvements in long-run fiscal policy.
II. Gains from Stabilization:

A Baseline Calculation

In the rest of the lecture, I want to apply the
public finance framework just outlined to the
assessment of gains from improved stabilization
policy. Such an exercise presupposes a view of
the workings of the economy in which short-run
monetary and fiscal policies affect resource allocation
in ways that are different from the
supply side effects I have just been discussing.
One possibility is that instability in the quantity
of money or its rate of growth, arising from
government or private sources, induces inefficient
real variability. If that were all there was
to it, the ideal stabilization policy would be to
fix the money growth rate. (Of course, such a
policy would require the Federal Reserve to
take an active role in preventing or offsetting
instabilities in the private banking system.) But
this cannot be all there is to it, because an
economy in which monetary fluctuations induce
real inefficiencies-indeed, any economy in
which money has value-must be one that operates
under missing markets and nominal rigidities
that make changes in money into
something other than mere units changes. Then
it must also be the case that these same rigidities
prevent the economy from responding efficiently
to real shocks, raising the possibility that
a monetary policy that reacts to real shocks in
some way can improve efficiency.
If we had a theory that could let us sort these
issues out, we could use it to work out the
details of an ideal stabilization policy and to
evaluate the effects on welfare of adopting it.
This seems to me an entirely reasonable research
goal-I have been thinking success is
just around the comer for 30 years-but it has
not yet been attained. In lieu of such a theory, I
will try to get quantitative sense of the answer to
the thought-experiment I have posed by studying
a series of simpler thought-experiments.
In the rest of this section, I ask what the effect
on welfare would be if all consumption variability
could be eliminated.4 To this end, consider
a single consumer, endowed with the
stochastic consumption stream

(1) c, = Aete- (l/2)r2e,

where log(se) is a normally distributed random
variable with mean 0 and variance oC2. Under
these assumptions

E(e-(1/2)t02tt) = 1

and mean consumption at t is Ae t. Preferences
over such consumption paths are assumed to be
(2) E{ ;-y},

1 + p 1-y '

where p is a subjective discount rate, y is the
coefficient of risk aversion, and the expectation
is taken with respect to the common distribution
of the shocks so, el ... .

Such a risk-averse consumer would obviously
prefer a deterministic consumption path
to a risky path with the same mean. We quantify
this utility difference by multiplying the risky
path by the constant factor 1 + A in all dates
and states, choosing A so that the household is
4 This calculation replicates the one I carried out in
Lucas (1987, Ch. Il).

VOL. 93 NO. I

3


### ---Economics-2003-0-05.txt---
THE AMERICAN ECONOMIC REVIEW

indifferent between the deterministic stream
and the compensated, risky stream. That is, A is
chosen to solve

(3) E: E t ((1 + A)ct) lMany

questions have been raised about this
estimate, and subsequent research on this issue
has pursued many of them, taking the discussion
deep into new scientific territory. In the
next four sections, I will review some of the
main findings.

III. Removeable Variance: Two Estimates
(Aegt)l -Y

t=0

where ct is given by (1). Canceling, taking logs,
and collecting terms gives

(4) A ^ YI '2.

A 2 y r

This compensation parameter A-the welfare
gain from eliminating consumption riskdepends,
naturally enough, on the amount of
risk that is present, cr2, and the aversion people
have for this risk, y.

We can get an initial idea of the value to the
economy as a whole of removing aggregate risk
by viewing this agent as representative of U.S.
consumers in general. In this case, to estimate A
we need estimates of the variance o02 of the log
of consumption about its trend, and of the coefficient
y of risk aversion. Using annual U.S.
data for the period 1947-2001, the standard
deviation of the log of real, per capita consumption
about a linear trend is 0.032.5 Estimates of the
parameter y in use in macroeconomics and public
finance applications today range from 1 (log
utility) to 4. Using log utility, for example, the
formula (4) yields the welfare cost estimate
(5) A = (0.032)2 = 0.0005,

about one-twentieth of 1 percent of consumption.
Compared to the examples of welfare gains
from fiscal and monetary policy changes that I
cited above, this estimate seems trivially small:
more than an order of magnitude smaller than
the gain from en

ing parameter 400 is 0.022.

Even if we do not know exactly how much
consumption risk would be removed by an optimal
monetary and fiscal policy, it is clear that
it would fall far short of the removal of all
variability. The major empirical finding in macroeconomics
over the past 25 years was the
demonstration by Finn E. Kydland and Prescott
(1982), replicated and refined by Gary D.
Hansen (1985) and by many others since then,
that technology shocks measured by the method
of Robert M. Solow (1957) can induce a reasonably
parameterized stochastic growth model
to exhibit nearly the same variability in production
and consumption as we see in postwar U.S.
time series. In the basic growth model, equilibrium
and optimal growth are equivalent, so that
if technology shocks are all there is to postwar
business cycles, resources are already being allocated
efficiently and a variance-reducing
monetary-fiscal policy would be welfare reducing.
Even if the equilibrium is inefficient, due to
distorting taxes, missing markets or the like, in
the face of unavoidable technology and preference
shocks an optimal monetary and fiscal
policy will surely be associated with a positive
level of consumption variance. We need to estimate
the size of that part and remove it from
the estimate of ao used in (4).
Matthew D. Shapiro and Mark W. Watson's
(1988) study is one of several relatively atheoretical
attempts to break down the variance of
production and other variables into a fraction
due to what these authors call "demand" shocks
(and which I will call "nominal" shocks) and
fractions due to technology and other sources.
Their study represents quarterly U.S. time series
over the period 1951-1985 as distributed lags of
serially independent shocks. The observables
include first differences of a measure of hours
worked, a log real GDP measure, and the corresponding
implicit price deflator. To these
three rates of change are added an ex post real
interest rate (the t


### ---Economics-2003-0-06.txt---
LUCAS: MACROECONOMIC PRIORITIES
minus the inflation rate) and the change in the
relative price of oil. The coefficients of an invertible
vector autoregression are estimated,
subject to several restrictions. This procedure
yields time series of estimated shocks g t and
decompositions of the variance of each of the
five variables into the fractions "explained" by
the most recent k values of each of the five
shocks.

Shapiro and Watson apply a variety of theoretical
principles to the interpretation of
their estimates. They do not consistently follow
the general-equilibrium practice of interpreting
all shocks as shifts in preferences,
technologies, or the behavior of policy variables,
but they have in mind some kind of
monetary growth model that does not have a
long-run Phillips curve.6 Real variables, in
the long run, are determined by real factors
only. Nominal shocks can affect real variables
and relative prices in the short run but not in
the long run. This idea is not tested: Long-run
neutrality is imposed on the statistical model.
In return it becomes possible to estimate separately
the importance of nominal shocks to
the short- and medium-run variability of output,
hours, and real interest rates.7
In the five-variable scheme that Shapiro and
Watson use, there are two nominal variablesthe
inflation rate and the nominal interest rateand
three real ones-output, hours, and the
relative price of oil. They assume as well five
shocks, two of which are nominal in the sense
of having no effect on real variables in the long
run. They are not able to measure the effects of
the two dimensions of nominal instability separately.
The other three shocks are taken to be
real. The assumed exogeneity of oil price
shocks plus a long-run neutrality hypothesis on
hours are used to estimate the importance of
three distinct real shocks. This aspect of their
identification seems to me questionable, and in
any case it is of an entirely different nature from
the neutrality of nominal shocks. I will just
lump the effects of the real shocks together, as
6 To remove any doubt on the latter point, they quote
from Milton Friedman's (1968) Presidential Address.
7 A similar, and similarly motivated, identification pro-
cedure was used in Olivier J. Blanchard and Danny Quah
(1989). Thomas J. Sargent and Christopher A. Sims (1977)
is a predecessor in spirit, if not in detail.
TABLE 1-PERCENTAGE OF VARIANCE DUE TO NOMINAL
SHOCKS AT DIFFERENT FORECAST HORIZONS
Quarter Output Hours Inflation Interest rate
1 28 36 89 83

4 28 40 82 71

8 20 31 82 72

12 17 27 84 74

20 12 20 86 79

36 8 12 89 85

x0 0 0 94 94

Shapiro and Watson do with the two nominal
shocks, and interpret their paper as partitioning
the variance of output and hours into nominal
and real sources. The resulting Table 1 is a
condensation of their Table 2.
The two zeroes for output and hours in the
last, long-run, row of Table 1 are there by the
definition of a nominal shock. But the two 94-
percent entries in this row for inflation and the
nominal interest rate could have come out any
way. I take the fact that these values are so close
to 1 as a confirmation of Shapiro and Watson's
procedure for identifying nominal shocks. According
to Table 1, these nominal shocks have
accounted for something less than 30 percent of
short-run production variability in the postwar
United States. This effect decays slowly, with
no change after one year, a reduction to 20
percent after two years, and so on.
One can ask whether a better estimate of the
importance of nominal shocks could have obtained
by using Ml or some other observable
measure of monetary shocks. Many studies
have proceeded in this more direct way,8 and
much has been learned, but in the end one does
not know whether the importance of monetary
shocks has been estimated or just the importance
of a particular, possibly very defective,
measure of them. Information on future prices is
conveyed to people by changes in monetary
aggregates, of course, but it is also conveyed by
interest-rate and exchange-rate movements, by
changes in the fiscal situation that may lead to
tighter or easier money later on, by changes in
financial regulations, by statements of influential
people, and by many other factors. Shapiro
and Watson's method bypasses these hard
8 For example, Lawrence J. Christiano, et al. (1996).
VOL. 93 NO. 1

5


### ---Economics-2003-0-07.txt---
THE AMERICAN ECONOMIC REVIEW

measurement questions and goes directly to an
estimation of the importance of nominal shocks
in general, those we know how to measure and
those we do not, whatever they may be.
A second reason for preferring the procedure
Shapiro and Watson used is that the effects of
nominal shocks as they estimate them include
the effects of real shocks that could have been
offset by monetary policy but were not. Whatever
it is that keeps prices from rising in proportion
to a given increase in money must also
keep relative prices from adjusting as neoclassical
theory would predict they should to, say,
an increase in the OPEC-set price of oil. Effects
of either kind-those initiated by monetary
changes and those initiated by real shocks-will
last only as long as the rigidity or glitch that
gives rise to them lasts, vanishing in the long
run, and will be identified as arising from the
"nominal," or "demand," shock under the Shapiro
and Watson identification procedure. Thus
I want to interpret the estimates in columns 2
and 3 of Table 1 as upper bounds on the variance
that could have been removed from output
and hours at different horizons under some
monetary policy other than the one actually
pursued. The table gives no information on
what this variance-minimizing monetary policy
might have been, and there is no presumption
that it would have been a policy that does not
respond to real shocks.

Shapiro and Watson applied the theoretical
idea that nominal shocks should be neutral in
the long run to obtain an estimate of the fraction
of short-run output variability that can be attributed
to such shocks. Prescott (1986a) proceeded
in a quite different way to arrive at an estimate
of the fraction of output variability that can be
attributed to technology shocks. He used actual
Solow residuals to estimate the variance and
serial correlation of the underlying technology
shocks. Feeding shocks with these properties
into a fully calibrated real-business-cycle model
resulted in output variability that was about 84
percent of actual variability.9 In a complementary
study, S. Rao Aiyagari (1994) arrived at an
estimate of 79 per

tput variance are proposed.

technology shocks, based on comovements of
production and labor input over the cycle.
Shapiro and Watson find that at most 30
percent of cyclical output variability can be
attributed to nominal shocks. Working from the
opposite direction, Prescott and Aiyagari conclude
that at least 75 percent of cyclical output
variability must be due to technology shocks.
These findings are not as consistent as they may
appear, because there are important real factors
besides technological shocks-shocks to the tax
system, to the terms of trade, to household technology,
or to preferences-that are cyclically
important but not captured in either of the categories
I have considered so far.l? Even so, on
the basis of this evidence I find it hard to imagine
that more than 30 percent of the cyclical
variability observed in the postwar United
States could or should be removed by changes
in the way monetary and fiscal policy is
conducted.

IV. Risk Aversion

The estimate of the potential gains from stabilization
reviewed in Section II rests on assumed
consumer preferences of the constant
relative risk aversion (CRRA) family, using but
two parameters-the subjective discount rate p
and the risk-aversion coefficient y-to characterize
all households. This preference family is
almost universally used in macroeconomic and
public finance applications. The familiar formula
for an economy's average return on capital
under CRRA preferences,

(6) r = p + yg,

where g is the growth rate of consumption,
makes it clear why fairly low 3y values must be
used. Per capita consumption growth in the
United States is about 0.02 and the after-tax
return on capital is around 0.05, so the fact that
p must be positive requires that y in (6) be at
most 2.5. Moreover, a value as high as 2.5
would imply much l


### ---Economics-2003-0-08.txt---
LUCAS: MACROECONOMIC PRIORITIES
tials than those we see between fast-growing
economies like Taiwan and mature economies
like the United States. This is the kind of evidence
that leads to the use of y values at or near
1 in applications.

But the CRRA model has problems. Rajnish
Mehra and Prescott (1985) showed that if one
wants to use a stochastic growth model with
CRRA preferences to account for the entire
return differential between stocks and bondshistorically
about 6 percent-as a premium for
risk, the parameter y must be enormous, perhaps
50 or 100.11 Such values obviously cannot
be squared with (6). This "equity premium puzzle"
remains unsolved, and has given rise to a
vast literature that is clearly closely related to
the question of assessing the costs of
instability.12

One response to the puzzle is to adopt a
three- rather than two-parameter description
of preferences. Larry G. Epstein and Stanley
E. Zin (1989, 1991) and Philippe Weil (1990)
proposed different forms of recursive utility,
preference families in which there is one parameter
to determine intertemporal substitutability
and a second one to describe risk
aversion. The first corresponds to the parameter
y in (6), and can be assigned a small
value to fit estimated average returns to capital.
Then the risk-aversion parameter can be
chosen as large as necessary to account for
the equity premium.

Thomas D. Tallarini, Jr. (2000) uses preferences
of the Epstein-Zin type, with an intertemporal
substitution elasticity of 1, to construct a
real-business-cycle model of the U.S. economy.
He finds an astonishing separation of quantity
and asset price determination: The behavior of
aggregate quantities depends hardly at all on
attitudes toward risk, so the coefficient of risk
aversion is left free to account for the equity
premium perfectly.13 Tallarini estimates a welfare
cost of aggregate consumption risk of 10
percent of consumption, comparable to some
11 See also Lars Peter Hansen and Kenneth J. Singleton
(1983).

12 Two especially informative surveys are John H.
Cochrane and Hansen (1992) and Narayana R. Kocherlakota
(1996).

13 Similar results, obtained in a closely related context,
were reported by Hansen et al. (1999).
of the supply-side gains cited in Section I, and
two orders of magnitude larger than the estimate
I proposed in Section II.14 As Maurice Obstfeld
(1994) shows, this result is basically the formula
(4) with a coefficient of risk aversion two
orders of magnitude larger than the one I used.
Fernando Alvarez and Urban J. Jermann
(2000) take a nonparametric approach to the
evaluation of the potential gains from stabilization
policy, relating the marginal cost of business-
cycle risk to observed market prices
without ever committing to a utility function.
Their estimation procedure is based on the observation
that consumption streams with a wide
variety of different risk characteristics-or
something very nearly equivalent to them-are
available for sale in securities markets. They
use a mix of asset-pricing theory and statistical
methods to infer the prices of a claim to the
actual, average consumption path and alternative
consumption paths with some of the uncertainty
removed. They call the price differentials
so estimated marginal welfare costs, and show
that they will be upper bounds to the corresponding
total cost: my compensation parameter
A. The basic underlying hypotheses are that
asset markets are complete and that asset-price
differences reflect risk and timing differences
and nothing else.

The gain from the removal of all consumption
variability about trend, estimated in this
way, is large-around 30 percent of consumption.
15 This is a reflection of the high risk aversion
needed to match the 6-percent equity
premium, and can be compared to Tallarini's
estimate of 10 percent. But the gain from removing
risk at what Alvarez and Jermann call
business-cycle frequencies-cycles of eight
14 James Dolmas (1998) uses still another preference
family, obtaining much higher cost estimates than mine.
Like Tallarini, Christopher Otrok (1999) develops and analyzes
a complete real-business-cycle model. He uses a
preference family proposed by John Heaton (1995). His cost
estimates are close to mine. A recent paper by Anne
Epaulard and Aude Pommeret (2001) contains further
results along this line, and provides a very useful quantita-
tive comparison to earlier findings.
15 Alvarez and Jermann offer many estimates in their
Tables 2A-2D. My summary is based on Table 2D, which
uses postwar (1954-1997) data and requires that consump-
tion and dividends be cointegrated. From this table, I follow
the authors and cite averages over the columns headed "8
years" and "inf."

VOL. 93 NO. 1

7


### ---Economics-2003-0-09.txt---
THE AMERICAN ECONOMIC REVIEW

years or less-is two orders of magnitude
smaller, around 0.3 percent. Most of the high
return on equity is estimated to be compensation
for long-term risk only, risk that could not be
much reduced by short-run policies that are
neutral in the long run.

Accepting Shapiro and Watson's finding that
less that 30 percent of output variance at
business-cycle frequencies can be attributed to
nominal shocks, the lower Alvarez and Jermann
estimate of 0.3 should be reduced to 0.1 if it is
to serve my purpose as an estimate of the value
of potential improvements in stabilization policy.
But it is important to keep in mind that this
estimate is not smaller than Tallarini's because
of a different estimate of risk aversion. Tallarini's
estimate of y - 100 is the parametric analogue
of Alvarez and Jermann's "market price of
risk," based on exactly the same resolution of
the equity premium puzzle. The different cost
estimate is entirely due to differences in the
consumption paths being compared.
Resolving empirical difficulties by adding
new parameters always works, but often only by
raising more problems. The risk-aversion levels
needed to match the equity premium, under the
assumption that asset markets are complete,
ought to show up somewhere besides securities
prices, but they do not seem to do so. No one
has found risk-aversion parameters of 50 or 100
in the diversification of individual portfolios, in
the level of insurance deductibles, in the wage
premiums associated with occupations with
high earnings risk, or in the revenues raised by
state-operated lotteries. It would be good to
have the equity premium resolved, but I think
we need to look beyond high estimates of risk
aversion to do it. The great contribution of
Alvarez and Jermann is to show that even using
the highest available estimate of risk aversion,
the gain from further reductions in businesscycle
risk is below one-tenth of 1 percent of
consumption. The evidence also leaves one free
to believe-as I do-that the gain is in fact one
or two orders of magnitude smaller.
V. Incomplete Markets and Distribution Effects
The calculations I have described so far treat
households as identical and individual risks as
diversifiable. But as Per Krusell and Anthony A.
Smith, Jr. (1999) observe, "it is quite plausible
that the welfare costs of cycles are not so high
on average, but may be very high for, say, the
very poor or currently unemployed members of
society." Several recent studies have pursued
this possibility.16 Doing so evidently requires
models with incomplete risk sharing and differently
situated agents.

Krusell and Smith (1999, 2002) study a
model economy in which individual families
are subject to three kinds of stochastic shocks.
There is an aggregate productivity shock that
affects everyone, and employment shocks that
differ from person to person. Families are infinitely
lived dynasties, but every 40 years or so
a family draws a new head, whose subjective
discount rate is drawn from a fixed distribution.
Dynasties with patient heads will accumulate
wealth while others will run their wealth
down.17 The sizes of these shocks are chosen so
that the model economy experiences realistic
GDP fluctuations, unemployment spells have
realistic properties, and the overall wealth distribution
matches the U.S. distribution: In the
model, the wealthiest 5 percent of households
own 54 percent of total wealth; in reality, they
hold 51 percent.

It is essential to the substantive question that
motivates this study that neither the employment
shocks nor the uncertainty about the character
of the household head can be diversified
away. Otherwise, the individual effects of the
aggregate productivity shocks would be the
same as in the representative agent models I
have already discussed. One may argue over
why it is that markets do not permit such diversification,
but it seems clear enough that they do
not: Where is the market where people can be
insured against the risk of having irresponsible
or incompetent parents or children?
These exogenous forces acting differentially
across households induce different individual
choices, which in turn lead to differences in
individual capital holdings. The state space in
this economy is


### ---Economics-2003-0-10.txt---
LUCAS: MACROECONOMIC PRIORITIES
anything people were working with numerically
15 years ago, and without the method developed
in Krusell and Smith (1998) it would not have
been possible to work out the predictions of this
model. A key simplification comes from the fact
that the impact on any one family of the shocks
that hit others has to work through two prices,
the real wage and the rental price of capital.
These prices in turn depend only on the total
stock of capital, regardless of the way it is
distributed, and total employment, regardless of
who has a job and who does not. By exploiting
these features, solutions can be calculated using
an iterative procedure that works like a dream:
For determining the behavior of aggregates,
they discovered, realistically modeled household
heterogeneity just does not matter very
much.

For individual behavior and welfare, of
course, heterogeneity is everything. In the
thought-experiments that Krusell and Smith run
with their model, removal of the business cycle
is defined to be equivalent to setting the aggregate
productivity shock equal to a constant. It is
important to be clear on what the effect of such
a change would be on the behavior of the employment
shocks to which individuals are subject,
but the magical character of the experiment
makes it hard to know how this question is best
resolved. I will describe what Krusell and Smith
did, and deal with some other possibilities later on.
Suppose that a shock y = az + - affects an
individual's behavior, where z is the aggregate
shock and e is idiosyncratic. We project the
individual shock on the aggregate, e = cz + 7J,
where the residual qr is uncorrelated with z, and
then think of an ideal stabilization policy as one
that replaces

y = az + e = (a + c)z + r1

with

y = (a + c)E(z) + *1.

Not only is the direct effect of the productivity
shock z removed but also the indirect effects of
z on the individual employment shocks e.18 In
18 This is a linear illustration of the more generally
defined procedure described in Krusell and Smith (1999).
this particular application, removing the variance
of the aggregate shock is estimated to
reduce the standard deviation of the individual
employment shocks by 16 percent.19
The first such thought-experiment Krusell
and Smith describe involves a comparison between
the expected utility drawn from the
steady state of the economy with aggregate
shocks and the expected utility from the steady
state of the economy with aggregate shocks and
their indirect effects removed in the way I have
just described. The welfare gain from eliminating
cycles in this sense turns out to be negative!
In a model, like this one, in which markets for
risk pooling are incomplete, people will engage
in precautionary savings, overaccumulating
capital in the effort to self-insure. This implies
larger average consumption in the more risky
economy. Of course, there are costs to accumulating
the higher capital stock, but these costs are
not fully counted in a steady-state comparison.
In any case, as Krusell and Smith emphasize,
there is nothing really distributional about a
steady-state comparison: Every infinitely lived
dynasty is assigned a place in the wealth distribution
at random, and no one of them can be
identified as permanently rich or poor. The
whole motivation of the paper is to focus on the
situation of people described as "hand-to-mouth
consumers," but a steady-state comparison
misses them. This observation motivates a second
thought-experiment-one with much more
complicated dynamics than the first-in which
an economy is permitted to reach its steadystate
wealth distribution with realistic aggregate
shocks, and then is relieved of aggregate risk.
The full transition to a new steady state is then
worked out and taken into account in the utility
comparisons. In this experiment, we can identify
individuals as "rich" or "poor" by their position
in the initial wealth distribution, and discuss
the effects of risk removal category by category.
The average welfare gain in this second experiment
is about 0.1 of 1 percent of consumption,
about twice the estimate in Section II of
this paper. (Krusell and Smith also assume log
utility.) But this figure masks a lot of diversity.
Low wealth, unemployed people-people who
19 Here and below, the numbers I cite are taken from
Krusell and Smith (2002).

VOL. 93 NO. 1

9


### ---Economics-2003-0-11.txt---
THE AMERICAN ECONOMIC REVIEW

would borrow against future labor income if
they could-enjoy a utility gain equivalent to a
4-percent perpetual increase in consumption.
Oddly, the very wealthy can also gain, as much
as 2 percent. Krusell and Smith conjecture that
this is due to the higher interest rates implied by
the overall decrease in precautionary savings
and capital. Finally, there is a large group of
middle wealth households that are made worse
off by eliminating aggregate risk.
These calculations are sensitive-especially at
the poor end of the distribution-to what is assumed
about the incomes of unemployed people.
Krusell and Smith calibrate this, roughly, to current
U.S. unemployment insurance replacement
rates. If one were estimating the costs of the depression
of the 1930's, before the current welfare
system was in place, lower rates would be used
and the cost estimates would increase sharply.20 It
would also be interesting to use a model like this
to examine the trade-offs between reductions in
aggregate risk and an improved welfare system.
Storesletten et al. (2001) study distributional
influences on welfare cost estimates with methods
that are closely related to Krusell and
Smith's, but they obtain larger estimates of the
gains from removing all aggregate shocks. They
use an overlapping generations setup with 43
working age generations, in which the youngest
cohort is always credit constrained. In such a
setting, the young are helpless in the face of
shocks of all kinds and reductions in variance
can yield large welfare gains. But if the age
effects are averaged out to reflect the importance
of intrafamily lending (as I think they
should be) the gains estimated by Storesletten et
al. under log utility are no larger than Krusell
and Smith's.21 In contrast to earlier studies,
however, the Storesletten et al. model implies
that estimated welfare gains rise faster than
proportionately as risk aversion is increased:
From Exhibit 2, for example, the average gain
increases from 0.6 of a percent to 2.5 as y is
increased from 2 to 4.

Two features of the theory interact to bring
this about.22 First, an

ery helpful in this regard.

ence in the way reductions in the variance of
aggregate shocks affect risks faced at the individual
level. In the Storesletten et al. simulations,
a bad realization of the aggregate
productivity shock increases the conditional
variance of the idiosyncratic risk that people
face, so aggregate and individual risks are compounded
in a way that Krusell and Smith rule
out. A second difference is that idiosyncratic
shocks are assumed to have a random walk
component, so their effects are long lasting. A
bad aggregate shock increases the chances that
a young worker will draw a bad individual
shock, and if he does he will suffer its effects
throughout his prime working years.
The effects of these two assumptions are
clear: They convert small, transient shocks at
the aggregate level into large, persistent shocks
to the earnings of a small fraction of households.
Whether they are realistic is question of
fact. That individual earnings differences are
highly persistent has been clear since Lee
Lillard and Robert Willis's pioneering (1978)
study. The fanning out over time of the earnings
and consumption distributions within a cohort
that Angus Deaton and Christina Paxson (1994)
document is striking evidence of a sizeable,
uninsurable random walk component in earnings.
The relation of the variance of earnings
shocks to the aggregate state of the economy,
also emphasized by N. Gregory Mankiw (1986)
in connection with the equity premium puzzle,
has only recently been studied empirically.
Storesletten et al. find a negative relation over
time between cross-section earnings means and
standard deviations in Panel Studies of Income
Dynamics data. Costas Meghir and Luigi
Pistaferri (2001) obtain smaller estimates, but
also conclude that "the unemployment rate and
the variance of permanent [earnings] shocks
appear to be quite synchronized" in the 1970's
and 1980's.

These issues are central to an accurate description
of the risk situation that individual
agents face, and hence to the assessment of
welfare gains from policies that alter this situation.
The development of tractable equilibrium
models capable of bringing cross-section and
panel evidence to bear on this and other macroeconomic
questions is an enormous step forward.
But Krusell and Smith find only modest
effects of heterogen


### ---Economics-2003-0-12.txt---
LUCAS: MACROECONOMIC PRIORITIES
fare gains from the elimination of aggregate
risk, and even accepting the Storesletten et al.
view entails an upward revision of a factor of
only about 5.

The real promise of the Krusell-Smith model
and related formulations, I think, will be in the
study of the relation of policies that reduce the
impact of risk by reducing the variance of
shocks (like aggregate stabilization policies) to
those that act by reallocating risks (like social
insurance policies). Traditionally, these two
kinds of policies have been studied by different
economists, using unrelated models and different
data sets. But both appear explicitly in the
models I have reviewed here, and it is clear that
it will soon be possible to provide a unified
analysis of their costs and benefits.
VI. Other Directions

My plan was to go down a list of all the
things that could have gone wrong with my
1987 calculations but, as I should have anticipated,
possibilities were added to the list faster
than I could eliminate them. I will just note
some of the more interesting of these possibilities,
and then conclude. The level of consumption
risk in a society is, in part, subject to
choice. When in an economy that is subject to
larger shocks, people will live with more consumption
variability and the associated loss in
welfare, but they may also substitute into riskavoiding
technologies, accepting reduced average
levels of production. This possibility shows
up in the precautionary savings-overaccumulation
of capital-that Krusell and Smith (1999,
2002) found. As Garey Ramey and Valerie A.
Ramey (1991) suggested, this kind of substitution
surely shows up in other forms as well.
In an endogenous growth framework, substitution
against risky technologies can affect rates
of growth as well as output levels. Larry E.
Jones et al. (1999) and Epaulard and Pommeret
(2001) explore some of these possibilities,
though neither study attributes large welfare
gains to volatility-induced reductions in growth
rates. Gadi Barlevy (2001) proposes a convex
adjustment cost that makes an erratic path of
investment in knowledge less effective than a
smooth path at the same average level. In such
a setting, reducing shock variability can lead to
higher growth even without an effect on the
average level of investment. He obtains welfare
gains as large as 7 percent of consumption in
models based on this idea, but everything
hinges on a curvature parameter on which there
is little evidence. This is a promising frontier on
which there is much to be done. Surely there are
others.

VII. Conclusions

If business cycles were simply efficient responses
of quantities and prices to unpredictable
shifts in technology and preferences, there
would be no need for distinct stabilization or
demand management policies and certainly no
point to such legislation as the Employment Act
of 1946. If, on the other hand, rigidities of some
kind prevent the economy from reacting efficiently
to nominal or real shocks, or both, there
is a need to design suitable policies and to
assess their performance. In my opinion, this is
the case: I think the stability of monetary aggregates
and nominal spending in the postwar
United States is a major reason for the stability
of aggregate production and consumption during
these years, relative to the experience of the
interwar period and the contemporary experience
of other economies. If so, this stability
must be seen in part as an achievement of the
economists, Keynesian and monetarist, who
guided economic policy over these years.
The question I have addressed in this lecture
is whether stabilization policies that go beyond
the general stabilization of spending that characterizes
the last 50 years, whatever form they
might take, promise important increases in welfare.
The answer to this question is "No": The
potential gains from improved stabilization policies
are on the order of hundredths of a percent
of consumption, perhaps two orders of magnitude
smaller than the potential benefits of available
"supply-side" fiscal reforms. This answer
does depend, certainly, on the degree of risk
aversion. It does not appear to be very sensitive
to the way distribution effects are dealt with,
though it does presuppose a system of unemployment
insurance at postwar U.S. levels. I
have been as explicit as I can be on the way
theory and evidence bear on these conclusions.
When Don Patinkin gave his Money, Interest,
and Prices the subtitle "An Integration of Monetary
and Value Theory," value theory meant, to
VOL. 93 NO. 1

11


### ---Economics-2003-0-13.txt---

 ## Economics-2004-0


### ---Economics-2004-0-02.txt---
Social Securityt

By PETER DIAMOND*

I frequently find economists who express a
view of the system that is very far from mine.
For example, many young economists and economics
students say that they expect to get no
benefits at all from Social Security. This expectation
does not seem sensible to me. If there is
no legislation changing Social Security, trust
fund assets and payroll tax revenue (and revenue
from the taxation of benefits) are projected
to be sufficient to pay all the benefits scheduled
under current law until 2042 (Board of Trustees
of Social Security and Medicare, 2003). After
the trust fund assets are exhausted the payroll
tax revenue would continue to be available to
pay benefits, with the flow of revenues at that
time sufficient to pay roughly three-quarters of
the benefits scheduled in current law. The estimate
for the end of the 75-year projection period
shows enough revenue to pay roughly
two-thirds of scheduled benefits. With initial
benefits indexed to earnings, average real benefits
would be higher than today, although replacement
rates would only be roughly 60
percent of current levels for the medium
worker. This projection is a far cry from no
benefits.

Moreover, I anticipate that Congress will act
before the trust fund is exhausted, both lowering
t Presidential Address delivered at the one hundred fifteenth
meeting of the American Economic Association,
January 4, 2004, San Diego, CA.
* Department of Economics, E52-344, Massachusetts In-
stitute of Technology, 50 Memorial Drive, Cambridge, MA
02142 (e-mail: pdiamond@MIT.EDU). I am grateful to
Robert Hall for suggesting a framework for approaching
Social Security for this address and to him and Henry
Aaron, Alan Auerbach, Abhijit Banerjee, Nicholas Barr,
Olivier Blanchard, Jeffrey Brown, Courtney Coile, Dora
Costa, Thomas Davidoff, Esther Duflo, Jonathan Gruber,
David Laibson, Jeffrey Liebman, Assar Lindbeck, Alicia
Munnell, Peter Orszag, James Poterba, Virginia Reno, Ber-
nard Saffran, Eytan Sheshinski, Robert Solow, and Peter
Temin for valuable comments on previous drafts, and to the
National Science Foundation for financial support under
Grant No. SES0239380. The address draws heavily on the
thinking behind and the content of my book with Peter R.
Orszag (2004). I have greatly enjoyed and profited from the
collaboration on this and earlier work.
benefits relative to those scheduled under current
law and providing additional revenues to
finance higher benefits than are payable after
2042. After all, the financial problem of Social
Security is not so very large (unlike the larger
and more complex set of financial problems of
Medicare and Medicaid).1 An increase in tax
revenue of just over 15 percent of currently
projected payroll tax revenues would handle the
projected cash flow problem for 75 years on a
present value basis. On an annual cash flow
basis, the share of GDP needed to provide all of
the benefits scheduled in current law would
increase from 4.4 percent of GDP today to 7.0
percent in 2077.2 Like almost everyone else, I
do not favor addressing the projected deficit by
simply adding more revenues with no other
changes. Nor do I picture that solution as having
any political prospects. But solving the problem
with a mix of benefit reductions and revenue
increases does not require large changes, nor
does it require a fundamental restructuring of
the program.

It is not just in the perception of the projections
and the forecast of politics that I find
myself in disagreement with opinions that I
often hear. More generally, I think the system
works better than many economists think. I
hope to convince you that the approach inherent
in the current U.S. system broadly makes good
sense. In particular, I will argue that it makes
sense to mandate taxes to finance a reasonable
replacement of earnings after retirement; that it
makes sense to mandate that retirement benefits
be paid as an annuity; that it makes sense to
mandate protection for family members, both
young children and surviving spouses; that it
I also note that the financial problems of Social Secu-
rity are smaller than the problems in many other countries,
a comparison that seems to surprise some people.
2 To put this change in historic context: Spending on
public education when the baby boomers were children
increased by 2.8 percent of GDP over 25 years; defense
spending during the Cold War increased by 2.5 percent of
GDP in 2 years; defense spending in the 1980's increased
by 1.5 percent of GDP in 6 years; defense spending in the
1990's decreased by 2.6 percent of GDP in 10 years.
1


### ---Economics-2004-0-03.txt---
THE AMERICAN ECONOMIC REVIEW

makes sense to have a progressive benefit formula;
that it makes sense to limit benefits to
those who are old enough and stop working or
are even older (whether they stop or not); and
that having been generous to early cohorts, it
makes sense now to continue with a system that
is only partially funded.

This is not to say that I agree with all of the
details of the current structure by any means. Of
course we should change benefit and tax rules
so that we restore actuarial balance-so that
projected revenues are sufficient to pay for projected
benefits over at least 75 years. And other
changes would be desirable as well. I am just
arguing that the overall design of Social Security
makes good sense. In addition to presenting
the basis of my support for the broad structure
of Social Security, I will identify some rules
needing change and I will speculate on why
some economists seem to have a different view
from mine.

But I will not get into the debate of whether
there should be fully funded individual accounts
financed from existing payroll tax revenues
(carve-out accounts). Nor shall I discuss the
political and economic issues associated with
the potential role of stocks in Social Security.
Those controversial subjects would take up
most of the address and I prefer to write about
more fundamental issues. Those interested in
my view as to why carve-out accounts would
not be good policy in the United States today
can turn to Chapter 8 in my book with Peter R.
Orszag (2004), which also contains a package
of changes to restore actuarial balance and
strengthen protection of some vulnerable
groups. We discuss the potential role of stocks
as well.

I will not say much about the advantage of a
mandate to save for retirement-there is little
call for eliminating such a mandate. After a
discussion of a framework for thinking about
Social Security (Section I), I will consider annuitization
(Section II), treatment of the family
(Section III), the interaction among income distribution,
insurance, and labor supply (Section
IV), the degree of funding (Section V), and
adjustments over time to benefits and taxes
(Section VI).3 Not

nt pension systems in Assar

in mind are the supporting antipoverty programs
as they affect the elderly [Supplemental
Security Income (SSI) and Medicaid], and the
provision of medical insurance for the elderly
(Medicare). Nor will I discuss the Disability
Insurance program, which is a critical part of
Social Security.

I. Providing Retirement Income
One-third of the elderly received at least 90
percent of their income from Social Security in
2001, with nearly two-thirds receiving at least
half (Social Security Administration, 2003). Yet
Social Security was always meant to be a foundation
for retirement income and not a level to
be relied on exclusively. The average new
award for a retired worker in 2002 was just over
$900 per month. For a worker retiring in 2002 at
age 62 (the modal retirement age), a worker in
the middle of the earnings distribution received
a benefit of roughly one-third of (wageindexed)
lifetime average earnings in 2000 dollars.
If the worker had a nonworking spouse of
the same age, the benefit would be largerabout
one-half of the worker's lifetime average
earnings.4 These are low replacement ratesyou
would not want to retire on one-third to
one-half of what you had earned on average in
your lifetime. Benefits would look even lower
compared to earnings over the last decade of
work for a worker with the typical age-earnings
profile. As a foundation for retirement income,
Social Security is something substantial to build
on. As a level to live on, it is clearly inadequate.
Excessive reliance on Social Security, despite
its relatively low replacement rates, together
with a more general picture of many workers
with inadequate w


### ---Economics-2004-0-04.txt---
DIAMOND: SOCIAL SECURITY

the best evidence for evaluating whether workers
make adequate preparation for retirement.
I prefer the term "inadequate preparation" to
"insufficient savings." Preparation involves
multiple decisions. Indeed, one decision is to
save, to have less consumption than after-tax
earned income. Another is investing well. A
third is getting adequate insurance for earnings
risk to have a satisfactory outcome in retirement
despite a possibly adverse earnings experience.
And a fourth is using insurance to arrange income
flows after retirement. There are lots of
ways that workers could end up with inadequate
consumption after retirement relative to what
might sensibly and efficiently be done with earlier
earnings.

In addition to having low savings, many
workers have problems converting savings in
different years into retirement incomes in later
years in different states of nature. We know
from 401(k) studies that many workers do not
diversify sensibly and many do not choose a
sensible portfolio for long-term investments.5
The tendency of many workers to accept the
default allocation set by their employers is suggestive
that they do not have a clear view of
how to choose a portfolio. Outside employerorganized
retirement savings, others pay advisors
as much as 1 percent of assets each year to
help them select mutual funds (in what are
called wrap accounts). Paying 1 percent extra
per year reduces the accumulation at the end of
a 40-year career by roughly 20 percent.6 Mutual
funds, even very similar ones, come with quite
different annual charges. While the average of
charges of mutual funds containing equities is
currently 1 1/4 percent of assets per year (including
a prorating of front loads), some workers
pay much more. A fee of the average size takes
away roughly 25 percent from what would be
there at retirement without any fee. Thus many
workers find it harder to accumulate enough for
retirement than they might, than an idealized
theory says they should. To be clear, I am not
proposing that these market opportunities be
banned-although improvement in regulation
would be welcome. Rather, I am saying that
analysts of Social Security should be realistic
5 For an overview on 401(k)'s, see Alicia Munnell and
Annika Sund6n, 2004.

6 Deposits into an account each year for 40 years are
present in the account for roughly 20 years on average.
about the actual functioning of the market
alternatives.

Investing is only part of the story. We lack
market institutions to provide good insurance
for the risk in earnings trajectories, thereby affecting
the realized pattern of assets at retirement
relative to earnings potential. In the
Arrow-Debreu framework, workers have deterministic
budget constraints from selling their
labor supplies conditional on all the states of
nature in which they have labor that they choose
to sell. That is, they transfer resources across
states of nature to those where the purchasing
power is needed more. Making the same point
in a finance vocabulary, markets do not currently
exist for directly hedging the risks in
earnings opportunities, and if they did exist I do
not think we would see many workers using
them.7

In addition to problems in converting earnings
opportunities into wealth devoted to retirement
consumption, the wealth that is privately
allocated to retirement consumption does not
make adequate use of annuities. This problem
would be more severe without the annuities
provided by Social Security, since the utility
value of the marginal annuity decreases with the
extent of existing annuitization.
These shortcomings in providing for retirement
income fall on surviving spouses even
more heavily than on workers. While 5 percent
of elderly married couples have incomes below
the poverty line, with another 3 percent near
poverty, these figures more than triple when we
consider widows. Indeed, widowhood is associated
with a roughly 30 percent drop in income
relative to needs (Karen Holden and Cathleen
Zick, 1998).8 This is strongly suggestive of
inadequate protection of family members.
To my mind, the heart of the context for
thinking about Social Security is that it substitutes
for poor decision making and for missing
insurance opportunities (missing perhaps because
poor decision making implies low demand)
. The various shortcomings that are
apparent even in the presence of Social Security
7 If they existed, one could hedge some of earnings risk
through traded indices of wages in different regions and
industries. One could try to approximate that through trad-
ing in assets correlated with the indices.
8 Of course this is dependent on the relative measures of
need of singles and couples.

VOL. 94 NO. 1

3


### ---Economics-2004-0-05.txt---
THE AMERICAN ECONOMIC REVIEW

would be more severe in the absence of a program.
These different shortcomings in preparation
for retirement relate to different issuesinadequate
overall provision for retirement
relates to having a mandatory program, inadequate
annuitization relates to providing benefits
in annuitized form, inadequate protection of
family members relates to providing benefits for
surviving spouses and young children. I start
with the annuitization issue, since there is little
overt move to end the mandatory nature of
Social Security as a whole. But there are calls
for decreasing the role of annuities in Social
Security.

But first a word on the Arrow-Debreu framework.
I have referred to it above as a way to
describe the properties of a Pareto-optimal allocation.
I think that when economists quickly
consider economic issues outside their own subdisciplines,
they frequently think implicitly in
terms of the Arrow-Debreu model with its connection
to first-best outcomes (also incorporating
overlapping generations for some issues). In
contrast, economists thinking about issues in
their subdisciplines often share a framework
that is more complex and are more inclined to
do second-best analyses, which are more directly
policy-relevant. As in other subdisciplines,
analysts of Social Security are well
aware of the issues I have identified, although
the issues are not present in all analyses by any
means. The Arrow-Debreu model tends to start
our thinking in terms of the standard, fully
rational model of individual decision making
and in terms of a complete set of markets. That
is a reasonable place to start as long as it is not
also the end of modeling and thinking. For
example, simulations of Social Security reforms
that assume an overlapping generations model
with fully rational lifetime utility maximization
should not be taken as the whole story for Social
Security policy-making. It is inadequate and
potentially misleading to study the effects of
Social Security in models in which there is no
particular reason for Social Security to exist in
the first place. This would be akin to treating
Pigouvian taxation to correct externalities as
distortive by ignoring the externalities.
Interest in the description of behavior that
deviates from that in the Arrow-Debreu model
has grown enormously lately. Long before behavioral
economics became a hot topic, public
policies reflected recognition that the model of
homo economicus, while very useful, is not a
fully adequate basis for the design of all policies.
For example, federal legislation introduced
a "cooling-off' period during which contracts
with door-to-door salespeople could be cancelled
without penalty precisely because of deviations
from homo economicus. And social
security discussions have long recognized inadequate
savings for retirement by many workers
and inadequate annuitization by most. In addition,
social security systems have been concerned
about protection of the family and not
just the worker.9 Also possibly relevant, but not
much studied, is whether significant numbers of
workers retire too soon for their own good.
These issues of poor choices in the presence of
available opportunities are in addition to insurance
market limitations that come from
market incompleteness and from asymmetric
information.

Inadequate attention to the future in general
and its stochastic structure in particular implies
some form of time inconsistency. Normative
criteria for evaluating institutions become more
complicated without time consistency throughout
the population. Insofar as individuals are not
time-consistent, it seems essential to do normative
evaluations in terms of shorter periods (e.g.,
years) as well as in terms of lifetimes. We care
about actual consumption levels as well as the
levels of lifetime resources.

This requires more than just a positive theory
of how people determine consumption but also
normative criteria for evaluating consumption
at different times. The vocabulary of someone
being different selves at different times is suggestive,
although I am concerned that taking it
too literally, failing to recognize the tight links
between the different selves who are the same
person at somewhat different ages, is failing to
address adequately


### ---Economics-2004-0-06.txt---
DIAMOND: SOCIAL SECURITY

since the political process is not equivalent to a
consistent approach to policy over time (which,
it seems to me, is an essential property of democracy
given divergent preferences and
views), we must consider issues from multiple
perspectives.11

An education built around the Arrow-Debreu
model may lead to overvaluing the fundamental
welfare theorem. The wonderful properties of
competitive equilibrium in certain unrealistic
circumstances lead the profession to be very
aware of distortions that prevent first-best outcomes.
But some distortions are associated with
redistribution and with easing other deviations
from first-best rules. Stressing the distortions
caused by government policies and not giving
equal weight to the redistribution and insurance
and revenue generation accomplished by these
policies, effectively doing partial first-best
thinking rather than complete second-best
thinking, can lead to unbalanced inferences
about policies.12

II. Annuitization

Some mandate for retirement saving is not
particularly controversial among policy-oriented
economists, so I begin with mandatory annuitization.
First, let us consider the point of payments
that are conditional on being alive. With
some saving for retirement (over and above
precautionary balances) a worker can learn of
rates of return (and risks) available in the market
for investing for different lengths of time
(that is, including an illiquidity premium). Anyone
investing for some period of time (for example,
bank certificates of deposit, insurance
contracts, mutual funds with an early withdrawal
penalty, direct loans) could wonder how
much more might be paid if the investor were
still alive provided there was no payment at all
if the investor were no longer alive. With a
what a model is all about. On this subject, see Alfred
Marshall (1948, p. 366).

11 That is, thinking only in terms of lifetime utilities and
sustained government policy rules seems to me inadequate.
12 For example, when Congress removed the retirement
test between the age of full benefits and age 70, some
wanted to remove the test for all those over age 62. Noting
only that the test discourages work, without noting its ef-
fects on the timing and size of benefit receipt would be an
example of such partial first-best thinking.
noticeable probability of the investor's dying
before reaching the end of the contract period
and little cost for checking whether the investor
is still alive, it would be worthwhile for a borrower
(bank, insurance company, mutual fund,
or direct borrower) to offer some additional
payment in return for being freed from payment
in the event of the death of the investor.
This is the essence of an annuity and the
essence of why for an investor with no interest
in bequests and a tolerance for some illiquidity,
an annuitized asset dominates the same asset
without an annuity feature. This is how the
Arrow-Debreu model works when markets are
complete-the gain from annuitization can be
thought of as a lowering of the price of future
consumption by forgoing deliveries after one's
death. The formal argument for the dominance
of annuitization was made by Menahem Yaari
(1965) in the context of a conventional annuity
that guarantees payments over the rest of one's
life. But the argument is much broader than that
(Thomas Davidoff et al., 2003). Moreover, simulations
show a sizable quantitative importance
of annuity opportunities.

People do care about their children. But, a
bequest motive does not eliminate the advantage
of some annuitization. With a bequest motive
and complete Arrow-Debreu markets, one
would determine how much of one's lifetime
budget constraint to give away and when to give
it (e.g., when children reach some age). It would
seem very odd to prefer to have one's children
receive an amount in present value that was
conditional on how long one lived (even if one
did not want to make a transfer before dying).
So, one would still use annuities, the purchase
of commodities conditional on being alive, for
all of one's own planned consumption. That is,
having a bequest motive is not a basis for doing
no annuitization in a complete market settingunless
one was roughly risk neutral about both
the amount of bequest and its timing. Without
complete markets, a willingness to invest in
illiquid assets for future consumption leads to
the same advantage for some annuitization.
13 This is the way a defined benefit system works for
workers without any dependents eligible for benefits. The
risk could be shifted from the borrower to the set of investors
by distributing a fraction of the amount not paid to
deceased investors to surviving investors. This is the way
that CREF annuities work.

VOL. 94 NO. 1

5


### ---Economics-2004-0-07.txt---
THE AMERICAN ECONOMIC REVIEW

Despite the advantages of annuities, we see
only a small fraction of people doing voluntary
annuitization.14 Furthermore, those who do annuitize
make very odd choices. They buy nominal
annuities. There is wide popularity of what
are called guarantees-continued payments after
death up to some limit.15 Such guarantees
undo some of the underlying annuitization, and
are a relatively expensive form of holding nonannuitized
wealth (given the relative administrative
costs on annuities and other accounts).
They represent an increase in the riskiness of
one's bequest, not a decrease. That is, an annuity
without a guarantee costs less, allowing one
to leave one's heirs a determinate amount in
present value, rather than a random amount
depending on the date of death.'6 More generally,
many features of insurance markets are
hard to reconcile with sensible decisions by
households and the equilibrium industry response
we would expect in the presence of
sensible demands. The extremely limited options
available for annuitization seem to reflect
the natural response of the supply of insurance
to the nature of demand.17

Some have tried to explain this limited use of
annuities by the degree of annuitization that
already exists in government programs. But voluntary
annuitization, while present for centuries
before the creation of these programs, was not
extensive in the population and is unlikely to
become extensive if the programs were removed.
Asymmetric information is another candidate
for explaining this situation, and it does
cause an adverse sele

date of the

first payment.

would discourage some individuals from annuitizing.
While large systematic differences in
life expectancy do exist, much of the difference
is readily attributed to easily measured factors,
so insurance companies could do more to overcome
this problem, given the potential for large
gains to the insured.18 In the United Kingdom,
there is a sizable market for individual purchase
of annuities because of a large tax incentive for
their purchase from assets in tax-favored individual
retirement accounts. In the presence of
this demand, suppliers are offering annuities
with better prices for those with "impaired
lives." We do not see this risk classification in
the United States, presumably because there is
not a ready market in which firms could take
advantage of selection by risk classification and
better pricing since so few households purchase
annuities on an individual (nongroup) basis. So,
adverse selection alone can not explain the low
level of annuitization that is present.'9
I believe the major issue behind this pattern
of insurance demand is the failure of many to
understand the advantages of annuitization.
This plausibly relates to the failure of much of
the population to understand the properties of
stochastic variables, as has been documented by
cognitive psychologists. It is to be expected that
the set of insurance products that are marketed
will reflect the shortcomings of consumer
understanding-it is very expensive to try to
sell a product the virtues of which potential
customers do not understand. I think that without
Social Security, inadequate annuitization
would be even more widespread than inadequate
savings.

In any event, social security systems in advanced
countries typically provide benefits as
annuities, annuities that are generally indexed to
prices or wages (or a combination). This is a
simple application of the view that in a mandatory
program, individuals should be given what
we think they


### ---Economics-2004-0-08.txt---
DIAMOND: SOCIAL SECURITY

informed and well-educated. The presence of
mandatory annuitization does not prevent bequests,
although it raises the cost and requires
action to do so. Those surviving to the start of
their benefits and with sufficient life expectancy
can use part of their monthly Social Security
benefits to finance a long-term life insurance
contract, thereby providing a bequest with an
explicit choice of the relationship between the
real size of the bequest and the date of death.
This action contrasts with simply leaving unspent
funds to one's heirs, a strategy that leaves
an amount dependent on the history of consumption
relative to the income earned on
assets.20

In other words, the government's choice between
providing retirement benefits as annuities
or as lump sums can be considered as a choice
of a default, one which most individuals could
reverse-by purchasing life insurance if provided
an annuity or purchasing an annuity if
offered a lump sum (B. Douglas Berheim,
1991). Reversing the government choice,
though, takes time, thought, and effort and it has
a cost. That is, the government provides annuitization
at a far lower cost than does the private
market. The absence of selling costs (other than
equivalent information provision) and economies
of scale contribute to this advantage. Administrative
costs of Social Security are less
than 1 percent of annual expenditures, and a
great deal of that is due to the disability program,
which is naturally more expensive to run.
In contrast, privately provided insurance has
higher costs-life insurance company accounting
generally recognizes over 10 percent of
premia used for administrative costs and profit.
The private market is more expensive and does
not do a better job of delivering annuity products
that people need.21 This is one reason to
20 The timing of the purchase of both life insurance and
annuities is important for obtaining insurance. Waiting to
insure passes up insurance opportunities. The effects of
waiting depend on the extent of risk classification in the
pricing of insurance.

21 With any commodity, selling costs could be avoided
by mandating payment for government provision. What
distinguishes retirement annuities from most other products
are the similarity of needs of different workers (compared
with the diversity of tastes for different commodities) and
the reasons why a retirement income mandate is needed.
These reasons suggest that public provision is not blocking
an otherwise creative dynamic product development. The
have government provision rather than a mandate
to purchase an annuity in the private market.
22 As with many other settings, we expect
individuals to undo little of what is provided.23
So it makes sense to offer what we think people
might sensibly want. Moreover, the wider functioning
of the life insurance market than the
annuities market suggests a further advantage to
using substantial annuitization as the default.
A mandatory retirement income program requires
a choice of the form of benefit and it is
hard to think of a basis for choosing the form
which is other than what makes sense for the
bulk of the population. It seems to me that this
is an annuity in some form.24

A. Lifetime Income Distribution
Mandated annuitization affects lifetime
income distribution.25 Suppose one were
nature of the cash-in, cash-out annuity product also suggests
that we are not missing cost improvements that would
otherwise come. The small size of replacement provided by
Social Security leaves lots of room for such private devel-
opments if they did represent a significant opportunity.
22 With mandated private purchase, if allowed, we would
get risk classification and separate pricing, as has happened
in the United Kingdom. Separate risk classification has
advantages and disadvantages, and it is unclear whether it
would be better.

23 While many of the elderly have life insurance policies,
these appear to arise mostly from coverage for funeral
arrangements, small old policies that were not terminated,
and tax avoidance among the wealthy, rather than from a
conscious attempt to undo annuitization.
24 Mandatory annuitization in a social security program
raises the interesting question of how monthly benefit
should vary over time-with prices, wages, and possibly
other variables such as rates of return. Relevant for this
issue are the age structure of optimized expenditures, the
relative importance of both real and relative consumption,
and the allocation of risk bearing between the elderly and
the rest of the population. Currently, benefits in force are
increased for inflation as measured by the CPI. While this is
a reasonable solution, I suspect it would be better, on a
revenue neutral basis, to have lower initial benefits that then
grew faster (for example as a weighted average of prices
and wages). This would help the longer-lived more than the
shorter-lived, but the effect on expected lifetime income
distribution could be partially adjusted by changing the benefit
formula. But this issue has not received detailed analysis.
25 A full analysis of the income distribution effects of
Social Security should consider the disability program along
with the retirement income program since there is a negative
correlation between life expectancy and the likelihood of
collecting disability benefits and dying young enough to
leave children who collect young survivor benefits.
7

VOL. 94 NO. I


### ---Economics-2004-0-09.txt---
THE AMERICAN ECONOMIC REVIEW

comparing two mandatory programs, one with
lump-sum payments and one with annuities.
This comparison would be easy if individual
choices between annuitization and nonannuitization
were unaffected by the government
choice. Then one would simply compare the
implicit price of the trade-off between annuities
and lump sums in the alternative mandatory
programs with the explicit price at which
individuals could make transactions. For example,
if everyone annuitized and the market
had a single price for all annuities, then we
would compare the price implicit in the comparison
of the programs with the actual uniform
price. In this case, we would find
mandatory annuitization attractive because
the government would be likely to have a
better price than the market.26 Conversely, if
everyone would purchase life insurance to
undo a mandatory annuity (and rates were
uniform), then we would find mandated annuities
unattractive since the private market
price for life insurance would likely be larger
than the implicit price if the government
switched from annuities to lump sums.
The story becomes a little more complicated
if we assume that everyone annuitizes
and the market would offer different prices to
different people. This might happen with a
mandate to purchase annuities in the private
market if the market had some degree of price
diversity by risk class.27 Then, in addition to
the difference from the average price with and
without the government annuity, we would
note the differences in prices for different
people. Relative to annuities priced differently
for different groups, the uniform
annuitization implicit in the mandated annuitization
would favor those with longer expected
lives-women relative to men, male
high earners relative to male low earners,
female high earners relative to female low
earners. A progre

labor

market incentives.

used to offset the systematic variation in life
expectancy with earnings within genders.
For this or any income distribution comparison,
we must have a counterfactual, preferably
a plausible one. Without a mandate, the
relevant counterfactual is that approximately
no one would annuitize. Pretty much everyone
would lose the insurance gains from annuitization.
28 We can compare the mandate

with this counterfactual in two steps-first the
value of annuitization assuming actuarially
fair pricing and then the difference, described
above, between fair and uniform pricing.
Since those groups with shorter life expectancies
have more to gain from fair annuitization
[assuming CRRA preferences in the
usual range and realistic mortality rates (Jeffrey
Brown, 2003)] this counterfactual shows
much less diversity in the utility value of
annuitization than the previous comparison.29
Indeed, Brown finds that the utility value of
annuitization (relative to wealth) is similar for
groups divided by gender, race, and education.
Thus the differences in expected payments
from different life expectancies have
less distributional impact in utility terms than
in exp


### ---Economics-2004-0-10.txt---
DIAMOND: SOCIAL SECURITY

B. Labor Incentives

The implicitly uniform-price annuitization in
Social Security also affects labor market incentives.
The use of uniform annuity pricing (overall
or within still heterogeneous risk classes)
violates the conditions for first-best optimization.
Compared to first-best pricing, the decision
that would be distorted is that of labor
supply. If annuity pricing is breakeven, then
some are being taxed on work while others are
being subsidized compared to a system where
annuities are priced for individual life expectancies.
An alternative counterfactual would be
a failure to annuitize at all. Without annuitization,
we would have more accurate labor market
incentives person-by-person, but earnings would
finance less satisfactory consumption trajectories.
We would fail to insure not only life expectancy
realizations but also changes in life
expectancy as information accrues. That is,
even unfair annuities can raise individual welfare
if the alternative is no annuities.
I conclude that having a mandated retirement
income program provide its benefits as annuities
is sensible.

III. Workers and Families: Young Child,
Spouse, and Survivor Benefits

Social Security provides more than just retirement
benefits for workers. It provides benefits
for disabled workers and their families, for
young children of a deceased worker, and for
elderly spouses and surviving spouses. In addition,
a divorced spouse may be eligible for the
same benefits as a spouse if the marriage lasted
at least ten years.31 Benefits other than worker
benefits are referred to as auxiliary benefits.
These benefits are subject to a maximum
rule-a beneficiary receives the largest benefit
he or she is eligible for-with no increment for
also being eligible for a smaller benefit. That is,
if someone has worked at least ten years, on
retirement he or she is eligible for a retired
worker benefit. He or she is also eligible for a
spouse benefit if married to a retired worker
beneficiary. But the total amount of benefit is
equal to the maximum of the two benefits. Sim-
31 There is a family maximum that proportionally re-
duces all benefits except those of the worker if it binds.
ilarly, someone eligible for a worker benefit and
also eligible for a survivor benefit receives only
the larger benefit.32 A central design feature is
that these auxiliary benefits are not paid for on
an individual basis-workers with the same
earnings history receive the same retired worker
benefits whether or not they have family members
or ex-spouses collecting auxiliary benefits.
Auxiliary benefits raise four questions. Does
it make sense to mandate benefits for family
members and ex-spouses? Does it make sense to
base benefits on a maximum rule? Does it make
sense to finance all of the auxiliary benefits
from the program as a whole rather than in part
or in full from the benefits of the retired worker?
Are the details of benefit determination rules as
well-designed as might be?

Let me start with the first, most basic question.
It makes sense to provide auxiliary benefits
since studies suggest that significant numbers of
workers do not insure their lives adequately and
would not make good choices between singleand
joint-life annuities. More generally we are
learning more about the ways in which the
allocation of resources within the family does
not conform to a single maximization with a
single budget constraint. Since the government
cares about the different family members (and
not just the worker), direct allocations to family
members matter since they will change the allocation
of resources within the family. Protecting
family members is a role governments have
recognized for centuries.

The other questions are more complex and
need more detailed analyses. Two issues are
central here. These are the positive and normative
issues of how consumption is actually allocated
within families and how to combine
evaluations and rules that affect both individuals
and families. Research on the determination
of allocations within the family is still in an
early stage of development. And normative
analysis has not progressed much beyond identification
of the dilemma in recognizing both
32 If the spouse or survivor benefit is larger, the person is
referred to as having dual eligibility. In 2002, 38 percent of
elderly women received only a worker benefit, 34 percent
received only an auxiliary benefit and 28 percent were
"dually entitled." These fractions are expected to change as
more and more women with substantial careers reach retirement
age.

VOL. 94 NO. 1

9


### ---Economics-2004-0-11.txt---
THE AMERICAN ECONOMIC REVIEW

individuals and families.33 So my answers here
are speculative and primarily meant to identify
where research might lead us to policy
improvements.

Offhand, the maximum rule does not provide
labor incentives well (incentives are stronger for
workers who have spouses likely to collect auxiliary
benefits since two benefits are increased
by more earnings, and weaker for workers who
are likely to receive a larger spouse benefit since
further earnings by someone receiving a spouse
benefit do not increase that benefit). A similar
(but less extreme) issue arises with income taxation
of a lower-earning spouse.

Offhand, the cost of auxiliary benefits should
be shared between a worker and the program as
a whole. The benefit formula is progressive,
with a higher replacement rate for lower earners,
reflecting differences in retirement needs.
As part of responding to needs, it seems right to
recognize dependents in determining need and
so benefits. But the current rule is not the only
way to do that. Some provision of auxiliary
benefits for children from the general program
makes sense, in keeping with our generalized
support of children in education; some provision
for spouses is relevant in the role of
progressivity-two can not live as cheaply as
one. But the current system has gone too far and
I share in the criticism that too much is given to
the nonworking spouses of high earners.34 Using
system resources to finance large transfers
to those in the upper tier of the earnings distribution
offsets too much of the progressivity in
other portions of the system. Designing a different
system would be politically sensitive and
complex and would need detailed analysis.
The determination of survivor benefits has
also received consi

-

erable further analysis.

ing the role of couples in sharing resources at
least partially, it makes sense to relate the benefits
of an elderly survivor to the benefits that
had been received by the couple-a survivor
replacement rate. Currently, survivor replacement
rates vary with the past earnings of husband
and wife, usually, but not always, between
one-half and two-thirds.35 A uniform survivor
replacement rate seems more likely to approximate
relative needs than the current system. The
much higher poverty rate of widows than of
couples, noted above, suggests a higher survivor
replacement rate is needed, with threequarters
having been suggested by some

analysts.36 The change to a uniform and higher
survivor replacement rate could be financed out of
a suitable mix of the total resources of the program
and the benefits of the couple while both are alive.
The current recognition of divorce is to allow
benefits for unremarried divorced spouses and divorced
surviving spouses after at least ten years of
marriage. Since there is a family maximum, some
of these benefits are paid by the system as a whole
and some out of the other auxiliary benefits. The
adaptation of the system for the growth in divorce
seems to me a major issue for research. I do not
know if we can design a system that would be
better, recognizing both labor market incentives
and income distribution issues, or if such a design
could survive political hurdles.37 But it


### ---Economics-2004-0-12.txt---
DIAMOND: SOCIAL SECURITY

In sum, mandating benefits for the families of
workers is important, along with mandating
savings and mandating annuitization-the inclusion
of family benefits in Social Security
makes sense. There is good reason to think that
the current rules can be improved, but research
difficulties and political hurdles will need to be
overcome if we are to make improvements.
IV. Income Distribution, Insurance, and Labor
Supply38

In determining retirement benefits, Social Security
first averages the best 35 wage-indexed
annual earnings,39 then it uses a progressive
benefit formula to determine what real benefits
would be if first claimed at the age for full
benefits (commonly, if somewhat misleadingly,
called the normal retirement age),40 and then it
adjusts benefits for the age at which they start.
Moreover, between age 62 (the earliest age at
which retirement benefits can be claimed) and
the age for full benefits (which is in transition
from 65 to 67), benefits are only paid if earnings
are low enough, referred to as an earnings or
retirement test. Each of these steps in determining
benefits affects income distribution, insurance,
and labor supply. I will skip over
implications of using 35 years (as opposed to
more or fewer years or all of lifetime earnings
subject to tax)41 and of using a wage index to
weight the earnings in different years in determining
benefits (as opposed to using an
interest rate)42 and concentrate on the effects
38 For a discussion of the links among tax theory, incom-
plete market theory, and social security, see Diamond
(2002).

39 Earnings after age 60 enter in nominal terms, not in
indexed form. This should be changed to have labor incen-
tives not vary this way with inflation.
40 Such progressivity is missing in Europe, where in-
come distribution issues are addressed more fully in other
parts of retirement income provision.
41 Depending on the nature of the underlying stochastic
process of wage rates, both underweighting early years
(relative to the use of interest rates) and not counting some
low years may or may not help with insuring lifetime
earnings-this is not an area that has received much re-
search attention.

42 We can contrast the earnings incentives between ben-
efit formulas that accumulate earnings with a wage index
and those that accumulate with a (presumably higher on
average) interest rate. This is most readily done in a break-
even comparison, assuming the same level of resources for
of a progressive benefit formula and a retirement
test.43

Consider the stochastic process of earnings
opportunities. Individual workers face considerable
risks that are only partially correlated with
the economywide average earnings used in indexing.
Wages move differently by industry and
firm and region and some individuals have career
opportunities strongly affected by industry
and firm and region developments. We do not
have trading in the type of indexes Robert J.
Shiller (1993) has proposed in order to give
workers the ability to hedge these aspects of
their risks.44 Even if we managed to have trading
in such indices, it is beyond credibility that
most workers would take appropriate advantage
of these opportunities. When many workers can
not sort out the basics of portfolio diversification
in their 401(k)s, there is no reason to anticipate
successful execution of far more
complex financial strategies. By having replacement
rates that are higher for lower levels of
lifetime earnings, a social security system that
a cohort in each case. Using a wage index gives less weight
to the earlier years and more weight to the later years than
does use of an interest rate. Thus, use of a wage index would
be implicit taxation on younger workers and implicit subsidization
of older workers and would be a distortion in a
first-best world. However, the annual income tax is progres-
sive, so that an upward-sloping age-real earnings profile
implies that on average older workers have higher marginal
income tax rates than younger workers. Thus, the sum of the
marginal income tax plus the implicit Social Security tax is
smoother across ages with a wage index than with weight-
ing by interest rate. Also, preferences are not intertempo-
rally additive. The standard of living to which retirees have
become accustomed is more affected by earnings later in
life than earlier in life. While the effect of introducing a
standard-of-living effect into annual utility has been ex-
plored in simulations of the value of annuities (Davidoff et
al., 2003), no similar analysis has been done for the weight-
ing given earnings in different years.
43 I do not discuss the issues behind the choice of 62 as
the earliest age of eligibility for retirement benefits (EEA).
Increasing the EEA helps those who would otherwise retire
too early for their own good and hurts those who are right
in their early retirement decision and are hurt by the illi-
quidity from benefit nonavailability until the EEA. Measur-
ing the size of the two groups would be very hard and only
a little has been done in identifying people who are affected.
Increasing the EEA would have little effect on long-run
Social Security financing as explained below. Similarly, it
would be hard to design a good method for automatically
indexing the EEA.

44Robert C. Merton (1983) has examined the role of
Social Security in sharing aggregate earnings risks more
widely.

VOL. 94 NO. I

11


### ---Economics-2004-0-13.txt---
THE AMERICAN ECONOMIC REVIEW

makes benefits a progressive function of lifetime
earnings offers insurance about lifetime
earnings that is not available in the market.
If the taxes and benefits for a cohort broke
even in present value terms, the use of a progressive
benefit formula would imply that the
labor supplies of lower earners were being subsidized
and those of higher earners were being
taxed.45 This is the familiar pattern with insurance
with asymmetric information-a combination
of insurance and incentives neither of
which satisfy the conditions for first-best optimization.
This effect of progressivity is in
addition to the effects from annuity pricing discussed
above. Some of the effects of annuitization
and progressivity would be offsettingthose
with higher earnings of each gender tend
to live longer-and some would be compounding-
women on average have lower earnings
and longer lives. That taxes and benefits do not
break even on a cohort basis is discussed in the
next section.

The progressivity in the benefit formula uses
taxes that distort labor supply in order to redistribute
income and provide insurance. The progressive
annual income tax also redistributes
income, provides insurance against earnings uncertainty,
and distorts labor supply. Since these
two institutions work on different tax bases and
provide payments at different times, there is
room for each of them to contribute despite the
presence of the other. Annual income taxation
recognizes short-term needs, coming from borrowing
constraints and from behavior that is not
time-consistent. It also recognizes capital income
as part of determining tax rates. Ex post,
all of one's Social Security taxable earnings (in
the best 35 years) contributed to benefits in a
way that varies with age but not with the level
of annual earnings, given lifetime earnings.
This avoids the distortions coming from having
different marginal tax rates in different years as
a function of annual earnings, or annual capital
income. The use of a lifetime measure also
separates out issues of lifetime earnings from
the age-earnings

bsidies to

marginal taxes.

tion.46 While both annual income taxation and
lifetime social security have received analyses
of the trade-off among redistribution, insurance,
and distortions, there has not been much work
considering the simultaneous use of both
institutions.

A. Retirement Test

For a mandate to save for later consumption
to have bite, workers can not be allowed to
claim benefits whenever they want, including
immediately. To claim Social Security retirement
benefits, a worker must be at least 62. The
system could simply start paying benefits at age
62. Instead, between age 62 and the age for full
benefits, workers can only start receiving benefits
if their current earnings are low enough,
corresponding to full or partial retirement for
many workers.47 Any delay in the start of benefits
increases their monthly amount, tending to
counterbalance the delay in the start of benefits.
The impact of this retirement test on labor market
incentives is in addition to effects discussed
above that apply to each year of labor supply.
That is, the effect of Social Security on incentives
for continued work past age 62 has two
parts. One is the effect of a delay in the start of
benefits together with their later increase as a
consequence of th


### ---Economics-2004-0-14.txt---
DIAMOND: SOCIAL SECURITY

second is the extent to which additional work,
and so additional payroll taxes, increase the
measure of lifetime earnings and so add to
benefits.

For an average worker at ages 62 and 63,
Social Security had a roughly zero marginal tax
for the average worker when the age for full
benefits was 65.49 With the increase in the age
for full benefits there will be a small tax at these
ages. While implicit taxes used to be much
larger above the age for full benefits, the retirement
test has been eliminated for those ages.
With differences in life expectancy, a zero tax
on an average worker implies that some workers
are taxed and some are subsidized by the pres-
ence of the retirement test.50
The retirement test has two effects. One is to
raise (delayed) monthly benefits for those continuing
to work. To the extent that a worker
would have consumed out of benefits received
while still working, the delay in the start of
benefits raises later consumption (for both the
worker and possibly a surviving spouse) since
more is saved. This is advantageous to the extent
that consumption falls too much after retirement.
51 The combination of a delay and
increase in benefits is also redistribution across
workers based on life expectancy along the lines
discussed above. On the other hand, benefit
ineligibility while continuing to work discourages
work for those not fully valuing their increased
later benefits and those with shorter life
expectancy. Empirical estimates find that the
overall labor supply effect is modest, suggesting
that the increase in monthly benefits effect is
more important. The retirement test also helps
49 Courtney Coile and Jonathan Gruber (2001). The mar-
ginal tax reflects the loss of a year of benefits, the increase
in benefits thereafter, the payment of the payroll tax, and the
increase in benefits from an increase in AIME, since
earnings in a late year are likely to be among the top 35.
For example, of the Health and Retirement Survey sam-
ple of those born in 1931, over one-third of men and
four-fifths of women had fewer than 35 years of positive
earnings when entering the year in which they turned 61.
Without a tax on continued work, increasing the age at
which benefits can first be claimed, without other
changes, would not save money for Social Security on a
permanent basis.

50 Of course, these earnings are also subject to the annual
income tax.

51 Also valuable is that the higher monthly benefits come
as an annuity while savings from benefits would not provide
this insurance.

with the risk in earnings trajectories that comes
from how opportunities to earn (and disutilities)
develop toward the end of a career. The retirement
test addresses that risk to the extent that
there is taxation on continued work and those
continuing to work are less needy on average
than those who stop working earlier.52 Thus I
conclude that the retirement test does distort
labor supply, but that distortion is more than
offset by the gains from improved lifetime consumption
allocations and increased insurance.
Limiting the range of ages at which the retirement
test applies makes sense. Otherwise
some of those working to very advanced ages
would have replacement rates above 100 percent
and, if liquidity-constrained, would prefer
to have part of benefits while still working.
Currently the age for the end of the retirement
test is the age for full benefits. I am not aware of
any analysis of the optimal choice of an age for
the end of the retirement test.
B. Labor Supply at Younger Ages
I have focused on the retirement decision
since elasticities here are larger than those with
earlier labor supply decisions. But younger
workers pay payroll taxes and anticipate an
increase in benefits once they retire as a consequence
of the earnings that were subject to tax.
The effect on labor supply is relevant for choosing
the size of a mandatory retirement income
system. This incentive depends on the perceived
link between taxed earnings and retirement (and
disability) benefits. While those nearing retirement
age often gather information on the workings
of the system and seek advice on the
advantages of different timing of retirement,
younger workers are not well informed.53 Some
52 1 note that in a model with homogeneous life expect-
ancy, if we have fully rational workers and if the increase in
benefits implies an implicit tax, there is an increase in
insurance insofar as early retirement is a consequence of an
adverse realization of opportunities. This is a familiar optimal
insurance result-that one taxes observable variables in
the states where they signal a lower marginal utility of
consumption.

53 Information is also supplied in annual statements
which give individual benefit levels for different retirement
ages. Someone anticipating no benefits at all might see no
link (although inconsistent anticipations about the future are
common). Presumably that would change with a reform that
was widely perceived as restoring long-run sustainability.
VOL. 94 NO. 1

13


### ---Economics-2004-0-15.txt---
THE AMERICAN ECONOMIC REVIEW

simulations have assumed that younger workers
perceive no increase in future benefits as a consequence
of additional earnings. This leads to a
big boost in apparent efficiency from a switch to
individual accounts if it is also assumed that
money going into individual accounts has no
implicit tax. Both of these assumptions seem
wrong to me.

I believe that there is wide awareness of the
existence of some link between earnings and
later benefits, although understanding of how
the link works is not so wide. Misperception of
the link sometimes takes the form of imagining
that Social Security is like a corporate defined
benefit pension that heavily weights later years.
This perception would correspond to an implicit
tax at some ages and an implicit subsidy at
others, not a full tax at all ages. The extent to
which labor supply is affected by concern that
there will be no benefits would be greatly modified
by any reform that restored actuarial balance,
not just one with individual accounts.
Insofar as workers have high subjective discount
rates, mandating savings in any form affects
labor incentives and the exact link between
taxes and benefits is of reduced consequence.
My sense of a small difference between pension
systems in incentives for younger workers is
supported by the evidence of quite modest labor
market responses in Latin American countries
that have introduced individual accounts.
I have now argued for the use of a mandatory
retirement income system paying annuitized
benefits to workers and their families based on
a progressive benefit formula and using a retirement
test at some ages but not at others. I turn
next to two issues that bear more on reform
options, as well as reflecting the history of the
system. First I will discuss the redistribution
across cohorts and then the use of automatic
indexing as well as periodic legislation.
V. Benefits by Cohort

Social Security is often criticized for distorting
labor supply and savings. Despite the linking
of these two decisions, the issues are very
different. I have already noted that mandatory
annuitization with uniform pricing distorts labor
supplies relative to an idealized alternative, but
seems to be a welfare improvement relative to a
world with no annuities. And I discussed other
labor market issues where Social Security combines
incentives with redistribution and insurance.
In contrast, the rules of Social Security do
not distort savings. That is, Social Security certainly
affects savings and so national capital.
But the term distortion is usually reserved for an
intervention that would prevent Pareto optimality
in an economy that would otherwise satisfy
the conditions of the Fundamental Welfare Theorem.
To examine this meaning of distort (as
opposed to merely change) we need to consider
the impact of Social Security on the marginal
return to private savings (the size of a tax
wedge). By itself, Social Security has no effect
on the return to marginal savings since benefit
levels do not depend on capital income. Social
Security does interact with the income tax, but
the effect of the existence of Social Security on
the income taxation of the return on marginal
savings can have either sign for differently situated
workers, although it probably includes a
wedge on average.54

A mandate to pay taxes and receive benefits
would affect private savings even if there were
no marginal distortion at all. Effects come from
the requirement that people pay taxes at levels
and times when they might not have saved the
same amount. Effects also come from redistribution,
both within and across generations, that
is, from income effects as opposed to substitution
effects. I am not aware of any study of the
impact on savings from the progressive benefit
formula-the presence of higher benefits relative
to taxes for


### ---Economics-2004-0-16.txt---
DIAMOND: SOCIAL SECURITY

Cumulative: billions of 2002 dollars
14,000

400 188, 6 19 190 0- 12,000

,- S " >^ Cumulative Sum: net

transfers to all cohorts bom in
B,/~ i'~ r t h Y and prior to each year
300 / 10,000

Net transfers to cohort

born in each year

200 / / 8,000

100 < / ' 6,000

0 4,000

-100 / " 2,000

-200 I,,,,,,,,, , I I,,, I, , ,,, ,,,,,, , ,,,,,,,,,,,,, 0,,
1876 1886 1896 1906 1916 1926 1936 1946
Birth Year

FIGURE 1. NET INTERCOHORT TRANSFERS UNDER SOCIAL SECURITY
Source: Leimer, "Cohort-Specific Measures of Lifetime Net Social Security Transfers," Social Security Administration
Office of Research and Statistics Working Paper No. 59, February 1994, updated to present value 2002 dollars.
propensity to save than high earners.55 The redistribution
across generations has received particular
attention and has led to consideration of
the impact on national capital.
A. Transfers by Cohort

Everyone is aware of the decision to pay
earlier cohorts of retirees benefits far larger than
could have been financed by the taxes they paid
and the interest that could have been earned on
them. Figure 1, an updating to 2002 dollars of
analysis done by Dean R. Leimer (1994), shows
the lifetime transfers by cohort (left scale) and
the cumulative net payments by cohort (right
scale) for cohorts born through 1949, and so
55 The difference in propensities to save is also relevant
for the impact of any use of payroll tax revenues to lower
income tax rates.

turning 55 this year.56 The aggregate net transfers
to these cohorts is roughly $11.5 trillion.
How much did this early generosity reduce
national capital? We have some estimates but
they are surely not reliable. A believable timeseries
econometric study is probably not doable
and there is no consensus that one has been
done satisfactorily. Another approach would be
by simulation. But a credible simulation requires
modeling the appropriate underlying
behavior-the extent to which different workers
would save on their own without such a program.
Surely, a simulation with all workers
being fully rational lifetime utility maximizers
has no credibility. And we would also need to
track the effects on national savings from Social
56 To extend the figure to later cohorts, we would want to
consider how actuarial balance is restored, and I do not
present such a figure. Leimer assumed phased-in tax in-
creases to balance the present value of taxes and benefits.
VOL. 94 NO. 1

15


### ---Economics-2004-0-17.txt---
THE AMERICAN ECONOMIC REVIEW

Security displacing transfers to the elderly from
the government (through the program for the
poor elderly-Old Age Assistance, which became
SSI) and from individuals (through cash
gifts and shared housing).

While the impact on national capital would
be an interesting positive question if we could
answer it well, it is important to recognize the
additional issues needed for a normative analysis.
The goal of Social Security's early generosity
was to raise the consumption of early
cohorts of elderly. Apart from business-cycle
effects, higher consumption implies lower
savings-implying that lower national capital
was required by the goal, not an unintended side
effect. A normative evaluation of the impact of
the redistribution to early cohorts would consider
how much their wages were lower than
those of later cohorts and how little they had
saved, as well as the return on capital. It
would also consider the pattern of transfers
within benefited and paying cohorts. However
such an analysis would come out-balancing
very worthwhile transfers with some less
worthwhile ones-most of the transfers are
now history.

Given the infinite horizon present value budget
constraint of Social Security (in the absence
of transfers from general revenues) this early
generosity is the cause of lower benefits in the
future than could otherwise be afforded. That is,
the legacy of the early generosity of Social
Security shows up in assets that are not there. If
they were present, they would be earning interest
that could contribute to paying for benefits.
The cumulative curve in Figure 1 gives a sense
of the magnitude of the trust fund that is not
there because of Social Security's history. But,
Figure 1 is by cohort, and so does not show how
much larger the trust fund would be today if
every cohort had been on a breakeven basis.
Although such a calculation is doable, it would
not be the best basis for insight into reform
options. Rather, that comes from considering
the elements likely to constrain reform. Past
payments are history and political considerations
suggest that it is unlikely that benefits
will be directly reduced for those already retired
or those nearing retirement, although these benefits
might be affected by changes in tax treatment
or in the inflation indexing of benefits,
changes that would apply to everyone. A partial
picture of that constraint would be that cohorts
over 55 would not be affected by reform.57 The
measure is not exact since cohorts over 55
would be affected by any payroll tax change and
slightly younger cohorts are likely to have limited
changes in benefits as we phase in any
benefit reductions that are part of a reform. An
ideal definition of this constraint would conform
to a theory of political constraints on reform
coming from past generosity. We do not
have a full theory, but this gives a reasonable
sense of the size of the legacy that needs to be
financed from future cohorts.

Peter Orszag and I have referred to the missing
assets on this cohort basis as a legacy debt.58
Thus the legacy debt is not a debt in the traditional
sense of that word, but that term crystallizes
the need to allocate the cost of the assets
that are not there across cohorts. Spreading the
cost of that early generosity across cohorts is
inherent in any plan that restores actuarial balance.
While only an approximation to the real
constraint, the number is roughly $11.5 trillion
(a bit more than one year's GDP). If we were to
go to full funding, then this is roughly the cost
that would fall on the generations during the
buildup to full funding. Alternatively, instead of
ever achieving full funding, we can consider a
wider allocation of the legacy cost by aiming to
preserve a ratio of the legacy cost to taxable
payroll. This would parallel the idea of preserving
the ratio of the public debt (or the interest on
the public debt) to GDP. Spreading the legacy
cost over all future cohorts implies less than full
funding of Social Security. Without extensive
evaluation of its consumption transfers, the effect
of Social Security on national capital is not,
by itself, a basis for concluding that the system
should have been fully funded or should become
fully funded.

The baby boomers are much larger than earlier
cohorts. The 1983 legislation included payroll
tax revenues in excess of current outlays in
order to build a trust fund which would then be
used to finance the retirement of this very large
cohort. That is, tax


### ---Economics-2004-0-18.txt---
DIAMOND: SOCIAL SECURITY

them to be lower later.59 Politically, the trust
fund is very likely to be used for Social Security
purposes in the sense that the constraint on
future Social Security expenditures includes the
value of the assets in the trust fund. A separate
issue is the extent to which the higher payroll
taxes since 1983 increased national capital.60
This is a source of controversy, with a wide
range of presumptions and no ability to settle
the question econometrically.61 I believe that a
large part was saved-despite the large federal
deficit outside Social Security for the 1980's
and early 1990's. In my view, a larger unified
deficit, if Social Security had not been in surplus,
would not have had a strong effect on tax
and spending legislation. Congress had great
difficulty in legislating tax and spending
changes to lower the deficit. Without the Social
Security surplus, a somewhat larger unified deficit
would not have changed the basic character
of the situation-a deficit widely perceived as
being too large and a difficulty in raising taxes
and lowering spending. Looking beyond the
baby boomers we do not currently perceive a
need to single out a cohort that will differ
greatly from others and perhaps call for something
other than a smooth adjustment of taxes
and benefits.

Redistribution across cohorts has not been
done in a lump-sum fashion, but through the
choice of tax rates and benefit formulas. Thus
the redistribution has affected labor supplies as
well as savings decisions. In the early days, the
generous benefit formulas (in effect or antici-
59 That has shown up in the assets in the trust fund-
currently over $1.5 trillion or roughly 2.8 times annual
expenditures. This ratio of trust fund to expenditures is
projected to peak at 4.7 in 2016.
60 Note that the analysis needs to be done in terms of
taxes and benefits, the causes of changes in the trust fund.
The question of the impact of the trust fund on national
capital requires a distinction among different ways in which
trust fund size can be changed.
61 There has been a relatively short time during which
there is a plausible linkage between Social Security and the
rest of the budget. Moreover, specific pieces of legislation
imply different time shapes of revenue changes and spend-
ing changes over subsequent years. Thus, there is no simple
link between deficits and lagged deficits (or between unified
deficits and Social Security surpluses) that could reliably be
discovered by time-series analysis. In particular, it is not
credible to believe that econometric analysis could uncover
the counterfactual pattern of taxes and spending that would
have occurred if the 1983 Social Security legislation had
involved lower tax rates.

pated in the future) subsidized labor, just as the
lower benefits relative to taxes for younger
workers today taxes labor.62 This is similar to
the role of the progressive benefit formula discussed
above. Given the pattern of redistribution
by cohort shown in Figure 1, much of
redistribution served as an incentive for much of
the working life of recipients. This is in contrast
with analysis in two-period models where the
initial elderly recipients of transfers receive a
lump-sum transfer and all later cohorts have
implicit taxes on earnings to pay for it. Both the
transfers and the taxes have influenced labor
supply.

VI. Automatic and Legislated Adjustments to
Aggregate Realizations and Risk Allocation
The actuarial projection for the 1983 reform
envisioned a buildup of the trust fund, followed
by its decline back to the precautionary level of
one year's expenditures at the end of the 75-
year projection period. It has not worked out
that way. Instead of having just enough money
for 75 years of expenditures, plus a small trust
fund at the end, it is now projected that the trust
fund buildup will be sufficient to pay currently
scheduled benefits only until 2042. That is, the
policy that was designed for a 75-year horizon
will, if the projection is correct, cover all of
expenditures for only 60 years. By the scale of
preparing for long-term outcomes, that does not
seem to me to be too bad. Of course one could
argue, with hindsight, that Congress should
have looked further into the future than 75
years, although it was hard enough to reach
agreement on legislation even with that target.
Current discussions have extended the notion
of actuarial balance to include "sustainability"-
that the ratio of the trust fund to annual expenditures
not decline at the end of the horizon.
This criterion of sustainable solvency is meant
to avoid a repeat of the post-1983 experience
where the projected actuarial deficit returned
quickly (although the trust fund exhaustion date
was distant). Projected deficits returned quickly
because of what is called the terminal year
problem, or the cliff problem. That is, each year,
62 The effects of benefit increases of the already retired
(through 1972) affect labor supply insofar as they were
anticipated earlier. A similar issue arises for benefit in-
creases at different times during a career.
VOL. 94 NO. 1

17


### ---Economics-2004-0-19.txt---
THE AMERICAN ECONOMIC REVIEW

the realized net cash balance of Social Security
is added to the trust fund and another year is
added at the end of the 75-year horizon. With a
constant tax rate and the current benefit formula,
the added year is in worse fiscal shape
than the average of years before. Indeed of the
current 75-year actuarial deficit of 1.9 percent
of taxable payroll, a full 1.1 percent is due to the
fact that the projection now goes 20 years fur-
ther into the future than it did in 1983.
The 1983 legislation included future decreases
in benefits by increasing the age for full
benefits. At the time of the 1983 legislation,
there was still a tax rate increase on the books.
That was kept and took effect in 1990. Indeed
from the initiation of the program in 1935 until
1990, there was always a future tax rate increase
on the books. Given the political ease of raising
benefits or cutting taxes, and the political difficulty
of raising taxes or cutting benefits, having
future tax rate increases and future benefit decreases
on the books lowers the political cost of
preserving balance, since it is easier to legislate
future pain than current pain. Avoiding a recurrence
of actuarial imbalance a short time after
reform requires a substantial trust fund at the
end of the projection period, so that it can fall
for awhile without triggering imbalance,
and/or a change in the time shape of taxes and
benefits. A changed time shape can be legislated
directly (as we legislated an increase in
the full benefit age in 1983 and have legislated
future tax increases) or could be expected
from the adoption of further automatic
adjustments (for example, by including an
adjustment for life expectancy).
Before considering the choice between legislated
changes and automatic adjustment, let us
consider the allocation in a complete-market
Arrow-Debreu equilibrium. In the model, outcomes
are fully specified. Given subjective beliefs
about the probability structure of the states
of nature, one can express the value of equilibrium
for an individual. Also fully specified is
standard modeling of incomplete markets,
which replaces complete market auctioneerannounced
future allocations by accurate predictions
of future market equilibria as repeated
trading unfolds. Time-inconsistent individual
behavior does not interfere with the ability to
describe outcomes in this way, although it will
generally interfere with the efficiency properties
of equilibrium.

Most social security systems lack the completeness
that is needed to specify outcomes
solely in terms of underlying economic variables
(and the stochastic structure of states of
nature). U.S. legislation determines the payroll
tax rate for each year into the indefinite future.
The level of earnings that are subject to tax each
year is automatically indexed-thereby relating
taxable earnings to economic outcomes.63 Legislation
also sets down the rules for benefit
payments in terms of individual earnings histories
and price and wage indices. While each part
is fully specified, no mechanism ensures that the
Social Security budget constraint is satisfied.
Thus, there is the expectation that sooner or
later something will have to be changed. That is,
in order to model future labor and consumption,
we need to model future legislative outcomes.
This is hard.64 In some exercises, the Congressional
Budget Office has been instructed by law
to ignore some possible future legislation (such
as extensions of sunsets of income tax changes).
But this is not a satisfactory solution for academic
analysts, nor for individuals who are
making lifetime plans.

We have a theory using incomplete contracts
as part of the theory of the firm. In that theory
agents have well-defined property rights and
well-specified behaviors that determine the outcomes
not covered by the contracts, With incomplete
legislation, the future legislative
process plays a key role in determining outcomes
that are incompletely specified.6 Analyzing
an equilibrium that includes a legislative
process is difficult-requiring modeling the interaction
of the personal preferences of elected
officials with their concerns about reelection, as
well as election outcomes (R. Douglas Arnold,
1990). It is not that this is unknowable in principle,
but that w


### ---Economics-2004-0-20.txt---
DIAMOND: SOCIAL SECURITY

inely usable, empirically validated theory and
we are studying a process that generates very
limited data relating outcomes to underlying
factors.66

Incomplete specification is not a necessary
part of a mandatory social security system. For
example, in Chile workers are required to save
10 percent of covered earnings in mutual funds,
using the accumulation for an annuity purchase
or phased withdrawals after reaching benefit
eligibility. Thus the workings of the system are
fully specified in terms of economic outcomes.
7 This does not imply that the Chilean
government will never change the rules of the
system. Indeed, it has made frequent changes in
some details. But it does mean that we can
analyze the outcomes of the current system under
the assumption of no further legislation
without being internally inconsistent. We cannot
do that for the United States-there are
states of nature that require some legislative
change, indeed the probability of such a future
need is very high today.

The Chilean approach of a fully funded defined
contribution system is not the only way to
have a fully specified system. Sweden has one
too. In Sweden, the payroll tax rate is 18.5
percent. While 2.5 percent of payroll goes into
fully funded individual accounts, 16 percent is
used for a partially funded system, called a
notional defined contribution system (NDC).
An NDC system mimics a fully funded defined
contribution system in that it accumulates
a notional balance for each worker that
increases each year by taxes paid and a notional
interest rate.68 At retirement, this balance
is converted into an annuity based on the
66 For example, if we want to project a legislative re-
sponse to a possible drop in fertility, we do not have much
of a database for evaluating the relationship. With underly-
ing country differences being very significant in social
security politics, we may have basically one data point.
67 When workers purchase real annuities from insur-
ance companies, there is always the possibility that the
insurance companies will become unable to pay the con-
tracted amounts. But even with recognition of this
possibility, we still have a fully specified outcome-as
we do in models with incomplete markets and bankruptcy
rules.

68 That is, unlike the United States where benefits depend
on earnings subject to payroll tax, in Sweden benefits
depend on taxes paid. Since the Swedes currently seem
determined not to change the tax rates this difference is
likely to have little consequence for the future.
life expectancy of that cohort and the same
notional interest rate.69 The notional interest
rate is set administratively (with automatic
adjustment), not by returns realized on assets
held. In this way an NDC system mimics a
defined benefit system. Thus it is very much a
hybrid. Whether this system is referred to as a
defined benefit system or an unfunded defined
contribution system matters since the vocabulary
with which a system is described can
influence the politics of both creation and
adaptation.

By itself a well-structured NDC system, with
a decent size buffer stock of assets, will have
little probability of needing legislative intervention
as long as economic growth is large
enough. Even so, the Swedes have gone further
by introducing an automatic balancing system. I
will not digress to describe the Swedish automatic
balance rules. It suffices to say that if
economic growth is sufficiently slow, the notional
interest rate is automatically loweredreducing
both benefits in payment and future
benefits in response to this slower rate of
growth. Thus the Swedish system can be analyzed
for an indefinite future without an assumption
about the structure of future

legislation, so one does not need a fully funded
system to have that property. Sweden, like
Chile, puts all of the risk of future outcomes on
the side of benefits and none on the side of
taxes.70

Some simple ways for pretty much ensuring
automatic balance can illustrate alternative approaches.
In the French and German pension
systems, workers accumulate "points" based on
earnings that have been subject to tax. Think of
this as a sum of wage-indexed wages over a
worker's career. The accumulations of points
determine relative pensions for retirees. Unlike
what is actually done in France and Germany,
points could be converted into cash
benefits by automatically adjusting the value
of a point to exhaust available revenues. Conversely,
we could think of adjusting the tax
69 Also automatically adjusted is the relationship be-
tween the level of benefits and the age at which an individ-
ual starts them. There is not automatic adjustment for the
earliest age at which retirement benefits can be claimed.
70 To some, this is the heart of a defined contribution
system, rather than a relationship involving realized rates of
return on assets actually owned.
VOL. 94 NO. 1

19


### ---Economics-2004-0-21.txt---
THE AMERICAN ECONOMIC REVIEW

rate each year to produce enough revenues to
cover expenditures for given values of
points.71 Both types of adjustment need a
small buffer stock of assets (or borrowing
ability) because of lags between setting benefit
or tax rules and the determination of actual
expenditures and revenues.72

U.S. Social Security uses price and wage
indexing in the determination of both benefits
and the payroll tax base.73 This reliance on
automatic adjustments decreases the frequency
of the need for legislation.74 One popular proposal
is to extend automatic adjustments to include
an adjustment for life expectancy. Such a
change would play two roles-one is to have an
automatic adjustment rather than legislating in
anticipation of or in response to life expectancy
changes. The other is to decrease the actuarial
imbalance in a way that may be easier politically
than comparable direct changes.
But what mix of benefit and revenue changes
is the best response to

nce the sizes of the groups

Part of an approach to this question is to ask
how individual lifetime plans should vary with
life expectancy. This depends on how the potential
earnings trajectory and the difficulty
(disutility) of work change along with life expectancy.
If both opportunities and difficulties
in a year depended on the proportional position
of that year relative to life expectancy (and
mortality rates also depended on relative age),
then all of an optimal individual adjustment
would come in working longer. That is, optimal
work would be a fixed fraction of life expectancy.
The change in Social Security with the
same pattern has all of the adjustment in lower
benefits for each age of retirement.
However, I suspect that the proportional case
assumes too large a change in both earnings
opportunities and difficulty in work relative to
life expectancy. If the optimal outcome for an
individual were to work a smaller percentage of
life expectancy, then a sensible approach would
spread the implied drop in lifetime consumption
over both pre- and postretirement years. Decreased
preretirement consumption corresponds
to an increase in the Social Security tax rate. In
historic data, where the steady growth in life
expectancy has been accompanied by a steady
growth in real earnings, we have a steady decline
in the percentage of life expectancy
worked. This suggests a mix of tax and benefit
changes since we do expect a continued correlation
between life expectancies and earnings
levels.76 I also believe that, at least among academic
economists, the life cycle of productivity
relates to more than just health and it is
unclear how such other factors are correlated
with life expectancy. I think an automatic adjustment
for life expectancy that included adjustment
in both benefits and tax rates would be
a good idea.

Should we ha


### ---Economics-2004-0-22.txt---
DIAMOND: SOCIAL SECURITY

and so even less pressure for legislation? For
example, we could use additional adjustments
depending on real wage growth. Or we could go
directly to automatic adjustments based on
overall financial balance so Congress never
again needed to legislate.77 Such indexing
would need to choose the mix of revenue and
benefit changes in the response to imbalance. It
strikes me as implausible that a system with a
sensible tax rate would want to do all of the
adjustment on the side of benefits.78 That is, it
seems likely that the optimal size of a social
security system relative to the economy varies
with the same factors that affect actuarial
balance.

Relying on fully automatic adjustment
rather than assuming there will be periodic
new legislation bears some similarity to a
familiar distinction from macroeconomicsrules
vs. discretion for monetary policy. Parallel
issues include the concern about setting rules
without fully knowing how the economy adjusts
to the policy actions and recognition that the
economy may evolve so that currently good
rules may become less so in the future. But
there are also different issues. Social Security
set up for the indefinite future involves a level
of detail complexity that seems higher than setting
rules for the Fed. Moreover, Congress
could invite the Fed to set out a rule it will then
follow. Thus we need to ask whether Congress
would do a better job in setting out rules once
and for all rather than adjusting them from time
to time. While legislating from time to time is
an inherently easier intellectual problem, we
need to be concerned about the asymmetries in
the political ease of legislation addressing surpluses
and deficits, an asymmetry that is reduced
by legislating automatic adjustments.
Also there may be more similarity across the
77 With this approach, Social Security would become
fully specified and so easier to analyze and more in keeping
with Arrow-Debreu thinking. But, making it easier to ana-
lyze does not necessarily make it better. Just as mathemat-
ical convenience, for example from additive preferences,
while convenient for theoretical analysis, does not neces-
sarily add to empirical reliability.
78 If the tax rate is thought to be too high and politically
can not be lowered, then doing the adaptation to anticipated
higher cost outcomes fully in terms of lower benefits may
make sense as a political fallback. I, for one, do not think the
current tax rate and replacement rates in the United States
are too high.

political spectrum in normative evaluations of
the impacts of monetary policy than of the evaluations
of the sizes of taxes and benefits for
different workers and family structures. As with
monetary policy, I think that some discretion
can improve outcomes.

In considering possible automatic adjustments,
one can look at how adjustment is currently
debated and how it was done before. In
our last major reform in 1983, there was an
explicit sense of balancing benefit and revenue
changes (Paul C. Light, 1985). Currently, we
view both benefit reductions and tax revenue
increases as candidates to contribute to restoring
a projected position of financial balance. The
Commission appointed by President Bush put
forth two plans which restored actuarial balance
(Commission to Strengthen Social Security,
2002). One of them included new dedicated
revenues, and both of them included large transfers
of general revenues, which one cannot help
but think of as in large part coming from new
revenues and not just spending decreases and
certainly not benefit decreases. The plan that
Orszag and I have put forth explicitly divides
some of the proposed changes for restoring actuarial
balance (both one-time changes and new
automatic changes) between revenue changes
and benefit changes.

A. Fully Funded Defined Contribution and
Partially Funded Defined Benefit
The parallel to the workings of the ArrowDebreu
model and the completeness of the specification
makes economists more comfortable
thinking in terms of mandated fully funded defined
contribution systems than the type of partially
funded defined benefit system we have.
So, I want to draw out some comparisons. One
is that portfolio risk in a mandatory fully funded
defined contribution system is highly correlated
with the portfolio risk of the rest of individual
retirement savings. Thus the increased use of
defined contribution private pensions raises the
value of a defined benefit Social Security system
relative to individual accounts.79 In contrast, a
79 Social Security needs to be considered in the context
of all retirement income provision, not just by itself, recog-
nizing the great diversity in the extent to which people have
private sources of retirement income. The Social Security
reform debate has recognized non-Social Security retirement
VOL. 94 NO. 1

21


### ---Economics-2004-0-23.txt---
THE AMERICAN ECONOMIC REVIEW

system that is less than fully funded will have less
correlation with the returns on private retirement
savings, as it adjusts benefits in response
to the growth of tax revenues as well as the
returns on whatever assets are held.80 This comparison
holds even with initial benefits fully
automatically adjusted for the actuarial positionreturns
on assets and the growth of taxable earnings
are only partially correlated. Thus portfolio
diversification considerations suggest an advantage
to having (at least) some underfunding in
Social Security to complement private savings.
This diversification advantage comes with
the redistribution to earlier cohorts that is inherent
in a less than fully funded system.81 Thus
one can readily argue for a Pareto gain (ex ante)
from moving from a fully funded system to a
partially funded system with the risk associated
with the incomplete funding falling on benefits.
82 Note that the reverse argument does not
work-just adding funding to an unfunded system
that provides all of retirement benefits will
not generate a Pareto improvement. The gain
from diversification plays out over time, while
the redistribution required to build up funding
hurts the oldest cohorts who provide the funding
and are affected by the diversification argument
very little or not at all

might find a complex way to

argument by itself does not lead to the possibility
of a Pareto gain from adding funding to an
unfunded system, just from reducing funding of
a fully funded defined contribution system.
The comparison above assumed all of the
response in the unfunded system occurred in
benefits. By having some of the response to
aggregate shocks fall on taxes, a less than fully
funded system is capable of doing additional
risk sharing across generations that does not
occur with a fully funded defined contribution
system (Douglas Gale, 1990). Of course how
good a job Congress does in adapting such a
system (whether done automatically or by repeated
legislation) is a further issue that must be
recognized. Thus, the current system provides
insurance for individual earnings risk through
the progressive benefit formula and the retirement
test and provides insurance for aggregate
earnings risk through the defined benefit structure
with less than full funding.

My broad conclusion here is that the absence
of a complete specification of Social Security is
not by itself an argument that there is anything
wrong with our current approach.
VII. Concluding Remarks

Occasionally, I run into people who believe
that no one in his right mind would design a
retirement income system like the one we have.
Some of the details do seem far from satisfactory
to me. However, looking at the big picture,
this structure makes sense. Mandated savings
makes sense if you think that many workers
would not provide themselves a reasonable replacement
rate. This is not just an issue of
avoiding poverty, but one that extends quite far
up the income distribution. Mandating annuitization
makes sense if you think that workers do
not adequately understand the value of annuities.
Protection of spouses and children makes
sense if you think that many workers would not
do that adequately. Relating benefits to a measure
of lifetime earnings surely makes sense. A
progressive benefit formula makes sense to provide
higher replacement rates for lower earners,
in order to supp


### ---Economics-2004-0-24.txt---

 ## Economics-2005-0


### ---Economics-2005-0-01.txt---
Social insurance is a subject I have been
studying for nearly 40 years. The intellectual
and policy revolution in social insurance that is
occurring around the world is among the most
significant and positive developments of current
economics.1

Social insurance programs have become the
most important, the most expensive, and often
the most controversial aspect of government
domestic policy, not only in the United States
but also in many other countries, including developing
and industrialized nations. In the
United States, these programs include Social
Security retirement, disability, and survivor insurance,
unemployment insurance, and Medicare
insurance for those age 65 and older.
Together these programs accounted for 37
percent of federal government spending and
more than 7 percent of GDP in 2003. These
ratios have increased rapidly in the past and are
projected to increase even faster in the future
because of the more rapid aging of the
population.

I will discuss how the major forms of social
insurance could be improved by shifting to a
system that combines government insurance with
individual investment-based accounts: unemployment
insurance savings accounts (UISAs)
backed up by a government line of credit, per-
sonal retirement accounts (PRAs) that supplement
ordinary pay-as-you-go Social Security
benefits, and personal retirement health accounts
(PRHAs) that finance a range of Medicare
choices. I think that such reforms would
raise economic well-being and are also appealing
on broader philosophical grounds.
Several nations are now doing this for their
retirement programs, including such diverse
countries as Australia and Mexico, England and
China, Chile and Sweden (Feldstein, 1998a;
Feldstein and Horst Siebert, 2002). The focus
by governments around the world on social
insurance pension reform is driven in part by the
realization that the aging of their populations
implies that the tax rates required to fund social
insurance pension benefits will rise rapidly if
the programs are not changed.

The impetus for broader social insurance reform
comes from the recognition that existing
programs have substantial undesirable effects on
incentives and therefore on economic performance.
Unemployment insurance (UI) programs
raise unemployment. Retirement pensions induce
earlier retirement and depress saving. And health
insurance programs increase medical costs. Governments
are driven by a desire to reduce the
economic waste and poor macroeconomic performance
that these disincentives create and to avoid
the resulting tax consequences, as well as the
increased tax cost, of the aging population.
Economic research has helped policy officials
to recognize these undesirable effects and
to redesign social insurance programs. The pace
of reform and the nature of the program changes
differ from country to country, reflecting initial
conditions and local political realities. Reforms
are inevitably only partial and part of an ongoing
process. But the reforms generally make the
programs more economically efficient, providing
more protection relative to the financial
costs and the economic distortions. I will examine
some of the favorable changes that have
already occurred in U.S. unemployment, retirement,
and health care programs.

Before looking at these specific types of social
insurance, I want to discuss three general


### ---Economics-2005-0-03.txt---
questions. Just what is social insurance? Why
do, or should, we have such programs? And
what are the principles by which such programs
should be evaluated and redesigned?
I. Social Insurance and Welfare Programs
The word "insurance" is used to describe
these transfer programs because they deal with
risks: the risk of job loss, of health care expenses,
and of inadequate assets during retirement.
But social insurance is very different
from private insurance. The key distinction is
that participation in social insurance programs
is mandatory or is induced by substantial fiscal
subsidies.

Social insurance programs are also very different
from welfare programs. Welfare benefits
are means tested, i.e., they are paid only to those
with incomes (and assets) below some level. In
the United States, these means-tested programs
include Medicaid, food stamps, subsidized
housing, school lunches, and others.2 In contrast,
social insurance programs are "event conditioned.
" Benefits are paid when some event
occurs in an individual's life regardless of the
individual's income or assets. Unemployment
benefits are paid to those who lose their jobs and
Medicare benefits to those who are ill and over
65. Social Security benefits are available to
those over age 62, disability benefits to those
unable to work, and survivor benefits to the
widows and children of deceased workers.
Unlike welfare programs, social insurance
programs are not designed to be vehicles for
income redistribution. Although some fraction
of social insurance outlays is paid to those with
low incomes, most of the benefits go to middleand
higher-income households. This is particularly
true in the United States, where cash benefits
to retirees and the unemployed are positively
related to previous earnings and where health care
is provided by private hospitals and physicians,
even when financed by social insurance.
Social insurance may appear to be redistributing
income to the poor because benefits are
paid to those who are temporarily poor due to
the event that triggered the payment of benefits.
This ignores the permanent or lifetime income
of these recipients. It also ignores the effect of
the social insurance on the incentive to accumulate
funds for these rainy days. Social Security
benefits that replace 50 percent or more of
after-tax pre-retirement income reduce significantly
the incentive to save for old age and
therefore depress income in retirement. Unemployment
benefits with high replacement rates
have a similar effect on saving to finance spells
of unemployment.

The lack of redistribution is well illustrated
by the Social Security retirement program. Last
year, a new retiree who had annual earnings at
or above the Social Security program maximum
taxable amount ($87,900 in 2004) for at least 35
years received a benefit of $21,900. In contrast,
someone with lifetime earnings in the middle of
the earnings distribution received only about
two-thirds as much in retirement benefits. And
someone with low earnings (i.e., 45 percent of
the average wage) received benefits of less than
$9,000 a year. This lack of redistribution is
compounded by the rules governing benefits to
spouses and widows. A retiree who previously
had maximum taxable income and who retired
with a dependent spouse received more than
$32,000 a year from Social Security, while the
widow of a low-income earner would receive
less than $9,000 a year.

The Social Security program appears to be
redistributive because everyone pays the same
tax rate, while the ratio of benefits to lifetime
earnings is designed to fall as those earnings
rise. In practice, however, this apparent redistribution
is offset by the longer expected life of
higher-income individuals, their increased use
of spousal benefits, and the later age at which
they begin to work and to pay taxes. Research
by Jeffrey Liebman (2002), based on a large
sample of actual individual earnings histories,
showed that less than 10 percent of Social Security
benefits represented net redistribution
across income groups within the same birth
cohort. Julia Lynn Coronado et al. (2000)
showed that the combination of taxes and benefits
for the Social Security program leaves the
lifetime Gini coefficient of the population's income
essentially unchanged. In addition, the
general equilibrium effects of Social Security
tilt the pretax distribution of income toward
higher income individuals by reducing capital
accumulation, which in turn lowers real wages
and raises the return to capital.


### ---Economics-2005-0-04.txt---
Unemployment insurance also does not redistribute
to the poor. In Massachusetts, a state considered
to have a very generous UI program, the
UI benefits were financed in 2003 by a payroll tax
on only the first $10,800 of earnings (with a zero
marginal tax rate above that level), while basic
benefits were 50 percent of previous wages up to
more than $50,000 of wages per year.
An individual who earns $50,000 a year pays
the same tax as someone who earns $11,000 a
year but would receive benefits that are nearly
five times as high.3 Taken by itself this would
mean a substantial redistribution from lowwage
earners to higher-wage earners. Moreover,
since benefits are paid only to individuals who
have earned some minimum amount during the
past year, those with long spells of unemployment
may not be eligible for any benefits at all.
Although the same Medicare rules apply to
everyone over age 65, higher-income seniors
often get substantially more benefits than those
with lower incomes. Mark McClellan and
Jonathan Skinner (1997) concluded that Medicare
produced net transfers from the poor to the
wealthy as a result of the higher annual expenditures
and longer survival times of wealthier
Medicare beneficiaries. In the same spirit, Skinner
and Weiping Zhou (2004) found greater use
of mammography screening, diabetic eye exams,
and other indicators of good care among
high-income Medicare groups than among
those with lower incomes.

The very high level of spending on the middle
class social insurance programs hurts the
low-income population in another way: by putting
a drain on the government budget in a way
that reduces the funds available for helping the
poor. Social insurance programs cost $800 billion
in 2003, while federal spending on all
means-tested programs, except Medicaid, was
less than $150 billion.4 Over the past four decades,
the spending on means-tested programs
(except Medicaid) has remained relatively constant
(rising from 1.0 percent of GDP to 1.3
percent of GDP) while the social insurance pro-
grams that are not means tested rose from 2.7
percent of GDP to 7.4 percent of GDP.
The negative effect of social insurance spending
on means-tested programs is not only an
observed fact but is also what optimal tax theory
implies. The deadweight burden of an extra
dollar of taxes increases with the share of income
taken in taxes. The high level of taxes that is
needed to finance middle-class social insurance
programs therefore increases the deadweight burden
of any incremental taxes that would be used to
finance means-tested poverty programs. The large
social insurance programs thus reduce the optimal
size of means-tested poverty programs.
II. Why Social Insurance?

Some writers see social insurance in broad
philosophical terms, reflecting their specific
views of the appropriate role of government in
society. One such view, more common in Europe
than in the United States, is that social
insurance should be judged by its contribution
to social solidarity, i.e., to the sense that all of
the individuals in the nation are, in effect,
viewed as a single family and treated equally.
This leads to the principle of uniform health care
for all, although this is more often an asserted
political goal than a practical reality. Similarly,
they may reject any role for company-based
private pensions in order that all workers participate
in a common pay-as-you-go state plan.
The opposite philosophical view is that the
provision of health care or retirement income is
not a legitimate role for government because it
forces individuals to participate in a common
program rather than taking personal responsibility
and making decisions that reflect their
own preferences. Milton Friedman's Capitalism
and Freedom (1962) is the classic statement of
this view that social insurance programs are
inappropriate because they infringe upon individual
liberty.

The social solidarity view is often combined
with the statement that individuals are incapable
of making the complex decisions required to
plan for retirement income or to choose health
insurance or health care. The opposite view
emphasizes that individuals differ in their tastes
and are better able than governments to judge
what is in their own best interest.
I believe in the diversity of individual preferences
and the ability of most individuals to act
3 The unemployment tax is technically levied on the
employer but the incidence is likely to be primarily on the
employee.

4 Although Medicaid is a means-tested program, more
than half of its outlays are for nursing home care for the
very aged rather than care for those with low lifetime
incomes.


### ---Economics-2005-0-05.txt---
in their own self interest. But I also believe that
there is a role for government that justifies the
provision of social insurance benefits. I come to
this conclusion on utilitarian grounds rather
than from any philosophical commitment to social
solidarity.

There are two distinct reasons for providing
social insurance. Both reflect the asymmetry of
information. The first is that asymmetric information
weakens the functioning of private insurance
markets. The second is the inability of
the government to distinguish between those
who are poor in old age or when unemployed
because of bad luck or an irrational lack of
foresight from those who are intentionally
"gaming" the system by not saving in order to
receive transfer payments. Both problems show
that the case for social insurance cannot be rejected
simply by arguing that such programs force
people to act against their own best interests.
But these problems of asymmetric information
or any other market failures do not necessarily
justify government action. While a
perfect and benevolent government would be
better than a private market burdened by market
imperfections, actual governments are neither
perfect nor necessarily benevolent. Political actors
do not maximize a social welfare function,
but reflect political pressures and bureaucratic
preferences. Moreover, social insurance programs
impose costs that must be weighed
against the benefits of overcoming market imperfections.
Both require empirical evaluation.
Consider first the asymmetry of information
in insurance markets. To be specific, consider
the case of private annuities. If individuals can
buy annuities on actuarially fair terms they may
increase their expected utility by annuitizing
their assets at retirement. But if individuals differ
in their life expectancy and know more
about their mortality prospects than the insurance
company can learn, those with shorter life
expectancy will want to annuitize a smaller
portion of their wealth. Insurance companies
will recognize the resulting self-selection and
offer annuities with premiums that reflect the
mortality rates of the long-lived individuals
who are their most likely customers. This produces
a downward spiral in the demand for
annuities that is limited only when, at some
point, the risk-reducing value of annuitizing
outweighs the less than actuarially fair pricing
of individual annuities.

A mandatory social insurance program like traditional
Social Security circumvents this asymmetry
of information by providing everyone with a
retirement annuity rather than a lump sum at retirement
age. But whether this is better than an
imperfect private annuity market, in which some
annuitize little and others not at all, depends on the
implicit rates of return available on the social
insurance annuity, on the private annuity, and on
non-annuitized saving. It also depends on the degree
of diversity in preferred spending patterns in
retirement and in attitudes about bequests, since
complete annuitization at retirement would not
permit the purchase of retirement homes or other
major consumer outlays or the making of bequests
or inter vivos gifts.

The problem of information asymmetry in
private annuities could be reduced if individuals
purchased annuities at relatively young ages,
before they could accumulate much information
about their own likely mortality risks in old age.
Alternatively, a mandatory annuity could be
more attractive if it were based on the higher
return available in an investment-based program
rather than in a mature pure tax-financed
pay-as-you-go program.

Two conclusions follow from this. First, the
existence of asymmetric information may justify
a social insurance program (a government
annuity in this case) but does not necessarily do
so. The case for a mandatory annuity program
depends on calculations that could be done but
that have not yet been done. Second, the appropriateness
of a social insurance program and its
optimal size can be increased if the cost of the
social insurance option is reduced, something
that depends on how it is financed.
Consider now the second form of asymmetric
information that might provide a rationale for a
social insurance program: the government's inability
to distinguish those who are poor
through bad luck or inadvertence from those
who deliberately choose to act in a way that
leads to eligibility for free benefits. A primary
reason for social insurance programs is that
some individuals would not act in their own
interest, saving far too little for their retirement,
for health care after they are no longer working,
or to finance consumption when they are unemployed.
Although some economists may reject
the likelihood of such irrational behavior as a
basis for policy analysis, as individuals we all
recognize that such irrationality exists in practice.


### ---Economics-2005-0-06.txt---
Recent work on behavioral economics has
helped to make the possibility of such irrationality
or myopic behavior a part of mainstream
economics.

But such departures from rational saving by a
fraction of the population need not justify the
general provision of social insurance benefits.
Why not simply provide means-tested benefits
instead of the universal provision of social insurance
benefits? The primary reason for not
doing so is that some rational and farsighted
individuals would be induced by a means-tested
system to act in a way that allows them to
qualify for benefits. Doing so would impose tax
costs on the rest of the population that could
make overall well-being lower than in a universal
(i.e., not means-tested) program.
Consider a simple example of a means-tested
retirement program (Feldstein, 1987). Assume
that some fraction of the population is myopic
and would not save anything for retirement. A
means-tested program would provide a benefit
for all such myopic retirees. How would rational
working-age individuals respond to such a
system? They would have the choice of either
saving for their own retirement or consuming all
of their income before they retire and receiving
the means-tested benefit. The potential meanstested
benefit acts as a kind of tax on saving,
reducing the incremental retirement consumption
that saving would produce. A rational individual
would decide whether to act as if he or
she is myopic by comparing the lifetime utilities
with optimal positive saving and with no saving.
Those with relatively high incomes would
not be tempted by the means-tested benefit. But
others with lower incomes would have higher
lifetime utility by increasing their consumption
during working years even if the means-tested
benefits would only provide lower consumption
during retirement than optimal saving would
allow. Although they would achieve higher lifetime
utility through their action, their benefits
would be financed by tax-financed transfers,
which would make others worse off.
There is no way for the government to distinguish
between the genuinely myopic and
those who are rational utility maximizers gaming
the system. The government could, in principle,
set the means-tested benefit so low that
very few rational individuals would be tempted.
My judgment is that our relatively affluent society
would not accept that policy. The means-
tested benefits would be set at a higher level that
would tempt many rational individuals to save
nothing.

A policy of forcing everyone to save for his
or her own retirement would eliminate the problem
of those who game the system. The only
adverse effect of such a policy is that some
individuals might be required to shift more of
their lifetime consumption to their retirement
years than they would prefer. For them, part of
the mandatory saving would be a tax to the
extent that they valued the saving less than
current consumption.

The choice between such a mandatory saving
plan-essentially a kind of investment-based
Social Security pension-and a means-tested
benefit should depend on numbers. How many
people would receive means-tested benefits?
How much deadweight loss would be involved
in financing those benefits? How many people
in a mandatory saving plan would have to provide
more for their retirement than they want?
In the absence of such a mandatory investment-
based option, the policy choice is between
a means-tested program and a universal pay-asyou-
go retirement benefit. Such a pay-asyou-
go plan forces all individuals (after the
initial generation) to receive a lower rate of
return than they could obtain on private saving.
As such, it also imposes a tax on labor income
since each extra dollar of earnings would induce
an additional pay-as-you-go tax liability. The
reduction in saving that is induced by the payas-
you-go system also causes a fall in capital
income and therefore in corporate and personal
tax revenue that requires higher marginal tax
rates to recover the lost revenue.
Both of these examples of asymmetric information
show that a social insurance program may be
an appropriate response to a market failure but that
it need not be. Even when there is a market failure,
it may be better to do nothing or to have a meanstested
welfare program. The choice depends in
part on the relative costs of the different options,
and those in turn depend on the design of the
potential social insurance program. Whether such
a program is investment-based or purely tax financed
on a pay-as-you-go basis is an important
feature of that cost.

Economists can help to evaluate these choices
by estimating the relevant costs and benefits of the
different options. My own conclusion is that
investment-based social insurance programs for


### ---Economics-2005-0-07.txt---
retirement, unemployment, and health care of the
retired population are more appropriate than payas-
you-go programs, means-tested programs, or a
policy of doing nothing.

There is an important political economy
reason for economists to work on improving
the design of social insurance programs rather
than advocating means-tested programs for
unemployment and old age. Elected governments
will inevitably seek to create universal
benefits to capture political support from the
largest possible majority of voters. Otto von
Bismark introduced social insurance in Prussia
in 1881 in an attempt to win support for
his conservative government and to fend off
the appeal of the nascent social democrats.
Even if it were economically desirable to do
so, economists could not prevent the spread of
social insurance by arguing that means-tested
programs would be more efficient. If economists
don't analyze the effects of social insurance
programs and recommend rules that
reflect good economics, the political process
will inevitably produce inferior programs.
III. Principles of Social Insurance
Accepting that there is a reason for mandatory
social insurance programs does not imply the appropriateness
of the programs that we have inherited
from the past. Today's Social Security and
unemployment insurance were enacted nearly 70
years ago. Economic conditions, administrative
technologies, and assumptions about economic
behavior have all changed dramatically since then.
And yet during these past 70 years, the key social
insurance programs have expanded without fundamental
change.

Before I consider some of the specific ways
in which our basic social insurance programs
can be reformed and strengthened, I want to
discuss broader principles that can help us to
think about each of the specific programs. I'll
begin with three fundamental political principles
and then turn to four economic principles.
A. Three Political Principles

Political principles involve value judgments
to a greater extent than the economic principles
to which I will turn later. I can explain the
political principles that shape my view about
appropriate reforms but I cannot prove that they
should determine policy. You may or may not
agree with them. Of course, you might agree
with me about specific reforms even if you
reject some or all of these principles.
Permitting Individual Choice.-Individuals
differ in their preferences. We do not all have
the same risk aversion, the same time preference,
the same relative taste for goods and leisure.
Letting individuals choose among options
in a way that reflects their individual preferences
should be an important aspect of social
insurance design. For Milton Friedman, such
freedom to choose is paramount. For me, it is
important but not decisive. In cases where
asymmetric information creates serious efficiency
problems, I might restrict that choice.
But I prefer to allow as much choice as possible.
I think that allowing individuals to make their
own choices is morally correct and generally
improves individual, and therefore social,
well-being.

But allowing choice means that programs
should be designed so that choice enhances
economic efficiency rather than creating deadweight
losses. A good example of such a program
redesign was the Social Security reform
that introduced the actuarial adjustment for
early and delayed retirement in a way that, in
principle, will allow individuals to decide when
they will start collecting benefits without changing
the actuarial present value of their benefits.5
Creating Program Transparency.--Social
insurance programs involve complex rules
about the benefits to be received, the taxes to be
paid, and the link, if any, between them. Who
among you is confident about even the most
basic Social Security rules that determine benefits
at retirement? If you are a man, what
benefit would your wife receive if she collects
on her own rather than as your spouse? How
would that change if she earned more or worked
another year? If she decides to retire at age 62
rather than 65? I'm told that there are more than
2500 separate rules in the Social Security
handbook.

The complexity of the rules weakens the perceived
link between the payroll taxes paid and
SThis adjustment will provide actuarially equivalent
benefits with a 3 percent real rate of return.


### ---Economics-2005-0-08.txt---
subsequent benefits. Many employees may simply
regard their Social Security payroll tax as
similar to the income tax, thereby increasing the
perceived marginal tax rate and raising the
deadweight loss of the tax.

Lack of transparency also permits programs
to have effects that might not be politically
acceptable if they were more explicit. For example,
some defenders of the current Social
Security system believe it permits a substantial
amount of redistribution that Congress would
not be willing to build into an investment-based
system of individual accounts. Although the
Social Security rules do not actually achieve
that redistribution, the important political principle
is that it is inappropriate in a democracy to
use a deliberately opaque system to achieve a
redistribution of income that would be rejected
if proposed in a more transparent way.
The Social Security program lacks transparency
because it is a defined benefit system
rather than a defined contribution plan of the
sort that now characterizes most private pensions.
Converting Social Security to a defined
contribution plan-even an unfunded "notional"
system such as Sweden and Italy now
have-would allow individuals to see the link
between their taxes and the resulting benefits.
An explicit decision by Congress to supplement
the contributions of low-income earners
in such a defined contribution plan would
achieve income redistribution without a loss
of transparency.

Recognizing Political Dynamics.-When we
economists talk about policy design, we generally
think about enacting permanent reforms.
But experience shows that legislated rules do
change and that the initial conditions influence
the path of that change. When designing a particular
program or advocating a particular design,
economists should recognize that some
designs are more stable than others and should
anticipate how a program might evolve.
The Medicare drug legislation enacted in
2003 is a good example. Medicare beneficiaries
will pay the first $250 a year in drug expenses,
followed by a 25-percent coinsurance rate to a
maximum benefit limit. Patients must then pay
100 percent of the drug cost up to $3600 in
out-of-pocket payments (in 2006). Above that,
Medicare will pay 95 percent of any additional
drug costs.

This rather strange design was accepted to
limit the total cost of the plan while delivering
benefits to a very large number of senior citizen
voters. An economically more rational plan
with the same budget cost would have insured at
least part of the range that is currently uninsured
and kept total costs down by a larger deductible.
But that would have had the political disadvantage
of giving benefits to fewer individuals. It
seems likely that future legislation will address
the residual insurance gap in a way that will
raise the total cost of the program.
There is another and potentially more significant
aspect of the future evolution of this Medicare
drug program. If all of the drugs consumed
by seniors come to be covered by government
insurance, there will be strong pressure to regulate
the price of those drugs. Such price regulation
is, in turn, likely to discourage the
development of drugs for those diseases that
particularly affect the elderly. It would be sadly
ironic if an insurance plan initiated to improve
drug access for seniors led ultimately to a reduced
availability of new drugs for this group.
B. Four Economic Principles

Let me turn now from these three political
principles-permitting individual choice, creating
program transparency, and recognizing political
dynamics-to four economic principles.
Recognizing the Economic Effects of Social
Insurance Programs and Their Taxes.-Noneconomists
who write about social insurance
programs often implicitly assume that social
insurance programs do not affect the behavior
of beneficiaries or the overall performance of
the economy. Evidence shows that the opposite
is true. Social insurance programs have important
and sometimes harmful effects on the
economy that are not fully recognized by the
public, Congress, or the politically responsible
officials.

A substantial volume of work during the past
quarter century has shown the various ways in
which social insurance programs do affect individual
behavior and the overall economy. These
effects include reducing national saving, inducing
early retirement, raising the unemployment
rate, pushing up the cost of health care, and
crowding out private health insurance. Any serious
evaluation of social insurance programs,


### ---Economics-2005-0-09.txt---
and any attempt to improve their design, should
take these effects into account.
There is, of course, controversy about the
magnitude of these effects, just as there is about
most other economic parameters. Decisions
about program design have to use the evidence
that is available, even if parameter estimates
come with substantial uncertainty. But there is
clearly room for economists to use new data,
new statistical methods, and new natural policy
experiments to improve our knowledge and,
therefore, to improve policy design.
Adverse effects result from specific program
designs and are not inherent in the goals of the
programs. For example, John Gruber and David
Wise (1999) showed that the rules governing
retirement benefits induced early retirement in
several European countries but that different
rules at different times and in other countries
did not induce early retirement. The U.S. benefit
rules that now specify an almost actuarially fair
relationship between benefits and retirement
age reduces substantially the perceived bias in
favor of early retirement.

More generally, social insurance programs
not only distort economic behavior directly,
thereby creating deadweight losses, but also
create further deadweight losses because of the
taxes that are levied to finance those programs.
I believe that the deadweight losses of those taxes
are much larger than is generally recognized.
I will illustrate this with the effect of the
50-percent increase in the payroll tax rate that
could occur if there is no change in benefit
rules. Deadweight losses depend on marginal
tax rates. Consider an individual who now faces
a combined federal and state marginal rate of
income tax of 30 percent without social insurance.
The current 15.3 percent employeremployee
payroll tax rate,6 when adjusted for
the interaction with the income tax7 and for the
present actuarial value of the additional retiree
and survivor benefits that result from increased
taxable earnings, now increases the overall marginal
tax rate from 30 percent to about 37.7
percent.8 A 50-percent rise in the 15.3 percent
marginal tax rate, adjusted for the income tax
interaction, would increase this effective marginal
tax rate from 37.7 percent to 44.2
percent.9

The increase in the deadweight loss that
would result from this tax increase reflects both
the reduction in labor supply-broadly defined
to include not just working hours but also the
accumulation of human capital, the choice of
occupation, effort, etc., and the change in the
form of compensation-away from taxable cash
and to less valuable fringe benefits. Although
neither behavioral change can be measured explicitly,
the resulting deadweight loss can be
calculated empirically by estimating the extent
to which the higher payroll tax would reduce
taxable labor income. It is appropriate to focus
on the decline in taxable labor income without
evaluating the two separate effects because the
relative price of the two components-the marginal
tax rate on the reward for increased labor
supply and the marginal tax rate that determines
the net cost to the taxpayer of fringe benefitsremains
the same when the tax rate changes.
Taxable labor income is, therefore, a Hicksian
composite good that can be used to assess the
deadweight loss (Feldstein, 1999a).10


### ---Economics-2005-0-10.txt---
The elasticity of taxable labor income with
respect to the net-of-tax share, i.e., to one minus
the marginal tax rate on labor income, is much
greater than the traditional elasticity of labor
supply as measured by labor force participation
and average hours worked. Estimating this elasticity
is now a subject of very active research
among public finance economists. Although a
wide range of estimates has been produced,
some studies are more reliable than others. I
believe that a conservative estimate is that the
compensated elasticity of taxable income with
respect to the net-of-tax rate is one-half.
Using this elasticity and the 2004 taxable
payroll implies that a rise in the effective marginal
tax rate from 37.7 percent to 44.2 percent
increases the annual deadweight loss by $96
billion, or nearly one percent of GDP.11 Since
the 6.5-percent increase in the marginal tax rate
applies only to taxable labor income (about 40
percent of GDP), the deadweight loss is equal to
about one-third of the incremental tax revenue.
Even this understates the relative size of the
deadweight loss because it ignores the reduction
in the tax base and therefore in the tax revenue
that results from the higher marginal tax. When
that reduction in taxable income is taken into
account, the incremental deadweight loss is
nearly 50 percent of the incremental revenue.12
The true cost per additional dollar of payroll tax
revenue is therefore $1.50.

Note that this is just the deadweight loss or
excess burden-i.e., the pure waste-associated
with the incremental tax. It does not include the
deadweight loss of the existing tax or the direct
burden of the taxes themselves. And it does not
include the deadweight loss caused by the program
distortions.

Although scaling back the rise in future benefits
would reduce the increase in the deadweight
loss, it would also reduce the protection
that Social Security provides to future retirees.
An alternative approach is therefore to redesign
the program so that the increased financing required
for the aging population has less of the
character of a tax.

One way to do that is to strengthen the taxpayers
perception of the link between taxes paid
and future benefits. That is one of the advantages
of shifting from the existing complicated
defined benefit rules to a defined contribution
system, even to a notional defined contribution
system. Although a notional defined contribution
plan would remain a pay-as-you-go system,
it would clearly link each worker's social insurance
tax payment to his or her resulting future
benefits.

A defined contribution system would provide
a tax-benefit link for those groups in the population
that now receive no extra benefits at all in
exchange for their additional taxes. For them,
the Social Security payroll tax is a pure tax just
like the income tax. These include both young
and older workers who are not in the top 35
wage-indexed earning years of their life, the
basis on which Social Security benefits are calculated,
as well as working women who will
eventually claim benefits based on their husbands'
earnings.

Although an unfunded notional defined contribution
system would provide some remedy,
the very low implicit rate of return in an unfunded
system implies that the payroll tax
would retain much of its distorting character. A
pay-as-you-go plan that substitutes a 2-percent
real return for private saving that would otherwise
earn a 5-percent real return is equivalent
over a lifetime of saving and dissaving to a tax
rate of about 75 percent.13

ing distortion is not the elasticity of saving with respect to
the net-of-tax return but the elasticity of future consumption
with respect to that net of tax return; Feldstein (1978b).
Saving is equivalent to expenditure on future consumption.
The relevant elasticity is therefore much larger than the elas-
ticity of saving. If saving does not respond to changes in the net
interest rate, the relevant compensated elasticity is equal to one
minus the marginal propensity to save.
" The formula for the increase in the deadweight loss is
0.5 E (t22 - t12) TLI/(1 - t,) where TLI is taxable labor
income, t2 = 0.442 and t, = 0.377. With TLI equal to 40
percent of GDP or $4.5 trillion and E = 0.5, the implied
increase in the deadweight loss is $96 billion.
12 With a compensated elasticity of 0.5 and an income
effect of 0.15, raising the marginal tax rate from 37.7
percent to 44.2 percent on an initial tax base of $4.5 trillion
reduces taxable income by $199 billion and therefore re-
duces tax revenue by $88 billion. The net tax increase is
thus reduced from $293 billion to $205 billion and the
deadweight loss per dollar of incremental revenue increases
to 46 percent.

13 An individual who can get a real net of tax rate of
return of (say) 5 percent in an individual retirement account
(IRA) or a 401(k) plan converts one dollar saved at age 45
(in the middle of his working life) to $4.32 at age 75 (in the
middle of his retirement). But if the implicit return on a
pay-as-you-go Social Security tax is only 2 percent, the


### ---Economics-2005-0-11.txt---
A much more substantial reduction in the
effective tax rate would be achieved by financing
the increased cost of Social Security and
Medicare by a funded system that would permit
future benefits to be financed without a large
increase in the tax rate. Moreover, to the extent
that the additional saving that individuals do
earns a favorable rate of return, they might not
consider it a tax at all. I will return to this issue
later when I discuss Social Security reform
more fully.

Designing Programs by Balancing Protection
and Distortion, while Seeking Reforms that
Improve the Available Tradeoff.-Social insur-
ance programs generally involve a tradeoff of
protection and distortion. Social insurance programs
protect individuals against undesirably
low levels of consumption during old age, spells
of unemployment, or when hit by large medical
bills. They also protect individuals from the
need to work longer than health warrants, to
accept a job when additional searching would
be adequately productive, or to forego appropriate
medical care because of an inability to pay.
But the same social insurance programs also
distort incentives in ways that cause inefficient
use of resources: early retirement, low saving,
unproductively long job searches, and excessive
consumption of medical care.

Social insurance program parameters should
be chosen to balance protection and distortion.
The level of Social Security benefits should
reflect the fact that high benefits relative to
previous income improve the protection against
reduced consumption in old age but also depress
saving and may induce early retirement. A high
level of unemployment insurance benefits helps
the unemployed to maintain consumption but
also encourages longer spells of unemployment
and the choice of jobs that have a greater likelihood
of leading to a layoff. Low co-payments
in health insurance reduce the risk of foregoing
needed care or suffering a major drop in other
consumption, but they also lead to an increased
demand for care that is worth less than its cost
of production. More complete protection in
each program also raises the program cost and,
therefore, creates greater distortions through the
tax system.

As protection becomes more complete, the
marginal value of protection declines and the
incremental distortion rises. The primary goal
of social insurance should, therefore, generally
be to prevent catastrophic losses: poverty in old
age, long-term loss of income when unemployed,
very expensive out-of-pocket medical
costs, and the consequences of permanent
disability. More generally, at the optimum,
the marginal value of additional protection
should just equal the marginal cost of the
distortion. Economists can help the policy
process by evaluating the protection and distortion
created by different changes in program
design.

Useful economic analysis can go beyond selecting
an optimal point on a protection-distortion
frontier. It is important to seek ways to shift the
frontier, permitting less distortion at each level of
protection. Reforms based on individual accounts
that I describe later in the paper would achieve
that improvement.

Redesigning Programs to Keep Pace with
Changing Conditions.--Three important changes
that should influence the design of our social insurance
programs have occurred since those
programs began: a changed economy, new technology,
and a different understanding of the effect
of government programs on individual behavior.
The Social Security and unemployment insurance
programs were created during the depression
of the 1930s when individual savings
had been destroyed by widespread bank failures
and when many individuals had been unemployed
for a year or more because of a lack of
aggregate demand. Keynesian economists in the
1940s like Harvard's Seymour Harris praised
the unfunded character of the new Social Security
program for its ability to depress national
saving and stimulate aggregate demand (Harris,
1941). In contrast to those depression years,
conditions in the past half-century have been
very different, with relatively low unemployment
rates and a system of government deposit
insurance that protects individual savings. The
unemployment insurance and Social Security
that may have been appropriate in the 1930s is
no longer appropriate for the economy of the
twenty-first century.


### ---Economics-2005-0-12.txt---
A second relevant change has been in the
technology of financial administration made
possible by the introduction of computers.
When Social Security was created, President
Roosevelt wanted it to be a funded system
rather than a pay-as-you-go system.14 There
was of course no way to have individually controlled
personal retirement accounts and the Republicans
in Congress did not want to trust the
government to manage a large pool of funds.
And since the Congressional Democrats were
eager to start paying benefits, the result was a de
facto shift to a pay-as-you-go structure. The
creation of individual investment accounts for
every adult, which would have been technically
impossible in the 1930s, is no longer even difficult.
Today more than 90 million Americans
own mutual funds, including IRAs and 401(k)
plan accounts. In contrast to the formidable task
in the 1930s of keeping track of everyone's
Social Security account without the help of
computers, the creation of a system of individual
investment-based accounts would now be
relatively easy.

The third important change has been in the
economic profession's understanding of how
fiscal incentives affect individual behavior. In
the 1930s, economists assumed that individuals
were so unresponsive to taxes and benefits that
any behavioral response could simply be ignored.
Most economists continued to ignore the
adverse incentive effects even when the top
marginal tax rate was over 90 percent, as it was
from 1944 to 1963. The adverse effects of high
unemployment insurance benefits on job search
and on the choice of jobs were also ignored.
Today economists recognize that high marginal
rates of income tax and the marginal tax
rates implicit in various benefit rules reduce
taxable income and create substantial deadweight
losses.

These three changes imply that if Social Security
and unemployment insurance were being
created now, the programs would likely be significantly
different from those in current law.
Economists today would regard the adverse effect
of Social Security on saving and capital
accumulation as a deterrent to growth rather
than as a favorable source of Keynesian demand.
The widespread ownership of mutual
funds, IRAs, and 401(k)s would be a natural
starting point for any new social insurance program.
And every aspect of behavior would be
assumed to be more responsive to tax rates and
program design.

More generally, the reforms that will be enacted
in the future will inevitably evolve as
economists learn more and as the set of feasible
options changes. An interesting example of this
changing perception of what is feasible is the
possible transition to personal retirement accounts
in an investment-based Social Security
program. About 20 years ago, when I served as
chairman of the Council of Economic Advisers
in the Reagan administration, the Social Security
retirement program was on the verge of a
crisis. The trust fund was about to reach zero
and the projected taxes to be collected over the
next few years were not large enough to pay the
benefits specified in law. President Reagan appointed
a bipartisan commission to find a solution.
The resulting plan called for an acceleration of
scheduled future tax increases, the taxation of
Social Security benefits, and a variety of other
smaller measures.

President Reagan was unhappy with these
proposals and asked a small group of us in the
Administration whether there wasn't something
better to be done, perhaps along the
lines of the Chilean reform that used investment-
based personal retirement accounts
instead of a pay-as-you-go system. None of us
could design a feasible transition to such a
plan. It looked to me and to the others as if
accumulating funds to finance such personal
retirement account annuities would involve
a double burden on the transition generation
that was both unfair and politically
infeasible.

I now know that that was wrong. Research
that Andrew Samwick and I have done in recent
years (Feldstein and Samwick, 1998a, 1998b,
2002) shows that it would be possible to transition
gradually to a completely investment-based
plan without ever increasing the combination of
pay-as-you-go taxes and personal retirement account
(PRA) saving by more than 2 percent of
payroll earnings, or about 1 percent of GDP, and
without reducing the benefit that retirees receive
from the combination of the traditional taxfinanced
program and the new investment-based
14 See Sylvester J. Schieber and John B. Shoven (1999)
for a valuable discussion of the origins of the Social Secu-
rity legislation.


### ---Economics-2005-0-13.txt---
annuities.15 The key, we learned, is to have a
transition in which the personal retirement account
annuities gradually substitute for pay-asyou-
go benefits, allowing the pay-as-you-go tax
rate to decline and the PRA contribution rate to
increase until the transition is complete.
Of course, the demonstration that such a transition
is feasible doesn't mean that it is desirable.
A pure investment-based PRA plan would
involve more risk than many individuals might
want, a subject to which I will return later. But
a shift to a mixed system that avoids an increase
in the payroll tax rate or in private saving might
be an economic improvement. I'm sorry that I
couldn't offer that solution when President Reagan
asked for it.

My point in recalling this is that economic
research has changed what we regard as feasible.
Similarly, future research can and will develop
new ways to provide social insurance
protection with greater economic efficiency and
more responsiveness to individual tastes. A basic
principle of designing social insurance policies
should be a willingness to accept such
ideas when they become available.
Separating Social Insurance from Income
Redistribution.-As I indicated earlier, social
insurance programs are not means tested. Eligibility
for benefits does not depend on the income
or wealth of the recipient but on an event
like reaching age 65, beginning a spell of unemployment,
or incurring a medical problem.
Not surprisingly, the evidence that I cited makes
it clear that today's social insurance programs
do not redistribute income to the poor. Indeed,
the positive correlation of income and longevity
tilts the net benefit of Social Security and
Medicare to households with higher lifetime
incomes. The structure of unemployment
insurance rules causes a similar shift in that
program.

There is, of course, a role for means-tested
programs that are more narrowly focused on
individuals who demonstrate that they have low
income or assets. Although I doubt the desirability
of the myriad of existing in-kind programs
like food stamps and housing subsidies
(Moffitt, 2003), I have no doubt about the appropriateness
of transferring income to the very
poor.

There is, moreover, a clear case for being
more generous to some demographic groups
than to others. The existing Supplemental Security
Income program provides means-tested
benefits to those over age 65 whose Social Security
benefits plus private resources do not
together reach some minimal level. A more
generous means-tested program, targeted at individuals
over age 75, would not distort labor
supply to the same extent that it would for
younger ones. It is possible, therefore, to have
more protection with less distortion in such a
means-tested program. It is a shameful feature
of our Social Security system that, even with the
Supplemental Security Income program, 10 percent
of those over age 65 are in poverty while
Social Security provides nearly $500 billion a
year in benefits to individuals who are financially
more comfortable.

To the extent that distributional concerns motivate
the design of social insurance, the emphasis
should be on eliminating poverty and not on
the overall distribution of income or the general
extent of inequality. Like most economists, I
accept the Pareto principle that an economy is
better off if someone gains and no one loses.
This is true even if the gainer has above-average
income, causing a Gini coefficient measure of
income distribution to shift to greater inequality.
Although there may be spiteful egalitarians
who reject this Pareto principle, I believe that
most economists agree with me. To see if you
do, ask yourself whether you think it would be
a good thing if everyone reading this article
received $50 by some magical process that did
not decrease the income or wealth of anyone
else. Since we are an above-income group, national
inequality would rise. Nevertheless, I
think there are few who would reject bestowing
this extra wealth on us all.

This brings me to the end of my four economic
principles of social insurance. I turn now to discuss
how the three major forms of social insurance
could be improved by shifting to a system that
combines government insurance with individual
investment-based accounts: unemployment insurance
savings accounts (UISAs) backed up by a
government line of credit, personal retirement accounts
(PRAs) that supplement ordinary pay-asyou-
go Social Security benefits, and personal


### ---Economics-2005-0-14.txt---
retirement health accounts (PRHAs) that finance a
range of Medicare choices.

IV. Unemployment Insurance

Although unemployment insurance is a relatively
small program with total federal and state
outlays in 2003 of $39 billion, it is particularly
important because of its impact on macroeconomic
performance. It is also significant as an
illustration of how reforms have been able to
reduce distortion while retaining protection for
those who need it. Moreover, it is a form of
social insurance where further reforms through
investment-based accounts could achieve substantial
economic gains.

The unemployment insurance program in the
United States was created in 1935 in the depth
of the depression. The program is administered
by the individual states but under federal rules
that substantially restrict the scope of state governments'
actions. Benefits of a typical recipient
are 50 percent of previous earnings and can be
collected for up to six months. The European
unemployment benefit programs are substantially
more generous in both the relative level
and the duration of benefits, with clearly adverse
effects on European unemployment rates.
Thirty years ago, when I began doing research
on unemployment insurance (Feldstein,
1973a, 1973b), there was a general perception
that unemployment benefits were relatively low
and that they had little or no effect on economic
behavior. People were assumed to be unemployed
solely because there was inadequate aggregate
demand. Reformers focused on seeking
increases in the level and duration of benefits to
help those who were unemployed for what were
assumed to be reasons beyond their own control.
We now know that perception was wrong.
Unemployment insurance benefits raise the unemployment
rate in a variety of ways that economists
have now analyzed and measured. But
back in the 1960s and 1970s, the higher unemployment
rates that were actually induced by
unemployment insurance were instead incorrectly
perceived as due to inadequate demand.
When the government tried to reduce this high
structural unemployment with expansionary
monetary and fiscal policies, the result was rising
inflation. Fortunately, this is now better
understood. Monetary policy no longer tries to
reduce structural unemployment. But although
unemployment insurance is therefore no longer
a source of increased inflation, it continues to
raise the rate of unemployment. This is a particularly
serious problem in Europe where unemployment
rates remain close to 10 percent.
The old notion that unemployment benefits
were too low to affect the economy was the
result of a misleading comparison of the average
weekly unemployment benefit and the average
weekly wage. Although the average
benefit was only about 30 percent of the average
wage of all workers, the unemployed had substantially
lower pre-unemployment wages than
the labor force as a whole. Unemployment insurance
benefits actually averaged about 50 percent
of the pre-unemployment income of those
who received benefits, with even higher replacement
rates in states that supplemented the
basic benefit with payments for spouses and
children. But even this substantially understated
the relevant replacement rate because benefits
were not subject to the income and payroll taxes
that were levied on wages. Since the combined
marginal rate of income and payroll tax for the
spouse of a high-earning individual could then
easily exceed 50 percent, the ratio of untaxed UI
benefits to the individual's net-of-tax potential
earnings could exceed 100 percent. For such a
person, it was possible to have a higher net
income by remaining unemployed than by returning
to work.

Even significantly lower benefit replacement
rates could have substantial adverse incentive
effects, as a number of studies eventually
showed. Although macroeconomists came to
recognize that much unemployment was not of
an involuntary Keynesian type but was a productive
search for good job matches, the accumulating
evidence showed that UI benefits were
inducing excessively long periods of searching
in which the gain from the marginal search was
less than the value of the foregone output. For
example, Larry Katz and Bruce Meyer (1990)
showed that the probability that an unemployed
person takes a job rises dramatically in the few
weeks just before their benefits would expire.
Jim Poterba and I (1984) found that the median
value of the reported reservation wage of new
UI recipients was actually higher than the wage
on their previous job, that it was an increasing
function of the UI replacement rate, and that it
came down only very slowly during their spell
of unemployment.


### ---Economics-2005-0-15.txt---
Longer durations of unemployment are not
the only adverse effect of UI benefits. The practice
of temporary layoffs in which unemployed
individuals have a spell of unemployment but
return to their original employer is substantially
encouraged by high UI replacement rates (Feldstein,
1976, 1978a). High benefits also encourage
individuals to accept work in firms with
high seasonal or cyclical layoffs. That reduces
the wage that such firms have to pay and thus
subsidizes the expansion of those high unemployment
industries.

As all of this became clear, the most obvious
first reform was to include unemployment benefits
in taxable income. Although there was
initially strong opposition to this idea, it was
hard to argue with the position that cash income
is cash income and should be taxed. The notion
that taxing unemployment insurance would inappropriately
burden the poor was clearly contrary
to the fact that the income tax allows a
substantial exclusion of income before any tax
is levied. A poor UI recipient would pay no tax.
The initial legislative compromise was to include
only half of UI benefits in taxable income
and to do so only for relatively high-income taxpayers.
This provided a natural experiment that
Gary Solon (1985) used to show that the relative
duration of unemployment fell for those whose
benefits were taxed. Later, in the Tax Reform Act
of 1986, the UI benefits were fully subject to the
income tax like all other forms of labor income.
Taxing UI benefits eliminated the possibility
that an individual could have a higher net income
from UI benefits than by working. It is
hard to know what the aggregate effect on unemployment
has been, but my personal estimate
is that the unemployment rate probably fell by
about one-half percentage point after benefits
were taxed, an effect equal to more than
500,000 jobs at any time.

The evidence that UI benefits cause substantial
distortion led to analytic studies of the level
of benefits that optimally balances distortion
and protection. Martin Bailey (1978) presented
an analytic model in which the optimal level of
benefits depends on the individual's coefficient
of relative risk aversion and on the elasticity of
the duration of unemployment with respect to
the UI benefit replacement ratio. John Gruber
(1997) used this framework to derive an explicit
optimal UI benefit based on data on the effect of
unemployment on household food consump-
tion, concluding that the optimal replacement
rate should be much less than the 50 percent in
current law. More recently, however, Raj
Chetty (2003) showed that the measure of risk
aversion that is relevant to designing the optimal
UI benefit may be substantially greater than
the risk aversion that is relevant to financial
investments, because many types of household
spending cannot be adjusted in the short-run,
which is relevant to unemployment spells. Chetty'
s analysis points to optimal UI replacement
rates that are close to the levels that we observe
in the United States.

These calculations of optimal UI benefits assume
that individuals have no financial assets.
In contrast, if individuals save optimally, the
optimal value of UI benefits--especially for
short and moderate spells would be very much
less. Although there is evidence that individuals
who face greater income uncertainty have
somewhat higher saving rates, it would be
wrong to assume that in the absence of unemployment
insurance everyone would save

enough to finance consumption optimally during
spells of unemployment. Some individuals
would be too short-sighted to save for potential
unemployment.

What is the optimal response to this problem?
One possibility would be to continue the current
system of paying UI benefits but with the level
and time path of benefits selected to balance the
gain from protection and the loss from distortion.
Another possibility would be to shift to a meanstested
program, although that would induce some
individuals to game the system, saving nothing so
that they could qualify for means-tested benefits
when they became unemployed. The same problem
of asymmetric information would prevail, as
in the case of Social Security retirement benefits
that I discussed before: the government could not
distinguish individuals who were too short-sighted
to save from those who were gaming the system.
On efficiency grounds, the choice between the
current system and government means-tested benefits
would depend on the response of unemployment
to the benefit level and on the relative
number of those who would save optimally, those
too shortsighted to save, and those who would
choose not to save in order to qualify for the
means-tested benefits.

A third possibility is to require everyone to
have an unemployment insurance savings account
earmarked to pay benefits if unemployment


### ---Economics-2005-0-16.txt---
occurs. Dan Altman and I (Feldstein and
Altman, 1998) explored a variety of such possible
plans. In a typical plan, each individual
would be required to accumulate funds in an
unemployment insurance savings account until
the balance was enough to pay benefits for two
spells of six months at 50 percent of the individual'
s current wage. These funds would be
invested and would earn a market rate of return.
After a transition period to accumulate account
balances, anyone who would be eligible for
unemployment benefits under today's UI rules
would instead be able to withdraw the same
amount from his unemployment insurance savings
account. If a balance remains in the account
when the individual reaches retirement
age, the funds would be available for the individual
to take and spend. An individual who
dies before retirement bequeaths the account
balance. In short, individuals would regard the
funds in the UISA as their own money. For
someone who expects to have a positive balance
in his account until retirement, the UISA plan
would provide the same income protection as
the current UI system, but without any
distortion.

What about individuals who experience so
much unemployment that they use up the funds
in their UISA? Such individuals would be able
to borrow from a government UI fund to receive
the same benefits that they would withdraw if
they had a positive account balance. After they
return to work, they would again save to repay
the loan with interest and to rebuild their UISA
balance. If they expect their account to accumulate
a positive balance in the future, the dollars
that they borrow would be a very real obligation
and the incentives to return to work would not
be distorted by the government loan. They
would have full protection and no distortion
while unemployed and would accumulate personal
wealth after they returned to work.
It is only those who expect that they will have
a negative balance in their account when they
retire for whom this plan would represent no
improvement over current law. For them, the
protection and distortion would be the same as
it is with the current UI rules.
The extent of the gain from introducing unemployment
insurance savings accounts therefore depends
on the proportion of the unemployed who
expect to retire with negative balances and on the
sensitivity of unemployment to the change in in-
centives. Dan Altman and I did some preliminary
empirical analysis of this approach using a sample
of men in the National Longitudinal Survey. We
found that, even with no favorable behavioral response
of unemployment to the improved incentives,
less than 10 percent of benefits would be
paid to those who eventually retire with negative
balances (or who had negative balances when our
data sample ended).

Our analysis thus implied that the UI program
could be redesigned around individual unemployment
insurance savings accounts in a way
that substantially reduces the current distorting
effect while not reducing either the availability
of funds when unemployment occurs or the
protection against relatively large cumulative
amounts of lifetime unemployment. More research
on this potential form of unemployment
insurance would certainly be valuable.
V. Social Security16

Social Security is the largest social insurance
program in the United States, with expenditures
in 2003 of $470 billion, or 22 percent of total
federal government spending. It includes not
only annuities for retirees and survivors but also
a separate program of disability insurance that
accounts for some 15 percent of the total Social
Security outlays. Since the disability insurance
program involves a range of very different issues,
I will not be considering it in this article.
Social Security is now a defined benefit, payas-
you-go program in contrast to the defined
contribution, investment-based structure of most
private pensions. In a defined benefit program,
an individual's benefit at retirement depends on
his earnings during his working years and not
on the performance of asset prices during that
time. The program is a pay-as-you-go one because
most of the payroll taxes collected in each
year are used to pay concurrent benefits. There
is not the kind of asset accumulation and financial
investment that there would be in a private
16 Feldstein and Liebman (2002a) provides a survey for
the Handbook of Public Economics of the large theoretical
and empirical literature on Social Security. See also my Ely
Lecture to the American Economic Association (Feldstein,
1996a) and a forthcoming article in the Journal of Economic
Perspectives (Feldstein, 2005b) that deals with the current
reform debate in more detail.


### ---Economics-2005-0-17.txt---
pension. The benefits are financed by a payroll
tax on earnings currently up to about the eightyfifth
percentile of the distribution of wages
($87,900 in 2004). The payroll tax rate devoted
to the Social Security program other than disability
is now 10.6 percent, divided equally
between employers and employees.
Benefits take the form of an annuity that is
indexed to the CPI to retain its real value during
the individual's retirement years. The level of
benefits at retirement depends on the average
wage-indexed earnings of the individual during
his or her highest 35 earning years. The benefit
formula provides higher annual benefits per dollar
of previous earnings at lower earnings levels
than at higher levels. Individuals who retire
before their normal retirement age (now rising
gradually from 65 to 67) receive an actuarially
reduced benefit, while those who retire after
their normal retirement age receive an actuarially
increased benefit.

Couples may collect benefits either on the basis
of their separate earnings records or as 150 percent
of the benefits of the individual with the higher
benefit level. A surviving spouse can receive 100
percent of the benefit of the higher earning spouse.
These rules for spouse benefits have the effect of
causing many women who pay Social Security
taxes to get little or nothing back for their taxes
since their benefits are based on their husbands'
incomes. This is true not only for married women
but also for younger women who expect to marry
and for divorced women and widows who will
also expect to claim benefits based on their former
(or future) husbands' earnings.
I became interested in Social Security as a
graduate student in the 1960s when I realized
that the tests of consumption theory-Friedman'
s work on the permanent income hypothesis
and Modigliani's work on life cycle
saving- completely ignored the role of Social
Security even though it had become the major
source of retiree income. I realized also that the
theory of Social Security's effect on saving was
more complex than a simple displacement of
financial wealth by Social Security. To the extent
that Social Security induces earlier retirement,
it raises the desired level of financial
wealth. The net effect of Social Security on
saving therefore depends on the balance between
the positive induced retirement effect and
the negative wealth displacement effect, an issue
that could be settled only empirically.
My initial time series analysis (Feldstein, 1974)
implied that Social Security "wealth," the present
actuarial value of future Social Security benefits,
significantly reduced personal saving. Reestimating
this equation 22 years later with a corresponding
amount of additional data produced reassuringly
similar results (Feldstein,1996b) The
conclusion that Social Security depresses private
saving was also supported by household data and
cross-country analysis. Other researchers who
looked at this question generally supported the
primary conclusion, although with estimates of
varying magnitudes (Congressional Budget Office,
1998).

This adverse effect of Social Security on saving
is relevant to understanding the significance
of Paul Samuelson's very important 1958 overlapping
generations paper (Samuelson, 1958).
Samuelson showed that a pure pay-as-you-go
Social Security system in an economy without
capital and without technical progress would
generate an implicit rate of return on each generation'
s taxes equal to the rate of growth of the
population. This occurs because the number of
taxpayers is larger by the rate of growth of
population than the number of retirees. If technical
progress is added to this economy, the
implicit return in a pure pay-as-you-go system
is still the rate of growth of the tax base, now
the sum of the growth rate of population and the
growth rate of productivity.

Samuelson noted that this positive rate of return
meant that an unfunded Social Security program
could raise welfare in an economy that has no
capital assets. But what happens when we recognize
that all economies do have capital stocks and
that Social Security transfers act as a substitute for
real capital accumulation? With that more realistic
description, the reduction in the rate of saving
caused by the provision of pay-as-you-go annuities
can cause a reduction in the present value of
all current and future consumption.
To understand why, consider first an oversimplified
textbook economy in which there are no
capital income taxes. Each generation of workers
receives an implicit pay-as-you-go return equal to
the sum of population and productivity growth but
foregoes the larger return equal to the marginal
product of capital. For each such taxpayer generation,
there is, therefore, a cost of having a payas-
you-go Social Security program rather than
providing for retirement consumption by saving
and investing in real assets (or the financial assets


### ---Economics-2005-0-18.txt---
that represent a claim on those real assets.) However,
the initial generation of retirees had the good
fortune to receive benefits without ever having
paid taxes.

It can be shown that aggregating the consumption
losses of all generations of taxpayers back to
the initial date using a discount rate equal to the
marginal product of capital produces a present
value loss just equal to the windfall gain of the
initial retirees. (Feldstein and Liebman, 2002a;
Feldstein, 2005b) In short, with these simplified
"textbook" assumptions, the introduction of a payas-
you-go Social Security program does not reduce
the present value of national consumption
but rather redistributes it from later generations to
the first one. More generally, whenever the program
is expanded, those who are about to retire or
who will soon retire receive a windfall gain at the
expense of all future generations but with no
change in the present value of consumption over
all generations.

This neutrality result depends, however, on the
implicit assumption that there are no distorting
capital income taxes.17 With capital taxes, the
appropriate intra-generational rate of discount of
consumption is less than the marginal product of
capital. Moreover, the appropriate rate for aggregating
the consumption of different future generations
may not be a market rate at all but a
discount rate equal to the rate of decline in the
marginal utility of consumption. With a realistic
growth rate and any plausible value of the elasticity
of marginal utility with respect to consumption,
the appropriate rate of discount would be far
less than the marginal product of capital.'8 With
that discount rate, the net present value of the loss
of consumption due to introducing and then repeatedly
expanding a pay-as-you-go Social Security
would be very large. Similarly, shifting from
a pure pay-as-you-go program to a mixed program
with a substantial investment-based portion would
cause a large rise in the present value of
consumption.

The reduction in saving and in the present
value of consumption is not the only adverse
effect of a pay-as-you-go program. A second
important effect is the distortion of labor supply
and of the form in which compensation is paid
because of the increase in the marginal tax rate.
The relevant marginal tax rate is the statutory
rate net of the anticipated increase in the actuarial
value of benefits. The increase in benefits
is zero for many married women. It is also zero
for individuals who are not in one of their 35
highest earning years, typically when they are
either young or old, so that the higher effective
marginal tax rate comes when the individual's
attachment to work is relatively weak. Many
individuals may also underestimate the effect of
additional earnings on future benefits. In all of
these cases, the payroll tax may create a substantial
deadweight burden.

This incremental deadweight loss from distorting
the labor supply would be essentially
eliminated if individuals earned a market return
on their Social Security savings, as they would
in an investment-based system. That would
make the actuarial present value of their benefits
equal to their Social Security savings. The only
labor supply distortion would result if some
individuals were forced to do more saving for
retirement than they preferred or thought that
the implicit actuarial terms of the annuity did
not reflect their own mortality risk. A mixed
system that combines pay-as-you-go and investment-
based components would reduce but not
eliminate the labor supply distortion.
A third distortion caused by a traditional payas-
you-go system is the incentive to retire early
when an implicit tax results from the loss of
benefits caused by delayed retirement. Although
the United States has now largely eliminated
this by an appropriate actuarial adjustment, this
distortion remains a major problem in Europe
and elsewhere (Gruber and Wise, 1999). Early
retirement increases the annual cost of Social
Security benefits and reduces the available labor
income tax base. This leads to a higher marginal
rate of Social Security tax, further increasing
that source of deadweight loss. When combined
with formal or informal restrictions that prevent
reducing wages to offset the high payroll tax
rates, these taxes contribute to the high unemployment
rates that we see in Europe. The U.S.
experience shows that this problem can be eliminated
within the pay-as-you-go system. It
would, of course, also be eliminated in an
investment-based system in which retirement
17 For an explicit derivation of the neutrality result and an
examination of the implication of capital income taxes and of
other discount rates, see Feldstein and Liebman (2002a).
18 For example, with a per capita growth rate of two
percent and an elasticity of the marginal utility function of
two percent per year, the rate of decline of the marginal
utility of consumption would be 4 percent.


### ---Economics-2005-0-19.txt---
income is withdrawn from personal accounts,
which can be bequeathed if the individual dies
before exhausting the account or before annuitizing
the accumulated balance.

The analysis of these three types of distortion
should make it clear that the shift to a "notional"
defined contribution system would help only a
little to reduce the adverse effects of the current
pay-as-you-go system. A notional defined contribution
system is one in which each individual
has an account that is credited with his tax
payments and with a notional rate of return on
his accumulated balance, but in which there is
no actual investment in financial assets. The
notional rate of return that is feasible in the long
term is the modified Samuelson return, i.e., the
rate of growth of the tax base. Since there is no
real capital accumulation, the reduction in the
present value of consumption is not changed.
The distortion in labor supply and in the form of
compensation is reduced (but not eliminated)
because individuals can more clearly see the
link between their taxes and their future benefits.
A notional defined contribution system also
reduces the distortion in retirement decisions
because individuals reduce their future benefits
if they retire early and increase them by delayed
retirement. But even with this improved transparency,
the low implicit pay-as-you-go rate of
return leaves a substantial distortion in work
and compensation incentives.

Although the scope for reducing the substantial
deadweight losses of the pure pay-as-you-go
system and for increasing the present value of
all future consumption should provide a strong
incentive for a change in policy, they are not the
reason that has driven the political process in
many countries to move from a pure pay-asyou-
go to a mixed system or to consider such a
change. The primary driving force is the recognition
that the increasing age of the population
will require a very large tax increase or benefit
cut if nothing is done to change the existing
system. This is not a temporary effect of the
baby boom generation reaching retirement age
but a permanent result of the trend to increased
longevity. This demographic change is significant
not only because it drives the political
process but also because it increases the potential
gain of making such a change.

The desirability of shifting to an investmentbased
or mixed system depends on four issues: (a)
the transition process and its cost; (b) the ongoing
administrative costs; (c) the riskiness of financial
investments; and (d) the effect on the income
distribution and especially on the poorest group. I
have done work on these issues during the past
decade, both alone and with colleagues Jeffrey
Liebman, Elena Ranguelova, and Andrew Samwick.
I will now summarize what I have learned.
I commented earlier on the transition problem
when I discussed my experience with President
Reagan. The common view that the
transition from a pure pay-as-you-go system to
a mixed system requires the transition generation
to "pay double"-once to save for their
own retirement and once to meet obligations to
existing retirees-is wrong. As examples of
what could be done, Andrew Samwick and I
(1998a, 1998b, 2002) showed how the projected
rise in the pay-as-you-go tax rate to more than
19 percent19 could be avoided if individuals
contribute just 1.5 percent of wages out-ofpocket
to personal retirement accounts. The key
to the transition is using personal retirement
account annuities to supplement the pay-asyou-
go benefits. The growth of the PRA annuities
offsets the slowdown of the pay-as-gobenefits
that results from not increasing the tax
rate as the population ages.

There is no free lunch in this process. The
key is that additional saving during the transition
years can reduce long-run costs. This extra
saving could be voluntary, induced by a matching
PRA contribution out of existing payroll tax
receipts.20 Although the matching would reduce
the "trust fund" balances, the transition need not
involve borrowing by the Social Security system
from general revenue (Feldstein and Samwick,
2002).21


### ---Economics-2005-0-20.txt---
Even a transition in which the initial personal
retirement account deposits are financed wholly
by government borrowing could eventually
raise national saving and the present value of
future consumption. Financing the initial deposit
to personal retirement accounts by government
borrowing would have no immediate
direct effect on national saving because the increased
budget deficit would be offset by the
deposit of those funds into the personal retirement
accounts. Over time, the availability of the
PRA annuities could permit reducing the payas-
you-go benefits (relative to those projected in
current law) without lowering total retirement
income. This reduction in the pay-as-you-go
benefits would mean that the annual rise in the
budget deficit would be less than the amount
transferred to the PRAs. That difference would
be an increase in national saving.22
I turn next to the issue of the administrative
cost of PRAs. Some critics of PRAs argue that
the administrative costs of a PRA program
could offset all of their higher return relative to
the pay-as-you-go system. Although there have
been bad cost experiences in some countries,
this certainly need not be so. Sweden's recent
PRA program involves administrative costs
equal to between 30 and 100 basis points of the
assets, an amount that will come down further
as the total assets grow, since the administrative
costs depend on the number of transactions and
not on the value of the assets. TIAA-CREF
operates an individual account system with a
variable annuity for a charge of only 37 basis
points.23

The issue of risk is an important consideration
in both the pay-as-you-go and investment-based
systems. Although pay-as-you-go programs do
not have asset price risk, they have the political
risk that future taxpayers may not be willing to
raise taxes when demographic or economic
changes would make it necessary to do so in order
to finance promised benefits. The United States
enacted benefit cuts in 1983 by increasing the age
for full benefits. Many Latin American countries
cut cash benefits in the 1980s and 1990s. More
recently, Germany, Italy, and Japan have announced
or enacted reductions in state pension
benefits.24 Perhaps the most reliable way to avoid
future legislation that causes an unexpected reduction
in retirement income is to develop a mixed
system that does not require a future rise in the
payroll tax rate.

The issue of asset price risk is more complex.
On the basis of a substantial amount of research,
I believe that a suitable mixed system that combines
tax-financed pay-as-you-go benefits with
investment-based PRA annuities can satisfy
three conditions: a substantially lower longterm
cost of financing retirement income than
the tax projected for the pay-as-you-go system;
a higher expected level of benefits from the
combination of pay-as-you-go and the PRA annuities;
and a very low probability that the actual
level of combined benefits will be less than
the pay-as-you-go benefits projected in current
law.25 The low risk could be achieved by a
combination of three things: the floor on retirement
income provided by the pay-as-you-go
benefits in the mixed system, restrictions on the
investments that can be made in the PRAs, and
explicit guarantees provided by either the government
or the private market. Because individuals
differ in their risk preferences, the solution
that can best reflect those different preferences
may be the availability of a variety of alternative
guarantees from the private organizations
that manage the PRAs and PRA annuities (Feldstein,
2005a).

I turn, finally, to the issue of the distributional
effect of the shift from our pay-as-you-go system
to a mixed system. I have already discussed
the evidence that the current Social Security
system does very little redistribution and leaves
some 10 percent of seniors with incomes below
the poverty line. Women who were never married,
widows, and particularly those who were
widowed or divorced at a relatively early age
22 Feldstein and Liebman (2002a, section 7.1.3) used an
overlapping generation model to show how such a debt
financed transition could raise the present value of con-
sumption. The rise in saving does not happen in the first
period but begins after that. The debt service on the initial
borrowing cannot be financed by additional borrowing
alone; the growth of the debt must be less than the growth
of the economy.

23 See Shoven (2000) for several discussions of admin-
istrative issues. The chapter by Fred T. Goldberg, Jr., and
Michael J. Graetz (2000) shows how administrative costs
can be reduced while maintaining the individual account
structure.

24 See John McHale (2001) for more evidence on this
point. 25 See Feldstein (2005a, 2005b) for a more extensive
discussion of the risk issue.


### ---Economics-2005-0-21.txt---
generally have quite low benefits under current
law and could do substantially better under a
mixed system (Feldstein and Liebman, 2002b).
Divorced women would benefit if the total of
the husbands and wives' PRAs are pooled and
divided at the time of divorce.
While a PRA itself does not cause any income
redistribution, the redistributive structure
of the pay-as-you-go benefits could, in principle,
be changed to make the combined benefit
achieve any degree of redistribution.
In summary, it seems clear from the research
that has been done that the current pay-as-you-go
system could be gradually replaced with a mixed
system that includes investment-based personal
retirement accounts in a way that maintains or
exceeds the benefits that are projected in current
law, while sharply reducing the long-run cost of
achieving those benefits. This transition could be
financed with relatively small additional PRA saving
by individuals or by using existing payroll tax
revenue. Even if the tax revenue is used, the initial
fiscal deficit would not decrease national saving
because of the concurrent increase in private saving.
National saving would rise in the long run
because the PRA savings would exceed the increased
fiscal deficits.

A mixed system would eliminate the need for
a future increase in the Social Security payroll
tax and would, therefore, avoid the political risk
that future taxpayers would be unwilling to raise
taxes to finance promised benefits. It could be
designed so that, despite the asset price uncertainty,
there would be little risk that the combined
benefits would be less than the currently
projected pay-as-you-go benefits. The remaining
asset-price risk could be substantially reduced
by guarantees that could be produced by
the private financial market.

VI. Medicare

Medicare, the federal health care program for
those over age 65, is more difficult to reform
than either unemployment insurance or Social
Security. The program is more complex and the
reaction to proposed changes is often more
emotional. And yet without reform, Medicare
costs will rise even more dramatically than the
cost of Social Security retirement benefits, reflecting
the increasing numbers of the very old
(who consume relatively more medical care)
and the changing medical technology that pro-
vides new opportunities to spend money to prolong
life and increase the quality of life.
Before looking at Medicare, it is useful to
consider the basic theory of health insurance
and the current way that the government provides
a kind of quasi-social insurance for the
population under age 65 by its favorable tax
treatment of employer payments for health insurance.
This is important in itself and suggests
an approach that may be useful for reforming
Medicare.

The basic theory of insurance implies that a
risk-averse individual will prefer an actuarially
fair insurance policy to an uncertain and exogenous
distribution of potential losses. But the
distribution of potential health spending is not
exogenous and the gain from risk reduction
must be balanced against the distorting effect of
insurance on the demand for care. As insurance
becomes more complete, the marginal gain
from additional risk reduction declines and the
marginal deadweight loss from distorting the
demand for health care rises. At the optimum
level of health insurance (e.g., at the optimum
coinsurance rate) there is a deadweight loss
caused by the distortion in the demand for care
because individuals, advised by their doctors,
make decisions about diagnosis and treatment
based on a net price of care that is very much
less than the cost of producing that care. That
level of insurance is, however, efficient because
the deadweight loss from distorted demand is
less than the gain from risk reduction.
In actual practice, the demand for health insurance
is greatly increased by the tax treatment
of excluding employer payments for health insurance
from the taxable income of employees.
Allowing employees to buy health insurance
with pretax dollars in this way changes the
nature of health insurance. For someone with a
marginal tax rate of 40 percent, the ability to
buy health insurance at a cost of only 60 cents
per dollar of premium substantially increases
the demand for health insurance with low deductibles
and low coinsurance rates. This increases
the deadweight loss caused by the
distortion in demand for care.
Direct attempts to eliminate or even reduce
the tax subsidy or to constrain it by requiring
minimum deductibles or coinsurance rates have
not been politically successful. When the Reagan
Administration proposed to limit the employers'
deduction for health insurance premiums,


### ---Economics-2005-0-22.txt---
it was unable to get any member of the
President's own party to introduce the legislation
in Congress.

Recently, however, Congress enacted legislation
to shift the incentives away from excessive
health insurance. The new health savings account
(HSA) rules, enacted as part of the 2003
Medicare legislation, allow individuals or their
employers to deposit up to $5,000 of pretax
income into a health savings account if they
have a health insurance policy with an equally
large deductible and with protection against catastrophic
expenses. The individual foregoes the
advantage of the tax-free income in the form of
the employer-paid premium, but gets an even
larger tax-free income in the form of the health
savings account contribution. Assets in these
IRA-like accounts earn tax-free investment income.
Funds not spent in one year automatically
carry forward to the future and can be used to
finance any kind of health care without paying
tax. Funds can also be withdrawn for any other
kind of spending by paying tax at that time.
The health savings accounts create a strong
incentive to choose policies with high deductibles
instead of the current comprehensive lowdeductible
and low coinsurance policies. This,
in turn, should change the nature of the demand
for care. Until individual health spending
reaches the deductible limit, money spent on
health care is the individual's own money and
not that of an insurance company. Spending
below the high deductible limit, therefore,
would not have any of the distortion caused by
current policies with low deductibles and low
coinsurance rates. And the requirement that the
policies provide protection for catastrophic levels
of spending means that the most important
form of protection is retained or increased at the
same time that the distortions are reduced.26
Of course, anyone who spends several days
in a hospital will exceed the deductible limit. At
that point, the insurance company is paying for
care as it would today and the patient and his
doctor no longer have the incentive to be cost
conscious. The favorable incentive effects of
the HSA could be increased without reducing
the individual's insurance protection by replacing
the deductible with a 50-percent coinsur-
ance rate on spending up to twice the level of
the HSA saving deposit. For example, the limit
associated with the $5,000 HSA deposit would
shift from a $5,000 deductible to a 50 percent
co-payment on the first $10,000 of care, causing
significantly more individuals and health spending
to be in the cost-conscious range.
The shift from the current tax-induced comprehensive
insurance to large deductibles or coinsurance
is not only a way to limit excessive
health care spending, i.e., spending that individuals
and their doctors recognize as less valuable
to them than the cost of production. It is also a
way of making health spending reflect each
individual's preferences. While all of us want
good health, the lifestyles choices that individuals
make show that some of us value it more
than others. Although we all understand the
adverse health effects of obesity, smoking, and
the lack of exercise, not everyone acts on this
information. Many people knowingly make the
tradeoff to enjoy more eating, to smoke, and to
avoid the rigors of exercise. Just as people make
different lifestyle choices, some are more willing
than others to sacrifice more of other consumption
to increase spending on health care.
Health savings accounts will allow this expression
of taste in health care spending instead of
effectively inducing almost everyone to purchase
high-cost health care.

Health savings accounts may be a model for
Medicare reform. If nothing is done, the cost of
Medicare to the federal government will rise
from 2.4 percent of GDP to about 6 percent by
2030 and 8 percent by 2050. The rising cost of
Medicare is similar to, but even more dramatic
than, the rising cost of Social Security retirement
benefits. The remedy for this problem
should have two components: changing the
spending incentives to slow the growth of
Medicare outlays, and using a mixed financing
system to raise the needed funds without the
sharp tax increase that would otherwise be
needed (Feldstein and Samwick, 1997; Feldstein,
1999b).

If health savings accounts are successful, the
high deductibles and coinsurance that will
evolve because of the HSAs for those under age
65 may establish a precedent that will also affect
future Medicare benefits and therefore the
spending incentives of the Medicare population.
A mixed financing system for Medicare
could combine a tax-financed Medicare annuity
26I discussed this type of health insurance reform in
Feldstein (1971) and Feldstein and Gruber (1995).


### ---Economics-2005-0-23.txt---
for retirees geared to the then-current cost of
health care plus an opportunity for individuals
during their working years to accumulate funds
in retirement health savings accounts. These
combined funds could be used at retirement to
pay for the type of health plan that the retiree
prefers: a comprehensive insurance plan of the
type that Medicare now provides; membership
in a health maintenance organization that provides
a managed care plan; or a lower cost plan
with substantial deductibles and coinsurance.
Alternatively, working age individuals could
use the funds contributed annually to their retirement
health savings account to purchase
health insurance for their retirement years,
thereby minimizing the problem of asymmetric
information in policy choice at retirement
that would occur in buying insurance after
retirement.

VII. Conclusion

The reform of social insurance is clearly a
work in progress in the United States and in
other countries as well. Policymakers can do
much to improve the major social insurance
programs that protect the unemployed, the aged,
and the ill. Economists can contribute to this
process by improving our understanding of the
effect of social insurance rules and by deriving
new program designs. In this paper I have emphasized
the use of personal investment-based
accounts created and regulated by the government
and earmarked for unemployment benefits,
for retirement income, and for health care
during retirement. Such accounts have the potential
to provide a better tradeoff of increased
protection and reduced distortion. They also
give individuals greater discretion in tailoring
benefits to their own tastes.

I am an optimist about economic policy. I
have examined what is wrong with our current
social insurance programs and what could be
done to improve them in the future. I believe
that the policy process does evolve and that
economists have contributed to that evolution.
We see that in the important reforms of the past
two decades that I have described. But there is
still much for economists to do in designing
better policies for the future and in educating
the public and the political decision makers
about the desirability of making such changes.
 ## Economics-2006-0


### ---Economics-2006-0-03.txt---
By DANIEL MCFADDEN*

You cannot simply tell a person in dire
need, wait for the market to take care of
you. That is a most callous thing to say,
and only makes a person feel owned, and
with no control over his life.
Letter to the Editor,

New York Times, 2005

[I]t is not enough to simply liberate people
and assume that they will automatically
pursue economic prosperity. People
need to be instilled with certain beliefs,
like the belief that ... individuals have the
power to shape their own destiny. ... It's
important to understand the beliefs that
encourage people to work hard and grow
rich.

David Brooks,

New York Times, 2005

I. Consumers and Markets

Economic theories and ideologies are
founded on the principle that consumers have
well-defined preferences, and consistently behave
to advance their self-interest. Jeremy
Bentham (1789) said, "My notion of man is
that ... he aims at happiness ... in every thing he
does." Herbert Simon (1957) said, "The rational
man of economics is a maximizer, who will
settle for nothing less than the best." Some
economists have even taken self-interest to explain
choice tautologically:

t Presidential Address delivered at the one hundred seventeenth
meeting of the American Economic Association,
January 7, 2006, Boston, MA.

* Department of Economics, University of California,
Berkeley, 549 Evans Hall, #3880, Berkeley, CA 94720
(e-mail: mcfadden@econ.berkeley.edu). This paper was
prepared with the support of the National Institute of Aging
and the E. Morris Cox Endowment, University of Califor-
nia, Berkeley. I thank particularly Joachim Winter, and also
Sam Bowles, Frank Caro, Colin Camerer, Ernst Fehr, Flo-
rian Heiss, Dana Goldman, Elizabeth Goldstein, Arie
Kapteyn, Alan Krueger, Byunghill Jun, Rosa Matzkin, Ellen
Peters, Richard Suzman, David Weir, Robert Willis, and
David Wise for comments and suggestions.
An article can have no value unless it has
utility. No one will give anything for an
article unless it yield him satisfaction.
Doubtless people are sometimes foolish,
and buy things, as children do, to please a
moment's fancy; but at least they think at
the moment that there is a wish to be
gratified. Doubtless, too, people often buy
things which, though yielding pleasure for
the moment, or postponing pain, are in the
end harmful. But here ... we must accept
the consumer as the final judge. The fact
that he is willing to give up something in
order to procure an article proves once for
all that for him it has utility,-it fills a
want.

Frank Taussig, 1912

Consumers who know their own tastes, and
are relentlessly self-interested and self-reliant,
relish choice, and welcome market opportunities
that expand their options. Most economists
accept this concept of the consumer, and the
attendant economic theory that demonstrates
the efficiency and Pareto optimality of decentralized,
competitive markets. Over the past 30
years in the United States and elsewhere, these
market-oriented views have driven economic
policy, leading to deregulation of air and truck
transportation, telecommunications, and energy
markets; establishment of property rights and
markets to manage environmental externalities;
and globalization of international markets for
goods, capital, and services. Notable successes
were the deregulation of truck and air transportation,
and of telecommunications, where dysfunctional
regulation worked at cross-purposes
to competition. Another success was making
air pollution a property right, allowing Coasian
markets to internalize environmental externalities.
There have also been striking failures,
such as the breakdown of the incompletely deregulated
energy market in California a few
years ago, the rail transport deregulation in
Great Britain which got wrong the incentives
for track maintenance, and the British system
of private retirement accounts which allowed
5


### ---Economics-2006-0-04.txt---
excessive fees and overselling. However, the
sweep of decentralization and privatization is, I
believe, widely viewed by economists as an
almost universal success, with the failures
due to correctable flaws in market design. Romantics
of the economic right would carry the
concepts of self-interested consumers and
free markets even further, embracing a withering
of authority and a nirvana of Hayekian
self-reliance.

Most reasoned discussions of privatization
among economists concentrate on information
asymmetries, incentives, economies of scale
and scope, risk management, and the relative
efficiency and sustainability of alternative
forms of market organization. There are serious
economic questions as to whether, for example,
the technologies of network industries inevitably
lead to concentration, with an attendant loss
of choice and efficiency. There are serious questions
as to whether adverse selection will defeat
the efficiency gains from competition in multiple-
payer privatized insurance markets. It is a
worthy scientific enterprise to study these issues,
and look to the historical record of privatization
for answers, but not one that I will take
up in this paper. I will concentrate, instead, on
the decision-making of consumers, the market
outcomes they achieve as a result, and the influence
of these outcomes on their attitudes
toward markets.

In the general public we see widespread unease
about market solutions. Free trade and
globalization, privatization of social insurance,
and deregulation of energy markets all elicit
opposition from many consumers, sometimes
reasoned but often inchoate. It is no coincidence
that support for market solutions is concentrated
among the economically successful, and opposition
among the less successful. Free choice
has moral appeal, but moral fiber is strongest
when not cut by self-interest. Market mechanisms
have to compete for votes with alternative
resource allocation schemes more favorable to
the underdogs; and in this competition, fairness
to me is my primary concern, efficiency is
someone else's problem. In addition, there is
ideological opposition to market solutions. In
the liberal orthodoxy, markets are dominated by
the powerful and rapacious, and the motives of
government bureaucrats are purer than those of
private bureaucrats. In this ideology, the process
of privatization often serves the interests of the
politically connected. The Enrons and Haliburtons
of this world reinforce these views. However,
ideologies themselves are woven from
human sentiments, and antipathy to market solutions
is more than just doctrine.

My concern in this paper is that it is not
enough to find ways to handle information and
technology issues in privatization if consumers
are not up to the task of functioning satisfactorily
in such markets. The argument is not that
consumers should be coddled; they may need to
see the stick to get the incentives for selfreliance
right. However, the efficiency and stability
of an economy requires that all consumers
be part of the franchise, in reality and in perception,
so that good economic policies, including
privatization and free markets when they
make sense, receive broad support. I will discuss
these issues at two levels. First, I will give
a selective review of the behavioral evidence on
consumer decision-making, and how this influences
market outcomes and attitudes toward
markets. Second, I will summarize results that
my research group has obtained on a current,
concrete privatization issue, the new Medicare
Part D prescription drug program, which is offering
market choices within a social insurance
program. I will ask whether consumers are, in
fact, able to manage their choices adequately in
this new market, and whether they will, in fact,
gain from the added choice offered by privatization.
The following fundamental questions,
explored in pioneering papers by James J. Choi
et al. (2003) and Richard H. Thaler and Cass R.
Sunstein (2003), comprise an important scientific
agenda:

* Are consumers sufficiently consistent in advancing
their self-interest in specific markets
to achieve the levels of efficiency and wellbeing
that privatization promises?

* What can be done as part of the design of
privatization, such as information, instruction,
and support structures, to help consumers
satisfactorily pursue their self-interest?
* When privatization is in consumers' selfinterest,
how can they be enlightened and
convinced to support the change?


### ---Economics-2006-0-05.txt---
II. The Challenge of Choice

Agoraphobia (ayopa` + 6f3o(, literally
"fear of the marketplace") Fear of leaving
a safe place, fear of being in situations
from which escape might be difficult or
embarrassing; fear of losing control in a
public place such as a restaurant or shopping
mall.

Psychology Today

Studies of consumer perceptions, motivations,
and behavior give a complex picture of
self-interest and the determinants of well-being.
Consumers often find choice overwhelming,
and decision-making uncomfortable. In the
words of a Dutch proverb, "He who has choice
has trouble." We routinely use procrastination,
precommitments, habit, imitation, social norms,
defaults, and superstitions to avoid confronting
choice. We pass up trading opportunities, particularly
in unfamiliar situations. We are suspicious
of trading partners, and fearful of
deception, exploitation, or unfair treatment. In
short, we exhibit various degrees of agoraphobia,
a term that means literally "fear of the
marketplace," adapted by psychiatrists to mean
fear of leaving a safe place for a situation from
which it might be difficult or embarrassing to
escape. Reflect on the major decisions in your
own lives-choice of college, occupation, car,
house, and spouse-and in most cases you will
feel you made the right choice, but will recall
the choice process itself as an emotional, stressful
experience.

By rational calculation and accumulated experience,
we benefit from choice. Then, why do
consumers fear markets and find choice troubling?
First, there is market risk. Forget the
antiseptic, well-lighted budget sets and markets
of economics textbooks. Real-life markets are
rough, murky, tumultuous places where commodity
attributes shift, supply is uncertain,
prices are volatile, and information is imperfect.
Caveat emptor prevails, and caution and calculation
are vital. The sure-footed may thrive, but
their success may come in part from the failures
of the less experienced and nimble. Second,
there are personal risks, including the risk of
misperception and miscalculation, of misunderstanding
the available alternatives, of misread-
ing one's own tastes, of yielding to a moment's
whim and regretting it afterward. Finally, there
is social risk, the interactions between people
that trade requires; the stress of information
acquisition, search, and bargaining; the stress of
dealing with pushy or deceptive sales tactics;
and the risk of being embarrassed or defrauded.
How do consumers deal with these risks?
And what is it about these risks that leads to
broad biases against market-based resource allocation?
Perhaps such inference is rooted in
human psychology. Consumers often have the
perceptual illusion that other freeway lanes or
supermarket lines move faster than their own,
because the occasions on which this occurs are
particularly noticeable and irritating. Similarly,
they may have the perceptual illusion that they
are particularly unlucky, or subject to discrimination
and exploitation in markets, because
their bad experiences stand out. Markets that
work well for you are invisible, those that don't
are a source of frustration and grief.
III. The Consumer's Mind

What if everything is an illusion and nothing
exists? In that case, I definitely overpaid
for my carpet.

Woody Allen

To understand how consumers deal with market,
personal, and social risks, it is useful to
study how they think, and the social context of
thought and trade. While the mutual benefit of
trade is the aspect emphasized in economics,
trade is also a contest, with the issues, emotions,
and stresses that competitions entail: Is the playing
field level and the referee fair? Will my
opponent play by the rules? Can I match her
knowledge and skills? The competition itself,
not just the outcome, becomes a source of pleasure
or pain. Trade is part of the way that
humans as social animals define and defend
themselves, a process that is both cognitive and
visceral.

Mind and trade are linked in human prehistory.
I relate an evolutionary tale, adapted from
Matthew Ridley's book The Origins of Virtue.
A few million years ago, the great apes established
family groups that were successful in the
essentials-obtaining food, protecting themselves


### ---Economics-2006-0-06.txt---
from predators, and reproducing. In common
with other animals, they evolved a sense of
personal space sufficient to provide some defense
against attack, and a system of trust and
reputation that allowed them to suspend their
"fight or flee" defenses and live together with
family members. These spatial social interactions
had a physiological basis-reward pathways
in the brain and neurotransmitters that
facilitated social contact, reciprocity, and mutual
aid. Some of these apes discovered that
through division of labor, specialization, and
trade, they could be more productive and fertile,
and live better and longer. But trade, particularly
outside the family group, was risky business.
To get close enough to a stranger to trade
flints for furs, one had to risk being attacked.
The most successful apes dealt with this by
developing the ability to form bonds of trust
over larger social groups than the family. This
was accomplished by adapting the brain's visceral
reward pathways that already allowed
family units to function. Second, these apes
developed analytic, social, and communication
skills that allowed them to operate in larger
social and economic groups. These were cerebral
activities, and evolution selected species
with more cerebral capacity. Among these apes
were our ancestors. They gave us large brains,
with the capacity to explore the corners of our
universe, and to engage in sophisticated economic
activities. They also gave us an emotional
reward system that processes economic
actions in much the same visceral way that it
processes personal interactions: when to approach
and when to avoid, whom to trust, and
when to form personal or professional bonds.
The evolutionary tale I have just told is speculation,
based on observations of contemporary
apes and other animals, and fossil records.
However, the role of trust and reward pathways
in the brain, and how they affect economic
conduct, is something that we can investigate
experimentally, using the tools of brain science
and the new discipline of neuroeconomics to
study the processing of economic choice problems
at a physiological level. Brain measurements
include maps of energy consumption,
observed under experimental treatments that alter
electrochemistry and cognitive task. These
measurements fall short of Edgeworth's wistful
call in 1881 for a hedinometer to record pleasure,
but they provide some insight into the
sensations that economists call utility.
The early biologists observed that as the human
embryo developed, it seemed to go through
stages of evolution, from a simple one-celled
creature to its complex final form. That view
was superficial, but it does seem to be the case
that human physiology, and in particular the
structure of the brain, is consistent with a layering
of added functionality over a simpler and
more primitive core. The aspects of brain function
that we identify with being human-language,
the cognitive processes of deduction and
induction, the ability to empathize and interact
with others-are primarily sited in the frontal
lobe of the cerebrum, the outer layer of the brain
whose relative size and complexity in humans
differentiate us from most other species. The
more primitive limbic system, buried at the base
of the cerebrum, is heavily involved in emotion
and the reward pathways associated with sensations
of pain and pleasure. The limbic system
is active in animal behavior at a visceral level:
approach and avoidance, foraging, territory, and
reproduction. The electrochemistry of the limbic
system is similar in all animals, and on
the evolutionary scale clearly predates human
development.

Most people think of economic activity as
quite cerebral, learned through lengthy education
and shaped by culture. If the brain is the
hardware, then the utilitarian calculus might be
pictured as software, an operating system that is
stored and run at various, possibly relocatable,
hardware sites, and is modified, Linux-like, by
experience and selection. In this view, monitoring
the brain can tell you something about the
burden the software places on the hardware, but
relatively little about what the software is doing.
The picture that is now emerging, however, is
that economic behavior, like the brain itself, has
layers. Working a spreadsheet to balance a retirement
portfolio is indeed a high-level, learned
skill. Economic trading, however, also seems to
involve relatively primitive circuits in the limbic
system. Therefore, you should not be surprised
to learn that brain hardware is associated
with economic decisions in a substantial and
relatively direct way. Specifically, the limbic
system and its reward pathways qualify as the


### ---Economics-2006-0-07.txt---
brain's primary center for recording pleasure,
and are active when we are involved in matters
of threat, trust, sex, and economic trade.' If you
have ever dismayed over convincing students
that economics is a sexy subject, you can now
tell them that shopping and sex share the same
neurotransmitters and receptors.
The linkages from physiological sensation to
conscious interpretation and reasoning may be
complex, and physiology alone may give an
incomplete picture, just as computer hardware
monitoring gives an incomplete picture of what
software is doing. Nevertheless, it should be
clear than any ability to measure directly in the
brain the impact of economic choice tasks on
reward pathways is potentially an immensely
powerful tool for linking economic activities
and consumer well-being.

How do organisms process sensations of
pleasure and pain? The answer goes directly to
the question of whether there is a single, absolute
physiological scale of well-being or utility,
and whether the organism consciously or unconsciously
acts out of self-interest to maximize
this quantity. First, both behavioral observation
and brain studies indicate that organisms seem
to be on a hedonic treadmill, quickly habituating
to homeostasis, and experiencing pleasure
from gains and pain from losses relative to the
reference point that homeostasis defines (see
Sanfay et al., 2003). People quickly grow to
accept the city in which they are located, their
job, their mate, and their health status. They
may recognize and complain about unfavorable
absolute states, but their levels of satisfaction by
various measures are not nearly as differentiated
as they would have to be if their sensation of
well-being were experienced on an absolute scale.
Second, the picture that emerges from brain
studies is that the dopamine reward pathways in
the limbic region play a central role in experi-
1See Limo R. Becerra et al. (1999), Kent C. Berridge
(2003), Meghana Bhatt and Colin F. Camerer (2005), Mi-
chael A. Bozarath (1994), Camerer (2003), Antonio
Damasio (2005), John Dickhaut et al. (2003), Ernst Fehr et
al. (2005), de Quervain et al. (2004), Paul W. Glimcher et
al. (2005), David Laibson (2005), Kevin McCabe et al.
(2001), Samuel M. McClure et al. (2004), Michael Kosfield
et al. (2005), Aldo Rustichini et al. (2003), Alan G. Sanfey
et al. (2003), and Fehr and Tania Singer (2005).
encing pleasure, and also mitigate, with a lag,
the sensation of pain (see Becerra et al., 1999;
McClure et al., 2004). Adaptation to homeostasis
and differentiation between the pleasure and
pain circuits coincide with the powerful endowment
and loss aversion effects, and sensitivity to
framing and context, found in behavioral studies,
and suggest that these phenomena are tied
fundamentally to brain structure. This is good
news and bad news for utilitarians: the limbic
system reward pathways record pleasure and
pain on what seems to be close to a utilitarian
scale, but brain circuitry processes experience
in ways that are not necessarily consistent with
relentless maximization of hedonic sensation.
One of the interesting bits of contemporary
biology has been the establishment for a variety
of species of simple direct links from particular
genes to the production of, and receptors for,
specific neurotransmitters, and from this to specific
social behavior. One peptide, oxytocin, is
particularly involved in bonding and trust between
animals, most notably between parents
and their offspring. This is relevant to economics
because, in the words of Kenneth Arrow,
"every commercial transaction involves an element
of trust." In a study that strikes at the heart
of consumer sovereignty, Fehr et al. (2005) and
Michael Kosfield et al. (2005) administer oxytocin
or a placebo to subjects, and then ask them
to play the trust game. In this game, an investor
is given 100 MU. She has the option of placing
Y MU with an anonymous trustee, who through
the experimenter receives triple this amount.
The trustee then volunteers to send Z MU back
to the investor. The trustee's subgame is a dictator
game in which norms of fairness and reputation
matter, but the rational response in a
single-shot anonymous game is to return nothing.
By backward induction, the investor should
send nothing. In fact, both the investment and
the return are usually positive, with the level of
investment higher in subjects who are administered
the "trust" peptide oxytocin. Oxytocin has
no effect, however, on play of the dictator subgame,
where trust does not matter. The conclusion
is that economic perceptions and decisions
are sensitive to brain chemistry, and susceptible
to chemical manipulation.

Neuroeconomics is a new subject, and the
future will determine its potential and limits for


### ---Economics-2006-0-08.txt---
understanding economic choice behavior. It already
seems to confirm and explain, however,
that brain structure and chemistry are behind
some systematic anomalies in economic behavior,
particularly failures to form perceptions and
pursue self-interest consistently when confronted
with choices involving remote, uncertain,
or ambiguous outcomes, failures to recall
or anticipate in full color the sensations that
outcomes produce, and the quick adaptation to
circumstance, the hedonic treadmill.
IV. Personal Risk

What information consumes is rather obvious:
it consumes the attention of its
recipients. Hence a wealth of information
creates a poverty of attention, and a need
to allocate that attention efficiently among
the overabundance of information sources
that might consume it.

Herbert Simon, 1971

A large literature from behavioral economics
and psychology finds that people
often make inconsistent choices, fail to
learn from experience, exhibit reluctance
to trade, base their own satisfaction on
how their situation compares with others',
and in other ways depart from the standard
model of the rational economic
agent. If people display bounded rationality
when it comes to maximizing

utility, then their choices do not necessarily
reflect their "true" preferences,
and an exclusive reliance on choices to
infer what people desire loses some of
its appeal.

Daniel Kahneman and

Alan Krueger, forthcoming

The biological evidence that the human brain
is complex and layered, more an imperfect
meeting of minds than an optimizing computer,
follows and supports behavioral evidence from
cognitive psychology and experimental economics
showing that humans are, well, all too
human in the ways they retrieve and evaluate
information, and process decisions.2 In over-
view, these studies suggest that homo economicus-
sovereign in tastes, steely-eyed and
point-on in perception of risk, and relentless in
maximization of happiness-is a rare species.
While consumer behavior in familiar market
settings may have these characteristics, when
we approach the consumer from a different angle,
asking direct and unusual questions about
beliefs or values, or offering novel products and
services, we find alarming variations from the
story of consistent advancement of self-interest.
All these apparently normal consumers are revealed
to be shells filled with heuristics that
have been shaped by evolutionary selection and
experience. These heuristics often work. For
example, two of my rules which seem successful
are: "Never buy a Rolex from a street
vender" and "Never accept an e-mail offer to
transfer millions of dollars to my bank account.
" However, throw the consumer a curve
ball, in the form of a question that fails to fit a
standard heuristic for market response, and the
essential "irrationality" of the organism is revealed.
For most economists, this is the plot line
for "Stepford Consumers," a real horror movie.
Even if this bleak portrayal is true, however, it
does not mean that policy conclusions based on
consumer rationality are wrong, only that the
consumer may need to be coaxed and wheedled
into responding to market choices with sufficient
diligence to approximate rational promo-
tion of self-interest.

Most of the evidence on consumer decisionmaking
comes from laboratory experiments.
Economists reviewing the experimental evidence
sometimes comment that markets punish
inconsistencies, and consumers learn to avoid
them. They then conclude that while these flaws
may appear in experiments, they are not important
for economic behavior. This may be true in
repeated, familiar market settings where the
conduct and rewards of others provide good


### ---Economics-2006-0-09.txt---
TABLE 1-FACTUAL AND AFFECTIVE MEMORY
Effect Description

Affective attenuation Affective memories are recalled/anticipated with diminished intensity
Availability Memory reconstruction is tilted toward the most available and salient information
Primacy/recency Initial and recent experiences are the most available
Reconstructed memory Imperfect memories are rebuilt using contemporary cues and context, historical exemplars,
customary search protocols

Selective memory Coincidences are more available than noncoincidences
Subjective time History is compressed and attenuated, duration neglected
examples. Some consumers are slow learners,
however, and many markets are inconsistent
teachers, providing more irritation than illumination,
giving random awards and punishments
that consumers cannot always translate into accurate
road maps for successful behavior. Even
if consumers do learn from experience, remember
P. T. Barnum's comment that "there is a
fool born every minute," additional mugs for
the market game. Importantly, the sting of market
punishment breeds agoraphobia. Just as
children humiliated in the classroom may be
turned off rather than educated, consumers humiliated
in the marketplace may develop an
aversion to markets, where opportunities for
choice may be interpreted as opportunities for
mistakes, embarrassment, and regret.
A. Memory and Perceptions

There are now extensive experiments and
insights from cognitive psychology showing
that memory is imperfect and perceptions are
often biased and statistically flawed (for detailed
surveys see Matthew Rabin, 1998; McFadden,
1999). Consider, first, factual and
affective memory. Our memories guide our perceptions
of alternatives and our preferences, and
imperfections in remembering facts and sensations
can distort our perspective, leading to inconsistent
behavior and disappointment. Table
1 summarizes some of these effects; I will comment
on how they can lead to suboptimal market
outcomes.

What we store and retrieve from memory
is affected by mood and emotion. Laura
Carstensen (Susan M. Charles et al., 2003;
James J. Gross et al., 1997) finds that advertisements
are remembered better, and influence
choice more, when the affective content of the
ad matches the mood of the consumer. George
Loewenstein (1996) finds that emotional sensations
are more easily remembered than nonemotional
ones, but emotions themselves are
difficult to retrieve from memory-we remember
experiencing episodes of pleasure or pain,
and these memories can powerfully condition
our behavior-"once burned, twice shy"-but
we fortunately cannot relive the experiences in
their original intensity.

Finding and retrieving information from
memory is a complex cognitive task. The answer
may be on the tip of your tongue, but
sometimes the tip of your tongue is hard to find.
We use contemporary cues to guide memory
search, and to fill in and bluff when memory
fails. Consequently, what we remember is influenced
substantially by current context and
mood, and these are vulnerable to manipulation
in the presentation of choice alternatives.
Selective memory is the phenomenon in
which we remember what draws our attention.
Coincidences stick in our minds, noncoincidences
are forgotten. This influences probability
judgments. A good example is the belief in the
"hot hand" in athletics, the idea that players can
get in the groove for some period of time and
play consistently above their game. Objectively,
the hot-hand phenomenon does not exist-the
observed distribution of runs of success is consistent
with independent Bernoulli trials, not
with heterogeneous spurts and slumps. The explanation
is that long runs are coincidences that
are selectively remembered. One of the implications
of selective memory for market behavior
is that people build up elaborate and
complex beliefs about causal relationships between
events, taking natural events personally,


### ---Economics-2006-0-10.txt---
and persuading themselves that they are systematically
lucky or unlucky in handling market
risk.

Another important memory effect is subjective
time. You all know the canard, "Time flies
when you are having fun." We have trouble
keeping time scales straight in our memories.
We telescope time, so past events seem more
recent than they actually were. We are unsuccessful
in integrating sensation over time. In a
phenomenon studied by Daniel Kahneman,
Alan Krueger, and others (Kahneman and
Krueger, forthcoming; Donald C. Redelmeier
and Kahneman, 1996), episodes of pleasure or
pain are remembered selectively in terms of
peak and most recent sensation. This can lead
consumers to choices that "remember" better
than they "experience." There is a relationship
between subjective time and brain structure--
current sensation is recorded in the limbic system
and its reward pathways, memory of past
and anticipation of future sensations are processed
in the cerebrum, more analytic and less
colorful. David Laibson and colleagues have
studied this as the physiological explanation for
hyperbolic discounting (Fehr, 2001; Laibson,
2005; Laibson et al., 2005). A final comment is
that subjective time is not a new element in
explaining consumers' sensations and behavior.
Francis Y. Edgeworth (1881) proposed, following
William S. Jevons (1871), that the same
objective time may correspond to different rates
of thought and feeling in different periods, so that
the utility of an experience will be the subjective
time integral of the sensations involved.
Perceptions and beliefs are influenced by the
way we process information (see Table 2).
Memory plays a role, e.g., selective memory is
implicated in regression and representativeness
effects. We overemphasize recent, available experience
in forming beliefs, and depend heavily
on readily available cues to construct our perceptions
when we need them to make choices.
In experiments, consumers are often influenced
by the context and framing of perceptual
tasks and choices, and anchor their perceptions
to cues contained in the choice task. Anchoring
affects statements of willingness to pay (WTP)
for public goods obtained by direct elicitation
when consumers have incompletely articulated
tastes for these goods (see Green et al., 1998).
In addition, anchoring distorts responses to factual
questions in surveys. Beyond this, why
should economists be interested? The answer is
that anchoring effects appear clearly in market
transactions involving complex commodities.
For example, houses and automobiles are typically
sold by bargaining, starting from an initial
listing price or manufacturer's suggested retail
price. Field experiments with real estate agents
show that manipulation of initial offers can influence
bargaining outcomes. A study by Itamar
Simonson and Amos Tversky (1992) finds that
when products are positioned so that one appears
to be a bargain, a form of anchoring, then
consumers will flock to the apparent bargain
alternative. When I told a friend who owns a
Boston seafood restaurant that he could use this
result to reposition his wine list and increase his
profits, his response was "tell me something I
didn't learn in hotel school."
Anchoring is one example of how consumers
may be influenced by context and framing that
should be irrelevant to choice. A second important
example is the endowment effect, also
called a reference point or status quo effect, in


### ---Economics-2006-0-11.txt---
Median

O1 100

90

80

70

60

50

40

30

20

10

0 Ask

Bid

0 10 20 30 40 50 60 70 80 90 100
Quantity

FIGURE 1. PENCIL EXPERIMENT OFFERS
which consumers show a reluctance to trade
away from any position in which they are established.
The endowment effect appears in
stated preference studies, where WTP for an
increased amount of a commodity is typically
far less than willingness to accept (WTA) a
reduced amount of the commodity. Some gap is
expected, due to diminishing marginal utility,
but experiments show gaps far too large to be
explained by classical income and substitution
effects. For example, a study by McFadden et
al. (1988) of stated WTP for changes in reliability
of electricity supply found that mean stated
WTP for a change between two levels, neither
of them the status quo, was valued consistently
by consumers independently of their status quo,
but in comparisons between the status quo and
any alternative, the status quo was given extra
value, independent of its level. It appears that
the hedonic treadmill is at work, with people
habituating to their current state, and viewing
changes with distaste.

A dramatic illustration of the endowment effect
is the now-classic cup experiment of Jack
L. Knetsch (1989), in which a random assignment
of coffee cups in a class, followed by an
opportunity to trade, produced a large gap between
WTP and WTA, with far less trading than
should be needed to move from a random allocation
to a Pareto optimal one (see also Kahneman
et al., 1990). I repeated this experiment in
an introductory microeconomics course at
Berkeley, using pencils embossed with the
course name. About half of the 345 students,
172, were randomly assigned a pencil. Then, a
Vickery sealed-bid uniform-price double auction
was held to reallocate the pencils (see Kiho
Yoon, 2005). In this auction, each bidder has an
incentive to report her true value, independently
of the strategies of others. The income effect of
being endowed with a pencil is negligible, so
that with random assignment the distributions
of money marginal utilities of a pencil should be
the same for buyers and sellers. Then if consumers
are neoclassically rational, there should
be no endowment effect.

Consider a market with N participants with
values v, > "> VN, and K randomly allocated


### ---Economics-2006-0-12.txt---
pencils. In the incentive-compatible Vickery
double auction, successful buyers pay vK+ 1,
and successful sellers receive vK, with the market
operator covering the difference. The number
of pencils J initially allocated to the K
highest value participants has a binomial distribution,
b(K, KIN). The volume in the efficient
auction is then K - J, which has mean K(N -
K)IN and variance K2(N - K)/N2.
In the experiment, the expected volume is
86.25, with a standard deviation of 6.56. The
actual market-clearing price was vK+ 1 = K =
35, and the number of market-clearing transactions
was 32. Under the hypothesis of no endowment
effect, the probability of 32 or fewer
transactions is on the order of 10-16. The median
offer to buy was 10 cents and the median
offer to sell was 100 cents. A runs test confirms
(T = 12.5) that buyers and sellers do not have
the same value distribution. Thus, there is a
strong, trade-suppressing endowment effect,
generated instantaneously by a random allocation
of pencils. Either tastes are changing endogenously,
with quick habituation to the status
quo, or agoraphobia is real-consumers find
trade an edgy experience, instinctively mistrust
the market, and resist trading for small gains.
Consumer preferences among risky prospects-
lotteries-show a number of behavioral
anomalies that appear to be related to the endowment
effect. In summary, consumers appear
to evaluate lotteries as changes from a reference
point that may be sensitive to framing, and to
exhibit asymmetric loss aversion in which
losses loom larger than gains, with consumers
displaying risk aversion for gains and risk
seeking for losses, a certainty effect in which
there is a pure preference for sure things over
lotteries, and a prospect effect in which the
probabilities of low-probability events are overestimated.
One of the consequences of these
effects is that consumers will often refuse to
take any share of either side of an offered lottery,
a result consistent with the observed paucity
of real-world wagers. An additional reason
that individuals are suspicious of lotteries, and
often avoid them, is the superstitious belief that
there are hidden causal forces at work, interventions
that place the lottery in ambagious relationship
to the rest of life.

There is experimental evidence that endowment
effects are attenuated when traders are
experienced (see Mikhail Myagkov and Plott,
1997; John A. List, 2004). Thus, the observed
paucity of trades in lotteries may occur primarily
for novel events and inexperienced traders.
These facts are consistent with a proposition
that learning by observing and by doing may be
effective in selecting rational market behavior
rules in arenas with sufficient repetitiveness to
allow these effects to operate.
B. Calculation and Processing

The ideal rational consumer has the computational
power to value complex commodities
and consistently handle risk, discounting, and
option calculations, and the logical clarity to
work through the consequences of decisions
and optimize choices. In practice, both computational
and logical skills are limited. This may
be inconsequential for repeated short-lived
choices, such as picking out your breakfast cereal
or deciding when to change lanes, but these
limitations become critical for unfamiliar, not
easily reversed choices, such as occupation, job
change, house, automobile, children. The deficiencies
are most severe when choice involves
small, ambagious risks in the distant future, as
in the case of smoking and other addictive activities,
a perfect storm in which distortion of
perceptions of time, risk, and affect combine
with difficult computations of options and contingencies.
Table 3 lists some of the effects that
impede accurate processing and maximization
of preferences.

A first limitation is that we miss many choice
opportunities, and are barely conscious of others
we make almost automatically. Driving an
automobile is an example. We may ignore opportunities
to change lanes or pass, or may
decide to do so without conscious thought. Such
decisions are usually sensible; we develop habits
that work well and save scarce attention
time. They may not, however, be optimal. In
particular, lack of attention may lead to procrastination
and default choices that are, after the
fact, clearly not optimal.

I think it is remarkable on balance how well
most people function in markets, even people
with little academic aptitude. This may be beTABLE


### ---Economics-2006-0-13.txt---
3-DECISION CALCULATION AND PROCESSING DESCRIPTION
Effect Description

Awareness Recognition of choices, subjective definition of choice set
Construal/constructive Cognitive task misconstrued, preferences constructed endogenously
Disjunction Failure to reason through or accept the logical consequences of choices
Engagement Limited attention to and engagement in the cognitive task
Innumeracy Limited capacity to "run the numbers"
Suspicion/superstition Mistrust of offers and questioning of motives of others in unfamiliar situations; avoidance
of choices that "tempt fate"

cause we are adapted to trade, and because we
are good at copying successful behavior. Nevertheless,
such processing deficiencies as disjunction
and innumeracy do confuse choice.
Ellen Peters at Decision Research studies the
ability of people to understand and logically
relate numbers, an essential skill in trading that
involves prices or barter terms, or more complex
valuations requiring risk assessment or discounting.
Even if individuals do not consciously
"run the numbers" to determine choices, they
still have to form perceptions and make judgments
based on numerical information. The behavioral
evidence is that innumeracy rates are
high and significantly distort decisions. Peters
and her coauthors (Peters et al., forthcoming)
find that half the population is unable to read
and make sense of numbers in the newspaper.
Among those who score badly on a battery that
measures basic numerical and logical skills, one
finds errors such as altering ratings of risk and
choices when probabilities are presented as
number of successes out of a hundred, number
of failures out of a hundred, or as percent successes.
In one telling experiment, subjects are
offered a prize if they draw a red jellybean from
their choice of bowls. Bowl A contains 9 red
and 91 white beans, while bowl B contains 1 red
and 9 white beans, so the odds of success are
objectively better with bowl B. Nevertheless,
subjects who score low in numeracy often
choose bowl A because it "gives more chances
to win."

One could be hard-nosed about such people
and say that if they have not educated themselves
sufficiently to look after their own interests
in markets, the consequences are on their
shoulders. The economically unsuccessful can
vote, however, and they demonstrably have
used the vote at various times and places to pick
bad governments and bad economic policies.
The argument against "sink or swim" is that
when designing market mechanisms, it is in
society's interest to take a protective interest in
this segment of the population, building in information
and decision-making aids, and protection
from market wolves, which give these
people a chance of success, thereby increasing
the fairness of these mechanisms and support
for them. This argument becomes stronger
when one considers the sociality of choice, and
observes that there is more than "self' in
self-interest.

V. Social Risk

In risk perception, humans act less as
individuals and more as social beings who
have internalized social pressures and delegated
their decision-making processes to
institutions. They manage as well as they
do, without knowing the risks they face,
by following social rules on what to ignore.


Mary Douglas and

Aaron Wildavsky, 1982

Man is a social animal, identified with family
and kin, and with troops, tribes, clubs, ethnicities,
and nationalities. This has several consequences
for economic choice behavior. First,
individuals may look to their social networks
for information. Second, they may look to social
networks for approval, and use social accountability
to limit choice. Social norms can be
comforting, limiting options and regrets, but
they can also lead to embarrassment, ostracism,


### ---Economics-2006-0-14.txt---
and agoraphobia. Third, consumers may, out of
pure self-interest, engage in mutually beneficial
reciprocity, simple when the acts are synchronous,
involving more complex elements of reputation
and trust when they are not. Pursuing
comparative advantage, with division of labor
and trade, is a form of reciprocity. Fourth, they
may engage in genetic altruism, making choices
that are in the interest of their progeny rather
than themselves as individuals. Fifth, they
may exhibit altruistic behavior that does not
obviously serve their personal or genetic selfinterest,
such as incurring costs to sanction
greedy behavior.

A. Information

One major way sociality works is through
transmission of information, learning by imitation
rather than learning by doing. People
constantly make interpersonal comparisons,
judging the desirability of options from the apparent
satisfaction and advice of others. While
personal experience is the proximate determinant
of the utility of familiar objects, and may
be extrapolated to similar objects, our primary
sources of information on new objects come
from others, through observation, advice, and
association. McFadden and Kenneth E. Train
(1996) show that in innovation games with uncertain
payoffs, it may pay to wait, and learn by
observing rather than learn by doing. Charles F.
Manski (1991) has explored the possibility that
individuals faced with dynamic stochastic decision
problems that pose immense computational
challenges may simply look to others to infer
valuation functions to be used to judge the future
payoff of current acts, or to infer satisfactory
policies. An objection to such copycat
behavior is that it fails to take account of the
individual's idiosyncratic tastes, and correcting
this quickly gets the individual back into
the computational difficulties that imitation
was intended to circumvent. But if tastes as well
as perceptions are modified socially, the relevance
and value of the lessons from others
increases.

Economic demographer Hans Peter Kohler
(2001) has investigated the effect of word-ofmouth
communication from friends on choice
of contraceptive. He studies Korean peasant
women, who have access to relatively little public
information on efficacy, costs, and side effects
of new contraceptives. Choices within
villages show little diversity, but there is substantial,
persistent diversity across villages.
This pattern is not explained by income, education,
or price differences. Word-of-mouth
communication from friends was found to be
the important explanation of most women's
choices. Lack of inter-village mobility explained
multiple equilibria, with persistent intervillage
differences. Thus, some apparent taste
heterogeneity is due to the boundedly rational
practice of imitation in balkanized social networks.
The implications of social information
networks for economic policymakers is something
that is part of the bible of marketingproduct
launch and penetration is critical to
tipping network opinion and ensuring success.
Serious education of network information leaders
through demonstration and experience is
important not only for promotion of a product,
but also for its design.

In addition to providing information, social
networks may discipline the behavior of members
through consensus on social norms, accountability
for choices, and sanctions for
behavior that violates norms.3 The individual
gains from affiliation with such networks if
imitation and conformity save energy, if the
"expectation that one will be called upon to
justify one's beliefs, feelings, or actions, to others"
improves decision-making, and if approval
is itself a source of pleasure. The classical idea
of herd mentality is that social animals find it
easier and more comfortable to adhere to a
group, accept group roles, and mimic group
behavior than to act independently. Accountability
reinforces herd mentality in fixed groups,
and promotes safety in numbers. Individual
membership may be voluntary, as in the pellaton
of tightly packed riders in a bicycle race,
with riders tightly clustered and constrained
in order to save energy in preparation for
"breakaways."


### ---Economics-2006-0-15.txt---
B. Reciprocity and Altruism

Reciprocity is a simple form of social interaction,
present in economic trade and explained
by self-interest. Reciprocity is easy to
establish when it is synchronous, as in bilateral
barter. Asynchronous reciprocity, however,
requires reputation and trust. Norms for
fair practice, and sanctions for bad behavior,
may evolve in social networks to facilitate
asynchronous reciprocity, and individuals
may by habit or internalization conform to
these norms even in novel situations where
the normal cycle of approval and reputation is
suspended (see Fehr and Klaus M. Schmidt,
1999; Laetitia B. Mulder et al., 2005). Consider
the single-shot ultimatum game with
anonymous players. Player 1 proposes a division
of a prize of 100 units. If Player 2
accepts, the players get the proposed shares;
otherwise, they get nothing. It is rational for
Player 2 to accept any positive amount, and
thus rational for Player 1 to offer the minimum
positive amount. If, however, the probability
of acceptance a(s) by Player 2 is less
than one when the share s offered by Player 1
is low, then Player l's optimal strategy is to
maximize a(s) vg (1 - s). Students in a cross
section of developed countries play similarly.
Offers are usually 42 to 50 percent of the
prize, and offers less than 20 percent are
rejected about half the time. These results are
consistent with social norms for fairness in
which individuals altruistically incur costs to
punish greedy behavior.

Sam Bowles and a team of experimental
economists and ethnographers have conducted
anonymous ultimatum game experiments in 15
isolated societies whose ways of life provide
natural experiments on the influence of cultural
norms (see Joseph Henrich et al., 2001, 2004).
The findings overall are that cultures where
cooperative activity is important, and particularly
where people are exposed to markets, induce
offers in the ultimatum game that are more
equitable.

Genetic altruism is the phenomenon of
self-sacrifice for the good of your family or
kinship group. Genetic altruism appears to
explain cooperation in most species, and
seems to have a convincing evolutionary basis.
It has been a central theme of sociobiologists
in the past four decades, but the
concept itself is as old as the concept of
self-interest, as in a quote from Adam Smith
(1759):

Every man feels [after himself, the pleasures
and pains] of the members of his
own family. Those who usually live in the
same house with him, his parents, his
children, his brothers and sisters, are naturally
the objects of his warmest affections.
They are naturally and usually the
persons upon whose happiness or misery
his conduct must have the greatest
influence.

Despite its recognized importance, particularly
in economic models of the family and
of intergenerational transfers, genetic altruism
has not been systematically studied as a
determinant of economic behavior. The operation
of genetic selection could be very indirect.
Thus, the acquisition of language, the
exploitation of comparative advantage, the
formation of successful defenses against marauders
and disease, and a disposition to "fair
play" that reduces interpersonal conflict may
all arise from the selective advantage of group
traits that promote sociality. Then altruistic
behavior, including pure altruism with gifts
to unrelated individuals with no possibility
of personal gain, might be explained as an
indirect consequence of genetic self-interest,
as might the "warm glow" most humans experience
when placed in a supportive, cooperative
environment, the distaste people have
for aggressive, greedy traders, the potlatch
pride of being more generous than your
neighbors.

Summarizing, physiological, behavioral, and
sociological evidence indicate strongly that
consumers will often fail to promote their
self-interest reliably when choices involve risk,
ambiguity, integration of experience, and perceptions
of remote and/or unlikely events. Consumers'
failures will loom large, and this may
generate agoraphobia. Market-oriented economic
policy needs to take into account how
consumers' market experiences and outcomes
will influence well-being and acceptance of
market solutions.


### ---Economics-2006-0-16.txt---
VI. Consumers and Medicare Part D
Medicare's Part D drug plan is extraordinarily
complex. This government program
takes the cake, the candles, the
platter, and the crumbs.

Kathleen Pender,

San Francisco Chronicle

Medicare Part D is not that difficult to
understand. There has been a lot of confusing
information in the news about Part
D Medicare.

OregonHealthInsurance.com

The new Medicare Part D program that began
operation on January 1, 2006, provides prescription
drug coverage through Medicareapproved
plans offered by private insurance
companies and HMOs. Consumers in the Medicare
population can choose to opt out, or to
enroll in one of the private plans available in
their geographic area. This is a large and
complex government program that provides
substantial entitlements for the elderly and substantial
insurance against catastrophic drug
costs. If the entire eligible Medicare population
of 41 million were to enroll in this program,
then at current levels of prescription drug use,
the net subsidy from general government revenues
would be about $44.8 billion per year; this
includes some double counting of Medicaid,
veterans, and other programs that currently
cover prescription drug costs, and assumes that
all employer and union plans meet Medicare
requirements and qualify for the subsidy. There
is an adverse selection problem. If the approximately
27 percent of the elderly whose annual
pharmacy bills are currently below $842, the
breakeven point in 2006, were to delay enrollment
until health conditions warrant, the net
cost of the program would rise another $4.2
billion. However, moral hazard is the bigger
issue.4 In the Medicare population, people with
prescription drug coverage average 1.1 more
prescriptions than those without. If the 26 percent
of the population who currently pay all


their pharmacy bills enroll in Part D, experience
this increase in number of prescriptions, and
face the current average monthly cost of a new
prescription, $66, then this increases the cost of
the program by $6.8 billion. In these worst
cases, the effect of adverse selection and moral
hazard together is projected to increase the cost
of the program to $55.8 billion.
The creation of a market in which private
companies compete to offer coverage, and in
which consumers have choices of carriers and
plans, was an important element in the Part D
legislation. For economists, it is an interesting
economic policy experiment in whether the
benefits of competition can overcome the problems
of adverse selection and moral hazard that
always lurk in private insurance markets;
whether the Center for Medicare and Medicaid
Services (CMS) can efficiently manage its principle/
agent and underwriting relationship with
private insurers; and whether consumers can
understand and evaluate plan alternatives in
their own self-interest. In 2004, the National
Institutes of Health asked research groups working
on the economics of aging if they could
provide information on the impact of the Part D
program. My research group attempted to do
this by modifying a survey we were planning to
study health perceptions and choices of the elderly.
During the week of November 7-15,
2005, just before enrollment for Part D began,
we surveyed 4,739 persons age 50 and older and
gathered information on health conditions and
prescription drug use, knowledge and enrollment
intentions for Part D, and preferences
across different plans. Our initial findings are
given in Joachim Winter et al. (2005). I will
summarize a few findings here, with particular
attention to the question of whether consumers
are sufficiently self-reliant to take advantage of
the choices offered by the private market structure
of this program.

The Part D program is complex because of its
interactions with existing employer or unionprovided
drug coverage and with Medigap insurance,
and its provisions for means-tested cost
reductions for low-income consumers. There
are five main classes of eligible consumers:
* Standard Medicare, including those with
Medigap policies that do not cover drugs


### ---Economics-2006-0-17.txt---
TABLE 4-2006 PRESCRIPTION DRUG BENEFITS UNDER
MEDICARE PART D STANDARD PLAN

Annual Percent Percent of

pharmacy Patient Medicare paid by patients with
bill pays pays patient higher bills
$0 $ 0 $ 0 - 85.4%

$250 $ 250 $ 0 100% 80.5%

$500 $ 313 $ 188 63% 77.4%

$842 $ 398 $ 444 47% 73.0%

$1,000 $ 438 $ 563 44% 70.7%

$2,250 $ 750 $ 1,500 33% 49.4%
$5,100 $3,600 $ 1,500 70% 16.3%
$8,000 $3,745 $ 4,255 47% 6.4%
$12,000 $3,945 $ 8,055 33% 2.1%
$20,000 $4,345 $15,655 22% 0.4%
$40,000 $5,345 $34,655 13% 0.1%
Standard Medicare with Medigap policies
that cover drugs

Employee- or union-provided coverage, including
drugs

Medicare Advantage (HMO or PPO) policies
that cover drugs

Medicaid beneficiaries

Generally, those in the last three categories receive
Part D coverage by default. Those with
Standard Medicare will default out of Part D
if they do not take action, but have the choice
of enrolling in a privately offered plan, or of
converting to Medicare Advantage coverage.
In virtually all cases, there are Part D plans
that are more advantageous than Medigap
policy drug coverage. The analysis that follows
applies to the people currently on Standard
Medicare.

CMS has established a standard plan under
Part D that has an annual premium of $444, a
deductible of $250, pays 75 percent of prescription
drug pharmacy bills above $250 up to
$2,250, provides no additional benefits until
pharmacy bills reach $5,100, and pays 95 percent
of pharmacy bills above that level. CMS
requires approved private plans to offer comparable
coverage.

Table 4 summarizes consumer out-of-pocket
costs under the standard plan, not including the
annual premium, for various pharmacy bills.
The private insurers who provide drug coverage
within the Plan D framework may offer enhancements
to the standard plan, at higher
premiums, including coverage for the $250 deductible
and/or for the gap or "doughnut hole"
in the standard plan, which pays no added benefits
for pharmacy bills above $2,250 or below
$5,100. They may offer broader formularies
than Medicare requires, variations in the coinsurance
or copayment tier structure, and convenience
features such as broad pharmacy
participation and mail-order services. Approved
plans must have formularies that include at least
two drugs in each therapeutic category; the fraction
of the 100 most frequently prescribed drugs
included in currently approved formularies
ranges from 65 percent to 100 percent, with a
median of about 90 percent. Enrollees may
change plans annually. There are penalties for
late enrollment, currently a 1-percent increase
in premiums per month's delay past the initial
enrollment period, which ends in May 2006. In
evaluating alternatives, consumers need to take
into account not only their current pharmacy
bills, but also the probabilities of developing
new health conditions that will require treatment,
and the distribution of costs of these
treatments. As a result, consumers are being
asked to make relatively complex plan assessments,
generally with relatively incomplete information
on future prospects. Because of the
late enrollment penalties, there is not only a
current financial risk of making a poor decision,
but also an option pricing problem of determining
the value of enrolling to lock in current
premium rates. Not surprisingly, some seniors
are finding this a difficult choice, and the media
has had a field day publicizing Part D's complexity.
The economic policy question is this:
After the dust settles, will most consumers have
made good use of the choices offered by the
private market, so that a market-oriented design
contributes to consumer well-being? Is further
intervention on behalf of the vulnerable
needed?

Our survey, entitled the "Retirement Perspectives
Survey" (RPS-2005), was fielded as a
self-administered Internet questionnaire from
November 7-15, 2005, using a panel of subjects
enrolled by Knowledge Networks, a commercial
survey firm. This panel was recruited from
a random sample of the underlying population,
and all panel members were provided with identical
hardware (Web TVs) through which they


### ---Economics-2006-0-18.txt---
respond to periodic surveys. Members are compensated
for participation on the panel. For our
study, 5,879 members of the panel aged 50
and over were contacted. Of these, 4,738 individuals
completed the survey. Our present analysis
is restricted to those respondents who are in
the Medicare-eligible population, for the purposes
of our study defined as age 65 and older
(N = 1996).

The survey lasted about 22 minutes and covered,
in addition to questions about Part D,
questions about health status and conditions,
long-term care choices, prescription drug use
and cost, and attitudes toward risk. We also use
the 2001 Medicare Current Beneficiary Survey
(MCBS) distribution of annual pharmacy bills,
and an AARP survey giving median prices of
commonly prescribed drugs (as of April 2005)
for nine health conditions. Table 5 gives the
average numbers of prescriptions used by various
groups. Notable is the increase in the number
of prescriptions for those who have their
pharmacy bills paid by others, relative to those
who pay their own bills.

We find that despite the complexity of the
Part D program's competing plans, a majority
of the Medicare population has at least some
knowledge and intends to enroll. However, lowincome,
less educated elderly with poor health
or some cognitive impairment are significantly
less informed and may fail to take advantage of
the program. Table 6 gives the fractions of the
Medicare population who just before enrollment
started said they had little or no knowl-


edge of Part D. Table 7 gives the percentages of
the Medicare population who said just before
enrollment started they were unlikely to enroll
in a Part D plan. This does not include people
who will not enroll directly in Part D because
they already have prescription drug coverage
that is at least as good as the Medicare standard
plan. Overall, 17 percent say they are unlikely
to enroll. The percentages are higher for those
in good health, and those poorly informed. The
percentage differences are small, but statistically
significant.

A revealing assessment of the consistency of
individual intentions is obtained by comparing
enrollment choices with the alternatives that
minimize the expected present value (EPV) of
out-of-pocket cost (OPC). Underlying the enrollment
decision is an option value problem: If
an eligible person enrolls immediately in Part
D, her EPV of OPC in each year from 2006 to
the end of her life will be the $444 annual
premium plus her expected pharmacy bill, less
the Part D benefit. If, on the other hand, she
delays one year, then the EPV of her OPC is her
expected pharmacy bill for 2006 plus the EPV
of her OPC from 2007 forward, assuming that
she makes the decision to enroll or delay in
2007 and subsequent years to minimize EPV of
OPC, and assuming that these future decisions
take into account the new information she will
obtain on health and prescription costs as she
goes along, and the Medicare premium penalty
for late enrollment, which is 7 percent in 2007,
and 12 percent per year thereafter. With information
on the probabilities of developing new
health conditions, and the distributions of drug
costs for required therapies, this can be formulated
as a dynamic stochastic programming
problem, and solved by backward recursion to
determine a threshold depending on age, such
that if the current pharmacy bill is below the
threshold, an individual who seeks to minimize


### ---Economics-2006-0-19.txt---
m m m $1,000

$800

$600

$400

Enroll Now

Probably

Enroll Now

Probably

Delay

65 75 85 95

Age in 2005

FIGURE 2. ENROLLMENT THRESHOLDS MINIMIZATION OF EPV OF OPC
EPV of OPC cost will choose to delay. We
simplify this computation by approximating a
necessary condition for delay, ignoring the influence
on expected cost today of the additional
information and contingent decisions that will
be gained as future health conditions and pharmacy
bills are realized. This approximation was
found to be reasonably accurate in a study of
retirement decisions by Robin L. Lumsdaine et
al. (1994). We implement this calculation using
U. S. Life tables, estimates from the Health and
Retirement Survey of the annual probability of
developing a condition requiring a new prescription
drug therapy, and estimates from our
survey and the MCBS of the distribution of
annual drug costs for a new therapy.5
Figure 2 gives the thresholds we obtain using
this approximation; these apply to people who
do not receive means-tested premium reductions.
There are four factors that may modify
5 Some plans offer reduced or zero premiums, and may
be attractive to the healthy. However, most appear to be
available only to those who meet a low-income means test
or enroll in bundled HMO services.
this calculation for an individual. First, additional
information on health that will be revealed
in the future, and decisions contingent on
this information, give delay some added option
value. Second, risk aversion gives immediate
enrollment added insurance value. Trial calculations
indicate that the full option pricing calculation,
and risk aversion for a person with
moderate coefficient of absolute risk aversion,
have effects on the threshold for delay that are
relatively small, on the order of $100 or less.
Third, individuals may have different personal
probabilities for new health conditions and prescription
drug requirements than the ones we
have used. Fourth, individuals may have different
discount rates than the 5-percent discount
rate we have employed. For people with 2005
pharmacy bills above $802, the option of delaying
enrollment is "out of the money"-these
people can expect to reduce their OPC for prescription
drugs in 2006 with Part D coverage, in
addition to being insured against risks of high
future bills. The difference between the $802
threshold and the $842 break-even level for a
consumer's current pharmacy bill is the expected
value of the consumer's new pharmacy


### ---Economics-2006-0-20.txt---
bills in 2006. About 72.5 percent of the Medicare
population meet this condition. For those
with lower bills, there is an annual pharmacy
bill threshold that rises with age from just below
$500 to close to $750. Individuals who are
prepared to self-insure and are currently below
this threshold will probably find delay desirable,
while those between this threshold and $802
will probably find immediate enrollment desirable.
Approximately 24.4 percent of the Medicare
population falls in the region where delay
is probably desirable, and 3.1 percent in the
region where immediate enrollment is probably,
but not definitely, desirable.
Table 8 classifies enrollment intentions
against the action that minimizes EPV of OPC.
The table shows that the choice of 70.6 percent
of the population minimizes EPV of OPC.
However, there are 10 percent who intend to
delay even though it is likely in their selfinterest
to enroll. On the other hand, 19.4 percent
of those intending to enroll would achieve
lower EPV of OPC by delaying. Of course,
some of that group may want the insurance
against catastrophic costs in the future, and
these could be rational decisions if there is very
strong aversion to the risk of large, low-probability
losses.

A final part of our survey asked subjects for
their preferences among the alternatives of no
prescription drug coverage, the Medicare Part D
standard plan, and three hypothetical alternative
plans:

* Guaranteed Benefit Plan: Medicare pays 52.3
percent of approved prescription drug costs,
no matter how high or low these costs are.
The annual premium of $444 is the same as
the standard plan.

* Major Cost Protection Plan: Pays all approved
prescription drug costs above $2,444
per year, but nothing until your cost at the
pharmacy reaches this level. The annual premium
of $444 is the same as the standard
plan.

* No Copay Plan: You pay an up-front annual
premium of $1,889 per year, and all approved
prescription drug costs are then fully covered,
with no copayments.

The alternative plans all have the same actuarial
value as the standard plan for the Medicare
population, but differ in the degree to which
they provide insurance against major pharmacy
costs. The Major Cost Protection Plan and No
Copay Plan provide almost complete insurance
against major costs, with the latter eliminating
the deductible and charging an up-front premium
for the actuarial value of this replacement.
The Guaranteed Benefit Plan is more
favorable than the Major Cost Protection Plan at
low pharmacy bills, but entails substantial risk
at high bills. These hypothetical alternatives
vary more from the standard plan than most
products currently being offered, but preferences
among them provides some indication of
preferences for features of actual plans.
Enrollee choice among the alternative plans
is not explained well by cost minimization; only
36.3 percent of enrollees choose the plan that
minimizes EPV of OPC. Further, consumers do
not seem to place much value on the insurance
component of the alternative plans-among enrollees,
the Guaranteed Benefit Plan that offers
relatively poor insurance against catastrophic
drug costs is the minimum cost alternative in
only 3.2 percent of cases, but is preferred by
27.1 percent, while the plans that offer almost
complete insurance are preferred by only 26
percent, even though they include the
minimum-cost alternative for 51.2 percent. We
conclude that consumers are likely to have


### ---Economics-2006-0-21.txt---
difficulty choosing among plans to fine-tune
their prescription drug coverage, and do not
seem to be informed about or attuned to the
insurance feature of Part D plans.
VII. Conclusions

We conclude from our survey that significant
fractions of the Medicare population, particularly
among those with low SES, bad health,
and low cognitive ability, are poorly informed
about the Part D prescription drug program, and
risk making poor plan choices. Most of the
Medicare population, 89.2 percent, intend to
enroll, although this drops to 80.4 percent among
the poorly informed. When one compares preferences
with alternatives that minimize the expected
present value of out-of-pocket costs, one
finds that 10 percent of the elderly intend to
delay enrollment even though it increases their
expected costs, and 19.4 percent intend to enroll
immediately even though it increases their expected
costs. Choice among plans is erratic, and
shows little attention to or concern about the
insurance features of Part D plans. Procrastination
is a predictable behavioral response to the
complexity and ambiguity surrounding Part D,
making it likely that many who intend to enroll
will miss the May 15, 2006, enrollment deadline.
Consequently, there is likely to be considerable
churning and grumbling in this market in
the future.

How could the Part D market be managed
to overcome consumers' lack of information,
behavioral aversion to market choices, and
procrastination when faced with ambiguous alternatives?
First, CMS should pursue an aggressive
marketing program to find the vulnerable
who are insufficiently informed to act in their
self-interest, sell the neglected and undervalued
benefits of the insurance that Part D offers, and
coax consumers into making sensible plan
choices. This could include giving insurers incentives
to scour for vulnerable seniors. Marketing
of Part D should benefit consumers as
long as it is not done deceptively. Policies that
have proven effective in encouraging early retirement
in downsizing firms may also work in
this market. The most effective is "default in"
rather than "default out"-all individuals are
assigned a plan unless they choose a plan them-
selves or explicitly opt out; see Choi et al.
(2003). This could be done by providing stepby-
step decision forms that require seniors to
choose a plan, opt out, or let Medicare or an
ombudsman make a choice for them; one suggestion
is that these be called Plan D-EZ to
match simplified IRS forms. Another marketing
method that works for retirement is the use of
windows with attractive incentives. This could
be adapted to encourage Part D enrollment by
combining stiff late enrollment penalties with a
program to convert nonenrollees, such as a series
of "last ever" penalty amnesty windows in
the future, particularly for the vulnerable. A
number of private plans are being offered with
quite low premiums and basic coverage, which
encourage enrollment of the healthy. If CMS
ensured that a basic plan, with zero premium, a
limited formulary, and copayments sufficient
for actuarial balance, was always a market option,
then all seniors should enroll in either the
basic or a more comprehensive plan, assuring
affordable medications and catastrophic coverage
for the entire Medicare population.
The new Medicare Part D prescription drug
insurance market illustrates that leaving a large
block of uninformed consumers to "sink or
swim," and relying on their self-interest to
achieve satisfactory outcomes, can be unrealistic.
To make the Part D market work, in the
sense that it provides choices that consumers
want, and achieves the efficiencies it seeks,
CMS will have to make a diligent effort to
manage the market, and to reach all consumers
and provide them with information and assistance
in making wise choices. What the Part D
market, and other market privatization initiatives,
need is a component of Thaler and Sunstein'
s (2003) libertarian paternalism, in which
understanding consumers' limitations, helping
consumers to help themselves, and convincing
them that the market will serve their interests
are intrinsic parts of mechanism design.
 ## Economics-2007-0


### ---Economics-2007-0-03.txt---
Macroeconomics changed between the early
1960s and the late 1970s. The macroeconomics
of the early 1960s was avowedly Keynesian.
This was manifested in the textbooks of the
time, which showed a remarkable unity from
the introductory through the graduate levels.'
John Maynard Keynes appeared, posthumously,
on the cover of Time.2 Even Milton Friedman
was famously-although perhaps misleadinglyquoted:
"We are all Keynesians now."3 A little
more than a decade later Robert Lucas and
Thomas Sargent (1979) had published "After
Keynesian Macroeconomics." The love-fest was
over.

The decline of the old-style Keynesian economics
was due in part to the simultaneous rise
in inflation and unemployment in the late 1960s
and early 1970s. That occurrence was impossible
to reconcile with the simple nonaccelerationist
Phillips curves of the time.

But Keynesian economics also declined because
of a change in economic methodology.
The Keynesians had emphasized the dependence
of consumption on disposable income
and, similarly, of investment on current profits
and current cash flow.4 They posited a
Phillips curve, where nominal-rather than
real-wage inflation depended upon the unemployment
rate, which was used as an indication
of the looseness of the labor market.
They based these functions on their own introspection
regarding how the various actors
in the economy would behave. They also
brought some discipline into their judgments
by estimating statistical relations.5
But a new school of thought, based on classical


### ---Economics-2007-0-04.txt---
economics, objected to the casual ways of
these folks. New Classical critics of Keynesian
economics insisted instead that these relations
be derived from fundamentals. They said that
macroeconomic relationships should be derived
from profit-maximizing by firms and from utilitymaximizing
by consumers with economic arguments
in their utility functions.

The new methodology had a profound effect
on macroeconomics. Five separate neutrality results
overturned aspects of macroeconomics
that Keynesians had previously considered incontestable.
These five neutralities are: the independence
of consumption and current income
(the life-cycle permanent income hypothesis);
the irrelevance of current profits to investment
spending (the Modigliani-Miller theorem); the
long-run independence of inflation and unemployment
(natural rate theory); the inability of
monetary policy to stabilize output (the rational
expectations hypothesis); and the irrelevance of
taxes and budget deficits to consumption (Ricardian
equivalence).6 These results fly in the face
of Keynesian economics. They undermine its
conclusions about the behavior of the economy
and the impact of stabilization policy.
The discovery of these five neutrality propositions
surprised macroeconomists. They had
not suspected that radically anti-Keynesian conclusions
were the logical outcome of such seemingly
innocuous maximizing assumptions.
I. Neutralities and Preferences
How did macroeconomists react to the discovery
of the five neutralities? On the one hand,
the New Classical economists viewed their neutrality
results as a telltale: that Keynesian economists
of the previous generation had been
thinking in the wrong way. In their view, scientific
reasoning was producing a new, leaner,
more precise economics.

On the other hand, Keynesian economists, for
the most part, reacted differently. In due course
they came to view the neutralities as logically
impeccable. These New Keynesians accepted
the methodological dictums of the New Classical
economics: that constrained maximization
of profit and utility functions is the appropriate
microfoundation for macroeconomics. They
also viewed the neutralities as having a certain
sort of generality. The neutralities do commonly
describe equilibria of competitive economies
with complete information, irrespective of people'
s preferences-as long as those preferences
correspond to economists' typical descriptions
of them. The Keynesians then resurrected somebut
not all-of the Keynesian conclusions by adding
a variety of frictions to the New Classical
model. Those frictions include credit constraints,
market imperfections, information failures, tax
distortions, staggered contracts, uncertainty,
menu costs, and bounded rationality. This formulation
preserves many (but not all) Keynesian
conclusions regarding cyclical fluctuations and
macroeconomic policy.

This lecture will suggest a new stance in
regard to each of the five neutralities. Like New
Classical and New Keynesian economics, it will
derive behavior from utility and profit maximization.
That captures the purposefulness of economic
decisions. But this lecture will also
question the generality of the preferences that
lead to the five neutralities. There is a sense in
which those preferences are very narrowly defined.
They have important missing motivation-
since they fail to incorporate the norms of
the decision makers. Those norms reflect how
the respective decision makers think they and
others should or should not behave, even in the
absence of frictions. Preferences reflecting such
norms yield a macroeconomics with important
remnants of the early Keynesian thinking. They
also yield a macroeconomics that, in important
details, cannot be obtained only with frictions.
We shall see that, with such preferences, even
in the absence of frictions, each of the five
neutralities will be systematically violated. Specifically:


* A realistic norm regarding consumption behavior
will make consumption directly dependent
on current income, in violation of the
neutrality of consumption given wealth;
* A realistic norm will make investment directly
dependent on cash flow, in violation of
Modigliani-Miller;

* A realistic norm will make wages and prices
dependent on nominal considerations and
thus violate natural rate theory;
6 Of course, it took some time for the implications of
these neutrality results to be fully appreciated. For example,
life-cycle consumption and Modigliani-Miller were initially
considered as nothing more than useful codicils to Keynesian
thinking.


### ---Economics-2007-0-05.txt---
* A realistic norm will make income and employment
dependent on systematic monetary
policy, and thus violate rational expectations
theory; and

* A realistic norm will make current consumption
dependent on the current generation's
social security receipts, in violation of Ricardian
equivalence.

Additionally, insofar as the behavior assumed
by the early Keynesians differed from the behavior
that produces the neutralities, there is
likely to be a bias in favor of the Keynesians.
The Keynesians based their models on their
observation of motivations, rather than on abstract
derivations. If there is a difference between
real behavior and behavior derived from
abstract preferences, New Classical economics
has no way to pick up those differences. In
contrast, models with norms based on observation
will systematically incorporate such behavior-
although, of course, as with any method,
there is the possibility for error.
Inclusion of the "missing motivations in macroeconomics"
then combines the observations
of the Keynesians with the intentionality of
economic decisions in New Classical economics.
Such a synthesis yields the best of the two
approaches.

Two Disclaimers.--Before beginning in earnest,
let me offer two brief disclaimers. First,
none of the behavior revealing of the norms that
are introduced in this lecture will be new. On
the contrary, I have purposefully chosen phenomena
that have been emphasized since The
General Theory by macroeconomists who have
followed Keynes in voicing their continuing
doubts about classical interpretations of macroeconomic
behavior.

Second, this lecture will discuss different
norms that respectively correspond to the five
neutralities. I shall assume that these norms are
exogenous. Such assumptions of exogeneity are
standard in economic analysis. In a given problem
in a given time frame, some terms are
assumed constant, while others are allowed to
vary. I ask you to withhold your doubts regarding
whether such exogeneity is a correct assumption
or not. The incorporation of such
endogeneity is the next step-not the first
step-in the study of the effect of norms on
macroeconomics, especially since such endoge-
neity may sometimes dampen, but will rarely
nullify, the conclusions of this lecture.
II. The Five Neutrality Results
For clarity, this section will now give an
overview of each of the five neutrality results.
A. Dependence of Consumption on Wealth,
Not Income

Standard theory tells us that, under only
somewhat special conditions, consumption depends
on wealth, which is the value of current
assets plus the discounted value of future earnings.
7 Thus there is no tendency for people to
make their expenditures conform to the pattern
of their income receipts (as long as their wealth
is given).

Changes in the pattern of current income that
leave overall wealth constant are neutral in their
effects on current consumption.
B. The Modigliani-Miller Theorem
One version of the Modigliani-Miller Theorem
says that a firm's investment strategy is
totally independent of its liquidity position.8
Thus, for example, a corporation with an unexpected
windfall will not spend any additional
investment dollars. Instead, it will pass the
windfall on to shareholders or seek other financial
investments, since it will make only those
investments whose risk-adjusted rate of return
exceeds the rate of return on capital.
Changes in the firm's finances will thus be
neutral in their effect on current investment.
C. Natural Rate Theory

According to Natural Rate Theory, there is
some single rate of unemployment that is the
only level that could be permanently maintained
without ever-increasing inflation or everincreasing
deflation.9 A fiscal/monetary policy
mix that sought to maintain employment that
was any higher would result in permanently
increasing inflation. A fiscal/monetary mix that


### ---Economics-2007-0-06.txt---
sought to maintain employment that was any
lower would result in permanently decreasing
inflation. Fiscal/monetary mixes that yield different
levels of long-term (steady) inflation will
thus be neutral in their effects on long-term
unemployment.

D. Rational Expectations

According to Rational Expectations Theory,
a systematic response of monetary policy to the
business cycle will have no effect on the stability
of the macroeconomy.10 Wage and price
setters will foresee the systematic component of
the money supply; they will raise or lower
prices and wages exactly proportionally, and
thereby neutralize its effect on demand.
The stability of the economy is thus neutral
with respect to the systematic reaction of monetary
policy to the business cycle.

E. Ricardian Equivalence

According to Ricardian Equivalence, under
somewhat special conditions, a representative
consumer who receives a lump-sum intergenerational
transfer (for example, in the form of a
social security payment) will not spend a single
dime extra." Instead, she will pass on the whole
extra income, dollar-for-dollar, to her heirs,
who will have to pay the higher tax bills necessary
to retire the increased debt incurred in
funding the transfer to the previous generation.
The transfer is neutral in its effect on current
consumption.

III. The Missing Motivation: Norms'2
Each of the neutralities is based on the assumption
that the respective decision makers
are utility maximizers. But in each case the
utility functions of the decision makers have
been very narrowly described. They depend
only on real outcomes. For example, in the
consumption-neutrality models, utility depends
on consumption and leisure; in ModiglianiMiller,
it depends only on the discounted real
return to shareholders.

But as early as the beginning of the twentieth
century, Vilfredo Pareto pointed out that such
characterizations of utility missed important aspects
of motivation.13 According to Pareto, people
typically have opinions as to how they
should, or how they should not, behave. They
also have views regarding how others should, or
should not, behave. Such views are called
norms, and they may be individual14 as well as
social. The role of norms can be easily represented
in people's preferences by modifying the
utility function to include losses in utility insofar
as they, or others, fail to live up to their
standards.

Sociology has a further concept that gives an
easy and natural way to add those norms to the
utility function. Sociologists say that people
have an ideal for how they should or should not
behave. Furthermore, that ideal is often conceptualized
in terms of the behavior of someone
they know, or some exemplar whom they do not
know. The standard utility function is then modified
by adding a loss in utility, dependent on
the distance of behavior from that ideal.
Religion and religious identity give us a good
example of such norms. Consider the Gospels.
They are the most sacred texts of Christianity.
What do they describe? The life of Christ. How
should a Christian behave? "His life and conversation
ought to be worthy of the Gospel of
Christ [emphasis added]."'" How is a good
Christian supposed to feel when she has not
lived up to her conception of that ideal?
Ashamed.16

10 See Lucas (1972), Thomas J. Sargent (1973), and
Lucas and Sargent (1979).

11 See Robert J. Barro (1974) for the modem reincarnation
of these ideas, first discovered by Ricardo.
12 This section, including much of its exact wording, has
been taken from a joint manuscript with Rachel Kranton
(Akerlof and Kranton 2006). I should emphasize that these
insights have been developed jointly. The initial instigation
of our project is wholly due to Kranton. It is impossible for
me to say which ideas or wordings are mine and which are
hers. 13 See Pareto (1920). George C. Homans and Charles P.
Curtis (1934) give an excellent summary of Pareto that is
fully consistent with the emphasis here. Jon Elster (1989)
also presents a similar conception of norms.
14 For example, the protagonist of the novel Rice Mother
(Rani Manicka 2002) did not believe she should wear red
with black.

15 See http://www.orthodoxytoday.org/articles/StBasilBehavior.
php.

16 Of course, there are many interpretations of the Gos-
pel, and some of them are even contradictory. But that does
not affect whether the person should be ashamed or not. She


### ---Economics-2007-0-07.txt---
A. Importance of Norms in Motivation:
Some Examples

But religion is only one of the many realms
where people have such an ideal. To appreciate
the ubiquity of norms in motivation, it is useful
to see some further examples. Those examples
will demonstrate that people tend to be happy
when they live up to how they think they should
be; and they are, correspondingly, unhappy
when they fail to live up to those norms.
For the audience for this lecture, most of
whom are professors, teaching provides an especially
familiar example. We have a view of
what it means to be a good teacher. On our
lucky days, when we live up to our standards
and our classes go well, we tend to be happy; on
our off days, when something goes awry in
class, we may even feel quite miserable.
Such motivation in the workplace is the rule,
rather than the exception. Most workers, like
teachers, care about the conduct of their jobs.
Randy Hodson (2001), who surveyed ethnographies
of the US workplace, found that most
employees care about their dignity at work.
They want to conceive of what they do as useful.
And they feel a lack of dignity if they are
thwarted, either by their own actions or by the
actions of others. Those who are unable to get
such satisfaction are likely to show their displeasure
by acting up in some way or other.
Studs Terkel's Working (1972) captures in a
single volume much of the ethnographic findings
summarized by Hodson. Terkel interviews
people from many different occupations about
their feelings about their jobs and concludes that
people "search for daily meaning as well as
daily bread" (1972, xi). Some of the interviewees
are successful in this search: like the stone
mason, who cruises his Indiana county and
basks in pride as he not infrequently passes his
past work. At the opposite extreme is an Illinois
steelworker, whose work denies him the dignity
he seeks. He takes out his frustration at work by
being disrespectful, and, after hours, by getting
into tavern brawls. Most workers are somewhere
between these extremes, but in all cases,
following Terkel, they have a feeling for how
they should behave at work. It is not just about
the money; it is also about living up to an ideal
about who they think they should be.
Such belief regarding how people should behave,
and their behavior in accordance with
such belief, goes beyond the workplace. It affects
disparate areas, from playing golf to life in
the family. Betty Friedan's Feminine Mystique
gives what may be as good a description of
norms and their impact on people's lives as can
be found anywhere-in this case regarding the
norms for middle-class women of the previous
generation. Here is a brief sample of her description:


"Millions of women lived their lives in
the image of those pretty pictures of the
American suburban housewife, kissing
their husbands goodbye in front of the
picture window, depositing their stationwagonsful
of children at school, and smiling
as they ran the new electric waxer
over the spotless kitchen floor .... Their
only dream was to be perfect wives and
mothers; their highest ambition was to
have five children and a beautiful house,
their only fight to get and keep their husbands
.... They gloried in their role as
women, and wrote proudly on the census
blank: "Occupation, housewife" (Friedan
1963, 18).

Most women lived up to these norms. Some of
these were dissenters, like Friedan herself, who
disagreed with them, but felt compelled, nevertheless,
to follow a norm with which they disagreed.
Friedan says they suffered from "the
problem without a name." In our terms, they were
losing utility because they were failing to live up
to what one part of them thought they should do.
We may appeal to religious texts, to work
ethnographies, and, like Friedan, to women's
magazines to see the role of norms. But is there
yet harder data, some form of natural experiment,
that indicates the importance of norms?
The sociologist Erving Goffman has found such
an example. He observed the behavior of children
of different ages when they were brought
to the local merry-go-round. Because appropriate
activity differs by age, the children should
have predictably different reactions. For the
toddlers, riding a wooden horse is an accomplishment.
They show their joy at fulfilling what
they should do with smiles and waves as they
pass by. In contrast, for older children, there is


### ---Economics-2007-0-08.txt---
a gap between their conception of how they
should behave and riding the merry-go-round.
However much they may enjoy it, they also feel
the need to distance themselves from an activity
that is so age inappropriate. They manifest this
distance by riding a frog, rather than a "serious"
animal like a horse; alternatively they show off
by standing up "dangerously" during the ride. In
some way or other they play the clown.
Behavior at the merry-go-round is, of course,
just the stuff of kids. But Goffman supplements
it with a totally serious example. In surgical
operations, because of their inexperience, medical
students are given tasks that are ridiculously
easy.17 They respond in the same way as the
older children at the merry-go-round: they also
act the clown.18

In economics, as elsewhere, $500 bills do not
just lie on the street. If living up to norms is
such an important motivation, it must show up
in many economic examples, even if it is not
identified in exactly our language. Gary S.
Becker's Economics of Discrimination (1957)
offers an example of now-standard economics
that can also be interpreted in terms of such
norms. Becker's theoretical innovation was to
modify plain-vanilla economic utility by the
introduction of a discrimination coefficient. He
defined that as the loss in utility incurred by
exchange with someone from a different racefor
example, the loss of a white from an exchange
with a black. The natural interpretation
is that the discrimination coefficient represents
the loss in utility for the white from physically
engaging in an exchange with a black. But this
representation of the utility function can also be
interpreted in terms of norms. There is a code as
to how blacks and whites should behave toward
each other. The white has a view that she should
not deal with a black. She loses utility equal to
the value of the discrimination coefficient-not
from the physical association-but ipso facto
from the violation of the code. There is reason
to believe that such norm-based interpretation
better reflects the nature of discrimination than
a physical exchange-based theory. In the preCivil
Rights period, when Becker was writing,
there can be no doubt that discrimination, and
the code that upheld it, was stronger in the
South than in the North. Yet exchanges between
blacks and whites were surely much more com-
mon in the South than in the North. At least one
statistic reflects such a difference: there were
significantly lower levels of residential segregation
by race in the South than in the North.
B. Summary

Our examples are illustrative of behavior that
is pervasive. Sociology is dense in examples of
people's views as to how they and others should
behave, their joy when they live up to those
standards, and their discomfort and reactions
when they fail to do so.

We now turn to examining the role of
norms in each of the five macroeconomic neutralities.
20 In each case we shall ask whether
17 Goffman (1961) observed the behavior of such students
in medical operations.

18 Another example, the Milgram experiment (Stanley
Milgram 1963, 1965) demonstrates the strength of such
motivation-by showing the lengths that people will take to
do what they think they should be doing. To see this
interpretation of this experiment, which is only one of many
ways of viewing it, it is useful to give a brief description. On
arrival, subjects were told that they were involved in a
learning experiment. They were put in the role of the
"teacher," who should administer shocks to a "learner"
whenever he gave a wrong answer. The subjects are led to
identify with their role as teacher in this experiment, and
feel that they should obey the experimenter. Rather than
being another subject, and, rather than being wired, as it
appeared, actually the learner was an unwired, trained con-
federate of the experimenter. Subjects were then instructed
to administer shocks of escalating voltage as the learner
made errors. A surprising fraction of subjects escalated their
shocks to the maximum 450 volts-even though such a
dosage in real life would have been lethal. There are many
different versions of the experiment, but the version where
the confederate grunts and moans at 75 volts, asks to be let
out of the experiment at 150 volts, and refuses to give any
more answers at 300 volts, is typical. Here more than 60
percent of subjects went all the way. Nor is such motivation
limited to the laboratory. The rampage of the Nazi Reserve
Police Battalion #101 in Poland during World War II
(Christopher R. Browning 1999) gives a real-world mirror
of the behavior Milgram obtained in the laboratory. Like
Milgram's subjects, the members of this unit, were just
Ordinary Men (Browning's title). They were recruited from
the most prosaic civilian occupations.
19 See Douglas S. Massey and Nancy A. Denton (1993,
table 3.1, 64).

20 Some years ago, at a conference in Spoleto, Italy,
Edmund Phelps gave a still-unpublished lecture wondering
why the economics of the twentieth century had failed to
discover what was central to most of the arts, which was the
role of subjectivity. This paper is about the direct relevance
of such subjectivity for macroeconomics. I have very much
benefitted from enjoyable conversations with Professor


### ---Economics-2007-0-09.txt---
people's views as to how they should behave
will enter their utility function. In each case, we
shall see that such views will nullify the respective
neutrality result. Indeed, we shall also see
that in each case there will be a natural norm
broadly consistent with Keynesians' views of
economic behavior.21

IV. Ricardian Equivalence

We shall begin our detailed discussion with
Ricardian equivalence. It was chronologically
the last of the neutralities to be appreciated by
modern economists. But it is also the simplest.
That makes it the best place to begin.22 If there
is missing motivation in the utility function, it
should be easiest to see here.
A very simple model demonstrates the essence
of Ricardian equivalence, as it was rediscovered
by Robert Barro after a lapse of almost
two centuries.23 In the model, there are just two
periods, periods 1 and 2. There are just two
people, a parent and her child. The utility of the
parent depends directly upon her own consumption,
in period 1; it also depends upon the utility
of her child. That utility depends upon his consumption,
in period 2.

The parent's utility function can be expressed
simply as U,(cl, U2(c2)), where c1 is the consumption
of the parent, C2 is the consumption of
the child, U, is the utility of the parent, and U2
is the utility of the child. The parent chooses her
consumption in period 1 to maximize her utility.
Whatever wealth remains, she bequeaths to her
child.

Ricardian equivalence takes the following form
in this model. Suppose that the government
gives a transfer, which we shall call a social
security payment, to the parent in period 1; but
then in period 2 it taxes the child to retire the
debt caused by this transfer.24 In this case, the
consumption of a parent who maximizes the
utility function U, and who leaves a bequest to
her child will be unaffected by her receipt of
social security.

The logic of this result is simple. With and
without social security the discounted value of
consumption of the parent and of the child is
constrained by the discounted value of the family'
s earnings (plus its initial wealth). Social
security leaves that constraint unchanged. If the
parent found (c,, c2) to be the optimal division
of consumption between herself and her child in
the absence of a social security payment, this
same division of consumption between herself
and her child will optimize her utility with a
social security payment.

A vast literature explains why such Ricardian
equivalence is unlikely to be empirically descriptive.
25 The long list of reasons includes (a)
infinite, rather than finite, horizons; (b) strategic
bequests to obtain the attention of one's heirs
while alive; (c) childless families; (d) uncertainty,
including bequests made because of uncertainty
about the age of death; (e) differential
borrowing rates between the government and
the public; (f) growth of the economy in excess
of the interest rate, allowing steady debt issuance;
(g) lack of foresight regarding the effect
of social security on future taxes; (h) foreign
ownership of debt; (i) tax distortions;26, 27, 28 (j)


### ---Economics-2007-0-10.txt---
constraints on the consumption of parents (so they
do not leave bequests); (k) myopia of the parents
regarding children's future tax payments.29
The preceding list gives empirical reasons for
failure of Ricardian equivalence; but, lengthy as
it is, it still ignores its theoretical challenge.
According to that challenge, under economists'
standard assumptions, with perfect certainty and
with perfect foresight, Ricardian equivalence
will occur. Such a result had previously been
unsuspected by economists.30

Two possible conclusions can be drawn from
this surprise. On the one hand, we might continue
to assume that classical assumptions describe
economic behavior. The five neutralities
that are the subject of this paper concern the
realignment to macroeconomics that occurred
as economists gained understanding of the consequences
of classical assumptions from the
mid-1950s to the mid-1970s.

Economists may have been correct in drawing
the conclusion that the early Keynesian economics
was too simplistic and naive. But they
could have drawn another conclusion from this
surprise. In this view, Ricardian equivalence is
a telltale: we do not believe, even in the presence
of perfect foresight and perfect certainty,
that the parent will make an equal and opposite
offset of her social security transfer in terms of
an increased bequest to her child. Something
must be missing from the motivation in Barro's
model; otherwise, it would not have given rise
to results that are so surprising.
B. Douglas Bernheim and Kyle Bagwell
(1988) give further evidence suggesting that
Ricardian equivalence is such a telltale. They
show how the same logic would apply to a
network of gift-givers. Remarkably, any
member of such a network will be indifferent
whether she receives an extra dollar or any
other participant in the network is the recipient.
Such conclusions, suspect as they are,
suggest a problem with the model beyond the
lack of realism involved in perfect foresight
and perfect certainty. They also suggest missing
motivation.

James Andreoni (1989) has put his finger on
what that missing motivation might be.31 A
bequest is a type of gift. The parent will receive
utility from giving such a gift. Ricardian equivalence
will fail if the parent has utility from
gift-giving. With a social security transfer, more
money is hers, and the same consumption allocation
to herself entails a greater gift to her
child. With declining marginal utility for bequestgiving,
she will then divide an increased social
security transfer between additional consumption
for herself and an additional bequest to her child.32
Andreoni thus describes the utility missing
from the standard utility function as that arising
from the "warm glow" from giving. Such a
characterization may be accurate. It also sounds
as if it is very close to classical assumptionsthat
there is nothing fundamentally different
about this additional motivation. But this segment
of the utility function is, in fact, very
different from economists' usual characterization
of motivation. We know that the "warm
glow" does not come from the utility the parent
This argument suggests that a "bequest" is not really what it
seems. This is an argument where the preferences of the
parent do play a role, but quite different from the type of
reason that I think would have surprised the Keynesians. I
want to show that parents who make bequests for the
conventional reasons, because they care about the welfare of
their children, will still routinely violate Ricardian equiva-
lence, even in the absence of most of the commonplace
frictions that almost surely invalidate exact Ricardian equivalence.
29 This was Ricardo's own reason for dismissal of the
argument. He said that the parent would alter her bequest
because she would not take into account the added tax
payments of the child (see Gerald P. O'Driscoll Jr. 1977).
Uncertainty regarding the size of the future tax payments is
different from such myopia, in which the payment is altogether
ignored. But, with quadratic utility and expected
utility maximization, uncertainty regarding the child's fu-
ture tax payments will have no effect on the size of the
parent's bequest.

30 For example, Feldstein (1974) and Feldstein and Pel-
lechio (1979) engage in no theoretical soul-searching regarding
the negative effects of social security on current
savings. There is a voluminous literature (see Roberto Ric-
ciuti 2003) examining the empirical validity of Ricardian
equivalence. Largely because of the problem of endogene-
ity, it is difficult to come to firm conclusions regarding its
empirical validity. There are studies with findings both for
and against such crowding out.
31 See also John Laitner (2002), Laitner and Henry Ohls-
son (2001), Alan S. Blinder (1975) and Michael D. Hurd
(1989), who have also modeled the bequest motive as coming
from the utility of the parent from giving the bequest.
32 Formally, she trades off the marginal utility of her
own consumption against the marginal utility from gift-
giving and the marginal utility she gets from her child's
consumption. In making this trade-off, she takes due ac-
count of the fact that one unit of consumption today is
traded off against (1 + r) units of consumption next period.


### ---Economics-2007-0-11.txt---
derives from her own consumption; nor, yet
more tellingly, does it derive from the utility of
her child (as the child's utility depends on its
own consumption). It enters the utility function
as a separate term.

What, then, could account for a "warm glow"?
Parent-to-child bequests are a form of gift. If there
is any type of economic transaction that is governed
by norms, it is the giving of gifts.33 Parentto-
child bequests also occur within families.
Therefore, they should also be affected by the
norms of family life. We have already seen one
example of such norms (Friedan's portrait of the
proper place of women in the early 1960s).
The norms of family life are not constant. They
vary by culture. They also change over time. As
the nature of the ideal family has shifted, so has
the ideal bequest. Actual bequests have changed in
tandem. For example, the ideal sixteenth century
Anglo-Saxon family was dynastic. The lineage
passed from father to oldest son.34 Fathers then
left the bulk of their estates to their oldest sons. In
the twenty-first century, in the ideal family, siblings
are equal. Most bequests are now evenly
divided between them.35

Summary.-Economic outcomes, such as the
consumption of the parent and the utility of the
child, are one determinant of bequests. But another
possible determinant is parents' views regarding
how they should behave toward their
children. Just as Friedan's suburban housewives
waxed their floors, because they thought that is
what housewives should do, parents who leave
bequests derive a warm glow from bequests
because that is what they think they should do
for their children. Ricardian equivalence then
illustrates how odd neutralities can occur in
models that fail to take such norms into account.
A comment by David Romer (2001, 539)
tells us where we should venture next. He has
remarked that "quantitatively important" violations
of Ricardian equivalence and of the permanent
income/life-cycle hypothesis occur for
the same reasons. Ricardian equivalence is not
important for us as an empirical aspect of macroeconomics.
There are so many reasons other
than the role of norms for its violation. But it
does give us an initial window on the type of
motivation missing in classical macroeconomics.
Inclusion of such motivation will give us a
new perspective on the consumption function. It
allows us to return to a view in which consumption
will depend on current income, just as its
inclusion makes it natural to believe that social
security transfers will affect savings and consumption,
even in a world without frictions.
V. Consumption and Current Income
This takes us to the second neutrality. According
to this result, other than its contribution
to a consumer's wealth, current income has no
independent effect on the consumption of a
utility-maximizing consumer.

Milton Friedman (1957) derived such
consumption-income neutrality in the twoperiod
model of Irving Fisher. In this model, the
consumer chooses her consumption between
two periods. She maximizes her intertemporal
utility function, given by the function U(cl, C2):
c, denotes her current consumption in the first
period; c2 denotes consumption in the second
period.36 If she maximizes U(c1, c2), a dollar of
income earned today will have the same effect
on her current consumption as a discounted
dollar earned in the next period. Thus, her consumption
will depend only on the discounted
value of her current and future income and the
rate of interest. This proposition is easy to
prove. It generalizes to many different commodities
and to many different time periods,


### ---Economics-2007-0-12.txt---
and, with quadratic utility, to uncertain incomes.
37 In standard terminology, the value of
her discounted income is called her wealth; the
amount of that wealth that can be spent without
its depletion is called permanent income.38 An
alternative expression of Friedman's hypothesis
is that consumption depends on permanent
rather than on current income.39
The permanent income hypothesis may be in
accordance with most standard economic models.
Nevertheless, it contradicted prior thinking
about the consumption function. Keynes, and
his followers, believed that current income
played an especially important role in the determination
of current consumption.

"The fundamental psychological law [emphasis
added], upon which we are entitled
to depend with great confidence both a
priori from our knowledge of human nature
and from the detailed facts of experience,
is that men are disposed, as a rule
and on the average, to increase their consumption
as income increases, but not by
as much as the increase in income" (Keynes,
The General Theory, 1936, 96).
It is true that The General Theory discussed a
long list of other factors that could affect consumption.
The list was sufficiently rich to include
not only current income, but also all the
other determinants of wealth, such as expected
future income and the rate of interest. But that
does not make Keynes's theory identical to
Friedman's. In the Keynesian theory, consumers
are more sensitive to current income than to
other changes in income that have similar effect
on the consumer's wealth.

A. Empirical Results and Their Explanation
A large number of tests have demonstrated the
excess sensitivity of consumption to current income,
in concert with the Keynesian consumption
function. For example, John Y. Campbell and
N. Gregory Mankiw (1989) nested both Friedman'
s view that consumption depends solely on
wealth and the simplified Keynesian view that
consumption depends solely on income. They
suppose that a fraction of consumers A are pure
Keynesians, while a fraction (1 - A) behave according
to the permanent income hypothesis; they
estimate A from the extent to which consumption
overreacts to changes in income that would be
predictable from past changes in income and consumption.
Usefully, then, A gives a natural measure
of the departure from the permanent income
hypothesis. The estimates of A are significant statistically
and also of significant magnitude economically:
between 40 and 50 percent (depending
upon whether three or five periods are used to
predict the change in current income).
Other studies corroborate such excess dependence
on current income: John Shea (1985), for
union members whose contracts specified their
37 The simple proof is that her utility-maximizing con-
sumption will depend upon the intercept and the slope of the
budget line. The budget line states that the present dis-
counted value of consumption is the present discounted
value of her future income, which is what Friedman calls
her wealth. The intercept of the budget line is her wealth.
That is how much she could consume today if she consumed
nothing tomorrow. And the slope of the budget line is
determined by the rate of interest r: on the budget line for
every unit of c, she gives up (1 + r) units of c2. Her
consumption will be on the highest attainable utility indif-
ference curve. That will be the indifference curve that is just
tangent to the budget line. As a result, we see that, given the
utility function, c1 will be a function of W and r. Note that
current income does not come into this expression.
38 Formally, permanent income is the product of the rate
of interest and wealth.

39 The permanent income hypothesis also generalizes to
currently popular models of present bias. In these models
consumers have present bias in the form of "hyperbolic dis-
counting," which means that they put extra weight in their
utility functions on their current consumption. In this case, the
typical consumer's plans will not be consistent, but they can be
analyzed as if she has multiple selves. Her self today decides
on how much to consume today and then passes on the re-
maining assets to her self tomorrow. There is an exact analogy
to the parent's maximization in Barro's model of bequests. In
that model, today's consumer passes on assets to her child in
the next generation; in consumer theory, today's consumer
passes on assets to her new self in the next period. Since the
standard model of intertemporal consumption and Barro's
model of consumption are exactly isomorphic, Ricardian
equivalence then tells us that current consumption-which is
the consumption of the initial self-depends only on the con-
sumer's wealth. David I. Laibson (1997) thus shows that
consumption with forward-looking consumers with hyperbolic
discounting will balance the marginal utility of present con-
sumption out of wealth against the marginal utility of future
consumption according to an Euler condition. Such a condition
is wealth-based. It is the generalization of the tangency of the
utility indifference curve to the budget line in the two-period
model of Irving Fisher. Both Friedman and Laibson obtain
consumption that is determined solely by current income if
there is a constraint on current borrowing, and consumers'
desires for current consumption exceed their current income.
There is nothing inherent in the preferences in either case that
causes current consumption to be based on current income.


### ---Economics-2007-0-13.txt---
future wages; David W. Wilcox (1989), for social
security recipients who had been earlier notified of
changes in cost-of-living adjustments; Jonathan
Parker (1999), for payers of social security taxes
with predictable inter-year changes; Nicholes S.
Souleles (1999), for changes in disposable income
net of tax refunds; and James Banks, Richard
Blundell, and Sarah Tanner (1998), and Bernheim,
Jonathan Skinner, and Steven Weinberg
(2001), for retirees.

Textbooks explain such excess sensitivity by
a variety of frictions, particularly borrowing
constraints. For example, Rudiger Dornbusch
and Stanley Fischer (1987) say: "Given that the
permanent income hypothesis is correct [sic],
there are two possible explanations."40 They are
liquidity constraints for consumers and myopia
in their projections of future income.
Thus, we see the realignment that occurred
because of the life-cycle permanent income hypothesis:
excess sensitivity may occur, but only
in the presence of credit constraints or myopia.
Such a view cannot have been adopted because
of its empirical support. Few studies have tested
this proposition, but those that do have rejected
it. For example, credit constraints cannot explain
the reduction in consumption of retirees.
And, neither myopia nor credit constraint can
explain the reduction in union members' consumption
at the time of wage declines scheduled
in their union contracts (Shea 1995, 996).
The adoption of the permanent income/
life-cycle hypothesis then must rest on theoretical,
not empirical, reasons. But the theory fails
to take into account norms regarding what people
think they should, or should not, consume.
Such a norm-based theory will nest Keynes's
psychological law. Consumption-income neutrality
will occur only in a singular special case.
B. Consumption and the Role of Norms41
Why should consumption be overly sensitive
to income? This section presents an argument in
three steps. First, sociology gives motivations
for consumption that are very different from the
reasons for it in the life-cycle model. A major
determinant of consumption is what people
think they should consume. Second, what people
think they should consume can often be
viewed either as entitlements or as obligations.
Finally, in turn, current income is one of the
major determinants of these entitlements, and
obligations.

Sociology of Consumption.-The motivation
emphasized by sociologists for consumption is
very different from that in the life-cycle model.
Sociologists describe consumption as largely
determined by the norms regarding what people
should consume. These norms, in turn, are dependent
upon the individual's situation and also
who she thinks she is.

Two examples illustrate such dependence on
norms. Following Pierre Bourdieu (1984), people'
s consumption of cultural goods-the literature
they read, the music they hear, and the art
they buy-reflects not just their individual
tastes. The upper class should not make lowerclass
choices. Correspondingly, the lower class
should avoid appearing above their station.42
The epithet "lace curtain Irish" illustrates. To
the users of this phrase, those lace curtains were
indicative of those violating their social place.
Weber's analysis of the relation between religion
and savings further reflects the role of
people's views regarding who they should be.
In The Protestant Ethic and the Spirit of Capitalism,
43 Weber describes Calvinists as aspiring
to be "worldly ascetics." He concludes that
"economic acquisition is no longer subordi-
nated to man as the means for satisfaction of his
material needs.""44 Here the purpose of saving is
to live up to an ideal. The Calvinists are thrifty
because they think they should not be consuming.
That turns the motivation of the life-cycle


### ---Economics-2007-0-14.txt---
model on its head. There people save only because
of their desire for consumption in retirement.
Luigi Guiso, Paola Sapienza, and Luigi Zingales
(2003, 2006) have statistically affirmed
Weber's hypothesis that religion is correlated
both with attitudes toward savings and with
actual savings. In addition, they have more generally
affirmed the quantitative significance of
culture for savings and consumption; in their
regressions, variables reflecting culture have as
much power as variables derived from the lifecycle
hypothesis in explaining cross-country
savings ratios.45

Consumption Entitlements and Obligations.-
While sociology is useful in giving us the general
insight that consumption depends on cultural
norms, we need to be more specific. What
is the nature of those norms? They can frequently
be described in two ways: as entitlements
and, also sometimes, as obligations to
spend. Again some examples will illustrate.
First, oddly, people have obligations to
spend. Social history is full of the obligation to
keep up appearances. Most Wall Street bankers,
for example, do not live like mothers on welfare.
They do not want to. But, even if they did,
it would occasion gossip. It is not what they
should do. History is replete with stories of the
debt of aristocrats struggling to maintain their
social obligations.46 As just one example, the
debts to British merchants by Southern planters,
who were keeping up with the Joneses of the
eighteenth century, are considered a significant
factor underlying the Southern support of the
American Revolution.47

In addition to obligations to spend, there are
also entitlements. The lost-ticket paradox of
Amos Tversky and Daniel Kahneman (1981,
457) gives an illustration. Eighty-eight percent
of respondents to a questionnaire said they would
buy a $10 theater ticket if they arrived at a theater
to see a play and found that they had lost a $10
bill. In contrast, only 46 percent said they would
buy a new $10 ticket in the same situation if
they had lost a previously purchased ticket.
Tversky and Kahneman explain this difference
by "mental accounts," but an explanation
in terms of entitlements is equally valid. Tversky
and Kahneman say that those who have lost
the $10 bill do not connect that loss to the play.
In their mental account, its cost is just $10. But
those who have lost the ticket see themselves as
paying for it twice. In their mental account, its
cost is $20. Those with the lost ticket then tend
to opt out, because they see $20 as too much to
pay to see the play. But the difference in behavior
for those who lost the ticket and those who
lost the $10 bill could also have been interpreted
in terms of entitlements. Most people want to
think of themselves as responsible human beings.
When they lose the ticket, they do not feel
entitled to just buy another one. That is not the
type of person they aspire to be.
We should also observe that it is not coincidental
that the lost ticket paradox could be explained
both by mental accounting and by
norms. Formally, any model of mental accounting
can be translated into a model of norms: just
replace the rules of mental accounting as the
norms that people think they should follow.48
But even though norms and mental accounting
may be equivalent, interpretations in terms of
norms are important for this lecture. Mental accounting
has the connotation, whether rightly or
wrongly, of being a heuristic for quick decisions.
Such a heuristic will, of course, sometimes result
in cognitive error. Whether rightly or wrongly,
most economists would dismiss cognitive error as
unimportant. Why? because in their view people
are smart about what they want, and their decisions
are also very purposeful. But norms cannot
be dismissed so easily. As I argued earlier, people
feel strongly about adherence to them. Their
45 Guiso, Sapienza, and Zingales (2006, 39) report re-
gressions of savings ratios on GDP growth, dependency
ratios, and responses to the question: "Do you consider it
especially important to encourage children to learn thrift
and savings?" A one-standard-deviation difference to GDP
growth and to attitude toward thrift both produce a 1.8-
percentage-point difference in the savings ratio. (A one-
standard-deviation difference in the dependency ratio,
which could be the result both of cultural differences and of
life-cycle considerations, produces a 3.2-percentage-point
difference.)

46 See, for example, David Cannadine (1977).
47 See Woody Holton (1999).

48 But it turns out that there is quite possibly a substan-
tive difference between the two interpretations. With the
mental accounting interpretation the losers of the ticket
could be induced to buy one, if only a wise friend would
make them aware of the logical problems of their reasoning.
In contrast with the norms interpretation the friend cannot
be so helpful. Buying a new ticket is a departure from the
person's norm, and she loses utility by it.


### ---Economics-2007-0-15.txt---
absence from utility constitutes the missing
motivation of macroeconomics.

The Link of Entitlements and Obligations to
Current Income.-It remains to relate current
spending to current income. Norms may be
complex. But a web of evidence still reveals a
strong association between current income and
entitlements and obligations to spend. Such a
link, in turn, produces the excess sensitivity of
consumption on current income in Keynes's
Psychological Law.

A few examples follow.

* It is common practice in the United States for
parents, even for rich ones with no budget
constraint, to expect their children to assume
financial independence after their graduation
from college. They are indicating their belief
in the norm that the child is entitled to spend
what she earns. (Most parents, of course, give
their children a helping hand as they seek
their independence. But that does not mean
that they do not also strongly believe that their
children should live on their earnings, since that
norm is only one of their motivations.)
* In a thought experiment, consider a woman
living on $50,000 a year who learns that
her uncle will die in one year leaving her
$2,000,000. Even if she has considerable savings
in the bank, it would be unseemly for her
to run down her savings in anticipation of the
bequest. She is not entitled to do so. She
should stick to spending from her current
income. This gives another example in which
norms regarding entitlements to spend are
related to current income, in violation of the
life-cycle hypothesis.

* People's expenditures are supposed to reflect
their stations in life, and those stations usually
reflect their earnings. Thus, for example,
college students with little earnings are supposed
to live that way-like college students.
Their current spending is supposed to reflect
their current earnings, not what they will be
earning in the future. (At the other extreme,
as an obligation, the college president is often
expected to live in the presidential mansion.)
* Preliminary results from an experiment by
John Morgan and myself illustrate another
relation between entitlement and earnings. In
this experiment, subjects were asked to donate
to a charity before and after completing
a task. Those who were asked for the donation
afterward were more likely to keep the
money than those who were asked beforehand.
Those who had completed the task felt
that they had earned the money and were thus
entitled to keep it for themselves.49
* The mental accounting model by Hersh M.
Shefrin and Richard H. Thaler (1988) is especially
useful in our quest for a Keynesian consumption
function. Norms take many forms, so
their formal model is not unique.50 But it does
illustrate a possible link between consumption
and current income. In this model, people have
three separate mental accounts: current income,
current assets, and future income combined
with pension wealth. As consumers exhaust one
of these accounts and begin to use the next one
for their current consumption, they incur a discontinuous
"penalty." Those penalties are psychological
in nature-this is a model of mental
accounting-and they take the form of a loss in
utility.51 Corresponding to Shefrin and Thaler's
assumptions regarding the nature of these costs,
as consumption rises, consumers will first finance
it wholly from current income; then,
from current assets; and, finally, from future
income and retirement wealth.

As we discussed earlier, it should be no surprise
that there is an exact translation of such a
model into one with norms regarding entitle-
ments to consume. The rules of mental accounting
become the norms regarding how money
should be spent. The basic norm is that consumption
should come from current income.


### ---Economics-2007-0-16.txt---
And the discontinuous penalties correspond to
the losses of utility due to respective deviations
from that norm. In particular, Shefrin and Thaler
assumed that there is no such cost at all if
consumption comes only from current income.
That means that current income can be considered
as consumers' entitlement to spend, since
any consumption that is less than current in-
come entails no deviation at all from the norm
regarding the account that should finance it.
* Shefrin and Thaler give an impressive array
of econometric facts in support of their
model. Insofar as these facts support their
mental accounting model, they also equally
well support its reinterpretation-with the
norm that current income is an entitlement to
spend. Those facts include: differential savings
out of windfall and current income;52 a
less than one-to-one displacement of discretionary
saving by employee pension contributions;
53 undersaving for retirement;54 and a
marginal propensity to consume out of fully
anticipated bonuses that is much greater than
the marginal propensity to consume out of
monthly income.55

* Retired people are commonly believed to
tailor their consumption to a concept of
income rather than to the value of their
assets. Shefrin and Statman (1984) have
viewed this as another form of mental accounting.
They also present considerable
evidence regarding such behavior.
C. Summary

Considerable evidence suggests that people's
views regarding what they are entitled to spend
play a major role in their consumption choices.
It also suggests strongly that current income
plays a special role in those entitlements. Shefrin
and Thaler have explained such patterns by
mental accounting. A reinterpretation of their
model shows that they also could have explained
this behavior in terms of norms. Once
again we see that the current versions of the
life-cycle hypothesis have left out missing motivation
that easily justifies the excess sensitivity
of consumption to income in Keynes's
psychological law.

VI. Investment and Cash Flow

The debate concerning investment has been
surprisingly close to the debate about consumption.
The early Keynesians emphasized two
variables as determinants of investment: current
cash flow (with profits as a major component),
and the firm's current holdings of liquid assets.
Each of these variables is a measure of funds
available to firms for investment without seeking
outside finance.56 In contrast, the later literature
denied any special role of liquidity in the
investment function.

The first such questioning came from
Modigliani and Miller, who assumed that managers
maximize shareholder value and that markets
are frictionless and competitive. In this
case, a firm's financial position plays no role in
the value of the firm. The argument for this
independence proceeds as follows. By construction,
Modigliani and Miller show how a competitive
equilibrium changes if a firm increases
its debt and buys back shares. In the new equilibrium,
investment will be unchanged, and
shareholders will offset the increase in the
firm's debt by a compensating increase in the
bonds in their respective private portfolios. The
reason the equilibrium changes in this way is
straightforward: if the markets for debt cleared
in the old equilibrium, they will again clear in
the new. If managers' choice of investment
maximized shareholder value in the old equilibrium,
the same choice of investment maximizes
it in the new. Investment is therefore independent
of the firm's current financial position,
including its current liquidity position and its
current cash flow.

The advent of q-theory similarly questioned a
special place for current variables, such as cash
flow and liquid asset holdings in the investment
decision. In the original version of the theory,
James Tobin (1969) suggested that a firm's optimal
investment strategy arbitrages between
52 Shefrin and Thaler (1988, 619-20).
53 Shefrin and Thaler (1988, 622-24).
54 Shefrin and Thaler (1988, 626-27). Especially, they
say that there would be vast undersaving in the absence of
social security and forced private pensions to prevent it.
There is some ambiguity regarding whether there is under-
saving in the presence of these institutions to counteract it.
55 Shefrin and Thaler (1988, 633).
56 See, especially, John R. Meyer and Edwin Kuh
(1957).


### ---Economics-2007-0-17.txt---
the value at which it can sell a unit of its capital
and its investment costs to produce a new unit
of capital. In this case, firms should invest up to
the point where the marginal cost of a new unit
of capital is the valuation of such a unit of
capital in the stock market. That valuation is the
market value of the firm's shares divided by its
capital stock, called the q-ratio. If markets are
efficient, q is also the expected discounted value
of current and expected future profits per unit of
capital.57 Since q-theory says that firms should
invest in capital up to the point where the cost of
an extra unit of capital stock is equal to the
present discounted value of the stream of earnings
from a unit of capital, again, as in ModiglianiMiller,
investment is independent of the firm's
finance decision.58

The empirical testing of q-theory also has a
striking parallel to the empirical testing of the
consumption function. Just as Campbell and
Mankiw showed that there was excess sensitivity
to current income in the consumption function,
Steven M. Fazzari, R. Glenn Hubbard, and
Bruce C. Petersen (1988) showed that investment
depends not just upon q, but also upon the
current cash flows. Furthermore, as in the standard
explanation of excess consumption sensitivity,
Fazzari, Hubbard, and Petersen similarly
suggest that credit constraints are responsible
for the dependence of investment on cash flow.
They continue with the Modigliani-Miller/
q-theory assumption that managers maximize
stockholder value. But they posit that the difference
in information between managers and
financiers results in a wedge between the cost of
internal and external financing. This is clearest
for firms that are credit constrained-so that
credit-constrained firms will be especially sensi-
tive to available liquidity.59 But, as with creditconstraint
explanations of consumption, empirical
evidence, such as there is, rejects this hypothesis.
Steven N. Kaplan and Zingales (1997) analyzed
the subsample of firms that Fazzari, Hubbard,
and Petersen had considered most likely to be
credit constrained. They find credit constraint to
be rare. Furthermore, they also found that those
firms with the least constraint had the greatest
sensitivity to cash flows.60

There is, thus, remarkable similarity between
the consumption function and the investment
function. In both cases, economic theory suggested
rejection of earlier views regarding the
role of current flow variables-current income
in the case of consumption, cash flow in the
case of investment. In both cases, empirical
investigation showed the existence of excess
sensitivity to the current flow variable. In both
cases, these rejections support the previous
Keynesian theory. In both cases, economists
have sought to explain the divergence between
practice and theory by the presence of credit
constraints. In both cases, the empirical evidence,
such as it is, does not support the case
that credit-constraint explanations explain the
theoretical anomaly.

A. Theory of Excess Sensitivity of Investment
to Cash Flow

Whatever the similarities, consumption and
investment differ in one major respect. In the
case of investment, economists are already
aware of a fundamental reason why investment
will depend on current cash flow. ModiglianiMiller
and q-theory both assume that managers


### ---Economics-2007-0-18.txt---
maximize shareholder value. In the now-standard
theory of the firm, the interests of the shareholders
and the interests of the managers are viewed as
different. The managers are only the agents of the
owners, and accordingly they maximize their own
interests instead. Such incentives are said to turn
the managers into "empire-builders,"61 who will
use the resources they control to increase their
own domains.

Empire-building can result from two types of
motivations. On the one hand, managers may
have only strict economic interests in mind:
they care only about their take-home pay, and
their effort on the job. Such managers, for example,
will be biased in favor of investments
whose operation or construction enhances their
firm-specific human capital, and thereby increases
their bargaining power.

On the other hand, empire-building may be
pursued as a goal of its own, for its own sake.
We saw earlier that most workers have views
regarding how they should or should not perform
their jobs. Accompanying such views,
most managers and workers will have the further
view that the firm should be investing in
those jobs. For this reason, the agents making
the investment decision are likely to engage in
empire-building. We can represent such motivation
by adding a term to the utility function of
the agent-decision maker. Her utility function
will not only depend on her own pecuniary
returns and her expenditure of effort. It will also
include an additional term reflective of her
norms. She will lose utility insofar as the firm's
investment fails to live up to her ideal of what
she thinks it should be. In this case, the typical
norm is that she thinks that the firm should
engage in investment that will enhance her job
performance.

Following the logic of Michael Jensen (1986,
1993), empire-building, accompanied by the
abdication of corporate oversight in favor of
management interests, explains a correlation between
investment and cash flow. Furthermore,
this correlation will occur regardless of the motivation
for the empire-building, whether for purely
economic reasons as in the principal-agent model,
or, instead, because of managers' norms for how
they think they should behave. Jensen has given
many instances of lax corporate oversight in favor
of management interests. For example, he has
cited the excess exploration and drilling operations
of oil companies when retained earnings
were high, from 1975 to 1981,62 and the maintenance
of low-return operations in many US industries,
as in the investments of General Motors
throughout the 1980s.63 In Jensen's views, shareholders
would have fared better if profits had been
returned to them, giving them the option of investing
at a higher rate of return, or perhaps if profits
had been used for takeovers outside the industry.
To cure what he calls the "failure of corporate
internal control," Jensen has also suggested that
firms should issue large amounts of debt, perhaps
even by going private. In that case, the added debt
obligations act as a brake on excess investment.
Regarding investment behavior, Jensen is then on
the same page as Keynesian economists such as
Klein and Goldberger. They refer to "the preference
of many businessmen for internal as opposed
to external financing" (1955, 12-13) and also consider
it the major reason for the dependence of
investment on cash flow.

B. Sociology of the Corporation
Once again, we have seen a neutrality result
that depends on the goals of the respective decision
makers. Accordingly, the norms of corporate
decision makers are central to the
sociology of the corporation. For example, Dirk
M. Zorn (2004) has examined how the locus of
control has changed in large US firms over the
past 40 years. He has shown how this control
has shifted away from those with a production
or a sales orientation to those with a financial
orientation.64 Empirically, this is seen in the rise
of the chief financial officer. Prior to the 1960s,
corporate finances were handled by corporate
treasurers, whose duties were mainly restricted
to keeping the accounts and producing the budgets.
Now, most large corporations have replaced
them by a CFO. With the change in title
has come a change in function. CFOs are typically
central to major decisions. Such a change
affects investment decisions. If they are committed
to their missions, managers with sales or
61 Empire-building is especially emphasized by Jeremy
C. Stein (2003), following Jensen (1986, 1993).
62 Jensen (1986, 327).

63 See Jensen (1993, 853).

64 That distinction was emphasized earlier, for example,
by Neil Fligstein (1990).


### ---Economics-2007-0-19.txt---
production orientations will be empire-builders.
In contrast, the role of the conscientious CFO is
to curb those enthusiasms. Fifty years have
elapsed since the publication of ModiglianiMiller.
According to Zorn, when it first appeared,
it did not describe the investment
decision of large corporations. Now, quite possibly,
changes in corporate decision-making
since that time make it more realistic.65
C. Summary

The investment decision demonstrates once
again that the respective neutrality result depends
on the objective function of the decision makers.
VII. Natural Rate Theory

We now turn to natural rate theory. Once
again, the debate concerns the behavior of economic
decision makers. The early Keynesians
viewed wage setters, and possibly also price
setters, as setting nominal wages and prices,
respectively, without taking full account of inflationary
expectations. In contrast, New Classical
revisionists have assumed that wage and
price setters care only about relative wages or
prices, and therefore wage and price setting will
fully incorporate inflationary expectations. Such
behavior yields a long-run neutrality result with
severe limits on the ability of monetary and
fiscal policy to affect unemployment and output.
When wage and price setters care only
about relative wages and relative prices, accelerating
inflation will occur if unemployment is
below a critical level, called the natural rate;
accelerating deflation will occur if unemployment
is above it.

As we shall see, such spirals occur because,
at high levels of demand, the representative firm
will wish to set the price of its product relative
to the price of other firms' products-which we
call its real price-in excess of unity. A standard
natural rate model illustrates why this occurs. That
model assumes that in each period the typical firm
sets a desired real price for the following period;
in each period it also makes a bargain with its
labor regarding next period's real wages. Next
period's nominal price and nominal wage are then
respectively set by adjusting this desired real price
and this bargained real wage according to inflationary
expectations. When demand is higher, the
desired real price of the representative firm is
higher for two reasons: on the demand side, because
the demand for its product is higher, and, on
the cost side, because the bargained real wage is
higher. That bargained real wage is higher both
because the typical employee's opportunity costs,
which take into account her chances of being
unemployed, are higher, and because the firm's
desire for her labor is higher. Since the firm's
owners, customers, and workers care only about
real prices or real wages, a given level of real
aggregate demand will be associated with a given
real wage bargain between the firm and its workers,
and a given desired real price for the firm's
product. If unemployment is sufficiently lowbelow
the natural rate-that desired real price will
be in excess of unity. If unemployment is above
the natural rate, it will be less than unity.
It is now easy to explain the inflationary and
deflationary spirals in natural rate theory. Consider
what happens when the representative firm
wishes to set its price above that of other firms.
In this case, actual inflation will exceed expected
inflation. With such a positive gap between
actual and expected inflation, inflationary
expectations will rise, as inflationary expectations
are adjusted upward to conform to reality.
But the firm's desired real price, and therefore
the difference between actual and expected inflation,
will be unchanged as long as unemployment
is constant. There will be no abatement in
the rise in expected inflation. Inflationary expectations
will be forever increasing, and inflation
will rise with it, as nominal prices and
wages adjust the real wage bargains and the
desired real prices for these increasing inflationary
expectations. By similar logic, if unemployment
is above the natural rate, there will be a
deflationary spiral. The natural rate is the only
sustainable level of unemployment without accelerating
or decelerating inflation. It corresponds
to the exact level of demand where firms
wish to set a real price of exactly one.


### ---Economics-2007-0-20.txt---
A. Acceptance of Natural Rate Theory
Most macroeconomists do not just view natural
rate theory as a useful null hypothesis.
They also see it as a description of reality. Such
a view is revealed in textbook presentations.
Economists accept natural theory for theoretical
and empirical reasons.

Theoretically, they view the assumptions of
natural rate theory as realistic. A standard criterion
for an economic model is that participants
in the economy care only about real outcomes.
That is the fundamental assumption of natural
rate theory. Also, unlike our other neutrality
results, natural rate theory is insensitive to deviations
due to "frictions," such as imperfect
information, taxes, myopia, or transaction costs.
As long as these "frictions" can be expressed
solely in real terms, the neutrality result of
natural rate theory will be robust.
Empirical considerations have also been influential
in economists' acceptance of natural
rate theory. The original Phillips curve showed
a close fit between the rate of change of nominal
wages and the inverse of the unemployment rate
for 97 years of British data, between 1861 and
1957. There was no inflation adjustment in this
equation. In the United States in the late 1960s
and early 1970s, however, such a simple inverse
relation between changes in nominal wages and
unemployment broke down, as both price and
wage inflation rose, along with the unemployment
rate. Natural rate theory offered an explanation
for this occurrence: it explained the rise
in inflation by the large oil supply shock and
also an increase in inflationary expectations,
both of which shifted the Phillips curve outward;
it explained the rise in unemployment by
a decline in demand.

Furthermore, new estimates of Phillips curves
seemed to show that the theory closely fit the data.
If inflationary expectations are formed as a simple
lag of past inflation, estimates of Phillips curves
should find that the coefficients on past inflation
sum to one. Many Phillips curve estimates fail to
reject that this sum is equal to one.66' 67 The stan-
dard errors of such estimates are quite large; thus,
they also fail to reject sums whose departure from
one is of sufficient size to result in departures of
economically significant magnitude from natural
rate theory. But the standard treatment of the Phillips
curve ignores this inconvenient fact.
The textbooks thus typically present natural
rate theory as a "just-so" story. It runs as follows.
The previous Keynesian economists had
posited a Phillips curve without a dependence
on inflationary expectations. Friedman (1968)
and Phelps (1968) perceived that such a theory
could not result from models where the participants
in the economy are concerned only with
real variables. They modified the relationship so
that wage and price equations would be affected
one for one by inflationary expectations. Such
judicious use of economic theory explained the
otherwise-mysterious finding of the simultaneous
increases in inflation and unemployment
of the late 1960s/early 1970s. The theory is also
consistent with most econometric estimates.
B. Nominal Considerations in Wage Behavior
We now turn to the same question regarding
wages that we asked concerning consumption
and investment. Is there "excess sensitivity"
relative to the respective neutrality? Natural
rate theory is based on the assumption that
wages and prices are set only with real considerations
in mind. "Excess sensitivity" here
66 See, for example, Robert J. Gordon (1977, table 3,
lines 6 and 7, 260).

67 Given the importance of such findings, it is remark-
able that their robustness to specifications of time period,
data, and exact specification of the Phillips curve has never
been subjected to tough tests-even though everything else
about the Phillips curve, including the natural rate of un-
employment itself, is considered to be estimated with great
imprecision. Akerlof, William T. Dickens, and George L.
Perry (2000) show a range of estimates for both wage and
price equations with many different specifications. These
estimates, particularly when made for periods of low infla-
tion, show considerable variation in the sum of the coefficients
on lagged inflation, dependent on the specification.
Another bit of evidence that suggests such estimates will be
sensitive to specification comes from the high standard
errors on the natural rate itself (Douglas Staiger, James H.
Stock, and Mark W. Watson 1997); it would be surprising
that the sum of lagged coefficients could be estimated
precisely if another component of the Phillips curve, the
natural rate, could be estimated only with very low preci-
sion. Gordon's own estimates show very different values for
this sum of coefficients. Of course, there is a theoretical
reason why estimates of such a sum should not be robust.
With rational expectations, rather than a simple mechanical
theory of formation of inflationary expectations, Sargent
(1971) shows that there is no theoretical reason that they
should sum to one.


### ---Economics-2007-0-21.txt---
takes the form that nominal considerations affect
real wage or price setting in some way or
other.

Evidence of one form of violation of the
assumptions of natural rate theory is especially
stark. That evidence concerns downward wage
rigidity. Such wage behavior can easily be perceived
statistically by examining distributions
of wage-changes. These distributions are characterized
by a bunching of wage changes at
exactly zero; there are some wage changes just
above zero in these distributions, but almost no
wage changes just below.68 Careful studies have
documented such wage stickiness in Australia,
Canada, Germany, Japan, Mexico, New Zealand,
Switzerland, the United States, and the
United Kingdom.69,70 There seems to be no way
to account for such nominal wage rigidity with
the basic assumptions underlying natural rate
theory: that participants in the economy care
only about real prices and real wages.
Wage stickiness also explains a macroeconomic
observation that is an anomaly for natural
rate theory. Unemployment was so massive in
the Great Depression that inflation should have
been below inflationary expectations throughout
this long period. With any natural-rate adaptiveexpectations
Phillips curve, such high unemployment
would have caused a deflationary spiral.
Data on inflation are available for 12 countries for
the Great Depression. Not a single one of them
shows such a spiral.71 For example, the United
States experienced rapid deflation from 1929 to
1933, but inflation systematically neither rose nor
fell for the next decade. The predictions of natural
rate theory are thus grossly violated. But sticky
wages offer a good explanation for such behavior.
For example, a dynamic simulation of the US
economy with money wage rigidity and with
Depression-level unemployment fits the data all
but exactly (Akerlof, Dickens, and Perry 1996).72
Nominal wage rigidity may not only be statistically
perceptible; it can also be macroeconomically
important, even outside of Great
Depressions. Nominal wage rigidity imparts a
long-run trade-off between unemployment and
long-run inflation. This trade-off is of sufficient
size that it should deter central banks from
targeting very low levels of inflation. For example,
simulations of the US economy (Akerlof,
Dickens, and Perry 1996) show that an increase
of the inflation target from 0 to 2 percent will
permanently reduce unemployment by 1.5 percentage
points.73

Norms as Explanation for Sticky Money
Wages.-It seems to be impossible, or all but
impossible, to explain the existence of sticky
money wages, without relaxation of the basic
assumption that the utility functions of employees
or of employers contain real arguments. A
simple and natural amendment to the standard
model explains such sticky money wages: that
employees have a norm for what wages should
be. According to that norm, they will lose utility
from a money wage decline. Sticky money
wages then result, as the bargains between employers
and employees reflect the presence of
this ideal in the utility function.
Indeed, the study by Bewley (1999) gives


### ---Economics-2007-0-22.txt---
direct evidence that such a norm exists and is
responsible for wage stickiness. His extensive
open-ended interviews sought to elicit why employers
failed to cut money wages in the Connecticut
recession of 1991-1992. Bewley
concludes that, even though substitute labor was
easily available, employers were reluctant to cut
wages because of the negative effects of such
cuts on morale. He says that managers were
afraid that cuts in money wages would cause
workers no longer to "identify" with their companies.
74 There might be no immediate consequences
during the recession. But employers
thought that such cuts would cause workers to
shirk after the recession had ended. They also
feared that their best workers would be more
likely to quit. These stories indicate that workers
are not thinking about their wages only in
real terms, relative to the price level or the
wages received by others. They also have a
special aversion to cuts in wages below their
current nominal levels.75

Norms about Wage Increases.-The motivation
underlying resistance to money wage cuts
is so obvious, and the facts are so unexceptionable,
that most macroeconomists accept the possibility
that money wages are sticky. Even so,
they rarely appreciate the broader implications
of such violation of the assumptions of natural
rate theory. Their adjusted model is that price
and wage decisions are made only with real con-
siderations in mind, but desired wage changes
will be truncated insofar as they entail money
wage decreases. To my mind, such a view entails
a theoretical error. As we have seen, the
existence of money wage rigidity occurs because
workers have a norm, which affects their
utility function, that their employers should not
make such cuts. The message of this finding is
that norms in the utility function yield at least
one clear violation of natural rate theory. That
suggests the further empirical possibility that
workers (and also employers and customers)
may also have other norms regarding what nominal
wages (and prices) should be. All such
violations are exceptions to natural rate theory,
and yield reasons for long-run trade-offs between
inflation and unemployment.

Money wage rigidity is then potentially only
the tip of an iceberg. If there is one way in
which nominal wages enter utility functions,
because of employees' norms regarding what
their employers should or should not do, there
could also be many other ways.
There is another natural way whereby such
norms could enter utility functions: employees
may not only have a norm that they should not
take wage cuts. They may also have norms
regarding the nominal rate of increase of their
wages or salaries. For example, employees may
believe that their employer should give them a
nominal raise.

There is little research on the existence of
such norms. The two questionnaire studies that
have investigated it obtain strong and mutually
reinforcing results. Eldar Shafir, Peter Diamond,
and Tversky (1997) asked respondents to
comment on a vignette about two young women
who take their first jobs with the same initial
income. Specifically they asked respondents
who will be better off: Barbara, who receives a
5-percent raise in the presence of 4-percent inflation;
or Ann, who receives a 2-percent raise
when inflation is zero; 79 percent of respondents
correctly said that Barbara would be
worse off than Ann economically. Nevertheless,
64 percent of respondents also said that Barbara
would be happier.76 Such responses are contrary
to the natural rate hypothesis that employees
only care about real returns. But an
easy explanation for this phenomenon occurs if
74 In more detail, Bewley (1999, 1-2) summarizes his
findings: "Other theories fail in part because they are based
on unrealistic psychological assumptions that people's abil-
ities do not depend on their state of mind and that they are
rational in the simplistic sense that they maximize a utility
that depends only on their consumption and working con-
ditions, not on the welfare of others. Wage rigidity is the
product of more complicated employee behavior, in the face
of which manager reluctance to cut pay is rational. Worker
behavior, however, is not always rational and completely
understandable. A model that captures the essence of wage
rigidity must take into account the capacity of employees to
identify with their firm and to internalize its objectives. This
internalization and workers' mood have a strong impact on
job performance and call for material, moral, and symbolic
reciprocation from company leadership."
75 Following the argument by Raj Chetty and Adam
Szeidl (2006), some employers may have been concerned
with the fact that their employees had fixed mortgages that
they would find difficult to pay with cuts in nominal wages.
This puts the violation of natural rate theory in another
place: why were these financial contracts in nominal rather
than in real terms?

76 Shafir, Diamond, and Tversky (1997, 351-52).


### ---Economics-2007-0-23.txt---
Barbara and Ann both think that their employer
should give them a nominal wage increase.
Another study, with a different form of questionnaire,
independently found a similar response.
Robert Shiller found that 49 percent of
a sample of the general public either fully or
weakly agreed with the following statement: "If
my pay went up, I would feel more satisfaction
in my job, more sense of fulfillment, even if
prices went up as much." An additional 11
percent of the general public were undecided,
while only 27 percent completely disagreed. As
in the case of Ann and Barbara, such opinions
are consistent with the view that workers think
their employers should give them a nominal
wage increase: they will be disappointed when
it does not occur. Shiller's finding may be similar
to the public's view of Ann and Barbara.
But, as he reports, it is also in stark disagreement
with the view of professional economists
that underlies natural rate theory. Ninety percent
of economists weakly or strongly disagreed
with the statement; 77 percent were in complete
disagreement.77

Such norms-regarding the wage or salary increase
that employees think they should receive--
can be economically consequential. They cause
the long-run inflation-unemployment trade-off
to be downward sloping. With such a norm, at
higher levels of inflation workers will not experience
disappointment from receiving lower
nominal wage increases than they think they
should receive; therefore, at higher inflation,
ceteris paribus, wage bargains will result in
lower real wages, which will reduce the relative
price that the firm wants to set, and therefore
raise the rate of sustainable employment. There
is a need for further research following Shafir,
Diamond, and Tversky and Shiller regarding
whether workers have norms regarding the nominal
wage increases they think they should receive.
High Inflation.-The opinions expressed regarding
Barbara and Ann, and also the opinions
of Shiller's respondents, suggest that the longrun
trade-off between inflation and employment
is upward sloping. These answers were elicited
in the United States and thus are reflective of
respondents' views in an environment where
inflation has been low. But if inflation is very
high and therefore also very salient, the answers
to such questionnaires could be very different.
And they could impart a very different shape to
the trade-off between macroeconomic demand
and steady-state inflation.78 In such cases, people
may gain satisfaction only from wage and
salary increases that exceed inflation. Such
norms regarding how employers should behave
will then necessitate higher real wages (to maintain
the same level of satisfaction) at higher
levels of inflation. The long-run inflationemployment
relation will then be downward
sloping. Such behavior gives a much stronger
rationale, even than current rational-expectations
credibility models (Barro and Gordon 1983; Kenneth
Rogoff 1987), why central banks should
maintain price stability. Failure to appreciate this
realistic possibility again may be another case in
which the absence of norms from utility functions
has unduly blinkered macroeconomic thinking.79


### ---Economics-2007-0-24.txt---
C. Prices

We have just seen that employees' norms
regarding nominal wages may affect bargained
real wages, and therefore cause trade-offs between
long-run inflation and long-run unemployment.
Similarly, customers' norms regarding price levels
and price changes may also cause long-run tradeoffs
between output and inflation.

Indeed, models by Katsuhito Iwai (1981),
Julio J. Rotemberg (1982), and Andrew S. Caplin
and John Leahy (1991) all have long-run
trade-offs between inflation and unemployment.
Each of these models assumes that there are real
costs to nominal price changes. If, instead, there
were real costs to real price changes, the assumptions
of natural rate theory would still be
satisfied, and no such trade-off would occur.
These models then pose the question why there
should be such real costs from nominal price
changes. Iwai, Rotemberg, and Caplin and
Leahy all respectively assume that there is a
"menu" cost in making these changes known.80
But the physical costs of making such changes,
as in the printing of new menus, are trivially
small. Norms regarding price changes, however,
give an alternative reason why these costs
might--indeed-be of sufficient size to induce
a significant long-run trade-off between inflation
and unemployment. Customers may think
that firms should not raise prices. In that case,
price increases (or increases of greater size) are
likely to induce angry customers to search for
alternative suppliers. At higher steady-state inflation,
firms will be changing their nominal
prices more, and therefore will face more elastic
demands for their product. Producers' natural
microeconomic response to this increased elasticity-
a lower price for their product-will
produce a macroeconomic trade-off between inflation
and aggregate demand.

Just as sticky money wages indicated that
employees have norms regarding wage change,
similarly, sticky prices indicate that customers
have norms regarding price change. Thus, the
extensive evidence on price stickiness reveals
violation of the assumptions of natural rate theory,
and also the existence of norms regarding
price change. Like wage changes, price changes
also agglomerate at zero. Dennis Carlton (1986)
has shown that prices are often sticky for significant
periods of time.81 Furthermore, prices
seem to be especially sticky in customer markets.
82 Alan Kackmeister (2002) has compared
price changes at the end of the nineteenth century
to such changes a bit more than a century
later. Price changes of specific goods at retail
stores were recorded from June 1889 to September
1891; Kackmeister revisited the same
commodities and their price change for a comparable
period, from June 1997 to September
1999. Price change in the late twentieth century
was five times more frequent than a century
earlier. Furthermore, in the nineteenth century,
the average spell of constant price for an individual
good was very long. It was approximately
80 months.83 Such constancy of prices
can easily be explained by customer norms regarding
price change. The customers have a
notion of the price that they ought to pay at
stores where they are continued and knowing
customers. Kackmeister suggests that the decline
in long-term customer relationships is one
factor responsible for greater frequency of price
change today.

Emi Nakamura and J6n Steinsson (2005)
give an economic reason why customers would
have such a norm that firms should not change
prices. They view consumer purchases as habitforming.
Thus, by buying a particular brand, or
patronizing a particular store, consumers are
putting themselves in a position where they can
be exploited. Their loyalty puts the firm in a
have notions regarding what nominal wage increases should
or should not be. This, of course, is just one of many
anomalies in the form of indexed contracts.
80oMarika Karanassou, Hector Sala, and Dennis J.
Snower (2003) find considerable long-run trade-off between
inflation and unemployment in a model with nominal price
staggering and money growth.

81 See also Blinder and Don Choi (1990) and Blinder et
al. (1998).

82 The meaning of customer markets was especially explored
by Arthur Okun (1978).

83 derive this result from Kackmeister's data in the
following way. He finds that in the nineteenth century, only
5 percent of items changed their prices per month. This
means that the average spell of constant prices would have
been 20 months (the inverse). But that is a biased statistic
for the average length of time between price changes for an
item on the shelf. The difference between the average spell
of employment or unemployment and the average spell
being experienced by an individual suggests a rule of thumb
ratio for four to one. Using this ratio as a rule of thumb
suggests that the spell between price changes averaged over
the individual items on the shelf would be 80 months.


### ---Economics-2007-0-25.txt---
position where it can take advantage of the
consumer by raising prices. Firms then make an
implicit contract with their customers: they will
not change their prices unjustifiably. Since such
an implicit contract is easier to make (and enforce)
regarding nominal prices than real prices,
the implicit guarantee is in nominal terms. Nakamura
and Steinsson have also discovered a
phenomenon that suggests strikingly that firms
do behave this way. Goods in store 126 (chosen
for its completeness of data) of Dominicks Finer
Foods chain frequently go on sale; when the
sale ends, their nominal price returns to the
exact same level. Such behavior is consistent
with the view that consumers think that prices
should not change (for whatever reason); and
that they are also likely to retaliate (change
brands) when prices do change.
I should also remark that in countries where
inflation is very high, customers will expect
price changes to occur frequently, and possibly
be of large magnitude. The inhibitions against
price changes when inflation is low are eroded
at high inflation. Thus, while norms concerning
prices give a negative long-run trade-off between
inflation and unemployment at low inflation,
at high inflation that trade-off could very
well be reversed.

D. Summary

To summarize, there is considerable evidence
of violation of the assumptions and predictions
of natural rate theory. Wages and prices are
nominally rigid; there were no deflationary spirals
in the Great Depression; and questionnaire
respondents act as if they have a positive like for
nominal wage increases.84 This evidence suggests
that wage earners and customers have
views on what wages and prices should be. The
reflection of such views in utility functions produces
trade-offs between inflation and unemployment.
Those trade-offs have significant
implications for economic policy. On the one
hand, central banks should avoid very low targets
for inflation. On the other hand, they should
avoid high inflation, where the trade-offs between
inflation and unemployment may be
reversed.

VIII. Rational Expectations Theory
Our discussion of rational expectations piggybacks
on our previous discussion of the nat-
ural rate.

According to rational expectations theory, insofar
as the central bank changes the money
supply systematically in response to employment
conditions, the public will foresee that
response and change prices and wages exactly
to compensate. The public's anticipation will
then exactly offset the response. Monetary policy
is neutral.85

There are two key assumptions underlying
this neutrality. The obvious one is rational expectations.
To some, rational expectations regarding
the effects of the money supply on
prices and wages would seem to be beyond the
sophistication of most wage and price takers,
and also of most wage and price setters.
Even in the case where all those involved in
buying and selling goods and labor services
have rational expectations, however, the neutrality
results of rational expectations theory
require also that nominal considerations do not
enter into the setting of either wages or prices.
The previous descriptions of the ways in which
nominal wages and prices enter into preference
functions, via employees' views of the wages
that ought to be received and consumers' views
of the prices that ought to be paid, give further
reason why the neutrality results of rational
expectations will be violated. If prices and
wages are affected by people's notions of what
their nominal values should be, monetary policy
can be effective in stabilizing output-and possibly
in raising its long-run level-even in the
presence of rational expectations.
IX. Economic Methodology

We have seen that the absence of norms plays
a key role in each of the five neutralities. Why
have economists made such systematic omissions?
The omission of norms from macroeconomics,
as well as from economics more


### ---Economics-2007-0-26.txt---
generally, can be explained by economists' adherence
to positive economics.86 Friedman's
(1953) essay on positive economics describes
the methodological implications of such belief.
In particular, he says that economic theorists
should strive for parsimonious modeling. According
to Friedman, they should even forsake
realistic assumptions in pursuit of such parsimony.
Maximization models with only objective
arguments of utility have been defined as
more parsimonious than models where people,
additionally, lose utility insofar as they, or others,
fail to live up to their standards. As a result,
whatever the empirical validity or relevance of
such norms, positive economics has a methodological
bias against their consideration. It privileges
models without norms.

The prescriptions of positive economics regarding
the conduct of empirical investigation
compound the bias against norms. Friedman
says that economists should not pay heed to the
stated intentions of decision makers, which
would include their norms as to how they and
others should behave. Instead, empirical work
should test only hypotheses that economists
consider to be based on parsimonious models.
If economic tests had great power, then it
would be easy, of course, to follow Friedman's
dictum of making more and more refined tests
of hypotheses with decreasing parsimony. If
norms really do affect behavior, this method
would reject models without norms and in due
course would arrive at models where people's
views regarding how they should behave affect
decision making. But economic tests lack
power. All economic models are very imprecise
in their specification of the independent variable,
the nature of the dependent variables, the
nature of leads and lags, and the nature of
residuals. Yet worse, most economic problems
involve simultaneity (as in supply and demand),
making establishment of causality difficult. In
almost any instance, such a large number of
models can be fitted statistically that it is extremely
hard-and perhaps impossible-to statistically
reject all the variants of models
without norms. As a result, the program of
positive economics-with its initial nulls of
models based only on utility with objective
variables verified only by statistical hypothesis
testing-has severe bias against explanations of
economic phenomena where norms play a role.
Summers (1986) illustrates the severity of
this bias. The conventional test of the efficient
markets hypothesis-that stock prices are the
expected value of future returns-looks for autocorrelations
of the excess returns on stocks
relative to bonds. Following Summers, it would
take approximately 5,000 years of data with
such a test to obtain as much as 50 percent
rejection of an alternative model where stock
prices are more than 30 percent away from their
fundamentals 35 percent of the time. With such
lack of power, nulls are important. When they
are not rejected, alternative theories, such as
those with norms, are not even considered. This
lecture has illustrated such reversion to normless
nulls. Consumption behavior, investment
behavior, and wage and price behavior-the three
most important components of most macro models-
all display excess sensitivity relative to respective
neutralities. All of these violations could
be easily explained by norms. Yet in each case
economists have sought to explain such violations
of classical theory by norm-less models.
In contrast to reliance on statistical testing,
disciplines other than economics typically put
much greater weight on a naturalistic approach.
This approach involves detailed case
studies. Such observation of the small often
has been the key to the understanding of the
large. To me, the most dramatic example of
such a relation between the small and the
large occurs in the structure of life itself.
Francis Crick and James D. Watson87 conjectured
correctly that if they could describe the
crystalline structure of a single DNA molecule,
they would have unlocked the secret of
life. The duality between the structure of the
DNA molecule and the way in which organisms
are generated and reproduced is one of
the most beautiful findings of human knowledge.
It indicates the sense in which Crick and
Watson were, indeed, profoundly correct.
What are the implications for social science?
Positive economics, with its emphasis on statistical
analysis of populations, would suggest that
86 Some of the thoughts and wording in this section have
been presented in Akerlof (2005).
87 As dramatically described by Watson (1969).


### ---Economics-2007-0-27.txt---
the intensive study of a single molecule would
be an all-but-worthless anecdote. In the case of
DNA, we know that the exact opposite is true:
because DNA is a template that determines all
of the cells of the organism, and also its reproduction,
one molecule may not tell all, but it
does tell a great deal. Form follows function.
Is there some reason to believe that economic
behavior and economic units are any different?
Economic decisions may not be as duplicable as
biological processes, but the basic reason why
science intensively studies the microscopic applies
to economics as well. The individual economic
unit, be it a firm, a consumer, or an
employee, behaves the way it doesfor a reason.
And if these actors behave as they do for a
reason, we can expect to find those reasons from
the structures that we see in close observation;
and because of those structures their behavior
will also tend to be duplicated. This duality
between duplicability and structure explains
why much of science concerns very close observation,
as it also explains why the study of
even a single part of a single DNA molecule
will be revealing.

Standard economic methodology says that
it is impossible to infer motivation of individual
actors from intensive case studies. Anthropologists
and sociologists listen carefully
to individuals in such studies. When people
follow the norms, they use them to explain
their actions; when, on the other hand, they
violate the norms, they become the subject of
local gossip. Those case studies are revealing
because-like a language, which dictates how
one should speak-the norms are common
knowledge. In this lecture, we have seen one
prominent example of the use of such knowledge:
Bewley's interviews uncovered the
common understanding of the norms regarding
wage cuts among Connecticut employers
in the early 1990s.

Summary. -Positive economics systematically
denies that norms can be understood from
intensive case study. Precedence given to models
without norms because they are by definition
more parsimonious and statistical tests of low
power then jointly create a firewall against consideration
that norms play a role in determining
behavior. For these reasons, current economic
methodology inherently has created a biased
economics. In contrast, a more naturalistic approach
would prescribe a different methodology.
In this case, economists would observe
decision makers as closely as possible, with the
express intent of characterizing their motivation,
and would use such characterization as the
basis for modeling of economic structure. Indeed,
sociological and anthropological ethnographers
do precisely that: they depict their
subjects' motivation from close observation.
X. Endogeneity of Norms

It is now time to discuss the endogeneity of
the norms. There is a special reason for its
consideration. Robert Lucas discovered that,
with endogenous rational expectations regarding
inflation, monetary policy that was
intended to stabilize the macroeconomy
would, instead, be exactly neutral. Similarly,
is it not possible that endogeneity of the
norms, like Lucas's endogeneity of inflationary
expectations, will cause the neutralities
again to hold? We shall discuss this question
regarding all five neutralities. For the most
part, we find that the type of government
interventions being considered are usually of
such frequency, or of such order of magnitude,
that they should provoke relatively little
change in the norms. Endogeneity of the
norms should have little effect, then, on our
previous conclusions.

A. Ricardian Equivalence

Let's begin by returning to Ricardian equivalence,
which is still the simplest case. We
found that if people have a norm regarding the
amount of their bequest, then lump-sum transfers
to an older generation will not be neutral.
There remains the possibility that the source of
the warm glow to the older generation is not the
total bequest, but instead the bequest to the
younger generation net of the transfer. In this
case, if the transfers change, then the norm
changes. Ricardian equivalence will again be
valid. While such changes in norms with the
size of transfers are a theoretical possibility,
they also seem highly unlikely. The size of the
transfers involved--especially for those rich
enough to make large nonaccidental bequestswould
seem to be too small to warrant such a


### ---Economics-2007-0-28.txt---
sophisticated calculation. Our earlier discussion
did discuss at least one change in the norms
regarding bequests, but that resulted from a very
large change in people's orientation. It resulted
from changes in their conception of the family--
of their own place within it and of the place of
their heirs. That also occurred over a very long
run-over the course of centuries.
B. Life-Cycle Hypothesis

Regarding the life-cycle hypothesis, we argued
that consumption depends upon current
income because norms regarding how much
people think they should spend are linked to
it. But such a norm Wyould be highly unlikely
to change as a result'of the use of fiscal and
monetary policy for stabilization. In the first
place, such stabilization will make the adherence
to the norm less costly, not more costly,
in purely economic terms. Furthermore, macroeconomic
sources are responsible for only a
small fraction of the variation in individual
incomes. As a result, there is further reason
why the role of current income in norms is
unlikely to change as a result of macroeconomic
stabilization.

C. Cash Flow and Investment

The rise of the CFO suggests that norms
regarding investment have changed in large US
firms. Quite possibly, this change occurred because
firms realized the need for financial controls
that compared the returns on inside and
outside options. Such an endogenous response
would make Modigliani-Miller correct. But,
following Zorn (2004), this change took 40
years. In the meantime, in the short run, following
our earlier logic, investments would have
depended on cash flow. And, of course, even in
the long run the CFO, who is only one voice
among many in corporate decisions, may not be
fully effective.

D. Natural Rate Hypothesis and the Role of
Rational Expectations

Regarding the natural rate hypothesis and
also the rational expectations hypothesis, we
saw that they will no longer hold if norms of
price and wage setting have nominal components.
Regarding prices and wages, the most
powerful evidence in favor of norms comes
from employees' resistance to money wage
cuts and customers' resistance to nominal
price increases. As long as inflation is low, it
is doubtful that small changes in inflation will
affect such norms. People seem to find it
easier to think in nominal, rather than in real,
terms. Indeed the facilitation of such thinking
is one of the benefits of money according to
the textbook mantra on its three uses: for
transactions, as a store of value, and as a unit
of account. Money is useful as a unit of
account especially if people think in nominal,
rather than in real, terms. As a result, as long
as inflation is low, people are unlikely to
forsake making calculations in nominal terms,
especially regarding the norms of what wages
or prices should be. Of course, if inflation
increases to high levels, the norms for wages
and prices and the method of calculating
those norms will change. Exactly how they
change-with the possibility that they underadjust
to increases in inflation when it is low
and overadjust when it is high-should be
empirically investigated.

E. Where Do the Norms Come From?
We do not know the general answer to the
question where norms come from. This lecture
has tried to make the case that norms, such as
they are, could potentially play an important
role in macroeconomics. Hopefully, then, it has
added to the motivation for research on their
microfoundations.88

XI. Conclusion

This lecture has shown that the early Keynesians
got a great deal of the working of the
economic system right, in ways that are denied
by the five neutralities. As quoted from Keynes
earlier, they based their models on "our knowledge
of human nature and from the detailed
facts of experience." They used their intuitions
regarding the norms of how consumers, investors,
and wage and price setters thought they
88 This lecture has been very much influenced by the
insights of the Ph.D. thesis of Robert Akerlof (2006) on
preferences for beliefs. His thinking on this subject has
influenced many of the sections of this paper, especially on
consumption and the endogeneity of norms.


### ---Economics-2007-0-29.txt---
should behave. There is systematic reason why
such knowledge and experience are likely to be
accurate: by their nature, norms are generated
and known by a whole community. They are
known to those who abide by them, and those
who observe them as well.

We have shown ways in which macroeconomic
variables will be affected by norms. The neutralities
say that consumption should have no special
dependence on current income; investment should
be independent of current cash flow; wages and
prices should not depend on nominal considerations.
The very construction of those neutralities
denies the possibility that peoples' decisions
might be influenced by their views regarding how
they, and how others, should behave. In practice,
however, the neutralities are systematically violated.
Insofar as economists have felt it necessary
to explain these violations, they have appealed to
a variety of different frictions, such as myopia and
credit constraint. In so doing, they have failed to
consider that those violations would occur even in
the absence of those frictions: they will occur
because of decision makers' norms.
The incorporation of norms based on careful
observation imparts an appropriate balance to
macroeconomics. The New Classical research
program was correct in viewing models of the
early Keynesians as too primitive. They had not
been sufficiently attentive to the role of human
intent in choices regarding consumption, investment,
wages, and prices. But that research program
itself has failed to appreciate the extent to
which the Keynesians' views of macroeconomics
were also reflective of reality, since they
were based on experience and observation.
A macroeconomics with norms in decision
makers' objective functions combines the best
features of the two approaches. It allows for
observations regarding how people think they
should behave. It also takes due account of the
purposefulness of human decisions.